{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_halving_search_cv # required for HalvingGridSearchCV\n",
    "from sklearn.model_selection import train_test_split, HalvingGridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.callbacks import EarlyStopping, History\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw PD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steering_angle</th>\n",
       "      <th>distance_to_road_center</th>\n",
       "      <th>angle_from_straight_in_rads</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124995</th>\n",
       "      <td>0.144081</td>\n",
       "      <td>0.147693</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>-3.835282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124996</th>\n",
       "      <td>0.142465</td>\n",
       "      <td>0.148484</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>-3.813875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124997</th>\n",
       "      <td>0.140811</td>\n",
       "      <td>0.149215</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>-3.792289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124998</th>\n",
       "      <td>0.139120</td>\n",
       "      <td>0.149889</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>-3.770526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124999</th>\n",
       "      <td>0.137394</td>\n",
       "      <td>0.150505</td>\n",
       "      <td>-0.006668</td>\n",
       "      <td>-0.201773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        steering_angle  distance_to_road_center  angle_from_straight_in_rads  \\\n",
       "0             2.588463                 1.026888                     2.956561   \n",
       "1             2.588463                 1.026888                     2.956561   \n",
       "2             2.588463                 1.026888                     2.956561   \n",
       "3             2.588463                 1.026888                     2.956561   \n",
       "4             2.588463                 1.026888                     2.956561   \n",
       "...                ...                      ...                          ...   \n",
       "124995        0.144081                 0.147693                     0.010285   \n",
       "124996        0.142465                 0.148484                     0.006000   \n",
       "124997        0.140811                 0.149215                     0.001745   \n",
       "124998        0.139120                 0.149889                    -0.002478   \n",
       "124999        0.137394                 0.150505                    -0.006668   \n",
       "\n",
       "          reward  \n",
       "0      -0.012300  \n",
       "1      -0.012300  \n",
       "2      -0.012300  \n",
       "3      -0.012300  \n",
       "4      -0.012614  \n",
       "...          ...  \n",
       "124995 -3.835282  \n",
       "124996 -3.813875  \n",
       "124997 -3.792289  \n",
       "124998 -3.770526  \n",
       "124999 -0.201773  \n",
       "\n",
       "[125000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/2548153325.py:15: RuntimeWarning: divide by zero encountered in divide\n",
      "  weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n"
     ]
    }
   ],
   "source": [
    "# import data from csv to dataframe\n",
    "filename = \"train_data/raw_data_pd.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "display(df)\n",
    "\n",
    "# split into input and target features, and weights\n",
    "X = df[['distance_to_road_center', 'angle_from_straight_in_rads']].values\n",
    "y = df['steering_angle'].values\n",
    "r = df['reward'].values\n",
    "\n",
    "# normalize rewards to [0, 1] range\n",
    "norm_r = (r - np.min(r)) / (np.max(r) - np.min(r))\n",
    "\n",
    "# calculate weights based on normalized rewards\n",
    "weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n",
    "\n",
    "# split into training and validation sets\n",
    "Xtrain, Xval, ytrain, yval, wtrain, wval = train_test_split(X, y, weights, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0311 - val_loss: 4.9459e-04\n",
      "Epoch 2/100\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 5.7531e-04 - val_loss: 7.1825e-04\n",
      "Epoch 3/100\n",
      "3125/3125 [==============================] - 2s 507us/step - loss: 4.4264e-04 - val_loss: 5.8055e-04\n",
      "Epoch 4/100\n",
      "3125/3125 [==============================] - 2s 522us/step - loss: 4.8484e-04 - val_loss: 3.8641e-05\n",
      "Epoch 5/100\n",
      "3125/3125 [==============================] - 2s 508us/step - loss: 2.5327e-04 - val_loss: 4.7775e-05\n",
      "Epoch 6/100\n",
      "3125/3125 [==============================] - 2s 521us/step - loss: 2.6589e-04 - val_loss: 2.4218e-04\n",
      "Epoch 7/100\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 2.0023e-04 - val_loss: 5.0007e-06\n",
      "Epoch 8/100\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 2.1890e-04 - val_loss: 6.4376e-06\n",
      "Epoch 9/100\n",
      "3125/3125 [==============================] - 2s 509us/step - loss: 1.4528e-04 - val_loss: 1.8688e-05\n",
      "Epoch 10/100\n",
      "3125/3125 [==============================] - 2s 510us/step - loss: 1.7536e-04 - val_loss: 1.3059e-05\n",
      "Epoch 11/100\n",
      "3125/3125 [==============================] - 2s 505us/step - loss: 1.1187e-04 - val_loss: 1.0243e-04\n",
      "Epoch 12/100\n",
      "3125/3125 [==============================] - 2s 515us/step - loss: 1.3933e-04 - val_loss: 2.0056e-05\n"
     ]
    }
   ],
   "source": [
    "# find optimal number of epochs using early stopping\n",
    "\n",
    "# define base model with default Huber loss delta = 1.0\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for steering angle prediction\n",
    "])\n",
    "\n",
    "model.compile(loss=Huber(delta=1.0), optimizer=Adam(learning_rate=0.001), weighted_metrics=[])\n",
    "\n",
    "# define early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=100, batch_size=32, validation_data = (Xval, yval, wval), callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0202 - mae: 0.0251 - mse: 0.0104 - val_loss: 1.5993e-04 - val_mae: 0.0052 - val_mse: 6.3100e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 7.1293e-04 - mae: 0.0093 - mse: 2.6307e-04 - val_loss: 0.0031 - val_mae: 0.0230 - val_mse: 9.9338e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 5.7935e-04 - mae: 0.0097 - mse: 2.3994e-04 - val_loss: 6.5062e-04 - val_mae: 0.0123 - val_mse: 3.5072e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 4.6699e-04 - mae: 0.0089 - mse: 2.1845e-04 - val_loss: 7.2402e-05 - val_mae: 0.0046 - val_mse: 4.5898e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 2.8915e-04 - mae: 0.0068 - mse: 1.2366e-04 - val_loss: 2.7315e-05 - val_mae: 0.0033 - val_mse: 1.6124e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 2.4844e-04 - mae: 0.0066 - mse: 1.1776e-04 - val_loss: 1.2982e-04 - val_mae: 0.0078 - val_mse: 9.2997e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 556us/step - loss: 2.2921e-04 - mae: 0.0064 - mse: 1.0785e-04 - val_loss: 1.6559e-04 - val_mae: 0.0049 - val_mse: 4.8297e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 1.6793e-04 - mae: 0.0057 - mse: 8.9933e-05 - val_loss: 3.0036e-04 - val_mae: 0.0115 - val_mse: 2.4888e-04\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.5048e-04 - mae: 0.0047 - mse: 7.5915e-05 - val_loss: 2.9277e-05 - val_mae: 0.0036 - val_mse: 1.9212e-05\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.4594e-04 - mae: 0.0044 - mse: 7.4576e-05 - val_loss: 1.1561e-05 - val_mae: 0.0019 - val_mse: 6.4723e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.2166e-04 - mae: 0.0040 - mse: 6.1894e-05 - val_loss: 2.1074e-05 - val_mae: 0.0029 - val_mse: 1.1959e-05\n",
      "Iteration 1: delta = 0.5, val_mae = 0.002943823579698801, val_mse = 1.1959174116782378e-05\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0179 - mae: 0.0315 - mse: 0.0141 - val_loss: 5.0269e-04 - val_mae: 0.0119 - val_mse: 2.3428e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 9.1817e-04 - mae: 0.0118 - mse: 3.6821e-04 - val_loss: 5.4259e-04 - val_mae: 0.0144 - val_mse: 3.1531e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 6.1241e-04 - mae: 0.0095 - mse: 2.3283e-04 - val_loss: 2.8834e-04 - val_mae: 0.0075 - val_mse: 1.3596e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 4.6370e-04 - mae: 0.0079 - mse: 1.8891e-04 - val_loss: 5.0387e-05 - val_mae: 0.0041 - val_mse: 3.0028e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 4.4266e-04 - mae: 0.0075 - mse: 1.6384e-04 - val_loss: 1.2292e-04 - val_mae: 0.0065 - val_mse: 8.0425e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 2.8406e-04 - mae: 0.0061 - mse: 1.2867e-04 - val_loss: 0.0011 - val_mae: 0.0171 - val_mse: 7.6907e-04\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 2.7603e-04 - mae: 0.0060 - mse: 1.2185e-04 - val_loss: 1.0649e-04 - val_mae: 0.0081 - val_mse: 8.3981e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 1.9524e-04 - mae: 0.0055 - mse: 1.0114e-04 - val_loss: 7.5181e-06 - val_mae: 0.0019 - val_mse: 6.3815e-06\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 585us/step - loss: 1.9535e-04 - mae: 0.0051 - mse: 9.1940e-05 - val_loss: 1.6227e-05 - val_mae: 0.0025 - val_mse: 9.9240e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 595us/step - loss: 1.8053e-04 - mae: 0.0049 - mse: 8.5892e-05 - val_loss: 1.2958e-04 - val_mae: 0.0055 - val_mse: 4.4025e-05\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 590us/step - loss: 1.3691e-04 - mae: 0.0039 - mse: 6.3916e-05 - val_loss: 0.0014 - val_mae: 0.0249 - val_mse: 0.0011\n",
      "Iteration 2: delta = 0.6, val_mae = 0.024870606139302254, val_mse = 0.0010972677264362574\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0114 - mae: 0.0257 - mse: 0.0094 - val_loss: 5.3391e-05 - val_mae: 0.0044 - val_mse: 3.7553e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 9.6109e-04 - mae: 0.0101 - mse: 3.9020e-04 - val_loss: 0.0014 - val_mae: 0.0215 - val_mse: 9.1424e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 4.7032e-04 - mae: 0.0080 - mse: 1.8341e-04 - val_loss: 2.6630e-04 - val_mae: 0.0081 - val_mse: 1.5600e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 6.6518e-04 - mae: 0.0071 - mse: 2.1967e-04 - val_loss: 1.0308e-04 - val_mae: 0.0078 - val_mse: 9.6577e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 4.7084e-04 - mae: 0.0070 - mse: 1.8236e-04 - val_loss: 2.1098e-05 - val_mae: 0.0024 - val_mse: 1.1410e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 2.2838e-04 - mae: 0.0055 - mse: 9.9337e-05 - val_loss: 5.1811e-04 - val_mae: 0.0174 - val_mse: 4.4689e-04\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 2.0463e-04 - mae: 0.0057 - mse: 9.5618e-05 - val_loss: 1.9682e-05 - val_mae: 0.0030 - val_mse: 1.3833e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.6610e-04 - mae: 0.0044 - mse: 6.7608e-05 - val_loss: 9.1411e-06 - val_mae: 0.0018 - val_mse: 7.1671e-06\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 2.3054e-04 - mae: 0.0049 - mse: 1.0475e-04 - val_loss: 0.0012 - val_mae: 0.0150 - val_mse: 3.6073e-04\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.6315e-04 - mae: 0.0043 - mse: 9.5132e-05 - val_loss: 1.8588e-05 - val_mae: 0.0019 - val_mse: 6.7877e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 1.9439e-04 - mae: 0.0043 - mse: 1.0390e-04 - val_loss: 1.7810e-06 - val_mae: 8.9890e-04 - val_mse: 1.7142e-06\n",
      "Iteration 3: delta = 0.7, val_mae = 0.0008989019552245736, val_mse = 1.7142334627351374e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0223 - mae: 0.0271 - mse: 0.0115 - val_loss: 0.0015 - val_mae: 0.0321 - val_mse: 0.0015\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 6.2845e-04 - mae: 0.0095 - mse: 2.5019e-04 - val_loss: 2.4619e-04 - val_mae: 0.0106 - val_mse: 2.0971e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 6.0896e-04 - mae: 0.0094 - mse: 3.1653e-04 - val_loss: 3.8566e-04 - val_mae: 0.0070 - val_mse: 1.2440e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 5.6446e-04 - mae: 0.0095 - mse: 2.9370e-04 - val_loss: 3.7102e-04 - val_mae: 0.0118 - val_mse: 2.7360e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 2.4134e-04 - mae: 0.0059 - mse: 9.6261e-05 - val_loss: 1.5849e-04 - val_mae: 0.0047 - val_mse: 5.4793e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 3.4626e-04 - mae: 0.0072 - mse: 1.6621e-04 - val_loss: 4.5183e-05 - val_mae: 0.0032 - val_mse: 2.0729e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 1.5767e-04 - mae: 0.0053 - mse: 7.7483e-05 - val_loss: 2.5716e-04 - val_mae: 0.0099 - val_mse: 2.1014e-04\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.8267e-04 - mae: 0.0056 - mse: 9.4568e-05 - val_loss: 6.7420e-05 - val_mae: 0.0032 - val_mse: 2.1570e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 556us/step - loss: 1.9465e-04 - mae: 0.0047 - mse: 8.9738e-05 - val_loss: 1.2042e-04 - val_mae: 0.0095 - val_mse: 1.3092e-04\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.6518e-04 - mae: 0.0050 - mse: 8.5304e-05 - val_loss: 2.0021e-05 - val_mae: 0.0023 - val_mse: 9.3320e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.1535e-04 - mae: 0.0037 - mse: 4.7635e-05 - val_loss: 3.0474e-06 - val_mae: 0.0010 - val_mse: 2.0951e-06\n",
      "Iteration 4: delta = 0.7999999999999999, val_mae = 0.0010287829209119081, val_mse = 2.0950956240994856e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0255 - mae: 0.0301 - mse: 0.0121 - val_loss: 3.8480e-04 - val_mae: 0.0080 - val_mse: 1.4563e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 8.5414e-04 - mae: 0.0105 - mse: 3.2599e-04 - val_loss: 2.2098e-05 - val_mae: 0.0032 - val_mse: 2.0943e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 556us/step - loss: 4.6625e-04 - mae: 0.0078 - mse: 1.8403e-04 - val_loss: 3.0350e-05 - val_mae: 0.0029 - val_mse: 1.6471e-05\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 5.0720e-04 - mae: 0.0073 - mse: 2.3415e-04 - val_loss: 2.1361e-05 - val_mae: 0.0024 - val_mse: 1.3373e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 3.0236e-04 - mae: 0.0061 - mse: 1.3501e-04 - val_loss: 1.2982e-05 - val_mae: 0.0018 - val_mse: 5.9104e-06\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 1.7702e-04 - mae: 0.0051 - mse: 7.9110e-05 - val_loss: 2.9651e-05 - val_mae: 0.0035 - val_mse: 2.0781e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 528us/step - loss: 1.7276e-04 - mae: 0.0047 - mse: 9.3099e-05 - val_loss: 0.0023 - val_mae: 0.0280 - val_mse: 0.0018\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 2.4888e-04 - mae: 0.0043 - mse: 1.0232e-04 - val_loss: 2.2918e-05 - val_mae: 0.0042 - val_mse: 2.6106e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 1.1057e-04 - mae: 0.0037 - mse: 5.9276e-05 - val_loss: 5.0996e-06 - val_mae: 0.0016 - val_mse: 4.2748e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 1.4172e-04 - mae: 0.0046 - mse: 6.8312e-05 - val_loss: 4.0365e-06 - val_mae: 0.0013 - val_mse: 3.3686e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 1.1190e-04 - mae: 0.0033 - mse: 5.2435e-05 - val_loss: 2.0810e-06 - val_mae: 0.0011 - val_mse: 2.5243e-06\n",
      "Iteration 5: delta = 0.8999999999999999, val_mae = 0.0011450344463810325, val_mse = 2.5242668471037177e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0221 - mae: 0.0245 - mse: 0.0082 - val_loss: 0.0014 - val_mae: 0.0218 - val_mse: 7.6380e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 7.3161e-04 - mae: 0.0098 - mse: 2.7063e-04 - val_loss: 1.0519e-04 - val_mae: 0.0048 - val_mse: 4.6909e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 6.0558e-04 - mae: 0.0089 - mse: 2.3874e-04 - val_loss: 1.4503e-04 - val_mae: 0.0058 - val_mse: 6.3119e-05\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 523us/step - loss: 3.1684e-04 - mae: 0.0072 - mse: 1.4621e-04 - val_loss: 6.0222e-04 - val_mae: 0.0152 - val_mse: 3.2410e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 521us/step - loss: 2.8636e-04 - mae: 0.0066 - mse: 1.2217e-04 - val_loss: 1.0486e-04 - val_mae: 0.0048 - val_mse: 4.2946e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 522us/step - loss: 1.8973e-04 - mae: 0.0056 - mse: 8.7885e-05 - val_loss: 1.7900e-04 - val_mae: 0.0047 - val_mse: 4.8652e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 2.5495e-04 - mae: 0.0048 - mse: 9.8091e-05 - val_loss: 4.8226e-06 - val_mae: 0.0014 - val_mse: 3.9607e-06\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 535us/step - loss: 1.8153e-04 - mae: 0.0037 - mse: 7.3064e-05 - val_loss: 0.0053 - val_mae: 0.0303 - val_mse: 0.0016\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 520us/step - loss: 1.3014e-04 - mae: 0.0039 - mse: 6.9618e-05 - val_loss: 1.8095e-06 - val_mae: 8.7120e-04 - val_mse: 1.7165e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 523us/step - loss: 9.9326e-05 - mae: 0.0032 - mse: 4.2085e-05 - val_loss: 2.3668e-06 - val_mae: 9.2389e-04 - val_mse: 1.6628e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 508us/step - loss: 1.0992e-04 - mae: 0.0039 - mse: 5.5732e-05 - val_loss: 2.7451e-06 - val_mae: 9.7746e-04 - val_mse: 1.7462e-06\n",
      "Iteration 6: delta = 0.9999999999999999, val_mae = 0.0009774580830708146, val_mse = 1.7462235746279475e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0248 - mae: 0.0229 - mse: 0.0078 - val_loss: 4.8171e-05 - val_mae: 0.0045 - val_mse: 4.0733e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 9.2655e-04 - mae: 0.0121 - mse: 4.1391e-04 - val_loss: 1.1910e-04 - val_mae: 0.0052 - val_mse: 5.1821e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 526us/step - loss: 5.7925e-04 - mae: 0.0096 - mse: 2.3423e-04 - val_loss: 6.7896e-04 - val_mae: 0.0105 - val_mse: 2.2890e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 524us/step - loss: 5.0893e-04 - mae: 0.0074 - mse: 1.9574e-04 - val_loss: 8.1922e-04 - val_mae: 0.0162 - val_mse: 3.5495e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 617us/step - loss: 3.1946e-04 - mae: 0.0070 - mse: 1.7556e-04 - val_loss: 3.5842e-04 - val_mae: 0.0160 - val_mse: 3.2989e-04\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 588us/step - loss: 2.2603e-04 - mae: 0.0061 - mse: 1.0574e-04 - val_loss: 7.4329e-04 - val_mae: 0.0159 - val_mse: 5.7037e-04\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 558us/step - loss: 2.8874e-04 - mae: 0.0066 - mse: 1.2610e-04 - val_loss: 2.8004e-04 - val_mae: 0.0120 - val_mse: 2.3289e-04\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 2.0672e-04 - mae: 0.0047 - mse: 9.4651e-05 - val_loss: 3.2528e-05 - val_mae: 0.0043 - val_mse: 2.2604e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 2.0818e-04 - mae: 0.0049 - mse: 9.5721e-05 - val_loss: 2.3110e-05 - val_mae: 0.0022 - val_mse: 9.1263e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.7532e-04 - mae: 0.0043 - mse: 8.3759e-05 - val_loss: 3.2591e-05 - val_mae: 0.0039 - val_mse: 2.3659e-05\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.0985e-04 - mae: 0.0045 - mse: 5.3047e-05 - val_loss: 1.7614e-04 - val_mae: 0.0078 - val_mse: 1.0241e-04\n",
      "Iteration 7: delta = 1.0999999999999999, val_mae = 0.007791235577315092, val_mse = 0.00010241157724522054\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0222 - mae: 0.0238 - mse: 0.0100 - val_loss: 2.2203e-04 - val_mae: 0.0057 - val_mse: 6.9195e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 8.8828e-04 - mae: 0.0098 - mse: 3.4518e-04 - val_loss: 1.8628e-05 - val_mae: 0.0029 - val_mse: 1.5384e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 6.4063e-04 - mae: 0.0091 - mse: 2.8052e-04 - val_loss: 4.4558e-04 - val_mae: 0.0091 - val_mse: 1.7246e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 3.9207e-04 - mae: 0.0080 - mse: 1.7642e-04 - val_loss: 1.1273e-04 - val_mae: 0.0048 - val_mse: 4.7311e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 3.3197e-04 - mae: 0.0076 - mse: 1.6882e-04 - val_loss: 4.8506e-05 - val_mae: 0.0029 - val_mse: 1.7219e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 2.3450e-04 - mae: 0.0053 - mse: 9.7111e-05 - val_loss: 3.2150e-04 - val_mae: 0.0063 - val_mse: 9.9308e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 557us/step - loss: 3.5725e-04 - mae: 0.0055 - mse: 1.5663e-04 - val_loss: 2.9813e-05 - val_mae: 0.0028 - val_mse: 1.3478e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.8142e-04 - mae: 0.0044 - mse: 8.9416e-05 - val_loss: 1.4668e-04 - val_mae: 0.0070 - val_mse: 1.0808e-04\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 555us/step - loss: 1.0711e-04 - mae: 0.0042 - mse: 5.3082e-05 - val_loss: 6.3507e-06 - val_mae: 0.0024 - val_mse: 7.9813e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 1.4143e-04 - mae: 0.0039 - mse: 6.4006e-05 - val_loss: 0.0011 - val_mae: 0.0212 - val_mse: 9.5096e-04\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.4094e-04 - mae: 0.0034 - mse: 5.3753e-05 - val_loss: 4.8846e-06 - val_mae: 0.0011 - val_mse: 2.5810e-06\n",
      "Iteration 8: delta = 1.2, val_mae = 0.001096259569749236, val_mse = 2.5810188617469976e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0304 - mae: 0.0230 - mse: 0.0107 - val_loss: 1.5114e-04 - val_mae: 0.0054 - val_mse: 5.8559e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 8.7749e-04 - mae: 0.0106 - mse: 3.3684e-04 - val_loss: 2.5765e-05 - val_mae: 0.0035 - val_mse: 2.1725e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 535us/step - loss: 5.9876e-04 - mae: 0.0084 - mse: 2.8471e-04 - val_loss: 5.2961e-05 - val_mae: 0.0054 - val_mse: 5.3889e-05\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 4.7338e-04 - mae: 0.0073 - mse: 1.8480e-04 - val_loss: 6.4867e-05 - val_mae: 0.0051 - val_mse: 4.2587e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 2.5211e-04 - mae: 0.0064 - mse: 1.2908e-04 - val_loss: 0.0027 - val_mae: 0.0139 - val_mse: 5.6304e-04\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 528us/step - loss: 2.6622e-04 - mae: 0.0062 - mse: 1.1705e-04 - val_loss: 1.0077e-05 - val_mae: 0.0022 - val_mse: 1.0210e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 1.5176e-04 - mae: 0.0048 - mse: 7.4001e-05 - val_loss: 3.1539e-05 - val_mae: 0.0036 - val_mse: 2.3778e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 532us/step - loss: 1.5780e-04 - mae: 0.0048 - mse: 9.1301e-05 - val_loss: 6.0171e-05 - val_mae: 0.0022 - val_mse: 1.4143e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 2.1998e-04 - mae: 0.0052 - mse: 1.4376e-04 - val_loss: 7.8193e-05 - val_mae: 0.0055 - val_mse: 3.8699e-05\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 9.8217e-05 - mae: 0.0035 - mse: 4.5323e-05 - val_loss: 1.8920e-05 - val_mae: 0.0020 - val_mse: 8.4045e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 528us/step - loss: 1.3668e-04 - mae: 0.0038 - mse: 5.4840e-05 - val_loss: 7.8829e-06 - val_mae: 0.0013 - val_mse: 4.0456e-06\n",
      "Iteration 9: delta = 1.3, val_mae = 0.0013006088556721807, val_mse = 4.045555670018075e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0286 - mae: 0.0258 - mse: 0.0099 - val_loss: 8.9270e-04 - val_mae: 0.0148 - val_mse: 4.5197e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 9.5435e-04 - mae: 0.0120 - mse: 4.5623e-04 - val_loss: 1.2477e-04 - val_mae: 0.0046 - val_mse: 4.3036e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 9.4698e-04 - mae: 0.0104 - mse: 3.7462e-04 - val_loss: 0.0092 - val_mae: 0.0475 - val_mse: 0.0033\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 586us/step - loss: 5.2611e-04 - mae: 0.0085 - mse: 2.0991e-04 - val_loss: 7.3887e-04 - val_mae: 0.0086 - val_mse: 1.9458e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 579us/step - loss: 4.0725e-04 - mae: 0.0074 - mse: 1.8848e-04 - val_loss: 1.3648e-04 - val_mae: 0.0050 - val_mse: 5.5901e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 2.1604e-04 - mae: 0.0063 - mse: 1.0755e-04 - val_loss: 1.4588e-05 - val_mae: 0.0020 - val_mse: 8.5403e-06\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 2.4824e-04 - mae: 0.0061 - mse: 1.0683e-04 - val_loss: 4.5263e-05 - val_mae: 0.0029 - val_mse: 1.7558e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 567us/step - loss: 1.6628e-04 - mae: 0.0052 - mse: 8.6930e-05 - val_loss: 2.3331e-04 - val_mae: 0.0042 - val_mse: 6.1065e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 603us/step - loss: 1.6646e-04 - mae: 0.0051 - mse: 8.4189e-05 - val_loss: 2.4806e-05 - val_mae: 0.0026 - val_mse: 1.0330e-05\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 594us/step - loss: 1.3747e-04 - mae: 0.0045 - mse: 8.1643e-05 - val_loss: 6.2283e-05 - val_mae: 0.0039 - val_mse: 2.4124e-05\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 659us/step - loss: 1.3672e-04 - mae: 0.0042 - mse: 6.9716e-05 - val_loss: 1.6083e-05 - val_mae: 0.0020 - val_mse: 8.3948e-06\n",
      "Iteration 10: delta = 1.4000000000000001, val_mae = 0.0019502261420711875, val_mse = 8.39479162095813e-06\n"
     ]
    }
   ],
   "source": [
    "# find best delta for loss function\n",
    "\n",
    "# function to train and evaluate model with a given delta\n",
    "def train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=delta), metrics=['mae', 'mse'], weighted_metrics=[])\n",
    "\n",
    "    history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    # get validation MAE, MSE and Huber loss\n",
    "    val_mae = history.history['val_mae'][-1]\n",
    "    val_mse = history.history['val_mse'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "    return val_mae, val_mse, val_loss\n",
    "\n",
    "# initialise delta value and step size\n",
    "delta = 0.5\n",
    "step_size = 0.1\n",
    "best_delta = delta\n",
    "best_val_mae = float('inf')\n",
    "best_val_mse = float('inf')\n",
    "\n",
    "# initialise arrays for plotting\n",
    "arr_val_mae = []\n",
    "arr_val_mse = []\n",
    "arr_val_loss = []\n",
    "\n",
    "# iterative search for optimal delta\n",
    "for i in range(10):\n",
    "    val_mae, val_mse, val_loss = train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval)\n",
    "\n",
    "    # print current results\n",
    "    print(f\"Iteration {i+1}: delta = {delta}, val_mae = {val_mae}, val_mse = {val_mse}\")\n",
    "\n",
    "    # update best delta if current results are better\n",
    "    if val_mae < best_val_mae and val_mse < best_val_mse: \n",
    "        best_delta = delta\n",
    "        best_val_mae = val_mae\n",
    "        best_val_mse = val_mse\n",
    "    \n",
    "    # store data\n",
    "    arr_val_mae.append(val_mae)\n",
    "    arr_val_mse.append(val_mse)\n",
    "    arr_val_loss.append(val_loss)\n",
    "    \n",
    "    # adjust delta for next iteration \n",
    "    delta += step_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best delta: 0.7\n",
      "Best validation MAE: 0.0008989019552245736\n",
      "Best validation MSE: 1.7142334627351374e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "deltas = np.arange(0.5, 1.5, 0.1)\n",
    "plt.plot(deltas, arr_val_mae, marker='o', linestyle='-', color='C0', label='MAE')\n",
    "plt.plot(deltas, arr_val_mse, marker='o', linestyle='-', color='C1', label='MSE')\n",
    "plt.plot(deltas, arr_val_loss, marker='o', linestyle='-', color='C2', label='Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Best delta: {best_delta}\")\n",
    "print(f\"Best validation MAE: {best_val_mae}\")\n",
    "print(f\"Best validation MSE: {best_val_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "3119/3125 [============================>.] - ETA: 0s - loss: 0.0124WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0124 - val_loss: 1.4695e-04\n",
      "Epoch 2/21\n",
      "3032/3125 [============================>.] - ETA: 0s - loss: 7.2352e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 503us/step - loss: 7.0489e-04 - val_loss: 3.3615e-04\n",
      "Epoch 3/21\n",
      "3085/3125 [============================>.] - ETA: 0s - loss: 4.9224e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 4.8774e-04 - val_loss: 1.1937e-04\n",
      "Epoch 4/21\n",
      "3032/3125 [============================>.] - ETA: 0s - loss: 3.0539e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 503us/step - loss: 2.9723e-04 - val_loss: 4.5692e-05\n",
      "Epoch 5/21\n",
      "3092/3125 [============================>.] - ETA: 0s - loss: 2.7563e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 2.7682e-04 - val_loss: 2.9324e-04\n",
      "Epoch 6/21\n",
      "3085/3125 [============================>.] - ETA: 0s - loss: 2.5836e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 2.5597e-04 - val_loss: 8.6236e-05\n",
      "Epoch 7/21\n",
      "3019/3125 [===========================>..] - ETA: 0s - loss: 1.7472e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 507us/step - loss: 1.9329e-04 - val_loss: 5.5759e-05\n",
      "Epoch 8/21\n",
      "3059/3125 [============================>.] - ETA: 0s - loss: 1.1668e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 517us/step - loss: 1.1497e-04 - val_loss: 2.0861e-04\n",
      "Epoch 9/21\n",
      "3111/3125 [============================>.] - ETA: 0s - loss: 1.6754e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 1.6720e-04 - val_loss: 1.1083e-04\n",
      "Epoch 10/21\n",
      "3078/3125 [============================>.] - ETA: 0s - loss: 1.4426e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 515us/step - loss: 1.4221e-04 - val_loss: 4.7471e-06\n",
      "Epoch 11/21\n",
      "3018/3125 [===========================>..] - ETA: 0s - loss: 1.1278e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 506us/step - loss: 1.1056e-04 - val_loss: 1.0144e-05\n",
      "Epoch 12/21\n",
      "3084/3125 [============================>.] - ETA: 0s - loss: 1.7072e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 1.6850e-04 - val_loss: 1.2806e-06\n",
      "Epoch 13/21\n",
      "3104/3125 [============================>.] - ETA: 0s - loss: 9.1941e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 9.1351e-05 - val_loss: 1.4919e-05\n",
      "Epoch 14/21\n",
      "3019/3125 [===========================>..] - ETA: 0s - loss: 1.0733e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 507us/step - loss: 1.0450e-04 - val_loss: 5.0571e-05\n",
      "Epoch 15/21\n",
      "3089/3125 [============================>.] - ETA: 0s - loss: 1.2246e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 1.2123e-04 - val_loss: 1.0353e-05\n",
      "Epoch 16/21\n",
      "3033/3125 [============================>.] - ETA: 0s - loss: 7.1371e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 504us/step - loss: 6.9639e-05 - val_loss: 1.1107e-05\n",
      "Epoch 17/21\n",
      "3074/3125 [============================>.] - ETA: 0s - loss: 9.0365e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 516us/step - loss: 8.9453e-05 - val_loss: 1.1710e-05\n",
      "Epoch 18/21\n",
      "3100/3125 [============================>.] - ETA: 0s - loss: 8.5348e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 8.5123e-05 - val_loss: 2.5125e-04\n",
      "Epoch 19/21\n",
      "3024/3125 [============================>.] - ETA: 0s - loss: 1.2449e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 505us/step - loss: 1.2051e-04 - val_loss: 1.3043e-06\n",
      "Epoch 20/21\n",
      "3089/3125 [============================>.] - ETA: 0s - loss: 7.9355e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 7.8451e-05 - val_loss: 5.0948e-07\n",
      "Epoch 21/21\n",
      "3030/3125 [============================>.] - ETA: 0s - loss: 8.6113e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 510us/step - loss: 8.3932e-05 - val_loss: 4.8413e-06\n",
      "Training Huber Loss: 8.393226016778499e-05\n",
      "Validation Huber Loss: 4.84133943245979e-06\n"
     ]
    }
   ],
   "source": [
    "# evaluate base model \n",
    "base_model = Sequential([\n",
    "                    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "                    Dense(32, activation='relu'),\n",
    "                    Dense(1)\n",
    "])\n",
    "\n",
    "base_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = base_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])\n",
    "\n",
    "# create folder for models\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# save model\n",
    "base_model.save(\"models/base_dnn_model_raw_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/base_dnn_model_raw_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "base_train_hl = history.history[\"loss\"][-1]\n",
    "base_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {base_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {base_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "3116/3125 [============================>.] - ETA: 0s - loss: 0.1236WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.1233 - val_loss: 0.0321\n",
      "Epoch 2/21\n",
      "3060/3125 [============================>.] - ETA: 0s - loss: 0.0195WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 0.0195 - val_loss: 0.0155\n",
      "Epoch 3/21\n",
      "3107/3125 [============================>.] - ETA: 0s - loss: 0.0136WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 4/21\n",
      "3100/3125 [============================>.] - ETA: 0s - loss: 0.0106WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 0.0106 - val_loss: 0.0088\n",
      "Epoch 5/21\n",
      "3047/3125 [============================>.] - ETA: 0s - loss: 0.0085WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 0.0085 - val_loss: 0.0072\n",
      "Epoch 6/21\n",
      "3121/3125 [============================>.] - ETA: 0s - loss: 0.0073WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 563us/step - loss: 0.0073 - val_loss: 0.0059\n",
      "Epoch 7/21\n",
      "3098/3125 [============================>.] - ETA: 0s - loss: 0.0082WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 0.0085 - val_loss: 0.0135\n",
      "Epoch 8/21\n",
      "3016/3125 [===========================>..] - ETA: 0s - loss: 0.0057WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 0.0057 - val_loss: 0.0080\n",
      "Epoch 9/21\n",
      "3067/3125 [============================>.] - ETA: 0s - loss: 0.0069WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 0.0068 - val_loss: 0.0050\n",
      "Epoch 10/21\n",
      "3019/3125 [===========================>..] - ETA: 0s - loss: 0.0055WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 0.0055 - val_loss: 0.0059\n",
      "Epoch 11/21\n",
      "3043/3125 [============================>.] - ETA: 0s - loss: 0.0055WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 555us/step - loss: 0.0055 - val_loss: 0.0046\n",
      "Epoch 12/21\n",
      "3050/3125 [============================>.] - ETA: 0s - loss: 0.0054WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 0.0054 - val_loss: 0.0044\n",
      "Epoch 13/21\n",
      "3076/3125 [============================>.] - ETA: 0s - loss: 0.0060WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 0.0060 - val_loss: 0.0044\n",
      "Epoch 14/21\n",
      "3020/3125 [===========================>..] - ETA: 0s - loss: 0.0048WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 15/21\n",
      "3048/3125 [============================>.] - ETA: 0s - loss: 0.0054WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 0.0054 - val_loss: 0.0042\n",
      "Epoch 16/21\n",
      "3025/3125 [============================>.] - ETA: 0s - loss: 0.0050WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 562us/step - loss: 0.0049 - val_loss: 0.0040\n",
      "Epoch 17/21\n",
      "3079/3125 [============================>.] - ETA: 0s - loss: 0.0058WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 0.0058 - val_loss: 0.0040\n",
      "Epoch 18/21\n",
      "3119/3125 [============================>.] - ETA: 0s - loss: 0.0045WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 594us/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 19/21\n",
      "3066/3125 [============================>.] - ETA: 0s - loss: 0.0049WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 0.0049 - val_loss: 0.0039\n",
      "Epoch 20/21\n",
      "3030/3125 [============================>.] - ETA: 0s - loss: 0.0048WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 0.0049 - val_loss: 0.0114\n",
      "Epoch 21/21\n",
      "3049/3125 [============================>.] - ETA: 0s - loss: 0.0045WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 519us/step - loss: 0.0045 - val_loss: 0.0037\n"
     ]
    }
   ],
   "source": [
    "# evaluate regularised model\n",
    "reg_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],), kernel_regularizer='l2'), \n",
    "        Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "reg_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = reg_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Huber Loss: 0.00447514234110713\n",
      "Validation Huber Loss: 0.003721917513757944\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "base_model.save(\"models/reg_dnn_model_raw_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/reg_dnn_model_raw_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "reg_train_hl = history.history[\"loss\"][-1]\n",
    "reg_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {reg_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {reg_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_95306/2299525531.py:28: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=20, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3913 - val_loss: 0.0504\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0354 - val_loss: 0.0248\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0214 - val_loss: 0.0172\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0180 - val_loss: 0.0160\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0132 - val_loss: 0.0119\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0102 - val_loss: 0.0092\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0085 - val_loss: 0.0077\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0079 - val_loss: 0.0072\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0071 - val_loss: 0.0065\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0065 - val_loss: 0.0062\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0067 - val_loss: 0.0061\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0057 - val_loss: 0.0051\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0051 - val_loss: 0.0049\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  47.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4063 - val_loss: 0.0449\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0310 - val_loss: 0.0262\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0198 - val_loss: 0.0165\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0155 - val_loss: 0.0141\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0125 - val_loss: 0.0111\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0094 - val_loss: 0.0086\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 638us/step - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0058 - val_loss: 0.0053\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0049 - val_loss: 0.0047\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  49.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3926 - val_loss: 0.0517\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0350 - val_loss: 0.0245\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0208 - val_loss: 0.0169\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0165 - val_loss: 0.0140\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0133 - val_loss: 0.0114\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0098 - val_loss: 0.0106\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0089 - val_loss: 0.0086\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0080 - val_loss: 0.0074\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0068 - val_loss: 0.0063\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0067 - val_loss: 0.0062\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 640us/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0057 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0053 - val_loss: 0.0048\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  49.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3740 - val_loss: 0.0547\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 537us/step - loss: 0.0360 - val_loss: 0.0254\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 523us/step - loss: 0.0214 - val_loss: 0.0179\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0173 - val_loss: 0.0184\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0138 - val_loss: 0.0121\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0116 - val_loss: 0.0103\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0090 - val_loss: 0.0085\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0077 - val_loss: 0.0072\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0071 - val_loss: 0.0064\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0065 - val_loss: 0.0063\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  48.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4054 - val_loss: 0.0512\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0364 - val_loss: 0.0256\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0220 - val_loss: 0.0176\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0169 - val_loss: 0.0141\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.0137 - val_loss: 0.0115\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0087 - val_loss: 0.0084\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0085 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 724us/step - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0069 - val_loss: 0.0060\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 705us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0064 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4548 - val_loss: 1.5654\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.4229 - val_loss: 1.5156\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.2551 - val_loss: 1.6405\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 1.4295 - val_loss: 1.2983\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 1.1793 - val_loss: 1.1226\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 1.1613 - val_loss: 2.0160\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2354 - val_loss: 1.1302\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 636us/step - loss: 1.1498 - val_loss: 0.7400\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 1.1035 - val_loss: 0.9781\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 1.0542 - val_loss: 1.7383\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 1.1506 - val_loss: 1.5261\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.2901 - val_loss: 1.2529\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0950 - val_loss: 0.9223\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 1.0057 - val_loss: 1.2830\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 1.1760 - val_loss: 1.9218\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.1963 - val_loss: 1.1139\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0766 - val_loss: 1.0442\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 1.0828 - val_loss: 0.9278\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 1.0241 - val_loss: 1.2778\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.0451 - val_loss: 1.1896\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 1.1658 - val_loss: 0.8618\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  47.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4894 - val_loss: 1.5536\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 681us/step - loss: 1.3949 - val_loss: 0.8424\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 1.2348 - val_loss: 2.4273\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 686us/step - loss: 1.3126 - val_loss: 1.0281\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 1.1545 - val_loss: 1.8219\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 1.2431 - val_loss: 1.7817\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 1.1064 - val_loss: 0.9372\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 1.1494 - val_loss: 1.1165\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 1.0696 - val_loss: 1.1350\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 1.2778 - val_loss: 1.2750\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.9945 - val_loss: 1.0512\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.9584 - val_loss: 0.8012\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 1.0732 - val_loss: 1.2680\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 1.0877 - val_loss: 0.7701\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.0771 - val_loss: 1.2036\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.1390 - val_loss: 1.3090\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 1.0615 - val_loss: 1.4114\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 1.1238 - val_loss: 0.6854\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.1122 - val_loss: 1.5386\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 1.0594 - val_loss: 1.5226\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 1.0355 - val_loss: 0.7678\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  48.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3110 - val_loss: 1.8916\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 1.5598 - val_loss: 0.9401\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2016 - val_loss: 3.6472\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 1.3655 - val_loss: 1.1979\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.1794 - val_loss: 0.9721\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 1.0479 - val_loss: 0.7877\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.1509 - val_loss: 1.0790\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.1672 - val_loss: 1.0041\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.1202 - val_loss: 1.1706\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.0972 - val_loss: 1.0665\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.1506 - val_loss: 0.9168\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 1.0471 - val_loss: 0.7631\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 1.0863 - val_loss: 1.4450\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 1.0688 - val_loss: 0.6892\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.0769 - val_loss: 1.9591\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 1.0388 - val_loss: 0.6801\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0011 - val_loss: 0.8488\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 1.0507 - val_loss: 1.0638\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 1.2632 - val_loss: 1.5597\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 1.0690 - val_loss: 1.5742\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 1.0258 - val_loss: 1.2597\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  47.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4352 - val_loss: 2.3524\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 1.4732 - val_loss: 1.8195\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 1.3513 - val_loss: 1.6046\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2815 - val_loss: 1.0628\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 1.2549 - val_loss: 0.8947\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 1.3595 - val_loss: 1.0425\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 1.2575 - val_loss: 1.1783\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 1.1457 - val_loss: 1.1053\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2386 - val_loss: 0.8328\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 1.1121 - val_loss: 0.7683\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 1.0800 - val_loss: 0.8122\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 1.1334 - val_loss: 1.0681\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.1360 - val_loss: 1.7227\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 1.1523 - val_loss: 0.8457\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 1.0846 - val_loss: 1.0112\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 661us/step - loss: 1.1795 - val_loss: 1.2654\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 1.1770 - val_loss: 2.9915\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 1.1655 - val_loss: 1.0430\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 1.1196 - val_loss: 0.6694\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 1.1168 - val_loss: 0.9777\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.9897 - val_loss: 1.5852\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  48.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2536 - val_loss: 1.4058\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.4792 - val_loss: 1.9845\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 1.3039 - val_loss: 1.2030\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 1.3195 - val_loss: 2.4667\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 1.2003 - val_loss: 0.9571\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 1.1969 - val_loss: 1.6163\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.0961 - val_loss: 1.3730\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0934 - val_loss: 1.9173\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 1.1045 - val_loss: 1.4569\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.9799 - val_loss: 0.7403\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.3255 - val_loss: 1.5843\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.2985 - val_loss: 1.4799\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 1.0522 - val_loss: 1.1813\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 1.0945 - val_loss: 1.1196\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 1.2515 - val_loss: 1.6401\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.1444 - val_loss: 1.0283\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 1.2275 - val_loss: 1.8738\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 1.0204 - val_loss: 0.6723\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 642us/step - loss: 1.1011 - val_loss: 0.8072\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 1.0705 - val_loss: 1.9125\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 1.0199 - val_loss: 1.0806\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  47.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3781 - val_loss: 0.4339\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.1522 - val_loss: 0.0624\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0513 - val_loss: 0.0401\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0392 - val_loss: 0.0306\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.0310 - val_loss: 0.0303\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0244 - val_loss: 0.0202\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0201 - val_loss: 0.0183\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0199 - val_loss: 0.0159\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0174 - val_loss: 0.0228\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 450us/step - loss: 0.0156 - val_loss: 0.0133\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0140 - val_loss: 0.0125\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0127 - val_loss: 0.0115\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 450us/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0114 - val_loss: 0.0098\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0106 - val_loss: 0.0093\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 448us/step - loss: 0.0103 - val_loss: 0.0094\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 512us/step - loss: 0.0098 - val_loss: 0.0091\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0092 - val_loss: 0.0087\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0086 - val_loss: 0.0082\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.6s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3577 - val_loss: 0.4205\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.1412 - val_loss: 0.0505\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 448us/step - loss: 0.0395 - val_loss: 0.0309\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0283 - val_loss: 0.0222\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0219 - val_loss: 0.0179\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0205 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0170 - val_loss: 0.0140\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0155 - val_loss: 0.0136\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0147 - val_loss: 0.0117\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 458us/step - loss: 0.0128 - val_loss: 0.0108\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0098 - val_loss: 0.0093\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0095 - val_loss: 0.0089\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 453us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3143 - val_loss: 0.3964\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.1338 - val_loss: 0.0543\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0444 - val_loss: 0.0361\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0357 - val_loss: 0.0506\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0293 - val_loss: 0.0239\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0274 - val_loss: 0.0221\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0238 - val_loss: 0.0209\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0212 - val_loss: 0.0182\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0176 - val_loss: 0.0222\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0164 - val_loss: 0.0135\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0154 - val_loss: 0.0128\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0158 - val_loss: 0.0131\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0169 - val_loss: 0.0113\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0136 - val_loss: 0.0106\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0147 - val_loss: 0.0102\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 442us/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0101 - val_loss: 0.0091\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 439us/step - loss: 0.0089 - val_loss: 0.0084\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 442us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.3639 - val_loss: 0.4114\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.1400 - val_loss: 0.0602\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0439 - val_loss: 0.0363\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 464us/step - loss: 0.0306 - val_loss: 0.0254\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 443us/step - loss: 0.0254 - val_loss: 0.0205\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0222 - val_loss: 0.0173\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0196 - val_loss: 0.0166\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0163 - val_loss: 0.0142\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0142 - val_loss: 0.0130\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0128 - val_loss: 0.0119\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 435us/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 439us/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 440us/step - loss: 0.0107 - val_loss: 0.0090\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0096 - val_loss: 0.0088\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 438us/step - loss: 0.0099 - val_loss: 0.0086\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 447us/step - loss: 0.0102 - val_loss: 0.0085\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0091 - val_loss: 0.0083\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  43.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3247 - val_loss: 0.4125\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.1398 - val_loss: 0.0532\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0414 - val_loss: 0.0324\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0292 - val_loss: 0.0250\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0263 - val_loss: 0.0200\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0218 - val_loss: 0.0180\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0196 - val_loss: 0.0212\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0181 - val_loss: 0.0150\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0166 - val_loss: 0.0907\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0165 - val_loss: 0.0143\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0138 - val_loss: 0.0122\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0123 - val_loss: 0.0116\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 464us/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 466us/step - loss: 0.0121 - val_loss: 0.0095\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 486us/step - loss: 0.0101 - val_loss: 0.0090\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 517us/step - loss: 0.0112 - val_loss: 0.0088\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0111 - val_loss: 0.0084\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 0.0088 - val_loss: 0.0082\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0082 - val_loss: 0.0082\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  44.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 811us/step - loss: 2.0495 - val_loss: 1.8333\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 556us/step - loss: 1.9853 - val_loss: 1.0758\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 537us/step - loss: 2.0177 - val_loss: 1.7394\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 4s 571us/step - loss: 1.8155 - val_loss: 0.6018\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.5840 - val_loss: 1.1553\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 539us/step - loss: 1.7037 - val_loss: 1.7619\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.6147 - val_loss: 1.4786\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 3s 558us/step - loss: 1.6448 - val_loss: 1.1432\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 542us/step - loss: 1.7160 - val_loss: 6.7320\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 559us/step - loss: 1.5923 - val_loss: 1.7710\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6038 - val_loss: 0.8006\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 539us/step - loss: 1.5625 - val_loss: 3.3897\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 4s 584us/step - loss: 1.6752 - val_loss: 3.7497\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 538us/step - loss: 1.5930 - val_loss: 1.3925\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 527us/step - loss: 1.7788 - val_loss: 1.2418\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.5695 - val_loss: 1.1778\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.4755 - val_loss: 1.0965\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6045 - val_loss: 0.8042\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 533us/step - loss: 1.6140 - val_loss: 2.1645\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 536us/step - loss: 1.6665 - val_loss: 3.2180\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 4s 566us/step - loss: 1.5921 - val_loss: 1.6398\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 795us/step - loss: 2.0826 - val_loss: 1.6291\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.8595 - val_loss: 2.8289\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 551us/step - loss: 1.7470 - val_loss: 1.6122\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 3s 526us/step - loss: 1.7286 - val_loss: 1.8451\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 532us/step - loss: 1.6521 - val_loss: 0.9254\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.7053 - val_loss: 2.2272\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.5833 - val_loss: 1.3344\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 4s 572us/step - loss: 1.7034 - val_loss: 1.0786\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 539us/step - loss: 1.6402 - val_loss: 2.0677\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.6700 - val_loss: 0.8380\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 4s 576us/step - loss: 1.5911 - val_loss: 0.9440\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.6941 - val_loss: 1.8238\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6054 - val_loss: 1.3997\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 4s 563us/step - loss: 1.6051 - val_loss: 1.0353\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 541us/step - loss: 1.5952 - val_loss: 0.9604\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 538us/step - loss: 1.6235 - val_loss: 1.1089\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 4s 573us/step - loss: 1.8128 - val_loss: 0.7699\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.7668 - val_loss: 0.8111\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.6631 - val_loss: 2.6912\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 4s 562us/step - loss: 1.7038 - val_loss: 0.7816\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.7381 - val_loss: 1.8475\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 824us/step - loss: 2.0848 - val_loss: 1.8964\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 537us/step - loss: 1.9604 - val_loss: 0.9799\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 538us/step - loss: 1.7706 - val_loss: 2.7968\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 4s 581us/step - loss: 1.8178 - val_loss: 0.8405\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.8514 - val_loss: 3.7617\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 542us/step - loss: 1.6464 - val_loss: 1.2769\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 4s 572us/step - loss: 1.7299 - val_loss: 2.0901\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6110 - val_loss: 3.6727\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.5695 - val_loss: 0.9895\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 546us/step - loss: 1.6302 - val_loss: 2.2467\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 3s 528us/step - loss: 1.7181 - val_loss: 2.0060\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.5945 - val_loss: 1.1329\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 4s 570us/step - loss: 1.7250 - val_loss: 0.9861\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.6462 - val_loss: 1.3101\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 536us/step - loss: 1.8206 - val_loss: 1.6681\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 553us/step - loss: 1.6675 - val_loss: 0.9190\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.5987 - val_loss: 0.9040\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 540us/step - loss: 1.6117 - val_loss: 0.9613\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 558us/step - loss: 1.7704 - val_loss: 0.7324\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6189 - val_loss: 3.5304\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 4s 574us/step - loss: 1.5071 - val_loss: 3.6898\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 791us/step - loss: 2.2296 - val_loss: 2.6894\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 554us/step - loss: 1.8376 - val_loss: 1.2147\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 541us/step - loss: 1.7435 - val_loss: 0.9304\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 3s 545us/step - loss: 1.8264 - val_loss: 1.1540\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 558us/step - loss: 1.8282 - val_loss: 1.8848\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.7473 - val_loss: 2.3212\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.8021 - val_loss: 2.3163\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 4s 563us/step - loss: 1.6223 - val_loss: 2.0090\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 549us/step - loss: 1.6435 - val_loss: 0.8118\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.7447 - val_loss: 4.3669\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 4s 574us/step - loss: 1.6616 - val_loss: 1.0755\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 541us/step - loss: 1.7779 - val_loss: 1.2148\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 3s 540us/step - loss: 1.6262 - val_loss: 3.5810\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 554us/step - loss: 1.6404 - val_loss: 1.0313\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.7011 - val_loss: 1.3704\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 532us/step - loss: 1.6487 - val_loss: 3.3312\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 4s 569us/step - loss: 1.7005 - val_loss: 0.7953\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 533us/step - loss: 1.6248 - val_loss: 1.2721\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 4s 560us/step - loss: 1.7068 - val_loss: 1.7680\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 557us/step - loss: 1.7333 - val_loss: 0.5884\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 3s 522us/step - loss: 1.6675 - val_loss: 0.9908\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.2min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 774us/step - loss: 2.1779 - val_loss: 1.4063\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 525us/step - loss: 1.8682 - val_loss: 3.0732\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 532us/step - loss: 1.8061 - val_loss: 0.7397\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 3s 536us/step - loss: 1.6944 - val_loss: 1.5711\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 545us/step - loss: 1.7009 - val_loss: 3.3226\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 4s 575us/step - loss: 1.6455 - val_loss: 1.0860\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 560us/step - loss: 1.8057 - val_loss: 1.3950\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 3s 527us/step - loss: 1.6595 - val_loss: 3.0940\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 554us/step - loss: 1.8185 - val_loss: 1.0772\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 4s 567us/step - loss: 1.7835 - val_loss: 2.3305\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 3s 522us/step - loss: 1.6996 - val_loss: 2.7528\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 556us/step - loss: 1.8290 - val_loss: 1.8732\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 3s 548us/step - loss: 1.7584 - val_loss: 2.3031\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.6866 - val_loss: 1.2577\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 553us/step - loss: 1.7467 - val_loss: 2.4673\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 552us/step - loss: 1.6664 - val_loss: 1.1721\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 3s 533us/step - loss: 1.8591 - val_loss: 3.9664\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 4s 570us/step - loss: 1.9158 - val_loss: 3.0424\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 546us/step - loss: 1.6148 - val_loss: 0.9112\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 529us/step - loss: 1.5801 - val_loss: 0.9372\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.6065 - val_loss: 0.7728\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 1.3138 - val_loss: 0.4532\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.2757 - val_loss: 0.5570\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.2025 - val_loss: 1.0355\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 1.1667 - val_loss: 1.0184\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 1.2145 - val_loss: 0.8794\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 1.0625 - val_loss: 0.8471\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 1.1149 - val_loss: 2.7763\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 1.1695 - val_loss: 1.3112\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 567us/step - loss: 1.1349 - val_loss: 0.7711\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.2292 - val_loss: 0.6008\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 596us/step - loss: 1.1854 - val_loss: 0.4148\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.1794 - val_loss: 1.3229\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.1348 - val_loss: 0.4279\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 564us/step - loss: 1.1109 - val_loss: 0.9632\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 574us/step - loss: 1.1120 - val_loss: 0.6256\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 574us/step - loss: 1.1666 - val_loss: 0.7041\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.0740 - val_loss: 2.1306\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 1.1004 - val_loss: 1.3319\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1979 - val_loss: 1.4518\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 559us/step - loss: 1.0777 - val_loss: 1.4715\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 573us/step - loss: 1.0911 - val_loss: 0.9016\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 1.4827 - val_loss: 1.4923\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.4227 - val_loss: 1.4094\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.3079 - val_loss: 1.0970\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.3872 - val_loss: 0.9075\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 595us/step - loss: 1.2271 - val_loss: 1.0691\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 572us/step - loss: 1.2792 - val_loss: 0.4303\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 565us/step - loss: 1.1838 - val_loss: 1.6258\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.2704 - val_loss: 2.6877\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1844 - val_loss: 0.9091\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.2034 - val_loss: 1.2839\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.3089 - val_loss: 1.0650\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 606us/step - loss: 1.2428 - val_loss: 1.8647\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 557us/step - loss: 1.1691 - val_loss: 1.6116\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 1.2043 - val_loss: 0.8117\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.0830 - val_loss: 0.8571\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.1902 - val_loss: 1.5128\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 570us/step - loss: 1.1331 - val_loss: 0.8823\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 1.1470 - val_loss: 1.1137\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 1.0958 - val_loss: 1.7578\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1036 - val_loss: 0.7895\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.0959 - val_loss: 2.5830\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 1.5119 - val_loss: 1.0155\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 579us/step - loss: 1.2930 - val_loss: 0.8148\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 581us/step - loss: 1.2643 - val_loss: 0.4344\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 1.3187 - val_loss: 2.0839\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.1392 - val_loss: 1.6559\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.2644 - val_loss: 0.8539\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.2365 - val_loss: 0.7371\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.1408 - val_loss: 0.6687\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.0993 - val_loss: 0.8836\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 1.0337 - val_loss: 0.7442\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.2246 - val_loss: 1.0844\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 0.9990 - val_loss: 1.1175\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.2760 - val_loss: 2.3433\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.1260 - val_loss: 0.6654\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.0991 - val_loss: 0.8295\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 0.9976 - val_loss: 1.2872\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 0.9738 - val_loss: 0.5575\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 0.9822 - val_loss: 0.5591\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.1437 - val_loss: 2.8904\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.1838 - val_loss: 0.6201\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.0390 - val_loss: 1.1287\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 1.3537 - val_loss: 3.2304\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 557us/step - loss: 1.2241 - val_loss: 0.7114\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.2891 - val_loss: 0.8875\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.2299 - val_loss: 1.2056\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.2072 - val_loss: 1.4694\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 565us/step - loss: 1.2006 - val_loss: 0.9376\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.2708 - val_loss: 0.5127\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 573us/step - loss: 1.2289 - val_loss: 0.4724\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 576us/step - loss: 1.1163 - val_loss: 1.9452\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 608us/step - loss: 1.1768 - val_loss: 2.2652\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.2898 - val_loss: 0.9334\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.1127 - val_loss: 0.5052\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.0420 - val_loss: 1.0299\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 574us/step - loss: 0.9830 - val_loss: 1.3125\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 572us/step - loss: 1.1674 - val_loss: 0.4857\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.1020 - val_loss: 0.7583\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.0609 - val_loss: 0.6556\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1073 - val_loss: 0.6437\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.1356 - val_loss: 1.4740\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.0726 - val_loss: 0.5107\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 583us/step - loss: 0.9805 - val_loss: 0.7234\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 1.5645 - val_loss: 0.9579\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.3933 - val_loss: 2.0165\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.2286 - val_loss: 0.7878\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.0514 - val_loss: 1.3322\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 570us/step - loss: 1.2913 - val_loss: 3.2380\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 576us/step - loss: 1.1876 - val_loss: 0.8611\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 1.1269 - val_loss: 1.5577\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 0.9962 - val_loss: 0.6705\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.2040 - val_loss: 1.1424\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 577us/step - loss: 1.2394 - val_loss: 1.0566\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 605us/step - loss: 1.1684 - val_loss: 1.4895\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 603us/step - loss: 1.1026 - val_loss: 0.7058\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.1220 - val_loss: 1.2121\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.1655 - val_loss: 1.4523\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.2646 - val_loss: 0.8966\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 567us/step - loss: 1.0991 - val_loss: 0.4141\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.0723 - val_loss: 1.9051\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 572us/step - loss: 1.0062 - val_loss: 1.5180\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.1790 - val_loss: 1.8682\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 558us/step - loss: 1.0427 - val_loss: 0.5334\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 590us/step - loss: 1.1291 - val_loss: 1.4961\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.5009 - val_loss: 0.1091\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.1017 - val_loss: 0.0809\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0805 - val_loss: 0.0724\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0688 - val_loss: 0.0596\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0604 - val_loss: 0.0537\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 704us/step - loss: 0.0544 - val_loss: 0.0579\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 864us/step - loss: 0.0499 - val_loss: 0.0574\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 864us/step - loss: 0.0440 - val_loss: 0.0389\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 698us/step - loss: 0.0364 - val_loss: 0.0321\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 667us/step - loss: 0.0315 - val_loss: 0.0298\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0292 - val_loss: 0.0270\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0276 - val_loss: 0.0246\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0261 - val_loss: 0.0240\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0253 - val_loss: 0.0297\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0238 - val_loss: 0.0218\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0230 - val_loss: 0.0213\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 696us/step - loss: 0.0223 - val_loss: 0.0222\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 675us/step - loss: 0.0217 - val_loss: 0.0201\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 720us/step - loss: 0.0209 - val_loss: 0.0205\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.0203 - val_loss: 0.0189\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 683us/step - loss: 0.0198 - val_loss: 0.0212\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  51.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4908 - val_loss: 0.1065\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0989 - val_loss: 0.0824\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 682us/step - loss: 0.0800 - val_loss: 0.0764\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0679 - val_loss: 0.0597\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 629us/step - loss: 0.0601 - val_loss: 0.0615\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0543 - val_loss: 0.0469\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0482 - val_loss: 0.0461\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0409 - val_loss: 0.0342\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0335 - val_loss: 0.0304\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 658us/step - loss: 0.0304 - val_loss: 0.0283\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0283 - val_loss: 0.0256\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0264 - val_loss: 0.0246\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0250 - val_loss: 0.0231\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0239 - val_loss: 0.0221\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 666us/step - loss: 0.0230 - val_loss: 0.0210\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0218 - val_loss: 0.0217\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0208 - val_loss: 0.0202\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 666us/step - loss: 0.0202 - val_loss: 0.0197\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0194 - val_loss: 0.0184\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 645us/step - loss: 0.0190 - val_loss: 0.0209\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 626us/step - loss: 0.0181 - val_loss: 0.0171\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  50.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4765 - val_loss: 0.1089\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.1041 - val_loss: 0.0823\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0826 - val_loss: 0.0745\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0701 - val_loss: 0.0706\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0614 - val_loss: 0.0549\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0554 - val_loss: 0.0477\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0499 - val_loss: 0.0434\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0441 - val_loss: 0.0420\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0370 - val_loss: 0.0333\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 637us/step - loss: 0.0319 - val_loss: 0.0290\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0290 - val_loss: 0.0263\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0274 - val_loss: 0.0249\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0260 - val_loss: 0.0242\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0250 - val_loss: 0.0231\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0240 - val_loss: 0.0228\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0229 - val_loss: 0.0211\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0222 - val_loss: 0.0232\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0215 - val_loss: 0.0200\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0209 - val_loss: 0.0196\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 692us/step - loss: 0.0200 - val_loss: 0.0209\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 740us/step - loss: 0.0196 - val_loss: 0.0180\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  49.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4379 - val_loss: 0.1000\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0946 - val_loss: 0.0831\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0760 - val_loss: 0.0678\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0650 - val_loss: 0.0618\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0583 - val_loss: 0.0513\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0532 - val_loss: 0.0471\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0491 - val_loss: 0.0448\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0456 - val_loss: 0.0413\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.0438 - val_loss: 0.0424\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0415 - val_loss: 0.0402\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0383 - val_loss: 0.0332\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0348 - val_loss: 0.0381\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0303 - val_loss: 0.0290\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0263 - val_loss: 0.0246\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0243 - val_loss: 0.0220\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0232 - val_loss: 0.0251\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0221 - val_loss: 0.0214\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 700us/step - loss: 0.0219 - val_loss: 0.0196\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 626us/step - loss: 0.0207 - val_loss: 0.0191\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0202 - val_loss: 0.0192\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0196 - val_loss: 0.0184\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  49.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4648 - val_loss: 0.1101\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.1003 - val_loss: 0.0799\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0791 - val_loss: 0.0689\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0678 - val_loss: 0.0584\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0600 - val_loss: 0.0537\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0543 - val_loss: 0.0600\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0501 - val_loss: 0.0457\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0464 - val_loss: 0.0417\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0429 - val_loss: 0.0384\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0397 - val_loss: 0.0369\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0363 - val_loss: 0.0344\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0319 - val_loss: 0.0279\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0276 - val_loss: 0.0249\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0254 - val_loss: 0.0237\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0240 - val_loss: 0.0219\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0228 - val_loss: 0.0230\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0219 - val_loss: 0.0201\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0214 - val_loss: 0.0200\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0209 - val_loss: 0.0193\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0203 - val_loss: 0.0191\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0198 - val_loss: 0.0182\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  48.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8257 - val_loss: 1.0841\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.7961 - val_loss: 1.8390\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.9293 - val_loss: 0.7291\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.7981 - val_loss: 0.4040\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.7961 - val_loss: 0.6338\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.7606 - val_loss: 0.5805\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.8357 - val_loss: 0.6535\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7379 - val_loss: 1.3158\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.7925 - val_loss: 0.8457\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.9728 - val_loss: 0.4094\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.8004 - val_loss: 0.7565\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.7480 - val_loss: 1.6881\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.6774 - val_loss: 0.7345\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.7003 - val_loss: 0.9248\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.8209 - val_loss: 0.5067\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.7357 - val_loss: 0.8436\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.7064 - val_loss: 0.7397\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.6986 - val_loss: 0.4369\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.6661 - val_loss: 0.7733\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.7657 - val_loss: 0.5325\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.8372 - val_loss: 0.6942\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  48.6s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7540 - val_loss: 0.5935\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 661us/step - loss: 0.9600 - val_loss: 0.5619\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.8767 - val_loss: 1.4013\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.7201 - val_loss: 0.8441\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.7841 - val_loss: 1.2540\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.9837 - val_loss: 0.5844\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.7907 - val_loss: 0.8751\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.8221 - val_loss: 0.4917\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.6586 - val_loss: 0.6788\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.8244 - val_loss: 0.7687\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.8070 - val_loss: 1.2177\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.7881 - val_loss: 0.3000\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.7595 - val_loss: 1.0335\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.7706 - val_loss: 1.2513\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.7115 - val_loss: 1.6043\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.8340 - val_loss: 0.4869\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.7587 - val_loss: 0.4795\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7453 - val_loss: 0.5054\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.8020 - val_loss: 1.2062\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.6782 - val_loss: 1.0712\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7217 - val_loss: 0.6947\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  49.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8259 - val_loss: 0.7914\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.8503 - val_loss: 1.4953\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 1.0301 - val_loss: 0.4811\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 695us/step - loss: 0.8100 - val_loss: 0.6207\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.8781 - val_loss: 0.6775\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.7969 - val_loss: 0.7816\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.7632 - val_loss: 0.5114\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.7226 - val_loss: 0.9786\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7845 - val_loss: 0.4548\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.6876 - val_loss: 1.2931\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.8842 - val_loss: 0.7475\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.7147 - val_loss: 0.8473\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.7138 - val_loss: 0.4855\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.8563 - val_loss: 0.6942\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.7323 - val_loss: 0.5321\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.7344 - val_loss: 0.4130\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.6635 - val_loss: 0.4413\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.7459 - val_loss: 0.5090\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7696 - val_loss: 0.6121\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.6943 - val_loss: 0.3167\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.7159 - val_loss: 0.4275\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  48.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8108 - val_loss: 0.6632\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 1.0144 - val_loss: 1.3635\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.9622 - val_loss: 0.7523\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.7466 - val_loss: 0.7003\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.8471 - val_loss: 2.2345\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 669us/step - loss: 0.7158 - val_loss: 0.6101\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.7460 - val_loss: 0.9688\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.8945 - val_loss: 2.0750\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.8393 - val_loss: 0.7837\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.7145 - val_loss: 1.3448\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.8506 - val_loss: 0.8265\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.7636 - val_loss: 0.4143\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.7345 - val_loss: 0.7574\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.9312 - val_loss: 0.8654\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.7928 - val_loss: 1.1743\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.8166 - val_loss: 0.6067\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.7407 - val_loss: 0.6288\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.8150 - val_loss: 1.3362\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.7802 - val_loss: 0.6273\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.6838 - val_loss: 0.8952\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.6871 - val_loss: 0.7546\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  49.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8026 - val_loss: 1.1283\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.9399 - val_loss: 1.0160\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.8103 - val_loss: 0.8321\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.8769 - val_loss: 1.0033\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 658us/step - loss: 0.6961 - val_loss: 0.8674\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.6724 - val_loss: 0.9293\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.8434 - val_loss: 0.4928\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 692us/step - loss: 0.7881 - val_loss: 0.8932\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.6938 - val_loss: 0.3938\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.8286 - val_loss: 0.4843\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 662us/step - loss: 0.6383 - val_loss: 0.5622\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.6835 - val_loss: 0.4465\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.8587 - val_loss: 0.9712\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.7464 - val_loss: 0.6541\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.6609 - val_loss: 0.6444\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.7408 - val_loss: 0.3988\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.7057 - val_loss: 0.6988\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.7602 - val_loss: 1.0903\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.7159 - val_loss: 1.4128\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.6291 - val_loss: 0.8044\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.6761 - val_loss: 0.3876\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  50.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3594 - val_loss: 0.2418\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.1824 - val_loss: 0.1319\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.1013 - val_loss: 0.0742\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0582 - val_loss: 0.0431\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0349 - val_loss: 0.0266\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0222 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 501us/step - loss: 0.0152 - val_loss: 0.0120\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0113 - val_loss: 0.0094\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0090 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0076 - val_loss: 0.0064\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0067 - val_loss: 0.0058\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0062 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0059 - val_loss: 0.0050\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 466us/step - loss: 0.0050 - val_loss: 0.0042\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 499us/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0044 - val_loss: 0.0041\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  42.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3683 - val_loss: 0.2460\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.1846 - val_loss: 0.1333\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.1021 - val_loss: 0.0790\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 537us/step - loss: 0.0586 - val_loss: 0.0434\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0350 - val_loss: 0.0276\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0222 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 506us/step - loss: 0.0151 - val_loss: 0.0120\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0111 - val_loss: 0.0091\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0088 - val_loss: 0.0074\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.0075 - val_loss: 0.0063\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0066 - val_loss: 0.0057\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0060 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0051 - val_loss: 0.0044\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0046 - val_loss: 0.0040\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  44.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3678 - val_loss: 0.2445\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.1842 - val_loss: 0.1327\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 497us/step - loss: 0.1018 - val_loss: 0.0746\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0584 - val_loss: 0.0435\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0350 - val_loss: 0.0267\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0222 - val_loss: 0.0172\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0152 - val_loss: 0.0124\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0113 - val_loss: 0.0092\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0090 - val_loss: 0.0079\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0061 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0050 - val_loss: 0.0043\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 492us/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0046 - val_loss: 0.0040\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  43.8s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3613 - val_loss: 0.2416\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.1823 - val_loss: 0.1308\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.1005 - val_loss: 0.0748\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0575 - val_loss: 0.0427\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0344 - val_loss: 0.0261\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0218 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0148 - val_loss: 0.0214\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0109 - val_loss: 0.0089\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0073 - val_loss: 0.0089\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0059 - val_loss: 0.0051\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 497us/step - loss: 0.0050 - val_loss: 0.0043\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 486us/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  44.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3676 - val_loss: 0.2454\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 506us/step - loss: 0.1854 - val_loss: 0.1341\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.1029 - val_loss: 0.0762\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0588 - val_loss: 0.0436\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0351 - val_loss: 0.0265\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0221 - val_loss: 0.0176\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0150 - val_loss: 0.0119\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0110 - val_loss: 0.0089\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0087 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0073 - val_loss: 0.0063\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0065 - val_loss: 0.0057\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0050 - val_loss: 0.0045\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 531us/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0043 - val_loss: 0.0037\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0041 - val_loss: 0.0055\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  44.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2769 - val_loss: 0.2397\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.1453 - val_loss: 0.1188\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.1091 - val_loss: 0.0981\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0972 - val_loss: 0.0922\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0899 - val_loss: 0.0761\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0847 - val_loss: 0.0765\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0800 - val_loss: 0.1352\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 499us/step - loss: 0.0759 - val_loss: 0.0872\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0728 - val_loss: 0.0686\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0702 - val_loss: 0.0866\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0674 - val_loss: 0.0599\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 514us/step - loss: 0.0640 - val_loss: 0.0720\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0635 - val_loss: 0.0534\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0602 - val_loss: 0.0559\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.0584 - val_loss: 0.0525\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 465us/step - loss: 0.0573 - val_loss: 0.0563\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0564 - val_loss: 0.0494\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0557 - val_loss: 0.0490\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0548 - val_loss: 0.0563\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0526 - val_loss: 0.0657\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0521 - val_loss: 0.0447\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  44.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2681 - val_loss: 0.2390\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.1428 - val_loss: 0.1090\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.1058 - val_loss: 0.0928\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0947 - val_loss: 0.0967\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0871 - val_loss: 0.0765\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0816 - val_loss: 0.0699\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 517us/step - loss: 0.0775 - val_loss: 0.0949\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0730 - val_loss: 0.0666\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0698 - val_loss: 0.0621\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0667 - val_loss: 0.0601\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0642 - val_loss: 0.0535\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.0614 - val_loss: 0.0562\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0605 - val_loss: 0.0663\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.0575 - val_loss: 0.0528\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0562 - val_loss: 0.0489\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0556 - val_loss: 0.1579\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0539 - val_loss: 0.0491\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0528 - val_loss: 0.0480\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0525 - val_loss: 0.0819\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0511 - val_loss: 0.0474\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0501 - val_loss: 0.0466\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  44.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3061 - val_loss: 0.2437\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.1441 - val_loss: 0.1102\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.1083 - val_loss: 0.0982\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0971 - val_loss: 0.0875\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 515us/step - loss: 0.0890 - val_loss: 0.1368\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0838 - val_loss: 0.0713\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0791 - val_loss: 0.0713\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 488us/step - loss: 0.0747 - val_loss: 0.0696\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0725 - val_loss: 0.0610\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0689 - val_loss: 0.0759\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0668 - val_loss: 0.0687\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0641 - val_loss: 0.0774\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0621 - val_loss: 0.0567\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0598 - val_loss: 0.0794\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0588 - val_loss: 0.0523\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 497us/step - loss: 0.0574 - val_loss: 0.0530\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 492us/step - loss: 0.0561 - val_loss: 0.0527\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 521us/step - loss: 0.0549 - val_loss: 0.0510\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0535 - val_loss: 0.0495\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0532 - val_loss: 0.0458\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0523 - val_loss: 0.0474\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  43.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2884 - val_loss: 0.2423\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.1446 - val_loss: 0.1139\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.1081 - val_loss: 0.0931\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0969 - val_loss: 0.0828\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0887 - val_loss: 0.0761\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0833 - val_loss: 0.0905\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0790 - val_loss: 0.0874\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0765 - val_loss: 0.0652\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0713 - val_loss: 0.0870\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0694 - val_loss: 0.0638\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 501us/step - loss: 0.0658 - val_loss: 0.0615\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0650 - val_loss: 0.0570\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0624 - val_loss: 0.1057\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 499us/step - loss: 0.0611 - val_loss: 0.0652\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.0585 - val_loss: 0.0502\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0583 - val_loss: 0.0559\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0575 - val_loss: 0.2498\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0557 - val_loss: 0.0494\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0549 - val_loss: 0.0465\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0541 - val_loss: 0.0468\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0525 - val_loss: 0.0620\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  43.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3054 - val_loss: 0.2460\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.1494 - val_loss: 0.1231\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.1100 - val_loss: 0.1390\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0962 - val_loss: 0.1000\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0885 - val_loss: 0.0773\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0813 - val_loss: 0.0716\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0769 - val_loss: 0.0676\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0726 - val_loss: 0.0648\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0693 - val_loss: 0.0599\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0667 - val_loss: 0.1222\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0645 - val_loss: 0.0580\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0622 - val_loss: 0.0556\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0600 - val_loss: 0.0534\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0586 - val_loss: 0.0526\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0568 - val_loss: 0.0568\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0560 - val_loss: 0.0509\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0540 - val_loss: 0.0488\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0534 - val_loss: 0.0505\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 506us/step - loss: 0.0532 - val_loss: 0.0480\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 510us/step - loss: 0.0510 - val_loss: 0.0433\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  44.3s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3760 - val_loss: 0.0559\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0393 - val_loss: 0.0275\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0236 - val_loss: 0.0183\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 661us/step - loss: 0.0168 - val_loss: 0.0143\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0136 - val_loss: 0.0118\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 647us/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 727us/step - loss: 0.0103 - val_loss: 0.0095\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0095 - val_loss: 0.0084\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 652us/step - loss: 0.0085 - val_loss: 0.0096\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0079 - val_loss: 0.0073\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 669us/step - loss: 0.0069 - val_loss: 0.0062\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0062 - val_loss: 0.0078\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0058 - val_loss: 0.0064\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 660us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 728us/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0053 - val_loss: 0.0052\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3673 - val_loss: 0.0431\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0298 - val_loss: 0.0211\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.0189 - val_loss: 0.0153\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0146 - val_loss: 0.0125\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0127 - val_loss: 0.0112\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0102 - val_loss: 0.0087\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0085 - val_loss: 0.0094\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0079 - val_loss: 0.0076\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0069 - val_loss: 0.0063\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 692us/step - loss: 0.0060 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0052 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0051 - val_loss: 0.0049\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4291 - val_loss: 0.0434\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0306 - val_loss: 0.0227\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0196 - val_loss: 0.0160\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0151 - val_loss: 0.0132\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 659us/step - loss: 0.0124 - val_loss: 0.0109\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0101 - val_loss: 0.0094\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 655us/step - loss: 0.0088 - val_loss: 0.0080\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 631us/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0068 - val_loss: 0.0060\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0059 - val_loss: 0.0056\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0055 - val_loss: 0.0064\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 651us/step - loss: 0.0054 - val_loss: 0.0056\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0050 - val_loss: 0.0067\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3826 - val_loss: 0.0523\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0364 - val_loss: 0.0265\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0220 - val_loss: 0.0188\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0161 - val_loss: 0.0136\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0135 - val_loss: 0.0117\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0097 - val_loss: 0.0086\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0086 - val_loss: 0.0082\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0061 - val_loss: 0.0059\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 755us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 668us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 711us/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 630us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0046 - val_loss: 0.0045\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.9s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3885 - val_loss: 0.0525\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0369 - val_loss: 0.0272\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 637us/step - loss: 0.0231 - val_loss: 0.0185\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 724us/step - loss: 0.0169 - val_loss: 0.0140\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 838us/step - loss: 0.0132 - val_loss: 0.0122\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 2s 990us/step - loss: 0.0100 - val_loss: 0.0090\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 625us/step - loss: 0.0087 - val_loss: 0.0083\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 712us/step - loss: 0.0085 - val_loss: 0.0112\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0077 - val_loss: 0.0071\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0057 - val_loss: 0.0061\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 636us/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 662us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  52.6s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3564 - val_loss: 0.2425\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.1828 - val_loss: 0.1314\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.1011 - val_loss: 0.0740\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0580 - val_loss: 0.0430\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0348 - val_loss: 0.0263\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0221 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0151 - val_loss: 0.0120\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0112 - val_loss: 0.0091\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 507us/step - loss: 0.0089 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0075 - val_loss: 0.0064\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0067 - val_loss: 0.0057\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0061 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0049 - val_loss: 0.0044\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 509us/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 514us/step - loss: 0.0042 - val_loss: 0.0037\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20>,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={'activation': ['relu', 'tanh'],\n",
       "                             'batch_size': [16, 32, 64],\n",
       "                             'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
       "                             'learning_rate': [0.001, 0.01, 0.1],\n",
       "                             'optimizer': ['sgd', 'adam']},\n",
       "              verbose=2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define base model function for bayes search\n",
    "def create_model(activation='relu', optimizer='adam', learning_rate=0.001, kernel_regularizer=None, batch_size=32):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'optimizer': ['sgd', 'adam'],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epochs, verbose=2)\n",
    "\n",
    "# perform random search hyperparameter tuning\n",
    "bayes_search = BayesSearchCV(estimator=keras_reg, search_spaces=param_dist, n_iter=10, cv=5, verbose=2, random_state=42, error_score='raise')\n",
    "bayes_search.fit(Xtrain, ytrain, sample_weight=wtrain, validation_data=(Xval, yval, wval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.897756</td>\n",
       "      <td>0.870543</td>\n",
       "      <td>0.156242</td>\n",
       "      <td>0.011116</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.004013</td>\n",
       "      <td>-0.003887</td>\n",
       "      <td>-0.003985</td>\n",
       "      <td>-0.004065</td>\n",
       "      <td>-0.004182</td>\n",
       "      <td>-0.004026</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.712921</td>\n",
       "      <td>0.397587</td>\n",
       "      <td>0.144377</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.920400</td>\n",
       "      <td>-0.372206</td>\n",
       "      <td>-0.663099</td>\n",
       "      <td>-0.555351</td>\n",
       "      <td>-0.749872</td>\n",
       "      <td>-0.652185</td>\n",
       "      <td>0.184047</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.110752</td>\n",
       "      <td>1.256021</td>\n",
       "      <td>0.151572</td>\n",
       "      <td>0.010048</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.006722</td>\n",
       "      <td>-0.005950</td>\n",
       "      <td>-0.007479</td>\n",
       "      <td>-0.006512</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>-0.006538</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135.101206</td>\n",
       "      <td>0.380900</td>\n",
       "      <td>0.378550</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>tanh</td>\n",
       "      <td>16</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 16, 'kern...</td>\n",
       "      <td>-1.221039</td>\n",
       "      <td>-2.241018</td>\n",
       "      <td>-2.258673</td>\n",
       "      <td>-1.308517</td>\n",
       "      <td>-1.526313</td>\n",
       "      <td>-1.711112</td>\n",
       "      <td>0.451005</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.599411</td>\n",
       "      <td>0.461664</td>\n",
       "      <td>0.228074</td>\n",
       "      <td>0.008765</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 32, 'kern...</td>\n",
       "      <td>-0.551183</td>\n",
       "      <td>-0.671337</td>\n",
       "      <td>-0.275650</td>\n",
       "      <td>-0.343786</td>\n",
       "      <td>-0.761923</td>\n",
       "      <td>-0.520776</td>\n",
       "      <td>0.186095</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49.627714</td>\n",
       "      <td>0.804522</td>\n",
       "      <td>0.150416</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.011203</td>\n",
       "      <td>-0.010994</td>\n",
       "      <td>-0.012591</td>\n",
       "      <td>-0.012352</td>\n",
       "      <td>-0.013580</td>\n",
       "      <td>-0.012144</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49.005421</td>\n",
       "      <td>0.492690</td>\n",
       "      <td>0.149297</td>\n",
       "      <td>0.007677</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.624877</td>\n",
       "      <td>-0.266007</td>\n",
       "      <td>-0.635726</td>\n",
       "      <td>-0.305873</td>\n",
       "      <td>-1.075089</td>\n",
       "      <td>-0.581515</td>\n",
       "      <td>0.291190</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43.598086</td>\n",
       "      <td>0.795214</td>\n",
       "      <td>0.150025</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.002285</td>\n",
       "      <td>-0.002225</td>\n",
       "      <td>-0.002348</td>\n",
       "      <td>-0.002100</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.002230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43.600693</td>\n",
       "      <td>0.447184</td>\n",
       "      <td>0.152037</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.041846</td>\n",
       "      <td>-0.029398</td>\n",
       "      <td>-0.051953</td>\n",
       "      <td>-0.034058</td>\n",
       "      <td>-0.031372</td>\n",
       "      <td>-0.037725</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50.836121</td>\n",
       "      <td>0.821611</td>\n",
       "      <td>0.152331</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.004155</td>\n",
       "      <td>-0.004024</td>\n",
       "      <td>-0.004070</td>\n",
       "      <td>-0.003794</td>\n",
       "      <td>-0.004121</td>\n",
       "      <td>-0.004033</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      48.897756      0.870543         0.156242        0.011116   \n",
       "1      47.712921      0.397587         0.144377        0.003170   \n",
       "2      42.110752      1.256021         0.151572        0.010048   \n",
       "3     135.101206      0.380900         0.378550        0.007319   \n",
       "4      76.599411      0.461664         0.228074        0.008765   \n",
       "5      49.627714      0.804522         0.150416        0.005282   \n",
       "6      49.005421      0.492690         0.149297        0.007677   \n",
       "7      43.598086      0.795214         0.150025        0.006603   \n",
       "8      43.600693      0.447184         0.152037        0.005516   \n",
       "9      50.836121      0.821611         0.152331        0.004190   \n",
       "\n",
       "  param_activation param_batch_size param_kernel_regularizer  \\\n",
       "0             relu               64                    l1_l2   \n",
       "1             tanh               64                       l1   \n",
       "2             relu               64                       l1   \n",
       "3             tanh               16                       l2   \n",
       "4             tanh               32                       l2   \n",
       "5             tanh               64                       l1   \n",
       "6             tanh               64                       l2   \n",
       "7             tanh               64                       l2   \n",
       "8             tanh               64                    l1_l2   \n",
       "9             relu               64                    l1_l2   \n",
       "\n",
       "  param_learning_rate param_optimizer  \\\n",
       "0               0.001            adam   \n",
       "1                 0.1            adam   \n",
       "2                0.01             sgd   \n",
       "3                 0.1            adam   \n",
       "4                 0.1            adam   \n",
       "5               0.001            adam   \n",
       "6                 0.1            adam   \n",
       "7                 0.1             sgd   \n",
       "8                0.01             sgd   \n",
       "9               0.001            adam   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.004013   \n",
       "1  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.920400   \n",
       "2  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.006722   \n",
       "3  {'activation': 'tanh', 'batch_size': 16, 'kern...          -1.221039   \n",
       "4  {'activation': 'tanh', 'batch_size': 32, 'kern...          -0.551183   \n",
       "5  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.011203   \n",
       "6  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.624877   \n",
       "7  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.002285   \n",
       "8  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.041846   \n",
       "9  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.004155   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0          -0.003887          -0.003985          -0.004065          -0.004182   \n",
       "1          -0.372206          -0.663099          -0.555351          -0.749872   \n",
       "2          -0.005950          -0.007479          -0.006512          -0.006029   \n",
       "3          -2.241018          -2.258673          -1.308517          -1.526313   \n",
       "4          -0.671337          -0.275650          -0.343786          -0.761923   \n",
       "5          -0.010994          -0.012591          -0.012352          -0.013580   \n",
       "6          -0.266007          -0.635726          -0.305873          -1.075089   \n",
       "7          -0.002225          -0.002348          -0.002100          -0.002193   \n",
       "8          -0.029398          -0.051953          -0.034058          -0.031372   \n",
       "9          -0.004024          -0.004070          -0.003794          -0.004121   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0        -0.004026        0.000097                2  \n",
       "1        -0.652185        0.184047                9  \n",
       "2        -0.006538        0.000552                4  \n",
       "3        -1.711112        0.451005               10  \n",
       "4        -0.520776        0.186095                7  \n",
       "5        -0.012144        0.000950                5  \n",
       "6        -0.581515        0.291190                8  \n",
       "7        -0.002230        0.000084                1  \n",
       "8        -0.037725        0.008277                6  \n",
       "9        -0.004033        0.000127                3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bayes_search_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "\n",
    "display(bayes_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/3143193033.py:24: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 11111\n",
      "max_resources_: 100000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 12\n",
      "n_resources: 11111\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 1ms/step - loss: 1.3465 - val_loss: 0.4055\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 435us/step - loss: 0.1402 - val_loss: 0.0562\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0448 - val_loss: 0.0366\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0348 - val_loss: 0.0322\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0338 - val_loss: 0.0500\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0288 - val_loss: 0.0334\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0286 - val_loss: 0.0230\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 466us/step - loss: 0.0245 - val_loss: 0.0188\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0235 - val_loss: 0.0199\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 445us/step - loss: 0.0237 - val_loss: 0.0168\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0213 - val_loss: 0.0153\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0208 - 88ms/epoch - 317us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0396 - 90ms/epoch - 324us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0186 - 90ms/epoch - 323us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0177 - 86ms/epoch - 311us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0195 - 93ms/epoch - 333us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0194 - 89ms/epoch - 319us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0162 - 86ms/epoch - 311us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0179 - 85ms/epoch - 307us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0256 - 86ms/epoch - 310us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0148 - 87ms/epoch - 313us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0187 - 86ms/epoch - 308us/step\n",
      "70/70 - 0s - loss: 0.0142 - 65ms/epoch - 929us/step\n",
      "278/278 - 0s - loss: 0.0142 - 82ms/epoch - 296us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  17.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3651 - val_loss: 0.4194\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 441us/step - loss: 0.1461 - val_loss: 0.0542\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 440us/step - loss: 0.0406 - val_loss: 0.0334\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 424us/step - loss: 0.0324 - val_loss: 0.0320\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0252 - val_loss: 0.0195\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 438us/step - loss: 0.0239 - val_loss: 0.0236\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0190 - val_loss: 0.0152\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 435us/step - loss: 0.0163 - val_loss: 0.0162\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 423us/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 431us/step - loss: 0.0163 - val_loss: 0.0134\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 426us/step - loss: 0.0142 - val_loss: 0.0132\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0174 - 88ms/epoch - 315us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0170 - 85ms/epoch - 305us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0153 - 85ms/epoch - 307us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0137 - 86ms/epoch - 308us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0216 - 85ms/epoch - 304us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0121 - 86ms/epoch - 308us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0148 - 85ms/epoch - 307us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0125 - 85ms/epoch - 306us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0124 - 85ms/epoch - 306us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0145 - 85ms/epoch - 307us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0172 - 85ms/epoch - 305us/step\n",
      "70/70 - 0s - loss: 0.0136 - 66ms/epoch - 945us/step\n",
      "278/278 - 0s - loss: 0.0136 - 81ms/epoch - 292us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  16.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3610 - val_loss: 0.3997\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 445us/step - loss: 0.1283 - val_loss: 0.0460\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 436us/step - loss: 0.0364 - val_loss: 0.0298\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 429us/step - loss: 0.0257 - val_loss: 0.0228\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0211 - val_loss: 0.0193\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 439us/step - loss: 0.0203 - val_loss: 0.0165\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0180 - val_loss: 0.0154\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 433us/step - loss: 0.0177 - val_loss: 0.0181\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0182 - val_loss: 0.0136\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0165 - val_loss: 0.0124\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0143 - val_loss: 0.0117\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0160 - 90ms/epoch - 322us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0149 - 88ms/epoch - 318us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0193 - 86ms/epoch - 309us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0144 - 86ms/epoch - 310us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0157 - 87ms/epoch - 313us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0123 - 89ms/epoch - 321us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0135 - 88ms/epoch - 317us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0132 - 91ms/epoch - 326us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0170 - 93ms/epoch - 333us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0137 - 93ms/epoch - 334us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0110 - 93ms/epoch - 334us/step\n",
      "70/70 - 0s - loss: 0.0105 - 68ms/epoch - 965us/step\n",
      "278/278 - 0s - loss: 0.0105 - 93ms/epoch - 336us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  17.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3553 - val_loss: 0.4157\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.1376 - val_loss: 0.0526\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 424us/step - loss: 0.0418 - val_loss: 0.0327\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0318 - val_loss: 0.0273\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 423us/step - loss: 0.0272 - val_loss: 0.0229\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 431us/step - loss: 0.0272 - val_loss: 0.3417\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0219 - val_loss: 0.0328\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 421us/step - loss: 0.0180 - val_loss: 0.0157\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 421us/step - loss: 0.0159 - val_loss: 0.0150\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 438us/step - loss: 0.0159 - val_loss: 0.0133\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 422us/step - loss: 0.0133 - val_loss: 0.0118\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0124 - 88ms/epoch - 317us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0117 - 87ms/epoch - 313us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0117 - 88ms/epoch - 315us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0115 - 87ms/epoch - 312us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0119 - 87ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0121 - 87ms/epoch - 311us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0121 - 87ms/epoch - 313us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0114 - 87ms/epoch - 314us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0141 - 87ms/epoch - 315us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0111 - 88ms/epoch - 316us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0110 - 87ms/epoch - 312us/step\n",
      "70/70 - 0s - loss: 0.0104 - 65ms/epoch - 923us/step\n",
      "278/278 - 0s - loss: 0.0104 - 81ms/epoch - 292us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  17.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3507 - val_loss: 0.4093\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 430us/step - loss: 0.1345 - val_loss: 0.0495\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 426us/step - loss: 0.0388 - val_loss: 0.0318\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 423us/step - loss: 0.0311 - val_loss: 0.0300\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 437us/step - loss: 0.0287 - val_loss: 0.0213\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 424us/step - loss: 0.0239 - val_loss: 0.0198\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0188 - val_loss: 0.0164\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 430us/step - loss: 0.0170 - val_loss: 0.0150\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0149 - val_loss: 0.0141\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0166 - val_loss: 0.0128\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 429us/step - loss: 0.0133 - val_loss: 0.0121\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0234 - 100ms/epoch - 358us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0244 - 87ms/epoch - 313us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0153 - 86ms/epoch - 310us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0148 - 87ms/epoch - 314us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0207 - 87ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0186 - 87ms/epoch - 312us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0124 - 99ms/epoch - 357us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0194 - 86ms/epoch - 311us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0161 - 86ms/epoch - 311us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0146 - 86ms/epoch - 311us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0146 - 87ms/epoch - 315us/step\n",
      "70/70 - 0s - loss: 0.0113 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.0113 - 83ms/epoch - 297us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  16.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3960 - val_loss: 0.0494\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0342 - val_loss: 0.0250\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0205 - val_loss: 0.0212\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0150 - val_loss: 0.0130\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0125 - val_loss: 0.0107\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0102 - val_loss: 0.0094\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0090 - val_loss: 0.0083\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0080 - val_loss: 0.0075\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0070 - val_loss: 0.0064\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0063 - 117ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0067 - 114ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0060 - 115ms/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0069 - 115ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0063 - 125ms/epoch - 448us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0059 - 122ms/epoch - 440us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0059 - 119ms/epoch - 428us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0061 - 119ms/epoch - 427us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0063 - 122ms/epoch - 440us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0058 - 123ms/epoch - 442us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0059 - 119ms/epoch - 428us/step\n",
      "70/70 - 0s - loss: 0.0057 - 68ms/epoch - 976us/step\n",
      "278/278 - 0s - loss: 0.0057 - 84ms/epoch - 302us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3854 - val_loss: 0.0480\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0310 - val_loss: 0.0213\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0177 - val_loss: 0.0149\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0147 - val_loss: 0.0121\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0100 - val_loss: 0.0089\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 0.0066 - val_loss: 0.0062\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0090 - 115ms/epoch - 413us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0058 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0058 - 114ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0058 - 114ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0057 - 113ms/epoch - 405us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0057 - 114ms/epoch - 409us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0062 - 114ms/epoch - 409us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0060 - 113ms/epoch - 408us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0056 - 114ms/epoch - 409us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0062 - 113ms/epoch - 406us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0056 - 113ms/epoch - 406us/step\n",
      "70/70 - 0s - loss: 0.0055 - 67ms/epoch - 961us/step\n",
      "278/278 - 0s - loss: 0.0055 - 81ms/epoch - 291us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  18.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3846 - val_loss: 0.0478\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0327 - val_loss: 0.0284\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0189 - val_loss: 0.0150\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0139 - val_loss: 0.0121\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0113 - val_loss: 0.0101\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0096 - val_loss: 0.0086\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0078 - val_loss: 0.0072\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 536us/step - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0060 - val_loss: 0.0056\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0077 - 117ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0057 - 115ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0056 - 114ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0056 - 115ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0058 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0056 - 116ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0055 - 115ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0055 - 115ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0055 - 116ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0056 - 115ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0059 - 113ms/epoch - 407us/step\n",
      "70/70 - 0s - loss: 0.0054 - 72ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0054 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3621 - val_loss: 0.0486\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0342 - val_loss: 0.0245\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0209 - val_loss: 0.0177\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 536us/step - loss: 0.0156 - val_loss: 0.0132\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0126 - val_loss: 0.0114\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0084 - val_loss: 0.0076\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0068 - val_loss: 0.0063\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0096 - 114ms/epoch - 412us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0063 - 113ms/epoch - 407us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0062 - 116ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0064 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0064 - 114ms/epoch - 409us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0088 - 114ms/epoch - 410us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0061 - 115ms/epoch - 412us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0061 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0061 - 115ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0066 - 116ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0060 - 116ms/epoch - 416us/step\n",
      "70/70 - 0s - loss: 0.0060 - 65ms/epoch - 924us/step\n",
      "278/278 - 0s - loss: 0.0060 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3908 - val_loss: 0.0493\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0328 - val_loss: 0.0224\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0191 - val_loss: 0.0156\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0140 - val_loss: 0.0121\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0115 - val_loss: 0.0100\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0096 - val_loss: 0.0090\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0087 - val_loss: 0.0078\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0070 - val_loss: 0.0065\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0078 - 118ms/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0056 - 123ms/epoch - 443us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0060 - 122ms/epoch - 437us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0056 - 119ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0055 - 120ms/epoch - 432us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0055 - 126ms/epoch - 452us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0056 - 117ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0063 - 118ms/epoch - 423us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0075 - 128ms/epoch - 459us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0054 - 122ms/epoch - 437us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0054 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.0054 - 65ms/epoch - 927us/step\n",
      "278/278 - 0s - loss: 0.0054 - 83ms/epoch - 299us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1026 - val_loss: 0.0409\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0401 - val_loss: 0.0304\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0293 - val_loss: 0.0273\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0295 - val_loss: 0.0323\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0276 - val_loss: 0.0268\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0277 - val_loss: 0.0266\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0286 - val_loss: 0.0279\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0282 - val_loss: 0.0261\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0277 - val_loss: 0.0262\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0275 - val_loss: 0.0254\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0287 - val_loss: 0.0260\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0483 - 117ms/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0281 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0317 - 115ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0271 - 113ms/epoch - 406us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0266 - 127ms/epoch - 458us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0262 - 125ms/epoch - 449us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0324 - 114ms/epoch - 411us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0299 - 120ms/epoch - 430us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0264 - 115ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0402 - 115ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0270 - 114ms/epoch - 409us/step\n",
      "70/70 - 0s - loss: 0.0267 - 66ms/epoch - 938us/step\n",
      "278/278 - 0s - loss: 0.0267 - 83ms/epoch - 297us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0995 - val_loss: 0.0342\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0355 - val_loss: 0.0484\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0305 - val_loss: 0.0325\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0276 - val_loss: 0.0263\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0306 - val_loss: 0.0280\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0307 - val_loss: 0.0276\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0269 - val_loss: 0.0261\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0282 - val_loss: 0.0266\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0266 - val_loss: 0.0258\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0295 - val_loss: 0.0274\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 651us/step - loss: 0.0265 - val_loss: 0.0285\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0566 - 117ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0273 - 113ms/epoch - 407us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0282 - 114ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0274 - 114ms/epoch - 410us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0267 - 114ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0266 - 115ms/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0265 - 116ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0285 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0429 - 115ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0285 - 116ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0275 - 122ms/epoch - 439us/step\n",
      "70/70 - 0s - loss: 0.0270 - 70ms/epoch - 996us/step\n",
      "278/278 - 0s - loss: 0.0270 - 82ms/epoch - 296us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0978 - val_loss: 0.0353\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0337 - val_loss: 0.0345\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0290 - val_loss: 0.0268\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0281 - val_loss: 0.0262\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0280 - val_loss: 0.0265\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0286 - val_loss: 0.0303\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0277 - val_loss: 0.0271\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0282 - val_loss: 0.0262\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0278 - val_loss: 0.0259\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0275 - val_loss: 0.0279\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0287 - val_loss: 0.0268\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0333 - 123ms/epoch - 444us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0267 - 121ms/epoch - 436us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0324 - 125ms/epoch - 449us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0307 - 122ms/epoch - 438us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0265 - 119ms/epoch - 430us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0352 - 119ms/epoch - 429us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0266 - 118ms/epoch - 424us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0293 - 118ms/epoch - 425us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0277 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0430 - 117ms/epoch - 422us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0274 - 110ms/epoch - 395us/step\n",
      "70/70 - 0s - loss: 0.0264 - 68ms/epoch - 977us/step\n",
      "278/278 - 0s - loss: 0.0264 - 88ms/epoch - 316us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1056 - val_loss: 0.0613\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0358 - val_loss: 0.0303\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0298 - val_loss: 0.0277\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0291 - val_loss: 0.0272\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0288 - val_loss: 0.0272\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0283 - val_loss: 0.0270\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0280 - val_loss: 0.0276\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0284 - val_loss: 0.0263\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0288 - val_loss: 0.0272\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0269 - val_loss: 0.0280\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0294 - val_loss: 0.0267\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0288 - 124ms/epoch - 447us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0526 - 121ms/epoch - 435us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0275 - 125ms/epoch - 450us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0279 - 117ms/epoch - 420us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0290 - 118ms/epoch - 426us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0275 - 131ms/epoch - 470us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0301 - 127ms/epoch - 456us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0273 - 116ms/epoch - 417us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0352 - 114ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0285 - 119ms/epoch - 428us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0337 - 142ms/epoch - 512us/step\n",
      "70/70 - 0s - loss: 0.0287 - 66ms/epoch - 936us/step\n",
      "278/278 - 0s - loss: 0.0287 - 85ms/epoch - 305us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1011 - val_loss: 0.0319\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0308 - val_loss: 0.0288\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0324 - val_loss: 0.0287\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0279 - val_loss: 0.0273\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0291 - val_loss: 0.0306\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0289 - val_loss: 0.0281\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0271 - val_loss: 0.0262\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0300 - val_loss: 0.0275\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0291 - val_loss: 0.0263\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0285 - val_loss: 0.0322\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0277 - val_loss: 0.0263\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0323 - 124ms/epoch - 447us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0305 - 115ms/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0282 - 118ms/epoch - 423us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0283 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0423 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0309 - 117ms/epoch - 420us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0267 - 117ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0272 - 109ms/epoch - 393us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0274 - 128ms/epoch - 460us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0273 - 104ms/epoch - 374us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0276 - 103ms/epoch - 370us/step\n",
      "70/70 - 0s - loss: 0.0269 - 69ms/epoch - 979us/step\n",
      "278/278 - 0s - loss: 0.0269 - 84ms/epoch - 303us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3300 - val_loss: 0.2675\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 531us/step - loss: 0.2860 - val_loss: 0.2474\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 525us/step - loss: 0.2707 - val_loss: 0.3840\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.2758 - val_loss: 0.2520\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.2681 - val_loss: 0.2504\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.3030 - val_loss: 0.2759\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.2590 - val_loss: 0.2493\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.2685 - val_loss: 0.4090\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.2641 - val_loss: 0.2842\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2743 - val_loss: 0.2543\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.2898 - val_loss: 0.2599\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3654 - 118ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.2581 - 115ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3261 - 112ms/epoch - 404us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2542 - 117ms/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2930 - 114ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2542 - 115ms/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.3043 - 117ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2996 - 117ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3365 - 116ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2554 - 114ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.3258 - 113ms/epoch - 407us/step\n",
      "70/70 - 0s - loss: 0.2511 - 70ms/epoch - 994us/step\n",
      "278/278 - 0s - loss: 0.2510 - 86ms/epoch - 311us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3370 - val_loss: 0.2700\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.2954 - val_loss: 0.2630\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.2865 - val_loss: 0.2740\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.2765 - val_loss: 0.2524\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.2759 - val_loss: 0.2738\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.2765 - val_loss: 0.2599\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.2716 - val_loss: 0.2615\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.2705 - val_loss: 0.2531\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.2752 - val_loss: 0.2520\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.2780 - val_loss: 0.2560\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2632 - val_loss: 0.2596\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4385 - 119ms/epoch - 429us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.4000 - 115ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.2614 - 118ms/epoch - 424us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2551 - 117ms/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3828 - 133ms/epoch - 478us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2856 - 120ms/epoch - 433us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.2560 - 115ms/epoch - 412us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3851 - 116ms/epoch - 419us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2599 - 117ms/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.4005 - 119ms/epoch - 429us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2574 - 117ms/epoch - 421us/step\n",
      "70/70 - 0s - loss: 0.2491 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.2490 - 82ms/epoch - 294us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3473 - val_loss: 0.2886\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.2792 - val_loss: 0.2545\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.2848 - val_loss: 0.2629\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.2880 - val_loss: 0.3089\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.2769 - val_loss: 0.2625\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2739 - val_loss: 0.2643\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.2778 - val_loss: 0.2504\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.2621 - val_loss: 0.2485\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.2746 - val_loss: 0.2556\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.2697 - val_loss: 0.2559\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.2861 - val_loss: 0.2588\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4729 - 115ms/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.2995 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3738 - 113ms/epoch - 406us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2598 - 114ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2608 - 114ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2660 - 113ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.2864 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.4161 - 113ms/epoch - 405us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2603 - 113ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3387 - 115ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2565 - 126ms/epoch - 452us/step\n",
      "70/70 - 0s - loss: 0.2542 - 64ms/epoch - 914us/step\n",
      "278/278 - 0s - loss: 0.2541 - 85ms/epoch - 306us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3284 - val_loss: 0.2611\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.2845 - val_loss: 0.2483\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.2944 - val_loss: 0.2620\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.2854 - val_loss: 0.2783\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.2849 - val_loss: 0.2550\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2712 - val_loss: 0.2555\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.2731 - val_loss: 0.2508\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.2727 - val_loss: 0.2477\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.2684 - val_loss: 0.2540\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.2748 - val_loss: 0.7859\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.2805 - val_loss: 0.2492\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4792 - 114ms/epoch - 411us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3824 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.2546 - 114ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2534 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2529 - 126ms/epoch - 453us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.3175 - 114ms/epoch - 411us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4005 - 113ms/epoch - 405us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2540 - 114ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2625 - 114ms/epoch - 409us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2538 - 114ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2534 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.2455 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.2456 - 82ms/epoch - 293us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3266 - val_loss: 0.2568\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.2785 - val_loss: 0.2578\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.2913 - val_loss: 0.2531\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.2769 - val_loss: 0.2475\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.2669 - val_loss: 0.2601\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.2981 - val_loss: 0.2510\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.2715 - val_loss: 0.2700\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.2651 - val_loss: 0.2603\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.2743 - val_loss: 0.2537\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.2679 - val_loss: 0.6151\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.2706 - val_loss: 0.2791\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3185 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3209 - 114ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3510 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2949 - 131ms/epoch - 471us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2534 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2539 - 114ms/epoch - 410us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.3196 - 115ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3097 - 114ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.4370 - 114ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2578 - 115ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2545 - 117ms/epoch - 419us/step\n",
      "70/70 - 0s - loss: 0.2485 - 65ms/epoch - 930us/step\n",
      "278/278 - 0s - loss: 0.2486 - 84ms/epoch - 302us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3513 - val_loss: 0.2317\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 450us/step - loss: 0.1737 - val_loss: 0.1265\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0955 - val_loss: 0.0702\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0534 - val_loss: 0.0398\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0307 - val_loss: 0.0233\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0184 - val_loss: 0.0143\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 458us/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0078 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0034 - 90ms/epoch - 325us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0033 - 90ms/epoch - 323us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 89ms/epoch - 321us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 316us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 316us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 87ms/epoch - 313us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0030 - 88ms/epoch - 315us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 90ms/epoch - 323us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 90ms/epoch - 323us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 89ms/epoch - 320us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 317us/step\n",
      "70/70 - 0s - loss: 0.0027 - 67ms/epoch - 960us/step\n",
      "278/278 - 0s - loss: 0.0027 - 87ms/epoch - 312us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3425 - val_loss: 0.2290\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 465us/step - loss: 0.1716 - val_loss: 0.1249\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0941 - val_loss: 0.0691\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0526 - val_loss: 0.0390\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 443us/step - loss: 0.0301 - val_loss: 0.0228\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0179 - val_loss: 0.0140\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 442us/step - loss: 0.0112 - val_loss: 0.0090\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 446us/step - loss: 0.0075 - val_loss: 0.0063\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0032 - 89ms/epoch - 318us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0031 - 87ms/epoch - 314us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0030 - 88ms/epoch - 317us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0030 - 88ms/epoch - 316us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 313us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 313us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0028 - 87ms/epoch - 312us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 316us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 316us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0027 - 87ms/epoch - 312us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 316us/step\n",
      "70/70 - 0s - loss: 0.0025 - 67ms/epoch - 953us/step\n",
      "278/278 - 0s - loss: 0.0025 - 85ms/epoch - 307us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3490 - val_loss: 0.2337\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.1752 - val_loss: 0.1276\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 482us/step - loss: 0.0962 - val_loss: 0.0706\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0538 - val_loss: 0.0400\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0309 - val_loss: 0.0234\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0185 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0078 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 447us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0033 - 104ms/epoch - 375us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0033 - 94ms/epoch - 337us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 93ms/epoch - 336us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 317us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0030 - 89ms/epoch - 320us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 89ms/epoch - 318us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0029 - 90ms/epoch - 323us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 314us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 89ms/epoch - 321us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 90ms/epoch - 326us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 90ms/epoch - 325us/step\n",
      "70/70 - 0s - loss: 0.0026 - 66ms/epoch - 947us/step\n",
      "278/278 - 0s - loss: 0.0026 - 84ms/epoch - 301us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3657 - val_loss: 0.2352\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.1762 - val_loss: 0.1282\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0966 - val_loss: 0.0709\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0539 - val_loss: 0.0401\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0309 - val_loss: 0.0234\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0184 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0116 - val_loss: 0.0093\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0034 - 93ms/epoch - 336us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0032 - 91ms/epoch - 326us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 102ms/epoch - 365us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 318us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0030 - 92ms/epoch - 332us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 94ms/epoch - 339us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0029 - 94ms/epoch - 338us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 94ms/epoch - 338us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 95ms/epoch - 341us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 89ms/epoch - 319us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 101ms/epoch - 364us/step\n",
      "70/70 - 0s - loss: 0.0027 - 66ms/epoch - 941us/step\n",
      "278/278 - 0s - loss: 0.0027 - 86ms/epoch - 311us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  18.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3586 - val_loss: 0.2408\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.1804 - val_loss: 0.1313\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.0990 - val_loss: 0.0727\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0553 - val_loss: 0.0412\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 449us/step - loss: 0.0317 - val_loss: 0.0240\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0189 - val_loss: 0.0148\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0119 - val_loss: 0.0096\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0080 - val_loss: 0.0069\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0058 - val_loss: 0.0050\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0033 - 90ms/epoch - 324us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0033 - 88ms/epoch - 318us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 87ms/epoch - 313us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 87ms/epoch - 314us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0030 - 87ms/epoch - 313us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 90ms/epoch - 324us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0029 - 89ms/epoch - 321us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 313us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 87ms/epoch - 314us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 90ms/epoch - 323us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 87ms/epoch - 314us/step\n",
      "70/70 - 0s - loss: 0.0027 - 66ms/epoch - 940us/step\n",
      "278/278 - 0s - loss: 0.0027 - 85ms/epoch - 304us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1058 - val_loss: 0.0093\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0054 - val_loss: 0.0032\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0038 - val_loss: 0.0025\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0028 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0026 - val_loss: 0.0015\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0017 - val_loss: 0.0058\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0030 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 9.5735e-04 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0128 - 116ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0070 - 114ms/epoch - 410us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0018 - 116ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0012 - 113ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0011 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0010 - 113ms/epoch - 405us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0015 - 115ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0012 - 116ms/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0086 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.0012 - 64ms/epoch - 910us/step\n",
      "278/278 - 0s - loss: 0.0012 - 82ms/epoch - 294us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1016 - val_loss: 0.0084\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0060 - val_loss: 0.0095\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0039 - val_loss: 0.0024\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0029 - val_loss: 0.0018\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0084 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0015 - 114ms/epoch - 411us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0014 - 117ms/epoch - 422us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0014 - 116ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0052 - 115ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0057 - 115ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0019 - 115ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0021 - 121ms/epoch - 436us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0013 - 117ms/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0013 - 117ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0057 - 112ms/epoch - 404us/step\n",
      "70/70 - 0s - loss: 0.0023 - 65ms/epoch - 933us/step\n",
      "278/278 - 0s - loss: 0.0022 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1327 - val_loss: 0.0125\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0065 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0046 - val_loss: 0.0030\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0032 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0028 - val_loss: 0.0057\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0022 - val_loss: 0.0237\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0015 - val_loss: 0.0027\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0076 - 118ms/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0012 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0021 - 113ms/epoch - 407us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0018 - 117ms/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0042 - 133ms/epoch - 477us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0011 - 116ms/epoch - 417us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0048 - 116ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0016 - 114ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0045 - 114ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0015 - 116ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0015 - 115ms/epoch - 415us/step\n",
      "70/70 - 0s - loss: 0.0014 - 65ms/epoch - 926us/step\n",
      "278/278 - 0s - loss: 0.0014 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0827 - val_loss: 0.0064\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0050 - val_loss: 0.0030\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0040 - val_loss: 0.0023\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0025 - val_loss: 0.0031\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0013 - val_loss: 9.8234e-04\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0032 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0085 - 114ms/epoch - 410us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0014 - 113ms/epoch - 406us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0020 - 113ms/epoch - 408us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0011 - 112ms/epoch - 404us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0011 - 114ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0016 - 114ms/epoch - 410us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0022 - 113ms/epoch - 407us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0045 - 113ms/epoch - 406us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0064 - 114ms/epoch - 408us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0013 - 114ms/epoch - 409us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0011 - 113ms/epoch - 408us/step\n",
      "70/70 - 0s - loss: 0.0011 - 67ms/epoch - 960us/step\n",
      "278/278 - 0s - loss: 0.0011 - 82ms/epoch - 293us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1294 - val_loss: 0.0102\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0064 - val_loss: 0.0056\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0040 - val_loss: 0.0025\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0039 - val_loss: 0.0023\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0028 - val_loss: 0.0020\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0027 - val_loss: 0.0015\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0102 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0026 - 117ms/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0018 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0013 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0040 - 122ms/epoch - 440us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0057 - 115ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0015 - 116ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0016 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0017 - 115ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0023 - 114ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0012 - 117ms/epoch - 422us/step\n",
      "70/70 - 0s - loss: 0.0011 - 65ms/epoch - 932us/step\n",
      "278/278 - 0s - loss: 0.0011 - 82ms/epoch - 296us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0322 - val_loss: 0.0051\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0106 - val_loss: 0.0178\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0089 - val_loss: 0.0029\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0065 - val_loss: 0.0134\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0037 - val_loss: 0.0060\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0066 - val_loss: 0.0028\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0027 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0034 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0037 - val_loss: 0.0466\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0599 - 125ms/epoch - 449us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0036 - 114ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0026 - 118ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0025 - 115ms/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0021 - 114ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0038 - 116ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0040 - 116ms/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0020 - 120ms/epoch - 431us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0015 - 129ms/epoch - 462us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0019 - 117ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0038 - 117ms/epoch - 420us/step\n",
      "70/70 - 0s - loss: 0.0013 - 65ms/epoch - 926us/step\n",
      "278/278 - 0s - loss: 0.0013 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0361 - val_loss: 0.0084\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0095 - val_loss: 0.0098\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0069 - val_loss: 0.0023\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0064 - val_loss: 0.0026\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0043 - val_loss: 0.0028\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0035 - val_loss: 0.0174\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0048 - val_loss: 0.0019\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0038 - val_loss: 0.0018\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0052 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0060 - 115ms/epoch - 415us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0016 - 114ms/epoch - 411us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0053 - 117ms/epoch - 422us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0014 - 119ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0018 - 119ms/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0061 - 115ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0017 - 116ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0080 - 135ms/epoch - 485us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0033 - 123ms/epoch - 443us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0019 - 116ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0087 - 116ms/epoch - 418us/step\n",
      "70/70 - 0s - loss: 0.0034 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.0034 - 94ms/epoch - 337us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0338 - val_loss: 0.0053\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0155 - val_loss: 0.0129\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0087 - val_loss: 0.0040\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0062 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0057 - val_loss: 0.0067\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0048 - val_loss: 0.0041\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0034 - val_loss: 0.0023\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0039 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0035 - val_loss: 0.0015\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0237 - 127ms/epoch - 455us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0018 - 124ms/epoch - 445us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0017 - 125ms/epoch - 451us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0016 - 120ms/epoch - 432us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0084 - 119ms/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0018 - 120ms/epoch - 430us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0030 - 120ms/epoch - 432us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0099 - 120ms/epoch - 432us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0017 - 121ms/epoch - 434us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0082 - 120ms/epoch - 430us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0022 - 122ms/epoch - 438us/step\n",
      "70/70 - 0s - loss: 0.0016 - 73ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0015 - 90ms/epoch - 322us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0385 - val_loss: 0.0204\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0093 - val_loss: 0.0036\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0064 - val_loss: 0.0029\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0068 - val_loss: 0.0022\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0042 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0041 - val_loss: 0.0067\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0069 - val_loss: 0.0027\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0051 - val_loss: 0.0018\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0194 - 118ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0019 - 125ms/epoch - 449us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0018 - 114ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0016 - 115ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0014 - 116ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0014 - 116ms/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0102 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0014 - 115ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0043 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0014 - 115ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0016 - 116ms/epoch - 419us/step\n",
      "70/70 - 0s - loss: 0.0071 - 65ms/epoch - 931us/step\n",
      "278/278 - 0s - loss: 0.0073 - 83ms/epoch - 298us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0353 - val_loss: 0.1669\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0122 - val_loss: 0.0094\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0068 - val_loss: 0.0024\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0051 - val_loss: 0.0022\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0065 - val_loss: 0.0037\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0034 - val_loss: 0.0021\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0053 - val_loss: 0.0368\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0024 - val_loss: 0.0036\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0337 - 117ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0086 - 138ms/epoch - 496us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0023 - 115ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0018 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0016 - 115ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0015 - 116ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0015 - 121ms/epoch - 435us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0036 - 118ms/epoch - 424us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0013 - 117ms/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0019 - 117ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0086 - 115ms/epoch - 412us/step\n",
      "70/70 - 0s - loss: 0.0019 - 65ms/epoch - 933us/step\n",
      "278/278 - 0s - loss: 0.0019 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0817 - val_loss: 0.0719\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0353 - val_loss: 0.0108\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0420 - val_loss: 0.0113\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0245 - val_loss: 0.0061\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0225 - val_loss: 0.0195\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0251 - val_loss: 0.0983\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0231 - val_loss: 0.0048\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0328 - val_loss: 0.0070\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0175 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0196 - val_loss: 0.0082\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0482 - val_loss: 0.0049\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0521 - 120ms/epoch - 431us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0598 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0818 - 115ms/epoch - 413us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0135 - 113ms/epoch - 408us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2159 - 113ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0829 - 113ms/epoch - 408us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0225 - 115ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0093 - 114ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0182 - 115ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2546 - 115ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0247 - 116ms/epoch - 415us/step\n",
      "70/70 - 0s - loss: 0.0053 - 64ms/epoch - 912us/step\n",
      "278/278 - 0s - loss: 0.0053 - 84ms/epoch - 302us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0878 - val_loss: 0.0192\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0237 - val_loss: 0.0060\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0388 - val_loss: 0.0034\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0459 - val_loss: 0.0518\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0217 - val_loss: 0.0048\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0228 - val_loss: 0.0159\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0335 - val_loss: 0.0099\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0216 - val_loss: 0.0160\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0268 - val_loss: 0.1032\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0221 - val_loss: 0.0091\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0214 - val_loss: 0.0045\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0828 - 117ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0220 - 116ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3516 - 114ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0303 - 113ms/epoch - 407us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0235 - 113ms/epoch - 407us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0064 - 113ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0081 - 113ms/epoch - 408us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.1599 - 113ms/epoch - 407us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0151 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0095 - 113ms/epoch - 407us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0903 - 112ms/epoch - 403us/step\n",
      "70/70 - 0s - loss: 0.0060 - 65ms/epoch - 925us/step\n",
      "278/278 - 0s - loss: 0.0059 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0877 - val_loss: 0.0093\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0334 - val_loss: 0.0140\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0528 - val_loss: 0.0086\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0287 - val_loss: 0.0060\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0217 - val_loss: 0.0053\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0349 - val_loss: 0.0100\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0271 - val_loss: 0.0046\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0251 - val_loss: 0.0037\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0172 - val_loss: 0.0033\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0357 - val_loss: 0.0203\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0447 - val_loss: 0.0059\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0229 - 125ms/epoch - 451us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0526 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0649 - 114ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0931 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0098 - 114ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0061 - 120ms/epoch - 433us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0743 - 117ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0225 - 119ms/epoch - 429us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.1106 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.1751 - 117ms/epoch - 419us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0106 - 115ms/epoch - 415us/step\n",
      "70/70 - 0s - loss: 0.0053 - 65ms/epoch - 933us/step\n",
      "278/278 - 0s - loss: 0.0052 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0882 - val_loss: 0.0172\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0399 - val_loss: 0.0151\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 635us/step - loss: 0.0260 - val_loss: 0.0048\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0364 - val_loss: 0.0051\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0262 - val_loss: 0.3343\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0240 - val_loss: 0.0113\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0266 - val_loss: 0.0071\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0250 - val_loss: 0.0041\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0323 - val_loss: 0.0075\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0234 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0325 - val_loss: 0.0572\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0198 - 123ms/epoch - 443us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.2494 - 115ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0991 - 115ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0154 - 115ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0154 - 116ms/epoch - 416us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.1370 - 115ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0135 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0518 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0127 - 126ms/epoch - 452us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0781 - 119ms/epoch - 427us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0349 - 137ms/epoch - 492us/step\n",
      "70/70 - 0s - loss: 0.0134 - 71ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0127 - 92ms/epoch - 330us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0691 - val_loss: 0.0070\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0339 - val_loss: 0.0592\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0340 - val_loss: 0.1339\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0350 - val_loss: 0.0085\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0499 - val_loss: 0.0041\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0263 - val_loss: 0.0104\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 718us/step - loss: 0.0294 - val_loss: 0.0107\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0311 - val_loss: 0.0084\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0200 - val_loss: 0.0118\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0191 - val_loss: 0.0047\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0217 - val_loss: 0.0036\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.1301 - 118ms/epoch - 426us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0081 - 133ms/epoch - 479us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0370 - 119ms/epoch - 429us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.1167 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0240 - 114ms/epoch - 409us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.1504 - 114ms/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0156 - 111ms/epoch - 398us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0087 - 117ms/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0406 - 116ms/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0828 - 117ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0310 - 121ms/epoch - 436us/step\n",
      "70/70 - 0s - loss: 0.0158 - 64ms/epoch - 918us/step\n",
      "278/278 - 0s - loss: 0.0153 - 84ms/epoch - 301us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.2734 - val_loss: 0.1981\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0732 - val_loss: 0.0428\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.0380 - val_loss: 0.0382\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 464us/step - loss: 0.0342 - val_loss: 0.0256\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0299 - val_loss: 0.0222\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0250 - val_loss: 0.0268\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 443us/step - loss: 0.0225 - val_loss: 0.0200\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0195 - val_loss: 0.0161\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 461us/step - loss: 0.0166 - val_loss: 0.0148\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0153 - val_loss: 0.0137\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0143 - val_loss: 0.0136\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0237 - 90ms/epoch - 322us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0154 - 84ms/epoch - 304us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0135 - 85ms/epoch - 305us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0197 - 84ms/epoch - 302us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0129 - 83ms/epoch - 299us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0122 - 83ms/epoch - 300us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0130 - 82ms/epoch - 294us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0124 - 84ms/epoch - 302us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0119 - 84ms/epoch - 301us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0124 - 92ms/epoch - 331us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0118 - 98ms/epoch - 352us/step\n",
      "70/70 - 0s - loss: 0.0114 - 69ms/epoch - 984us/step\n",
      "278/278 - 0s - loss: 0.0114 - 86ms/epoch - 310us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  17.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3252 - val_loss: 0.2172\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0797 - val_loss: 0.0475\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0443 - val_loss: 0.0407\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0380 - val_loss: 0.0327\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0325 - val_loss: 0.0257\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0257 - val_loss: 0.0217\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 461us/step - loss: 0.0220 - val_loss: 0.0191\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0203 - val_loss: 0.0173\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 484us/step - loss: 0.0226 - val_loss: 0.0162\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0184 - val_loss: 0.0160\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0170 - val_loss: 0.0142\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0205 - 84ms/epoch - 303us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0188 - 83ms/epoch - 299us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0218 - 83ms/epoch - 298us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0147 - 83ms/epoch - 298us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0172 - 83ms/epoch - 300us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0223 - 96ms/epoch - 346us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0152 - 82ms/epoch - 296us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0152 - 84ms/epoch - 302us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0146 - 84ms/epoch - 302us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0255 - 83ms/epoch - 299us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0138 - 84ms/epoch - 304us/step\n",
      "70/70 - 0s - loss: 0.0126 - 70ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0126 - 86ms/epoch - 310us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  17.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.2958 - val_loss: 0.2223\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0857 - val_loss: 0.0480\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 482us/step - loss: 0.0444 - val_loss: 0.0355\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0390 - val_loss: 0.0581\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0325 - val_loss: 0.0306\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0248 - val_loss: 0.0212\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0224 - val_loss: 0.0177\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0203 - val_loss: 0.0159\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0180 - val_loss: 0.0146\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0161 - val_loss: 0.0155\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0143 - val_loss: 0.0123\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0143 - 109ms/epoch - 392us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0156 - 95ms/epoch - 342us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0171 - 84ms/epoch - 303us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0132 - 82ms/epoch - 295us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0137 - 84ms/epoch - 301us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0145 - 89ms/epoch - 321us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0128 - 88ms/epoch - 316us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0139 - 86ms/epoch - 309us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0126 - 85ms/epoch - 304us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0129 - 82ms/epoch - 296us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0178 - 81ms/epoch - 293us/step\n",
      "70/70 - 0s - loss: 0.0111 - 67ms/epoch - 964us/step\n",
      "278/278 - 0s - loss: 0.0111 - 82ms/epoch - 296us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  17.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2390 - val_loss: 0.2058\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0847 - val_loss: 0.0552\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0486 - val_loss: 0.0368\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 486us/step - loss: 0.0365 - val_loss: 0.0277\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 482us/step - loss: 0.0295 - val_loss: 0.0229\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 488us/step - loss: 0.0265 - val_loss: 0.0210\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0207 - val_loss: 0.0187\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0192 - val_loss: 0.0156\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0174 - val_loss: 0.0149\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0166 - val_loss: 0.0135\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0137 - val_loss: 0.0140\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0189 - 94ms/epoch - 338us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0324 - 92ms/epoch - 332us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0128 - 86ms/epoch - 309us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0205 - 86ms/epoch - 308us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0169 - 87ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0150 - 86ms/epoch - 310us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0260 - 86ms/epoch - 309us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0233 - 86ms/epoch - 308us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0175 - 87ms/epoch - 314us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0160 - 86ms/epoch - 309us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0138 - 86ms/epoch - 308us/step\n",
      "70/70 - 0s - loss: 0.0115 - 73ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0115 - 84ms/epoch - 301us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  18.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2859 - val_loss: 0.2070\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0795 - val_loss: 0.0464\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0418 - val_loss: 0.2256\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0332 - val_loss: 0.0442\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0276 - val_loss: 0.0220\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0240 - val_loss: 0.0195\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0208 - val_loss: 0.0173\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0183 - val_loss: 0.0160\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0185 - val_loss: 0.0143\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 465us/step - loss: 0.0158 - val_loss: 0.0140\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0139 - val_loss: 0.0122\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0349 - 93ms/epoch - 333us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0385 - 85ms/epoch - 305us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0296 - 84ms/epoch - 301us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0230 - 84ms/epoch - 301us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0309 - 82ms/epoch - 293us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0208 - 91ms/epoch - 326us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0144 - 84ms/epoch - 302us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0137 - 85ms/epoch - 305us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0131 - 84ms/epoch - 300us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0285 - 81ms/epoch - 291us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0184 - 81ms/epoch - 290us/step\n",
      "70/70 - 0s - loss: 0.0118 - 69ms/epoch - 991us/step\n",
      "278/278 - 0s - loss: 0.0118 - 92ms/epoch - 330us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  18.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3932 - val_loss: 0.0532\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.0365 - val_loss: 0.0276\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0224 - val_loss: 0.0267\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0173 - val_loss: 0.0156\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0140 - val_loss: 0.0120\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0099 - val_loss: 0.0090\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0093 - val_loss: 0.0081\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0080 - val_loss: 0.0076\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0085 - val_loss: 0.0073\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0077 - val_loss: 0.0068\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0073 - 142ms/epoch - 509us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0073 - 116ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0109 - 119ms/epoch - 427us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0076 - 120ms/epoch - 433us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0066 - 115ms/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0066 - 114ms/epoch - 409us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0066 - 115ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0067 - 115ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0076 - 122ms/epoch - 440us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0067 - 117ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0074 - 116ms/epoch - 418us/step\n",
      "70/70 - 0s - loss: 0.0070 - 67ms/epoch - 958us/step\n",
      "278/278 - 0s - loss: 0.0070 - 83ms/epoch - 298us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4212 - val_loss: 0.0576\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0376 - val_loss: 0.0280\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0225 - val_loss: 0.0180\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0165 - val_loss: 0.0142\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0133 - val_loss: 0.0147\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0114 - val_loss: 0.0099\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0095 - val_loss: 0.0091\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0089 - val_loss: 0.0080\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0082 - val_loss: 0.0078\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0070 - val_loss: 0.0077\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0137 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0066 - 118ms/epoch - 426us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0065 - 115ms/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0065 - 116ms/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0064 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0082 - 122ms/epoch - 439us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0070 - 116ms/epoch - 419us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0067 - 118ms/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0065 - 121ms/epoch - 436us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0066 - 123ms/epoch - 441us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0098 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.0062 - 67ms/epoch - 963us/step\n",
      "278/278 - 0s - loss: 0.0062 - 80ms/epoch - 287us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3702 - val_loss: 0.0449\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.0314 - val_loss: 0.0240\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.0195 - val_loss: 0.0166\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0146 - val_loss: 0.0128\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0123 - val_loss: 0.0106\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0103 - val_loss: 0.0093\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 657us/step - loss: 0.0088 - val_loss: 0.0076\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0172 - 119ms/epoch - 430us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0065 - 118ms/epoch - 424us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0062 - 117ms/epoch - 421us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0062 - 117ms/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0061 - 119ms/epoch - 427us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0062 - 118ms/epoch - 425us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0065 - 116ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0063 - 118ms/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0061 - 116ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0064 - 116ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0061 - 131ms/epoch - 470us/step\n",
      "70/70 - 0s - loss: 0.0059 - 71ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0059 - 86ms/epoch - 309us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3909 - val_loss: 0.0514\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0353 - val_loss: 0.0250\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0216 - val_loss: 0.0170\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0167 - val_loss: 0.0136\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0134 - val_loss: 0.0116\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0115 - val_loss: 0.0102\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0100 - val_loss: 0.0091\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0090 - val_loss: 0.0084\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0083 - val_loss: 0.0076\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0078 - val_loss: 0.0072\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0114 - 121ms/epoch - 435us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0067 - 120ms/epoch - 432us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0067 - 118ms/epoch - 425us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0068 - 119ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0079 - 134ms/epoch - 481us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0066 - 119ms/epoch - 427us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0101 - 117ms/epoch - 422us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0065 - 117ms/epoch - 422us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0066 - 117ms/epoch - 422us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0069 - 119ms/epoch - 428us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0064 - 118ms/epoch - 426us/step\n",
      "70/70 - 0s - loss: 0.0064 - 70ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0064 - 84ms/epoch - 301us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3897 - val_loss: 0.0476\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0340 - val_loss: 0.0279\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0165 - val_loss: 0.0138\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0141 - val_loss: 0.0117\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0106 - val_loss: 0.0095\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0094 - val_loss: 0.0086\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0081 - val_loss: 0.0074\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0079 - val_loss: 0.0070\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0100 - 119ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0069 - 117ms/epoch - 422us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0070 - 120ms/epoch - 433us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0069 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0071 - 131ms/epoch - 470us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0078 - 116ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0087 - 118ms/epoch - 425us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0068 - 117ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0072 - 125ms/epoch - 450us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0073 - 117ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0075 - 118ms/epoch - 426us/step\n",
      "70/70 - 0s - loss: 0.0065 - 70ms/epoch - 999us/step\n",
      "278/278 - 0s - loss: 0.0065 - 85ms/epoch - 307us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1149 - val_loss: 0.0355\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0374 - val_loss: 0.0884\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0321 - val_loss: 0.0288\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0319 - val_loss: 0.0290\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0292 - val_loss: 0.0269\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0302 - val_loss: 0.0279\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0290 - val_loss: 0.0266\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0288 - val_loss: 0.0269\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0284 - val_loss: 0.0267\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0282 - val_loss: 0.0281\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0295 - val_loss: 0.0280\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0366 - 120ms/epoch - 430us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0283 - 117ms/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0270 - 118ms/epoch - 425us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0348 - 118ms/epoch - 425us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0282 - 116ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0301 - 118ms/epoch - 423us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0271 - 131ms/epoch - 471us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0449 - 118ms/epoch - 424us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0288 - 116ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0315 - 115ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0287 - 117ms/epoch - 421us/step\n",
      "70/70 - 0s - loss: 0.0287 - 70ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0287 - 86ms/epoch - 309us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1176 - val_loss: 0.0386\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0349 - val_loss: 0.0302\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0315 - val_loss: 0.0284\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0308 - val_loss: 0.0279\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0288 - val_loss: 0.0272\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0281 - val_loss: 0.0265\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0289 - val_loss: 0.0262\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0295 - val_loss: 0.0265\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0277 - val_loss: 0.0267\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0280 - val_loss: 0.0282\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0281 - val_loss: 0.0267\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0473 - 119ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0286 - 118ms/epoch - 426us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0267 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0278 - 118ms/epoch - 424us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0405 - 118ms/epoch - 425us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0270 - 117ms/epoch - 422us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0270 - 116ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0282 - 117ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0289 - 118ms/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0307 - 120ms/epoch - 433us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0851 - 120ms/epoch - 431us/step\n",
      "70/70 - 0s - loss: 0.0284 - 90ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0284 - 86ms/epoch - 309us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0966 - val_loss: 0.0323\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0338 - val_loss: 0.0286\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0311 - val_loss: 0.0276\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0289 - val_loss: 0.0269\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0288 - val_loss: 0.0261\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0295 - val_loss: 0.0263\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0283 - val_loss: 0.0283\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0284 - val_loss: 0.0257\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0275 - val_loss: 0.0278\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0286 - val_loss: 0.0261\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0269 - val_loss: 0.0370\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0373 - 120ms/epoch - 433us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0298 - 119ms/epoch - 426us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0272 - 117ms/epoch - 420us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0593 - 116ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0278 - 118ms/epoch - 423us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0268 - 117ms/epoch - 422us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0265 - 118ms/epoch - 424us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0274 - 117ms/epoch - 423us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0292 - 119ms/epoch - 429us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0280 - 118ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0269 - 118ms/epoch - 424us/step\n",
      "70/70 - 0s - loss: 0.0279 - 69ms/epoch - 987us/step\n",
      "278/278 - 0s - loss: 0.0279 - 86ms/epoch - 308us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1051 - val_loss: 0.0344\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0361 - val_loss: 0.0313\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0323 - val_loss: 0.0284\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0298 - val_loss: 0.0277\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0290 - val_loss: 0.0271\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 688us/step - loss: 0.0287 - val_loss: 0.0744\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0292 - val_loss: 0.0274\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0280 - val_loss: 0.0271\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0288 - val_loss: 0.0264\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0274 - val_loss: 0.0269\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0307 - val_loss: 0.0261\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0297 - 120ms/epoch - 431us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0287 - 119ms/epoch - 427us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0296 - 118ms/epoch - 423us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0367 - 117ms/epoch - 423us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0272 - 118ms/epoch - 426us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0282 - 118ms/epoch - 425us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0461 - 117ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0368 - 119ms/epoch - 427us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0297 - 118ms/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0274 - 119ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0271 - 120ms/epoch - 431us/step\n",
      "70/70 - 0s - loss: 0.0268 - 79ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0268 - 107ms/epoch - 386us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1078 - val_loss: 0.1346\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0369 - val_loss: 0.0307\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0344 - val_loss: 0.0296\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0305 - val_loss: 0.0307\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0293 - val_loss: 0.0279\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0310 - val_loss: 0.0272\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0278 - val_loss: 0.0495\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0319 - val_loss: 0.0282\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0289 - val_loss: 0.0266\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 638us/step - loss: 0.0295 - val_loss: 0.0266\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0279 - val_loss: 0.0269\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0339 - 124ms/epoch - 445us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0363 - 123ms/epoch - 441us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0283 - 121ms/epoch - 434us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0275 - 118ms/epoch - 426us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0315 - 118ms/epoch - 424us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0282 - 117ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0330 - 117ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0380 - 116ms/epoch - 419us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0290 - 117ms/epoch - 422us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0290 - 122ms/epoch - 438us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0345 - 129ms/epoch - 466us/step\n",
      "70/70 - 0s - loss: 0.0308 - 76ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0307 - 93ms/epoch - 334us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3782 - val_loss: 0.3166\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.3327 - val_loss: 0.4504\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.3195 - val_loss: 0.3393\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 640us/step - loss: 0.3142 - val_loss: 0.2991\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 655us/step - loss: 0.3164 - val_loss: 0.2992\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 645us/step - loss: 0.3190 - val_loss: 0.3262\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.3080 - val_loss: 0.4860\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.3168 - val_loss: 0.2888\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.3281 - val_loss: 0.3004\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.3165 - val_loss: 0.2895\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 626us/step - loss: 0.3248 - val_loss: 0.3144\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3540 - 118ms/epoch - 423us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3588 - 112ms/epoch - 403us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3080 - 116ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.3944 - 124ms/epoch - 444us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3076 - 130ms/epoch - 469us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.4744 - 116ms/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4531 - 120ms/epoch - 432us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2998 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3002 - 119ms/epoch - 429us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3556 - 128ms/epoch - 461us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2999 - 118ms/epoch - 423us/step\n",
      "70/70 - 0s - loss: 0.2939 - 68ms/epoch - 967us/step\n",
      "278/278 - 0s - loss: 0.2939 - 92ms/epoch - 330us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3947 - val_loss: 0.3179\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 638us/step - loss: 0.3194 - val_loss: 0.2996\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.3185 - val_loss: 0.2978\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.3373 - val_loss: 0.3016\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 642us/step - loss: 0.3158 - val_loss: 0.2999\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.3178 - val_loss: 0.2952\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.3218 - val_loss: 0.2971\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.3168 - val_loss: 0.2865\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.3151 - val_loss: 0.3037\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3192 - val_loss: 0.2902\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.3218 - val_loss: 0.2987\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4631 - 137ms/epoch - 492us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3052 - 125ms/epoch - 451us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.2990 - 119ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.4327 - 157ms/epoch - 564us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3723 - 123ms/epoch - 441us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2992 - 120ms/epoch - 432us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4223 - 191ms/epoch - 688us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3772 - 131ms/epoch - 471us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2995 - 126ms/epoch - 453us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2970 - 127ms/epoch - 458us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.3899 - 127ms/epoch - 458us/step\n",
      "70/70 - 0s - loss: 0.2943 - 79ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2942 - 95ms/epoch - 341us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  21.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3624 - val_loss: 0.3177\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.3419 - val_loss: 0.3196\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.3412 - val_loss: 0.3667\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 667us/step - loss: 0.3314 - val_loss: 0.3096\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.3364 - val_loss: 0.3324\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.3355 - val_loss: 0.3025\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.3217 - val_loss: 0.3061\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.3149 - val_loss: 0.3035\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 631us/step - loss: 0.3260 - val_loss: 0.3380\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3121 - val_loss: 0.2881\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3141 - val_loss: 0.2969\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3835 - 122ms/epoch - 438us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.4016 - 119ms/epoch - 429us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3005 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.4045 - 115ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.4195 - 116ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.3345 - 116ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.3063 - 125ms/epoch - 449us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2954 - 127ms/epoch - 458us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3579 - 125ms/epoch - 449us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3304 - 120ms/epoch - 433us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.3007 - 121ms/epoch - 434us/step\n",
      "70/70 - 0s - loss: 0.2859 - 82ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2858 - 86ms/epoch - 310us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  21.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3558 - val_loss: 0.3438\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.3256 - val_loss: 0.5836\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.3140 - val_loss: 0.2953\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.3133 - val_loss: 0.2914\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 637us/step - loss: 0.3147 - val_loss: 1.5693\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3286 - val_loss: 0.3493\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.3031 - val_loss: 0.2912\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.3061 - val_loss: 0.2818\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 625us/step - loss: 0.2972 - val_loss: 0.4654\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.3142 - val_loss: 0.2764\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 632us/step - loss: 0.3095 - val_loss: 0.2926\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3065 - 130ms/epoch - 468us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3538 - 136ms/epoch - 489us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3296 - 126ms/epoch - 452us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.3106 - 124ms/epoch - 448us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.4847 - 126ms/epoch - 452us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2962 - 130ms/epoch - 466us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.2911 - 122ms/epoch - 437us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2918 - 123ms/epoch - 442us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3779 - 141ms/epoch - 507us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3753 - 136ms/epoch - 489us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2926 - 133ms/epoch - 478us/step\n",
      "70/70 - 0s - loss: 0.2845 - 74ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2847 - 89ms/epoch - 320us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3848 - val_loss: 0.3250\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.3340 - val_loss: 0.2941\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.3317 - val_loss: 0.3055\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.3139 - val_loss: 0.3066\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.3352 - val_loss: 0.3064\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3177 - val_loss: 0.2936\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.3240 - val_loss: 0.3031\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.3333 - val_loss: 0.2881\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.3098 - val_loss: 0.3041\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.3106 - val_loss: 0.2827\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.3217 - val_loss: 0.3031\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3835 - 134ms/epoch - 484us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3741 - 119ms/epoch - 430us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3056 - 118ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.4580 - 124ms/epoch - 447us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3001 - 134ms/epoch - 483us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2970 - 127ms/epoch - 455us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4821 - 126ms/epoch - 454us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3075 - 124ms/epoch - 446us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3250 - 119ms/epoch - 428us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3180 - 126ms/epoch - 453us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.4951 - 130ms/epoch - 466us/step\n",
      "70/70 - 0s - loss: 0.2916 - 74ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2917 - 90ms/epoch - 325us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.5s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 4\n",
      "n_resources: 33333\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3532 - val_loss: 0.0434\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0252 - val_loss: 0.0167\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0149 - val_loss: 0.0125\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0101 - val_loss: 0.0089\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0088 - val_loss: 0.0078\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0064 - val_loss: 0.0060\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0059 - val_loss: 0.0055\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0053 - 379ms/epoch - 455us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0053 - 376ms/epoch - 450us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0050 - 373ms/epoch - 448us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0053 - 356ms/epoch - 427us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0050 - 384ms/epoch - 461us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0048 - 392ms/epoch - 470us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0050 - 495ms/epoch - 593us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0048 - 398ms/epoch - 477us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0047 - 385ms/epoch - 462us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0053 - 387ms/epoch - 464us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0046 - 395ms/epoch - 473us/step\n",
      "209/209 - 0s - loss: 0.0045 - 107ms/epoch - 510us/step\n",
      "834/834 - 0s - loss: 0.0045 - 247ms/epoch - 297us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3719 - val_loss: 0.0465\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0306 - val_loss: 0.0232\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0185 - val_loss: 0.0155\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0133 - val_loss: 0.0114\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0106 - val_loss: 0.0090\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0085 - val_loss: 0.0080\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0081 - val_loss: 0.0074\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 684us/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 648us/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 647us/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0055 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0056 - 341ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0053 - 341ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0051 - 339ms/epoch - 406us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0054 - 342ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0049 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0052 - 361ms/epoch - 433us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0051 - 343ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0047 - 340ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0048 - 355ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0049 - 343ms/epoch - 411us/step\n",
      "209/209 - 0s - loss: 0.0047 - 105ms/epoch - 504us/step\n",
      "834/834 - 0s - loss: 0.0047 - 223ms/epoch - 267us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3726 - val_loss: 0.0481\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0336 - val_loss: 0.0253\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0191 - val_loss: 0.0159\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0147 - val_loss: 0.0128\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0121 - val_loss: 0.0108\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0104 - val_loss: 0.0095\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0091 - val_loss: 0.0084\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0083 - val_loss: 0.0077\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0068 - val_loss: 0.0063\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0064 - val_loss: 0.0059\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0066 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0061 - 359ms/epoch - 431us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0068 - 358ms/epoch - 429us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0057 - 362ms/epoch - 434us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0065 - 366ms/epoch - 439us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0056 - 353ms/epoch - 423us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0064 - 356ms/epoch - 427us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0060 - 355ms/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0054 - 356ms/epoch - 427us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0055 - 350ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0069 - 354ms/epoch - 425us/step\n",
      "209/209 - 0s - loss: 0.0053 - 103ms/epoch - 491us/step\n",
      "834/834 - 0s - loss: 0.0053 - 225ms/epoch - 270us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3506 - val_loss: 0.0491\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0345 - val_loss: 0.0253\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0210 - val_loss: 0.0172\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0156 - val_loss: 0.0134\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0124 - val_loss: 0.0113\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0104 - val_loss: 0.0094\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0071 - val_loss: 0.0066\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0067 - val_loss: 0.0062\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0067 - 350ms/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0062 - 349ms/epoch - 419us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0065 - 345ms/epoch - 413us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0060 - 345ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0070 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0057 - 347ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0061 - 351ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0056 - 346ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0060 - 345ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0057 - 346ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0054 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0056 - 107ms/epoch - 512us/step\n",
      "834/834 - 0s - loss: 0.0056 - 225ms/epoch - 269us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3701 - val_loss: 0.0489\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 662us/step - loss: 0.0344 - val_loss: 0.0242\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0207 - val_loss: 0.0168\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0155 - val_loss: 0.0134\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0093 - val_loss: 0.0087\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0081 - val_loss: 0.0071\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0068 - val_loss: 0.0062\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0068 - 349ms/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0061 - 345ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0072 - 344ms/epoch - 413us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0062 - 345ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0058 - 343ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0060 - 345ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0065 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0059 - 345ms/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0055 - 343ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0057 - 348ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0062 - 346ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0054 - 105ms/epoch - 502us/step\n",
      "834/834 - 0s - loss: 0.0054 - 228ms/epoch - 273us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  23.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0315 - val_loss: 0.0136\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0130 - val_loss: 0.0045\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0078 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0068 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0051 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0081 - val_loss: 0.0022\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0027 - val_loss: 0.0075\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0031 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0049 - val_loss: 0.0014\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0058 - 350ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0056 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0018 - 420ms/epoch - 504us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0128 - 346ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0023 - 347ms/epoch - 416us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0031 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0048 - 353ms/epoch - 423us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0086 - 343ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0021 - 345ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0053 - 348ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0032 - 345ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0018 - 101ms/epoch - 485us/step\n",
      "834/834 - 0s - loss: 0.0018 - 228ms/epoch - 273us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0320 - val_loss: 0.0056\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0094 - val_loss: 0.0031\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0085 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0049 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0044 - val_loss: 0.0063\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0054 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0029 - val_loss: 0.0343\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0048 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0036 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0077 - 346ms/epoch - 415us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0266 - 341ms/epoch - 408us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0036 - 343ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0028 - 345ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0046 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0040 - 351ms/epoch - 420us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0030 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0026 - 343ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0062 - 342ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0043 - 340ms/epoch - 408us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0024 - 344ms/epoch - 412us/step\n",
      "209/209 - 0s - loss: 0.0013 - 105ms/epoch - 502us/step\n",
      "834/834 - 0s - loss: 0.0013 - 227ms/epoch - 272us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0328 - val_loss: 0.0047\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0079 - val_loss: 0.0023\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0048 - val_loss: 0.0018\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0042 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0041 - val_loss: 0.0020\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0020 - val_loss: 0.0141\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0032 - val_loss: 0.0016\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0032 - val_loss: 0.0542\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0023 - val_loss: 0.0085\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0047 - 342ms/epoch - 410us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0055 - 342ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0044 - 342ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0031 - 344ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0062 - 341ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0017 - 344ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0048 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0034 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0034 - 343ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0037 - 344ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0042 - 349ms/epoch - 419us/step\n",
      "209/209 - 0s - loss: 0.0013 - 102ms/epoch - 490us/step\n",
      "834/834 - 0s - loss: 0.0013 - 229ms/epoch - 274us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0305 - val_loss: 0.0046\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0101 - val_loss: 0.0074\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0069 - val_loss: 0.0057\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0050 - val_loss: 0.0064\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0037 - val_loss: 0.0030\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0046 - val_loss: 0.0017\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0033 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0024 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0064 - val_loss: 0.0017\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0024 - 346ms/epoch - 415us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0126 - 342ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0038 - 339ms/epoch - 407us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0018 - 345ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0088 - 344ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0018 - 341ms/epoch - 408us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0051 - 339ms/epoch - 407us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0058 - 343ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0016 - 343ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0040 - 343ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0045 - 342ms/epoch - 410us/step\n",
      "209/209 - 0s - loss: 0.0017 - 103ms/epoch - 494us/step\n",
      "834/834 - 0s - loss: 0.0017 - 234ms/epoch - 281us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0318 - val_loss: 0.0064\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0089 - val_loss: 0.0142\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0073 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0059 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0068 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0042 - val_loss: 0.0029\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0031 - val_loss: 0.0019\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0035 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0061 - 350ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0032 - 350ms/epoch - 419us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0018 - 346ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0019 - 349ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0027 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0022 - 349ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0043 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0016 - 347ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0019 - 349ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0024 - 349ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0037 - 349ms/epoch - 419us/step\n",
      "209/209 - 0s - loss: 9.8222e-04 - 104ms/epoch - 499us/step\n",
      "834/834 - 0s - loss: 9.8258e-04 - 232ms/epoch - 279us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3783 - val_loss: 0.2375\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 0.1780 - val_loss: 0.1296\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0977 - val_loss: 0.0717\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0545 - val_loss: 0.0405\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0312 - val_loss: 0.0236\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 526us/step - loss: 0.0186 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0116 - val_loss: 0.0093\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 521us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0032 - 290ms/epoch - 348us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 272ms/epoch - 326us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0028 - 265ms/epoch - 317us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0027 - 268ms/epoch - 321us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 283ms/epoch - 339us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 267ms/epoch - 320us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 265ms/epoch - 318us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0023 - 291ms/epoch - 349us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 291ms/epoch - 349us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0021 - 355ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 262ms/epoch - 314us/step\n",
      "209/209 - 0s - loss: 0.0021 - 108ms/epoch - 519us/step\n",
      "834/834 - 0s - loss: 0.0021 - 232ms/epoch - 278us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3651 - val_loss: 0.2418\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.1812 - val_loss: 0.1319\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 526us/step - loss: 0.0994 - val_loss: 0.0729\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 523us/step - loss: 0.0555 - val_loss: 0.0412\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0318 - val_loss: 0.0240\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0189 - val_loss: 0.0147\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0118 - val_loss: 0.0095\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0079 - val_loss: 0.0069\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 525us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0032 - 287ms/epoch - 344us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 286ms/epoch - 343us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0028 - 276ms/epoch - 330us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0027 - 272ms/epoch - 326us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0025 - 269ms/epoch - 323us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0024 - 269ms/epoch - 323us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 270ms/epoch - 323us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 268ms/epoch - 322us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0022 - 310ms/epoch - 372us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0022 - 314ms/epoch - 377us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0021 - 279ms/epoch - 334us/step\n",
      "209/209 - 0s - loss: 0.0020 - 107ms/epoch - 513us/step\n",
      "834/834 - 0s - loss: 0.0020 - 235ms/epoch - 282us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3778 - val_loss: 0.2500\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.1872 - val_loss: 0.1362\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.1027 - val_loss: 0.0754\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0573 - val_loss: 0.0426\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0329 - val_loss: 0.0249\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0196 - val_loss: 0.0152\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0123 - val_loss: 0.0099\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0082 - val_loss: 0.0069\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 524us/step - loss: 0.0059 - val_loss: 0.0051\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 526us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0034 - 263ms/epoch - 316us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0032 - 260ms/epoch - 312us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0030 - 262ms/epoch - 314us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0028 - 261ms/epoch - 313us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0027 - 261ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0026 - 275ms/epoch - 330us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0025 - 260ms/epoch - 311us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 261ms/epoch - 312us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 273ms/epoch - 328us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0023 - 280ms/epoch - 335us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 273ms/epoch - 327us/step\n",
      "209/209 - 0s - loss: 0.0021 - 107ms/epoch - 513us/step\n",
      "834/834 - 0s - loss: 0.0021 - 233ms/epoch - 279us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3615 - val_loss: 0.2347\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.1759 - val_loss: 0.1282\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0967 - val_loss: 0.0710\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0540 - val_loss: 0.0402\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 537us/step - loss: 0.0310 - val_loss: 0.0235\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0185 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0117 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0079 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0033 - 273ms/epoch - 327us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0031 - 259ms/epoch - 311us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0029 - 264ms/epoch - 317us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0028 - 258ms/epoch - 309us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 267ms/epoch - 320us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 261ms/epoch - 313us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 263ms/epoch - 316us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 273ms/epoch - 328us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 258ms/epoch - 310us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0022 - 271ms/epoch - 325us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 261ms/epoch - 313us/step\n",
      "209/209 - 0s - loss: 0.0021 - 111ms/epoch - 531us/step\n",
      "834/834 - 0s - loss: 0.0021 - 233ms/epoch - 279us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3648 - val_loss: 0.2338\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.1753 - val_loss: 0.1277\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 524us/step - loss: 0.0963 - val_loss: 0.0707\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0539 - val_loss: 0.0401\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0310 - val_loss: 0.0235\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 521us/step - loss: 0.0185 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 525us/step - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 520us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0033 - 257ms/epoch - 308us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0031 - 256ms/epoch - 307us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0029 - 253ms/epoch - 304us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0027 - 260ms/epoch - 312us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 270ms/epoch - 324us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 256ms/epoch - 307us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 260ms/epoch - 312us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 254ms/epoch - 304us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 255ms/epoch - 306us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0022 - 254ms/epoch - 304us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 262ms/epoch - 315us/step\n",
      "209/209 - 0s - loss: 0.0020 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0020 - 224ms/epoch - 268us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  21.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1108 - val_loss: 0.0109\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0066 - val_loss: 0.0037\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 694us/step - loss: 0.0039 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 665us/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 655us/step - loss: 0.0036 - val_loss: 0.0019\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 672us/step - loss: 0.0028 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 656us/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 684us/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0020 - val_loss: 0.0036\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0046 - 345ms/epoch - 413us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0034 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0022 - 341ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0024 - 342ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0019 - 344ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0030 - 339ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0033 - 338ms/epoch - 406us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 339ms/epoch - 406us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0026 - 338ms/epoch - 405us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0039 - 336ms/epoch - 402us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0048 - 352ms/epoch - 421us/step\n",
      "209/209 - 0s - loss: 0.0014 - 102ms/epoch - 488us/step\n",
      "834/834 - 0s - loss: 0.0014 - 228ms/epoch - 273us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  25.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1175 - val_loss: 0.0102\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0074 - val_loss: 0.0050\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0039 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0034 - val_loss: 0.0023\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0031 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0026 - val_loss: 0.0019\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0022 - val_loss: 0.0074\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0055 - 357ms/epoch - 428us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0016 - 349ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0068 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0016 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0021 - 350ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0026 - 349ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0031 - 347ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0013 - 346ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0016 - 347ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0054 - 348ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0021 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0013 - 108ms/epoch - 517us/step\n",
      "834/834 - 0s - loss: 0.0013 - 226ms/epoch - 271us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1157 - val_loss: 0.0088\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0036 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0032 - val_loss: 0.0018\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0023 - val_loss: 0.0057\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0014 - val_loss: 9.3695e-04\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0045 - 345ms/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 341ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0017 - 342ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0048 - 340ms/epoch - 408us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0022 - 341ms/epoch - 409us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0029 - 340ms/epoch - 408us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0015 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0018 - 340ms/epoch - 408us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0019 - 339ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0046 - 346ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0015 - 342ms/epoch - 410us/step\n",
      "209/209 - 0s - loss: 0.0011 - 103ms/epoch - 495us/step\n",
      "834/834 - 0s - loss: 0.0011 - 223ms/epoch - 267us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1163 - val_loss: 0.0086\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0065 - val_loss: 0.0037\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0042 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0034 - val_loss: 0.0028\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0030 - val_loss: 0.0103\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0018 - val_loss: 0.0051\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0053 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0032 - 344ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0026 - 343ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0026 - 346ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 341ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0034 - 339ms/epoch - 407us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0032 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0021 - 340ms/epoch - 408us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0026 - 343ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0018 - 362ms/epoch - 434us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0052 - 340ms/epoch - 407us/step\n",
      "209/209 - 0s - loss: 0.0014 - 107ms/epoch - 510us/step\n",
      "834/834 - 0s - loss: 0.0014 - 225ms/epoch - 270us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1215 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0071 - val_loss: 0.0083\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0044 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0031 - val_loss: 0.0099\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0026 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 657us/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0042 - 356ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0035 - 339ms/epoch - 407us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0020 - 339ms/epoch - 406us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0021 - 342ms/epoch - 410us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0048 - 338ms/epoch - 405us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0013 - 341ms/epoch - 409us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0019 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0031 - 339ms/epoch - 407us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0018 - 339ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0030 - 344ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0019 - 342ms/epoch - 410us/step\n",
      "209/209 - 0s - loss: 0.0012 - 102ms/epoch - 487us/step\n",
      "834/834 - 0s - loss: 0.0012 - 231ms/epoch - 277us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  23.7s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 99999\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0301 - val_loss: 0.0054\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0105 - val_loss: 0.0036\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0059 - val_loss: 0.0041\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0049 - val_loss: 0.0029\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0035 - val_loss: 0.0015\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0022 - val_loss: 0.0010\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0028 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0019 - val_loss: 8.6016e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0042 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0040 - 1s/epoch - 427us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 445us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 417us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 416us/step\n",
      "625/625 - 0s - loss: 0.0020 - 206ms/epoch - 329us/step\n",
      "2500/2500 - 1s - loss: 0.0020 - 661ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  35.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0373 - val_loss: 0.0112\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0137 - val_loss: 0.0131\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0075 - val_loss: 0.0059\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0090 - val_loss: 0.0027\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0055 - val_loss: 0.0028\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0075 - val_loss: 0.0028\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0035 - val_loss: 0.0018\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0034 - val_loss: 0.0214\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0046 - val_loss: 0.0015\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0040 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0055 - 1s/epoch - 422us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 417us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 418us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 426us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0035 - 1s/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0011 - 1s/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0016 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 7.4554e-04 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 7.4529e-04 - 655ms/epoch - 262us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  34.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0356 - val_loss: 0.0044\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0116 - val_loss: 0.0040\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0074 - val_loss: 0.0034\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0083 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0059 - val_loss: 0.0027\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0064 - val_loss: 0.0019\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 729us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0026 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0042 - 1s/epoch - 409us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 417us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 9.0551e-04 - 213ms/epoch - 341us/step\n",
      "2500/2500 - 1s - loss: 9.0586e-04 - 674ms/epoch - 270us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  35.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.0311\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0086 - val_loss: 0.0199\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0100 - val_loss: 0.0038\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0063 - val_loss: 0.0026\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0040 - val_loss: 0.0019\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0042 - val_loss: 0.0015\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0021 - val_loss: 0.0045\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0031 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0026 - val_loss: 9.5754e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0048 - 1s/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 413us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0041 - 1s/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 426us/step\n",
      "625/625 - 0s - loss: 0.0011 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 0.0011 - 657ms/epoch - 263us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  34.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0274 - val_loss: 0.0052\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0089 - val_loss: 0.0039\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0081 - val_loss: 0.0041\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0054 - val_loss: 0.0026\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0038 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0042 - val_loss: 0.0053\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0041 - val_loss: 0.0022\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0033 - val_loss: 0.0102\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0030 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0025 - val_loss: 9.5333e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0057 - 1s/epoch - 416us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0068 - 1s/epoch - 422us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 423us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 421us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 426us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 433us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 413us/step\n",
      "625/625 - 0s - loss: 7.8962e-04 - 213ms/epoch - 341us/step\n",
      "2500/2500 - 1s - loss: 7.8973e-04 - 659ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  35.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1327 - val_loss: 0.0109\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0065 - val_loss: 0.0054\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0045 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 977ms/epoch - 391us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0016 - 1s/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 413us/step\n",
      "625/625 - 0s - loss: 0.0010 - 216ms/epoch - 346us/step\n",
      "2500/2500 - 1s - loss: 0.0010 - 655ms/epoch - 262us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1080 - val_loss: 0.0094\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0063 - val_loss: 0.0040\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0030 - val_loss: 0.0025\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0025 - val_loss: 0.0058\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0020 - val_loss: 9.9945e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 412us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 9.6913e-04 - 212ms/epoch - 339us/step\n",
      "2500/2500 - 1s - loss: 9.6887e-04 - 661ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1281 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0067 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0042 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0034 - val_loss: 0.0025\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0031 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0017 - val_loss: 0.0087\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0035 - 1s/epoch - 417us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 421us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 424us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 418us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 424us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 415us/step\n",
      "625/625 - 0s - loss: 0.0013 - 213ms/epoch - 341us/step\n",
      "2500/2500 - 1s - loss: 0.0013 - 656ms/epoch - 262us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  35.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1250 - val_loss: 0.0110\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0069 - val_loss: 0.0038\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0047 - val_loss: 0.0027\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0035 - val_loss: 0.0025\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0024 - val_loss: 0.0029\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 411us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0016 - 1s/epoch - 414us/step\n",
      "625/625 - 0s - loss: 9.1906e-04 - 217ms/epoch - 347us/step\n",
      "2500/2500 - 1s - loss: 9.1922e-04 - 660ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1172 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0062 - val_loss: 0.0039\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0038 - val_loss: 0.0030\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0032 - val_loss: 0.0046\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 411us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 416us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 411us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0039 - 1s/epoch - 449us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0014 - 1s/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 421us/step\n",
      "625/625 - 0s - loss: 0.0011 - 233ms/epoch - 373us/step\n",
      "2500/2500 - 1s - loss: 0.0011 - 702ms/epoch - 281us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1129 - val_loss: 0.0086\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0060 - val_loss: 0.0034\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0041 - val_loss: 0.0024\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0035 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0029 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0022 - val_loss: 0.0039\n",
      "Epoch 1/11\n",
      "3125/3125 - 1s - loss: 0.0028 - 1s/epoch - 447us/step\n",
      "Epoch 2/11\n",
      "3125/3125 - 1s - loss: 0.0024 - 1s/epoch - 452us/step\n",
      "Epoch 3/11\n",
      "3125/3125 - 1s - loss: 0.0020 - 1s/epoch - 444us/step\n",
      "Epoch 4/11\n",
      "3125/3125 - 1s - loss: 0.0027 - 1s/epoch - 450us/step\n",
      "Epoch 5/11\n",
      "3125/3125 - 1s - loss: 0.0022 - 1s/epoch - 447us/step\n",
      "Epoch 6/11\n",
      "3125/3125 - 1s - loss: 0.0018 - 1s/epoch - 468us/step\n",
      "Epoch 7/11\n",
      "3125/3125 - 1s - loss: 0.0019 - 1s/epoch - 451us/step\n",
      "Epoch 8/11\n",
      "3125/3125 - 1s - loss: 0.0026 - 1s/epoch - 448us/step\n",
      "Epoch 9/11\n",
      "3125/3125 - 1s - loss: 0.0018 - 1s/epoch - 446us/step\n",
      "Epoch 10/11\n",
      "3125/3125 - 1s - loss: 0.0017 - 1s/epoch - 451us/step\n",
      "Epoch 11/11\n",
      "3125/3125 - 1s - loss: 0.0018 - 1s/epoch - 446us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;,\n",
       "                    param_grid={&#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                                &#x27;optimizer&#x27;: [&#x27;sgd&#x27;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4070&gt;]},\n",
       "                    verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;,\n",
       "                    param_grid={&#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                                &#x27;optimizer&#x27;: [&#x27;sgd&#x27;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4070&gt;]},\n",
       "                    verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(error_score='raise',\n",
       "                    estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940>,\n",
       "                    param_grid={'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
       "                                'optimizer': ['sgd',\n",
       "                                              <keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>,\n",
       "                                              <keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>,\n",
       "                                              <keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>]},\n",
       "                    verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search selected hyperparameters\n",
    "\n",
    "# define base model function for grid search\n",
    "def create_model(activation='relu', optimizer='adam', kernel_regularizer='l2', batch_size=64):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'optimizer': ['sgd', Adam(learning_rate=0.001), Adam(learning_rate=0.01), Adam(learning_rate=0.1)],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform grid search hyperparameter tuning\n",
    "grid_search = HalvingGridSearchCV(estimator=keras_reg, param_grid=param_dist, cv=5, verbose=2, error_score='raise')\n",
    "grid_search.fit(Xtrain, ytrain, sample_weight=wtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>n_resources</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>16.997419</td>\n",
       "      <td>0.293825</td>\n",
       "      <td>0.079295</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>l1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': 'sgd'}</td>\n",
       "      <td>-0.014213</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012012</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.014210</td>\n",
       "      <td>-0.013634</td>\n",
       "      <td>-0.010464</td>\n",
       "      <td>-0.010404</td>\n",
       "      <td>-0.011347</td>\n",
       "      <td>-0.012012</td>\n",
       "      <td>0.001605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.082942</td>\n",
       "      <td>0.169986</td>\n",
       "      <td>0.081252</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.005693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005604</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.005692</td>\n",
       "      <td>-0.005540</td>\n",
       "      <td>-0.005397</td>\n",
       "      <td>-0.005991</td>\n",
       "      <td>-0.005397</td>\n",
       "      <td>-0.005604</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.774823</td>\n",
       "      <td>0.208586</td>\n",
       "      <td>0.081163</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.026682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027134</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.026685</td>\n",
       "      <td>-0.026955</td>\n",
       "      <td>-0.026410</td>\n",
       "      <td>-0.028737</td>\n",
       "      <td>-0.026886</td>\n",
       "      <td>-0.027135</td>\n",
       "      <td>0.000823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.576868</td>\n",
       "      <td>0.366474</td>\n",
       "      <td>0.079455</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.251134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249694</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.251033</td>\n",
       "      <td>-0.248961</td>\n",
       "      <td>-0.254116</td>\n",
       "      <td>-0.245601</td>\n",
       "      <td>-0.248622</td>\n",
       "      <td>-0.249666</td>\n",
       "      <td>0.002820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>17.627119</td>\n",
       "      <td>0.196109</td>\n",
       "      <td>0.079965</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': 'sgd'}</td>\n",
       "      <td>-0.002666</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002632</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.002665</td>\n",
       "      <td>-0.002537</td>\n",
       "      <td>-0.002637</td>\n",
       "      <td>-0.002655</td>\n",
       "      <td>-0.002661</td>\n",
       "      <td>-0.002631</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.685975</td>\n",
       "      <td>0.251114</td>\n",
       "      <td>0.078898</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001442</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.001248</td>\n",
       "      <td>-0.002222</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>-0.001109</td>\n",
       "      <td>-0.001138</td>\n",
       "      <td>-0.001421</td>\n",
       "      <td>0.000412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.925349</td>\n",
       "      <td>0.165671</td>\n",
       "      <td>0.079988</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003046</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>-0.003359</td>\n",
       "      <td>-0.001547</td>\n",
       "      <td>-0.007301</td>\n",
       "      <td>-0.001889</td>\n",
       "      <td>-0.003083</td>\n",
       "      <td>0.002226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.040764</td>\n",
       "      <td>0.241711</td>\n",
       "      <td>0.079797</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.005327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009157</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.005273</td>\n",
       "      <td>-0.005871</td>\n",
       "      <td>-0.005183</td>\n",
       "      <td>-0.012737</td>\n",
       "      <td>-0.015338</td>\n",
       "      <td>-0.008880</td>\n",
       "      <td>0.004297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>17.926032</td>\n",
       "      <td>0.387758</td>\n",
       "      <td>0.083417</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': '...</td>\n",
       "      <td>-0.011382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011686</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.011381</td>\n",
       "      <td>-0.012582</td>\n",
       "      <td>-0.011095</td>\n",
       "      <td>-0.011546</td>\n",
       "      <td>-0.011830</td>\n",
       "      <td>-0.011687</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.120462</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>0.083020</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': &lt;...</td>\n",
       "      <td>-0.007023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006413</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.007033</td>\n",
       "      <td>-0.006217</td>\n",
       "      <td>-0.005919</td>\n",
       "      <td>-0.006353</td>\n",
       "      <td>-0.006546</td>\n",
       "      <td>-0.006414</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.078545</td>\n",
       "      <td>0.151325</td>\n",
       "      <td>0.092128</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': &lt;...</td>\n",
       "      <td>-0.028657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028504</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.028676</td>\n",
       "      <td>-0.028378</td>\n",
       "      <td>-0.027875</td>\n",
       "      <td>-0.026800</td>\n",
       "      <td>-0.030723</td>\n",
       "      <td>-0.028491</td>\n",
       "      <td>0.001286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.761524</td>\n",
       "      <td>0.236672</td>\n",
       "      <td>0.089498</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': &lt;...</td>\n",
       "      <td>-0.293942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.290032</td>\n",
       "      <td>0.004074</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.293926</td>\n",
       "      <td>-0.294151</td>\n",
       "      <td>-0.285779</td>\n",
       "      <td>-0.284657</td>\n",
       "      <td>-0.291711</td>\n",
       "      <td>-0.290045</td>\n",
       "      <td>0.004048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>24.141023</td>\n",
       "      <td>0.367304</td>\n",
       "      <td>0.119679</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.004456</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005074</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.004456</td>\n",
       "      <td>-0.004653</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>-0.005607</td>\n",
       "      <td>-0.005352</td>\n",
       "      <td>-0.005073</td>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>23.382346</td>\n",
       "      <td>0.121729</td>\n",
       "      <td>0.117038</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.001846</td>\n",
       "      <td>-0.001345</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>-0.001696</td>\n",
       "      <td>-0.000983</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>21.985600</td>\n",
       "      <td>0.174433</td>\n",
       "      <td>0.121637</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': 'sgd'}</td>\n",
       "      <td>-0.002051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.002024</td>\n",
       "      <td>-0.002124</td>\n",
       "      <td>-0.002068</td>\n",
       "      <td>-0.002035</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>24.130400</td>\n",
       "      <td>0.423410</td>\n",
       "      <td>0.118057</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>-0.001089</td>\n",
       "      <td>-0.001441</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>99999</td>\n",
       "      <td>34.752902</td>\n",
       "      <td>0.121369</td>\n",
       "      <td>0.224412</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001110</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.001972</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>-0.000906</td>\n",
       "      <td>-0.001142</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>-0.001111</td>\n",
       "      <td>0.000452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>99999</td>\n",
       "      <td>34.617896</td>\n",
       "      <td>0.145296</td>\n",
       "      <td>0.233857</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001054</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>-0.000969</td>\n",
       "      <td>-0.001307</td>\n",
       "      <td>-0.000919</td>\n",
       "      <td>-0.001052</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
       "0      0        11111      16.997419      0.293825         0.079295   \n",
       "1      0        11111      19.082942      0.169986         0.081252   \n",
       "2      0        11111      19.774823      0.208586         0.081163   \n",
       "3      0        11111      19.576868      0.366474         0.079455   \n",
       "4      0        11111      17.627119      0.196109         0.079965   \n",
       "5      0        11111      19.685975      0.251114         0.078898   \n",
       "6      0        11111      19.925349      0.165671         0.079988   \n",
       "7      0        11111      20.040764      0.241711         0.079797   \n",
       "8      0        11111      17.926032      0.387758         0.083417   \n",
       "9      0        11111      20.120462      0.067826         0.083020   \n",
       "10     0        11111      20.078545      0.151325         0.092128   \n",
       "11     0        11111      20.761524      0.236672         0.089498   \n",
       "12     1        33333      24.141023      0.367304         0.119679   \n",
       "13     1        33333      23.382346      0.121729         0.117038   \n",
       "14     1        33333      21.985600      0.174433         0.121637   \n",
       "15     1        33333      24.130400      0.423410         0.118057   \n",
       "16     2        99999      34.752902      0.121369         0.224412   \n",
       "17     2        99999      34.617896      0.145296         0.233857   \n",
       "\n",
       "    std_score_time param_kernel_regularizer  \\\n",
       "0         0.001660                       l1   \n",
       "1         0.002900                       l1   \n",
       "2         0.002130                       l1   \n",
       "3         0.003233                       l1   \n",
       "4         0.000889                       l2   \n",
       "5         0.000797                       l2   \n",
       "6         0.003639                       l2   \n",
       "7         0.002502                       l2   \n",
       "8         0.002099                    l1_l2   \n",
       "9         0.001753                    l1_l2   \n",
       "10        0.008104                    l1_l2   \n",
       "11        0.005330                    l1_l2   \n",
       "12        0.002844                       l1   \n",
       "13        0.001417                       l2   \n",
       "14        0.002785                       l2   \n",
       "15        0.003221                       l2   \n",
       "16        0.002090                       l2   \n",
       "17        0.006886                       l2   \n",
       "\n",
       "                                      param_optimizer  \\\n",
       "0                                                 sgd   \n",
       "1   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "2   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "3   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "4                                                 sgd   \n",
       "5   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "6   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "7   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "8                                                 sgd   \n",
       "9   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "10  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "11  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "12  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "13  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "14                                                sgd   \n",
       "15  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "16  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "17  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "\n",
       "                                               params  split0_test_score  ...  \\\n",
       "0    {'kernel_regularizer': 'l1', 'optimizer': 'sgd'}          -0.014213  ...   \n",
       "1   {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.005693  ...   \n",
       "2   {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.026682  ...   \n",
       "3   {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.251134  ...   \n",
       "4    {'kernel_regularizer': 'l2', 'optimizer': 'sgd'}          -0.002666  ...   \n",
       "5   {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001247  ...   \n",
       "6   {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001318  ...   \n",
       "7   {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.005327  ...   \n",
       "8   {'kernel_regularizer': 'l1_l2', 'optimizer': '...          -0.011382  ...   \n",
       "9   {'kernel_regularizer': 'l1_l2', 'optimizer': <...          -0.007023  ...   \n",
       "10  {'kernel_regularizer': 'l1_l2', 'optimizer': <...          -0.028657  ...   \n",
       "11  {'kernel_regularizer': 'l1_l2', 'optimizer': <...          -0.293942  ...   \n",
       "12  {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.004456  ...   \n",
       "13  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001846  ...   \n",
       "14   {'kernel_regularizer': 'l2', 'optimizer': 'sgd'}          -0.002051  ...   \n",
       "15  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001426  ...   \n",
       "16  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001968  ...   \n",
       "17  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001019  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         -0.012012        0.001611               14           -0.014210   \n",
       "1         -0.005604        0.000223               10           -0.005692   \n",
       "2         -0.027134        0.000823               15           -0.026685   \n",
       "3         -0.249694        0.002871               17           -0.251033   \n",
       "4         -0.002632        0.000044                7           -0.002665   \n",
       "5         -0.001442        0.000451                5           -0.001248   \n",
       "6         -0.003046        0.002154                8           -0.001318   \n",
       "7         -0.009157        0.004497               12           -0.005273   \n",
       "8         -0.011686        0.000507               13           -0.011381   \n",
       "9         -0.006413        0.000367               11           -0.007033   \n",
       "10        -0.028504        0.001293               16           -0.028676   \n",
       "11        -0.290032        0.004074               18           -0.293926   \n",
       "12        -0.005074        0.000441                9           -0.004456   \n",
       "13        -0.001427        0.000309                4           -0.001846   \n",
       "14        -0.002061        0.000035                6           -0.002052   \n",
       "15        -0.001290        0.000134                3           -0.001427   \n",
       "16        -0.001110        0.000450                2           -0.001972   \n",
       "17        -0.001054        0.000135                1           -0.001019   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            -0.013634           -0.010464           -0.010404   \n",
       "1            -0.005540           -0.005397           -0.005991   \n",
       "2            -0.026955           -0.026410           -0.028737   \n",
       "3            -0.248961           -0.254116           -0.245601   \n",
       "4            -0.002537           -0.002637           -0.002655   \n",
       "5            -0.002222           -0.001386           -0.001109   \n",
       "6            -0.003359           -0.001547           -0.007301   \n",
       "7            -0.005871           -0.005183           -0.012737   \n",
       "8            -0.012582           -0.011095           -0.011546   \n",
       "9            -0.006217           -0.005919           -0.006353   \n",
       "10           -0.028378           -0.027875           -0.026800   \n",
       "11           -0.294151           -0.285779           -0.284657   \n",
       "12           -0.004653           -0.005298           -0.005607   \n",
       "13           -0.001345           -0.001265           -0.001696   \n",
       "14           -0.002024           -0.002124           -0.002068   \n",
       "15           -0.001292           -0.001089           -0.001441   \n",
       "16           -0.000745           -0.000906           -0.001142   \n",
       "17           -0.000969           -0.001307           -0.000919   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0            -0.011347         -0.012012         0.001605  \n",
       "1            -0.005397         -0.005604         0.000223  \n",
       "2            -0.026886         -0.027135         0.000823  \n",
       "3            -0.248622         -0.249666         0.002820  \n",
       "4            -0.002661         -0.002631         0.000048  \n",
       "5            -0.001138         -0.001421         0.000412  \n",
       "6            -0.001889         -0.003083         0.002226  \n",
       "7            -0.015338         -0.008880         0.004297  \n",
       "8            -0.011830         -0.011687         0.000507  \n",
       "9            -0.006546         -0.006414         0.000371  \n",
       "10           -0.030723         -0.028491         0.001286  \n",
       "11           -0.291711         -0.290045         0.004048  \n",
       "12           -0.005352         -0.005073         0.000441  \n",
       "13           -0.000983         -0.001427         0.000309  \n",
       "14           -0.002035         -0.002061         0.000035  \n",
       "15           -0.001202         -0.001290         0.000134  \n",
       "16           -0.000790         -0.001111         0.000452  \n",
       "17           -0.001052         -0.001053         0.000135  \n",
       "\n",
       "[18 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "display(grid_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'kernel_regularizer': 'l2', 'optimizer': <keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>}\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/377624069.py:23: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 33333\n",
      "max_resources_: 100000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 3\n",
      "n_resources: 33333\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1467 - val_loss: 0.0128\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0077 - val_loss: 0.0043\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0045 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0032 - val_loss: 0.0068\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0050 - 352ms/epoch - 423us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0026 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0021 - 349ms/epoch - 418us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0057 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0015 - 349ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0027 - 348ms/epoch - 417us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0041 - 365ms/epoch - 437us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0022 - 358ms/epoch - 429us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0024 - 362ms/epoch - 435us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0027 - 365ms/epoch - 438us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0027 - 385ms/epoch - 462us/step\n",
      "209/209 - 0s - loss: 0.0019 - 112ms/epoch - 536us/step\n",
      "834/834 - 0s - loss: 0.0019 - 239ms/epoch - 287us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1279 - val_loss: 0.0116\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0064 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0039 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0016 - val_loss: 0.0111\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0057 - 351ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0019 - 347ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0021 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0034 - 348ms/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0022 - 349ms/epoch - 418us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0035 - 347ms/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0016 - 364ms/epoch - 437us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0033 - 346ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0012 - 347ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0038 - 344ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0017 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0012 - 107ms/epoch - 514us/step\n",
      "834/834 - 0s - loss: 0.0012 - 227ms/epoch - 272us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1154 - val_loss: 0.0111\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0069 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0023 - val_loss: 0.0070\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0045 - 351ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0041 - 344ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0014 - 343ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0059 - 346ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0024 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0016 - 344ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0021 - 343ms/epoch - 412us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0048 - 345ms/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 344ms/epoch - 413us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0014 - 343ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0039 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0014 - 105ms/epoch - 501us/step\n",
      "834/834 - 0s - loss: 0.0014 - 229ms/epoch - 274us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1286 - val_loss: 0.0118\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0044 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0036 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0027 - val_loss: 0.0047\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0021 - val_loss: 0.0083\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0056 - 354ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0017 - 351ms/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0033 - 350ms/epoch - 420us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0025 - 350ms/epoch - 420us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0030 - 352ms/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 351ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0015 - 349ms/epoch - 419us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0022 - 351ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0031 - 354ms/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0024 - 366ms/epoch - 438us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0018 - 351ms/epoch - 421us/step\n",
      "209/209 - 0s - loss: 0.0016 - 105ms/epoch - 503us/step\n",
      "834/834 - 0s - loss: 0.0016 - 230ms/epoch - 275us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1365 - val_loss: 0.0104\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0073 - val_loss: 0.0057\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0047 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0035 - val_loss: 0.0024\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0028 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0025 - val_loss: 0.0052\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0058 - 350ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 359ms/epoch - 431us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0021 - 372ms/epoch - 446us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0021 - 357ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0025 - 380ms/epoch - 456us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0029 - 386ms/epoch - 463us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0036 - 350ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0019 - 351ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0022 - 344ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0053 - 344ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0021 - 346ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0015 - 104ms/epoch - 497us/step\n",
      "834/834 - 0s - loss: 0.0015 - 230ms/epoch - 276us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0291 - val_loss: 0.0171\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0088 - val_loss: 0.0028\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0060 - val_loss: 0.0453\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0071 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0034 - val_loss: 0.0015\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 536us/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0035 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0032 - val_loss: 0.0037\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0033 - val_loss: 0.0065\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0038 - 363ms/epoch - 436us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0048 - 344ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0039 - 342ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0033 - 357ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0052 - 356ms/epoch - 427us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0017 - 365ms/epoch - 438us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0039 - 345ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0042 - 350ms/epoch - 419us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0030 - 354ms/epoch - 425us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0075 - 377ms/epoch - 453us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0043 - 364ms/epoch - 437us/step\n",
      "209/209 - 0s - loss: 0.0014 - 104ms/epoch - 499us/step\n",
      "834/834 - 0s - loss: 0.0014 - 229ms/epoch - 274us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.0063\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0096 - val_loss: 0.0039\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 672us/step - loss: 0.0065 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0069 - val_loss: 0.0104\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0046 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0039 - val_loss: 0.0044\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 630us/step - loss: 0.0038 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0046 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0038 - 394ms/epoch - 473us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0036 - 363ms/epoch - 435us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0058 - 369ms/epoch - 443us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0045 - 373ms/epoch - 448us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0052 - 352ms/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0029 - 345ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 377ms/epoch - 452us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0132 - 371ms/epoch - 445us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0025 - 401ms/epoch - 481us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0039 - 377ms/epoch - 451us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0031 - 390ms/epoch - 468us/step\n",
      "209/209 - 0s - loss: 0.0019 - 114ms/epoch - 544us/step\n",
      "834/834 - 0s - loss: 0.0019 - 241ms/epoch - 289us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  24.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0366 - val_loss: 0.0508\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0094 - val_loss: 0.0137\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0075 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0063 - val_loss: 0.0029\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0047 - val_loss: 0.0027\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0050 - val_loss: 0.0020\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0035 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0020 - val_loss: 9.9140e-04\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0018 - val_loss: 9.0679e-04\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0032 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0037 - 346ms/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0030 - 344ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0021 - 343ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0093 - 343ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0012 - 343ms/epoch - 411us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0056 - 346ms/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0016 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0024 - 343ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0030 - 345ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 344ms/epoch - 413us/step\n",
      "209/209 - 0s - loss: 0.0011 - 105ms/epoch - 501us/step\n",
      "834/834 - 0s - loss: 0.0011 - 231ms/epoch - 277us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0290 - val_loss: 0.1035\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0100 - val_loss: 0.0038\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0069 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0053 - val_loss: 0.0099\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0053 - val_loss: 0.0302\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0044 - val_loss: 0.0149\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0047 - val_loss: 0.0197\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0047 - val_loss: 0.0016\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0045 - 351ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0015 - 348ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0045 - 344ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0046 - 346ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0019 - 347ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0061 - 346ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0022 - 374ms/epoch - 449us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0037 - 348ms/epoch - 417us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0063 - 346ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0034 - 346ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0015 - 345ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0017 - 103ms/epoch - 494us/step\n",
      "834/834 - 0s - loss: 0.0017 - 230ms/epoch - 275us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0285 - val_loss: 0.0054\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0108 - val_loss: 0.0376\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 671us/step - loss: 0.0060 - val_loss: 0.0108\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0046 - val_loss: 0.0156\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0066 - val_loss: 0.0025\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0031 - val_loss: 0.0055\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0054 - val_loss: 0.0026\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0020 - val_loss: 9.9628e-04\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0032 - val_loss: 9.7153e-04\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0111 - 350ms/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0017 - 344ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0028 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0058 - 345ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0039 - 358ms/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0041 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0028 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0029 - 346ms/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0029 - 346ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0026 - 347ms/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 345ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0010 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0010 - 228ms/epoch - 273us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0932 - val_loss: 0.0262\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0370 - val_loss: 0.0082\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0169 - val_loss: 0.0052\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0259 - val_loss: 0.0095\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0276 - val_loss: 0.0130\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0364 - val_loss: 0.0138\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0217 - val_loss: 0.0053\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0214 - val_loss: 0.0038\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0522 - val_loss: 0.0041\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0293 - val_loss: 0.0068\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0220 - val_loss: 0.0050\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0542 - 349ms/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.1068 - 345ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0710 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0542 - 344ms/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0290 - 344ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0725 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0229 - 346ms/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0408 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0608 - 347ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0762 - 346ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0452 - 343ms/epoch - 411us/step\n",
      "209/209 - 0s - loss: 0.0050 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0051 - 228ms/epoch - 273us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0937 - val_loss: 0.0294\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0345 - val_loss: 0.0095\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0197 - val_loss: 0.0057\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0283 - val_loss: 0.0257\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0376 - val_loss: 0.4935\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0198 - val_loss: 0.0038\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0351 - val_loss: 0.0040\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0269 - val_loss: 0.0036\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0196 - val_loss: 0.0424\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0198 - val_loss: 0.0033\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0276 - val_loss: 0.0040\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0755 - 354ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0967 - 351ms/epoch - 421us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0351 - 357ms/epoch - 428us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0412 - 348ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0716 - 350ms/epoch - 420us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0417 - 350ms/epoch - 420us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0276 - 349ms/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 1.5169 - 352ms/epoch - 422us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 1.5450 - 351ms/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 1.5481 - 351ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 1.5502 - 352ms/epoch - 422us/step\n",
      "209/209 - 0s - loss: 0.8013 - 106ms/epoch - 507us/step\n",
      "834/834 - 0s - loss: 0.8014 - 227ms/epoch - 272us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1096 - val_loss: 0.1054\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0358 - val_loss: 0.0166\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0253 - val_loss: 0.1548\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0269 - val_loss: 0.0124\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0430 - val_loss: 0.0049\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0157 - val_loss: 0.0038\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0221 - val_loss: 0.0041\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0215 - val_loss: 0.0080\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0313 - val_loss: 0.0058\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0159 - val_loss: 0.0161\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0169 - val_loss: 0.0043\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.1425 - 349ms/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0109 - 349ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0512 - 349ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0479 - 344ms/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.1069 - 342ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0236 - 345ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0566 - 343ms/epoch - 411us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0248 - 343ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0627 - 349ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0987 - 345ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0457 - 345ms/epoch - 413us/step\n",
      "209/209 - 0s - loss: 0.0037 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0038 - 230ms/epoch - 275us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0934 - val_loss: 0.1095\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0387 - val_loss: 0.0405\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0565 - val_loss: 0.0036\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0447 - val_loss: 0.0185\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0229 - val_loss: 0.0273\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0298 - val_loss: 0.0308\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0259 - val_loss: 0.0738\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0297 - val_loss: 0.0254\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0316 - val_loss: 0.0104\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0342 - val_loss: 0.0136\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0268 - val_loss: 0.0074\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.1429 - 345ms/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0496 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0444 - 346ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0441 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0174 - 345ms/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.1005 - 348ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0664 - 345ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0206 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0744 - 347ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0719 - 343ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0807 - 348ms/epoch - 417us/step\n",
      "209/209 - 0s - loss: 0.0127 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0127 - 228ms/epoch - 273us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0922 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0301 - val_loss: 0.0130\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0355 - val_loss: 0.0867\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0348 - val_loss: 0.0072\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0178 - val_loss: 0.3569\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0355 - val_loss: 0.0067\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0301 - val_loss: 0.0554\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0603 - val_loss: 0.0047\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0203 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0334 - val_loss: 0.0084\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0160 - val_loss: 0.0043\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0593 - 353ms/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0395 - 348ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0196 - 351ms/epoch - 421us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0470 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0435 - 360ms/epoch - 432us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0784 - 357ms/epoch - 428us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0150 - 347ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0642 - 348ms/epoch - 417us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0654 - 348ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0266 - 349ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.1292 - 348ms/epoch - 418us/step\n",
      "209/209 - 0s - loss: 0.0062 - 104ms/epoch - 499us/step\n",
      "834/834 - 0s - loss: 0.0062 - 231ms/epoch - 277us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.8s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1\n",
      "n_resources: 99999\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0356 - val_loss: 0.0091\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0111 - val_loss: 0.0086\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0097 - val_loss: 0.0058\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0040 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0042 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0060 - val_loss: 0.0036\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0029 - val_loss: 0.0058\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0036 - 1s/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0061 - 1s/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0044 - 1s/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0047 - 1s/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0062 - 1s/epoch - 416us/step\n",
      "625/625 - 0s - loss: 0.0013 - 213ms/epoch - 340us/step\n",
      "2500/2500 - 1s - loss: 0.0013 - 664ms/epoch - 265us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.0074\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0107 - val_loss: 0.0040\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0084 - val_loss: 0.0031\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0057 - val_loss: 0.0024\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0045 - val_loss: 0.0025\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0045 - val_loss: 0.0027\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0029 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0040 - val_loss: 0.0073\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0027 - val_loss: 0.0109\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0038 - 1s/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 417us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 421us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 422us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 0.0015 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 0.0015 - 663ms/epoch - 265us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0366 - val_loss: 0.0047\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0097 - val_loss: 0.0032\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0084 - val_loss: 0.0027\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0090 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0039 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0036 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0027 - val_loss: 0.0011\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0039 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0028 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0024 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0055 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 424us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0036 - 1s/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0043 - 1s/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 419us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0035 - 1s/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 0.0011 - 207ms/epoch - 331us/step\n",
      "2500/2500 - 1s - loss: 0.0011 - 655ms/epoch - 262us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0361 - val_loss: 0.0061\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0099 - val_loss: 0.0056\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0063 - val_loss: 0.0025\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0066 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0044 - val_loss: 0.0015\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0058 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0039 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0041 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0042 - val_loss: 0.0014\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0058 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 420us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0061 - 1s/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0075 - 1s/epoch - 422us/step\n",
      "625/625 - 0s - loss: 0.0017 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 0.0017 - 660ms/epoch - 264us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0358 - val_loss: 0.0076\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0101 - val_loss: 0.0045\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0153 - val_loss: 0.0080\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0067 - val_loss: 0.0284\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0046 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0063 - val_loss: 0.0019\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0037 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0049 - val_loss: 0.0017\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0020 - val_loss: 9.4648e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0060 - 1s/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0039 - 1s/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 415us/step\n",
      "625/625 - 0s - loss: 7.8604e-04 - 210ms/epoch - 336us/step\n",
      "2500/2500 - 1s - loss: 7.8599e-04 - 666ms/epoch - 266us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0344 - val_loss: 0.0090\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0090 - val_loss: 0.0036\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0079 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0074 - val_loss: 0.0085\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0047 - val_loss: 0.0074\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0041 - val_loss: 0.0013\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0024 - val_loss: 0.0499\n",
      "Epoch 1/11\n",
      "3125/3125 - 1s - loss: 0.0041 - 1s/epoch - 450us/step\n",
      "Epoch 2/11\n",
      "3125/3125 - 1s - loss: 0.0030 - 1s/epoch - 449us/step\n",
      "Epoch 3/11\n",
      "3125/3125 - 1s - loss: 0.0024 - 1s/epoch - 454us/step\n",
      "Epoch 4/11\n",
      "3125/3125 - 1s - loss: 0.0031 - 1s/epoch - 445us/step\n",
      "Epoch 5/11\n",
      "3125/3125 - 1s - loss: 0.0030 - 1s/epoch - 452us/step\n",
      "Epoch 6/11\n",
      "3125/3125 - 1s - loss: 0.0025 - 1s/epoch - 447us/step\n",
      "Epoch 7/11\n",
      "3125/3125 - 1s - loss: 0.0030 - 1s/epoch - 452us/step\n",
      "Epoch 8/11\n",
      "3125/3125 - 1s - loss: 0.0022 - 1s/epoch - 447us/step\n",
      "Epoch 9/11\n",
      "3125/3125 - 1s - loss: 0.0035 - 1s/epoch - 450us/step\n",
      "Epoch 10/11\n",
      "3125/3125 - 1s - loss: 0.0032 - 1s/epoch - 448us/step\n",
      "Epoch 11/11\n",
      "3125/3125 - 1s - loss: 0.0019 - 1s/epoch - 451us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;,\n",
       "                    param_grid={&#x27;learning_rate&#x27;: [0.001, 0.01, 0.1]},\n",
       "                    verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;,\n",
       "                    param_grid={&#x27;learning_rate&#x27;: [0.001, 0.01, 0.1]},\n",
       "                    verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(error_score='raise',\n",
       "                    estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20>,\n",
       "                    param_grid={'learning_rate': [0.001, 0.01, 0.1]},\n",
       "                    verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search learning rates\n",
    "\n",
    "# define base model function for grid search\n",
    "def create_model(activation='relu', learning_rate=0.001, kernel_regularizer='l2', batch_size=64):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform grid search hyperparameter tuning\n",
    "grid_search = HalvingGridSearchCV(estimator=keras_reg, param_grid=param_dist, cv=5, verbose=2, error_score='raise')\n",
    "grid_search.fit(Xtrain, ytrain, sample_weight=wtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0052 - val_loss: 0.0031\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 629us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 7.4837e-04 - val_loss: 2.9767e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 7.7995e-04 - val_loss: 4.0676e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 6.9497e-04 - val_loss: 0.0013\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 4.3383e-04 - val_loss: 2.5998e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 4.0091e-04 - val_loss: 2.9744e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 4.1961e-04 - val_loss: 2.0497e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 4.5444e-04 - val_loss: 6.9645e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 2.9007e-04 - val_loss: 2.3507e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 3.3412e-04 - val_loss: 4.4008e-04\n"
     ]
    }
   ],
   "source": [
    "# evaluate best model\n",
    "best_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "    Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "best_model.compile(optimizer=Adam(learning_rate=0.01), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = best_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=64, validation_data=(Xval, yval, wval))\n",
    "\n",
    "# save the best model\n",
    "best_model.save(\"models/best_dnn_model_raw_pd.h5\")\n",
    "\n",
    "# save model history in models\n",
    "with open('models/best_dnn_model_raw_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Huber Loss: 0.00033411901677027345\n",
      "Best Validation Huber Loss: 0.0004400824836920947\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_train_hl = best_model.history.history[\"loss\"][-1]\n",
    "best_val_hl = best_model.history.history[\"val_loss\"][-1]\n",
    "print(f\"Best Training Huber Loss: {best_train_hl}\")\n",
    "print(f\"Best Validation Huber Loss: {best_val_hl}\")\n",
    "\n",
    "plt.plot(best_model.history.history['loss'], label='Training Huber Loss')\n",
    "plt.plot(best_model.history.history['val_loss'], label='Validation Huber Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed PD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steering_angle</th>\n",
       "      <th>distance_to_road_center</th>\n",
       "      <th>angle_from_straight_in_rads</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.411134</td>\n",
       "      <td>-3.017721</td>\n",
       "      <td>-1.561374</td>\n",
       "      <td>-2.491427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.411134</td>\n",
       "      <td>-3.017721</td>\n",
       "      <td>-1.561374</td>\n",
       "      <td>-2.491427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.411134</td>\n",
       "      <td>-3.017721</td>\n",
       "      <td>-1.561374</td>\n",
       "      <td>-2.491427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.411134</td>\n",
       "      <td>-3.017721</td>\n",
       "      <td>-1.561374</td>\n",
       "      <td>-2.491427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.411134</td>\n",
       "      <td>-3.017721</td>\n",
       "      <td>-1.561374</td>\n",
       "      <td>-2.493814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106692</th>\n",
       "      <td>1.952193</td>\n",
       "      <td>-0.279947</td>\n",
       "      <td>3.417991</td>\n",
       "      <td>-0.491446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106693</th>\n",
       "      <td>1.990733</td>\n",
       "      <td>-0.220655</td>\n",
       "      <td>3.403938</td>\n",
       "      <td>-1.018409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106694</th>\n",
       "      <td>2.028543</td>\n",
       "      <td>-0.161769</td>\n",
       "      <td>3.389249</td>\n",
       "      <td>-1.523784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106695</th>\n",
       "      <td>2.046814</td>\n",
       "      <td>-0.103238</td>\n",
       "      <td>3.344323</td>\n",
       "      <td>-2.048647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106696</th>\n",
       "      <td>2.083103</td>\n",
       "      <td>-0.045002</td>\n",
       "      <td>3.328064</td>\n",
       "      <td>-2.722282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106697 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        steering_angle  distance_to_road_center  angle_from_straight_in_rads  \\\n",
       "0            -3.411134                -3.017721                    -1.561374   \n",
       "1            -3.411134                -3.017721                    -1.561374   \n",
       "2            -3.411134                -3.017721                    -1.561374   \n",
       "3            -3.411134                -3.017721                    -1.561374   \n",
       "4            -3.411134                -3.017721                    -1.561374   \n",
       "...                ...                      ...                          ...   \n",
       "106692        1.952193                -0.279947                     3.417991   \n",
       "106693        1.990733                -0.220655                     3.403938   \n",
       "106694        2.028543                -0.161769                     3.389249   \n",
       "106695        2.046814                -0.103238                     3.344323   \n",
       "106696        2.083103                -0.045002                     3.328064   \n",
       "\n",
       "          reward  \n",
       "0      -2.491427  \n",
       "1      -2.491427  \n",
       "2      -2.491427  \n",
       "3      -2.491427  \n",
       "4      -2.493814  \n",
       "...          ...  \n",
       "106692 -0.491446  \n",
       "106693 -1.018409  \n",
       "106694 -1.523784  \n",
       "106695 -2.048647  \n",
       "106696 -2.722282  \n",
       "\n",
       "[106697 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_8512/1201342029.py:15: RuntimeWarning: divide by zero encountered in divide\n",
      "  weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n"
     ]
    }
   ],
   "source": [
    "# import data from csv to dataframe\n",
    "filename = \"processed_data_pd.csv\"\n",
    "df = pd.read_csv(f\"train_data/{filename}\")\n",
    "display(df)\n",
    "\n",
    "# split into input and target features\n",
    "X = df[['distance_to_road_center', 'angle_from_straight_in_rads']].values\n",
    "y = df['steering_angle'].values\n",
    "r = df['reward'].values\n",
    "\n",
    "# normalize rewards to [0, 1] range\n",
    "norm_r = (r - np.min(r)) / (np.max(r) - np.min(r))\n",
    "\n",
    "# calculate weights based on normalized rewards\n",
    "weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n",
    "\n",
    "# split into training and validation sets\n",
    "Xtrain, Xval, ytrain, yval, wtrain, wval = train_test_split(X, y, weights, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0054 - val_loss: 6.8445e-06\n",
      "Epoch 2/100\n",
      "2668/2668 [==============================] - 2s 601us/step - loss: 5.1792e-05 - val_loss: 3.2254e-05\n",
      "Epoch 3/100\n",
      "2668/2668 [==============================] - 2s 585us/step - loss: 3.4715e-05 - val_loss: 3.9242e-06\n",
      "Epoch 4/100\n",
      "2668/2668 [==============================] - 2s 584us/step - loss: 4.0076e-05 - val_loss: 6.3188e-06\n",
      "Epoch 5/100\n",
      "2668/2668 [==============================] - 2s 582us/step - loss: 2.8702e-05 - val_loss: 2.9887e-05\n",
      "Epoch 6/100\n",
      "2668/2668 [==============================] - 2s 588us/step - loss: 2.9418e-05 - val_loss: 6.9034e-06\n",
      "Epoch 7/100\n",
      "2668/2668 [==============================] - 2s 611us/step - loss: 2.1019e-05 - val_loss: 3.1088e-06\n",
      "Epoch 8/100\n",
      "2668/2668 [==============================] - 2s 589us/step - loss: 1.7690e-05 - val_loss: 2.2898e-06\n",
      "Epoch 9/100\n",
      "2668/2668 [==============================] - 2s 582us/step - loss: 1.2861e-05 - val_loss: 1.2565e-06\n",
      "Epoch 10/100\n",
      "2668/2668 [==============================] - 2s 591us/step - loss: 1.8398e-05 - val_loss: 3.5383e-06\n",
      "Epoch 11/100\n",
      "2668/2668 [==============================] - 2s 613us/step - loss: 1.0425e-05 - val_loss: 4.1321e-07\n",
      "Epoch 12/100\n",
      "2668/2668 [==============================] - 2s 630us/step - loss: 1.3102e-05 - val_loss: 2.6865e-04\n",
      "Epoch 13/100\n",
      "2668/2668 [==============================] - 2s 580us/step - loss: 5.4480e-06 - val_loss: 1.4632e-06\n",
      "Epoch 14/100\n",
      "2668/2668 [==============================] - 2s 588us/step - loss: 9.2396e-06 - val_loss: 4.2791e-07\n",
      "Epoch 15/100\n",
      "2668/2668 [==============================] - 2s 579us/step - loss: 5.7695e-06 - val_loss: 2.1163e-06\n",
      "Epoch 16/100\n",
      "2668/2668 [==============================] - 2s 604us/step - loss: 6.1781e-06 - val_loss: 2.4157e-07\n",
      "Epoch 17/100\n",
      "2668/2668 [==============================] - 2s 585us/step - loss: 5.2975e-06 - val_loss: 3.8977e-07\n",
      "Epoch 18/100\n",
      "2668/2668 [==============================] - 2s 586us/step - loss: 6.6762e-06 - val_loss: 8.3012e-06\n",
      "Epoch 19/100\n",
      "2668/2668 [==============================] - 2s 580us/step - loss: 5.1068e-06 - val_loss: 3.9285e-06\n",
      "Epoch 20/100\n",
      "2668/2668 [==============================] - 2s 579us/step - loss: 3.3371e-06 - val_loss: 1.3601e-06\n",
      "Epoch 21/100\n",
      "2668/2668 [==============================] - 2s 588us/step - loss: 4.7545e-06 - val_loss: 3.1340e-06\n"
     ]
    }
   ],
   "source": [
    "# find optimal number of epochs using early stopping\n",
    "\n",
    "# define base model with default Huber loss delta = 1.0\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for steering angle prediction\n",
    "])\n",
    "\n",
    "model.compile(loss=Huber(delta=1.0), optimizer=Adam(learning_rate=0.001), weighted_metrics=[])\n",
    "\n",
    "# define early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=100, batch_size=32, validation_data = (Xval, yval, wval), callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0027 - mae: 0.0117 - mse: 0.0057 - val_loss: 2.8208e-06 - val_mae: 0.0014 - val_mse: 4.5744e-06\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 627us/step - loss: 3.0040e-05 - mae: 0.0038 - mse: 4.2629e-05 - val_loss: 4.6602e-05 - val_mae: 0.0069 - val_mse: 6.5786e-05\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 631us/step - loss: 3.2924e-05 - mae: 0.0041 - mse: 4.6376e-05 - val_loss: 1.7812e-05 - val_mae: 0.0046 - val_mse: 2.5883e-05\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 655us/step - loss: 2.3937e-05 - mae: 0.0034 - mse: 3.3742e-05 - val_loss: 2.3363e-06 - val_mae: 0.0014 - val_mse: 3.9998e-06\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 663us/step - loss: 1.8774e-05 - mae: 0.0028 - mse: 2.6222e-05 - val_loss: 1.2047e-05 - val_mae: 0.0031 - val_mse: 1.5800e-05\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 638us/step - loss: 1.6471e-05 - mae: 0.0028 - mse: 2.3620e-05 - val_loss: 1.3409e-05 - val_mae: 0.0032 - val_mse: 1.7764e-05\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 656us/step - loss: 2.0460e-05 - mae: 0.0031 - mse: 2.9635e-05 - val_loss: 1.2763e-06 - val_mae: 0.0012 - val_mse: 2.1846e-06\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 630us/step - loss: 1.3774e-05 - mae: 0.0023 - mse: 1.9928e-05 - val_loss: 4.7001e-06 - val_mae: 0.0016 - val_mse: 6.1466e-06\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 633us/step - loss: 1.3275e-05 - mae: 0.0025 - mse: 2.0252e-05 - val_loss: 3.4239e-05 - val_mae: 0.0044 - val_mse: 4.2827e-05\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 649us/step - loss: 1.1220e-05 - mae: 0.0020 - mse: 1.7590e-05 - val_loss: 3.3358e-06 - val_mae: 0.0014 - val_mse: 4.1829e-06\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 629us/step - loss: 1.6124e-05 - mae: 0.0022 - mse: 2.5809e-05 - val_loss: 4.3798e-07 - val_mae: 6.3035e-04 - val_mse: 8.0228e-07\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 635us/step - loss: 6.5130e-06 - mae: 0.0015 - mse: 9.7292e-06 - val_loss: 5.2238e-07 - val_mae: 8.4274e-04 - val_mse: 9.9641e-07\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 630us/step - loss: 8.7630e-06 - mae: 0.0020 - mse: 1.3084e-05 - val_loss: 4.7048e-07 - val_mae: 8.0365e-04 - val_mse: 9.6175e-07\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 634us/step - loss: 6.7446e-06 - mae: 0.0017 - mse: 1.0301e-05 - val_loss: 3.2971e-06 - val_mae: 0.0017 - val_mse: 4.8294e-06\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 633us/step - loss: 9.6515e-06 - mae: 0.0015 - mse: 1.4028e-05 - val_loss: 2.0407e-05 - val_mae: 0.0045 - val_mse: 2.8702e-05\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 642us/step - loss: 7.4903e-06 - mae: 0.0016 - mse: 1.0780e-05 - val_loss: 3.1025e-06 - val_mae: 0.0016 - val_mse: 4.1071e-06\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 629us/step - loss: 5.4217e-06 - mae: 0.0015 - mse: 8.3099e-06 - val_loss: 2.6478e-06 - val_mae: 0.0011 - val_mse: 3.1184e-06\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 637us/step - loss: 1.7589e-05 - mae: 0.0017 - mse: 2.8229e-05 - val_loss: 4.3354e-07 - val_mae: 5.9685e-04 - val_mse: 6.1828e-07\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 631us/step - loss: 2.8009e-06 - mae: 9.3428e-04 - mse: 3.8929e-06 - val_loss: 4.1492e-06 - val_mae: 0.0018 - val_mse: 5.3796e-06\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 639us/step - loss: 7.7129e-06 - mae: 0.0016 - mse: 1.2082e-05 - val_loss: 4.4488e-07 - val_mae: 7.0695e-04 - val_mse: 6.8597e-07\n",
      "Iteration 1: delta = 0.5, val_mae = 0.0007069535786285996, val_mse = 6.859689847260597e-07\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0040 - mae: 0.0135 - mse: 0.0082 - val_loss: 3.6268e-06 - val_mae: 0.0017 - val_mse: 5.8397e-06\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 628us/step - loss: 4.1692e-05 - mae: 0.0041 - mse: 5.8673e-05 - val_loss: 1.8881e-05 - val_mae: 0.0037 - val_mse: 2.8784e-05\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 627us/step - loss: 4.0257e-05 - mae: 0.0042 - mse: 5.7001e-05 - val_loss: 2.0673e-05 - val_mae: 0.0045 - val_mse: 2.8730e-05\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 644us/step - loss: 3.6816e-05 - mae: 0.0038 - mse: 5.0765e-05 - val_loss: 1.4574e-05 - val_mae: 0.0039 - val_mse: 2.1791e-05\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 623us/step - loss: 2.2579e-05 - mae: 0.0034 - mse: 3.1707e-05 - val_loss: 2.3290e-04 - val_mae: 0.0115 - val_mse: 3.0103e-04\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 681us/step - loss: 2.5523e-05 - mae: 0.0033 - mse: 3.5813e-05 - val_loss: 2.8535e-06 - val_mae: 0.0017 - val_mse: 4.5730e-06\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 675us/step - loss: 2.2012e-05 - mae: 0.0030 - mse: 3.0783e-05 - val_loss: 1.0221e-05 - val_mae: 0.0025 - val_mse: 1.2461e-05\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 650us/step - loss: 1.9409e-05 - mae: 0.0029 - mse: 2.8323e-05 - val_loss: 3.4044e-05 - val_mae: 0.0046 - val_mse: 4.6811e-05\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 615us/step - loss: 1.9723e-05 - mae: 0.0024 - mse: 2.7314e-05 - val_loss: 1.7715e-06 - val_mae: 0.0013 - val_mse: 2.6395e-06\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 656us/step - loss: 1.5324e-05 - mae: 0.0022 - mse: 2.4813e-05 - val_loss: 1.5475e-06 - val_mae: 0.0010 - val_mse: 2.1940e-06\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 699us/step - loss: 1.2080e-05 - mae: 0.0020 - mse: 1.8225e-05 - val_loss: 5.3568e-07 - val_mae: 7.7094e-04 - val_mse: 9.1540e-07\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 691us/step - loss: 8.6148e-06 - mae: 0.0019 - mse: 1.3543e-05 - val_loss: 1.9474e-06 - val_mae: 0.0011 - val_mse: 2.4555e-06\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 652us/step - loss: 9.8638e-06 - mae: 0.0019 - mse: 1.4993e-05 - val_loss: 4.4174e-05 - val_mae: 0.0071 - val_mse: 7.2211e-05\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 660us/step - loss: 8.5429e-06 - mae: 0.0018 - mse: 1.3469e-05 - val_loss: 1.9065e-07 - val_mae: 4.3223e-04 - val_mse: 3.1473e-07\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 672us/step - loss: 1.2728e-05 - mae: 0.0015 - mse: 2.0384e-05 - val_loss: 1.2182e-06 - val_mae: 0.0011 - val_mse: 1.9787e-06\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 744us/step - loss: 5.1444e-06 - mae: 0.0013 - mse: 7.8650e-06 - val_loss: 9.5350e-07 - val_mae: 9.0453e-04 - val_mse: 1.4627e-06\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 714us/step - loss: 7.6973e-06 - mae: 0.0015 - mse: 1.2210e-05 - val_loss: 1.6403e-06 - val_mae: 9.6029e-04 - val_mse: 1.9731e-06\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 626us/step - loss: 8.9622e-06 - mae: 0.0018 - mse: 1.4302e-05 - val_loss: 1.8066e-07 - val_mae: 3.7538e-04 - val_mse: 3.1632e-07\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 645us/step - loss: 3.4846e-06 - mae: 0.0011 - mse: 5.1333e-06 - val_loss: 1.1713e-07 - val_mae: 3.0635e-04 - val_mse: 1.9807e-07\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 668us/step - loss: 5.2285e-06 - mae: 0.0014 - mse: 7.9734e-06 - val_loss: 1.1864e-06 - val_mae: 9.9979e-04 - val_mse: 2.0059e-06\n",
      "Iteration 2: delta = 0.6, val_mae = 0.0009997915476560593, val_mse = 2.0059496819158085e-06\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0047 - mae: 0.0150 - mse: 0.0083 - val_loss: 7.7995e-06 - val_mae: 0.0029 - val_mse: 1.3665e-05\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 662us/step - loss: 2.7776e-05 - mae: 0.0039 - mse: 3.9141e-05 - val_loss: 1.2171e-05 - val_mae: 0.0025 - val_mse: 1.6398e-05\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 630us/step - loss: 4.7035e-05 - mae: 0.0043 - mse: 6.7665e-05 - val_loss: 4.8355e-06 - val_mae: 0.0020 - val_mse: 6.8592e-06\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 637us/step - loss: 2.0456e-05 - mae: 0.0033 - mse: 2.8948e-05 - val_loss: 2.5820e-06 - val_mae: 0.0016 - val_mse: 4.4692e-06\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 678us/step - loss: 2.2689e-05 - mae: 0.0032 - mse: 3.2596e-05 - val_loss: 3.3606e-06 - val_mae: 0.0019 - val_mse: 5.3889e-06\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 631us/step - loss: 1.8398e-05 - mae: 0.0030 - mse: 2.5976e-05 - val_loss: 1.4572e-05 - val_mae: 0.0035 - val_mse: 1.9836e-05\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 639us/step - loss: 2.2016e-05 - mae: 0.0030 - mse: 3.1049e-05 - val_loss: 3.3447e-05 - val_mae: 0.0058 - val_mse: 4.7543e-05\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 655us/step - loss: 1.8421e-05 - mae: 0.0024 - mse: 2.6257e-05 - val_loss: 3.8449e-04 - val_mae: 0.0226 - val_mse: 7.1126e-04\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 646us/step - loss: 2.4709e-05 - mae: 0.0026 - mse: 3.5296e-05 - val_loss: 4.5793e-06 - val_mae: 0.0020 - val_mse: 8.1050e-06\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 632us/step - loss: 7.6567e-06 - mae: 0.0018 - mse: 1.1969e-05 - val_loss: 2.3800e-06 - val_mae: 0.0012 - val_mse: 3.9176e-06\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 767us/step - loss: 8.8308e-06 - mae: 0.0018 - mse: 1.3112e-05 - val_loss: 1.8766e-06 - val_mae: 0.0015 - val_mse: 3.1630e-06\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 660us/step - loss: 9.6928e-06 - mae: 0.0023 - mse: 1.4227e-05 - val_loss: 1.6976e-05 - val_mae: 0.0037 - val_mse: 2.2945e-05\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 648us/step - loss: 9.4637e-06 - mae: 0.0020 - mse: 1.4462e-05 - val_loss: 2.0907e-06 - val_mae: 0.0011 - val_mse: 3.0547e-06\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 682us/step - loss: 4.9051e-06 - mae: 0.0016 - mse: 6.8797e-06 - val_loss: 7.7860e-07 - val_mae: 7.4007e-04 - val_mse: 1.1718e-06\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 654us/step - loss: 7.8956e-06 - mae: 0.0016 - mse: 1.1252e-05 - val_loss: 3.0728e-07 - val_mae: 5.9721e-04 - val_mse: 6.4368e-07\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 652us/step - loss: 4.4292e-06 - mae: 0.0014 - mse: 6.8116e-06 - val_loss: 1.1785e-06 - val_mae: 0.0011 - val_mse: 1.9357e-06\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 646us/step - loss: 8.8392e-06 - mae: 0.0018 - mse: 1.4454e-05 - val_loss: 1.2549e-06 - val_mae: 8.8628e-04 - val_mse: 1.7504e-06\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 642us/step - loss: 4.1065e-06 - mae: 0.0012 - mse: 6.1138e-06 - val_loss: 1.4618e-05 - val_mae: 0.0039 - val_mse: 2.0169e-05\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 717us/step - loss: 6.0470e-06 - mae: 0.0013 - mse: 9.2112e-06 - val_loss: 3.2191e-07 - val_mae: 4.0966e-04 - val_mse: 4.8442e-07\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 627us/step - loss: 1.3071e-05 - mae: 0.0013 - mse: 2.0439e-05 - val_loss: 1.0904e-07 - val_mae: 3.0546e-04 - val_mse: 2.0723e-07\n",
      "Iteration 3: delta = 0.7, val_mae = 0.0003054589033126831, val_mse = 2.0723039995118597e-07\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0055 - mae: 0.0168 - mse: 0.0097 - val_loss: 8.5462e-06 - val_mae: 0.0024 - val_mse: 1.3224e-05\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 632us/step - loss: 2.7545e-05 - mae: 0.0034 - mse: 3.9043e-05 - val_loss: 2.8235e-05 - val_mae: 0.0053 - val_mse: 4.5890e-05\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 649us/step - loss: 3.1454e-05 - mae: 0.0043 - mse: 4.3903e-05 - val_loss: 2.4860e-06 - val_mae: 0.0014 - val_mse: 3.8368e-06\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 669us/step - loss: 2.7409e-05 - mae: 0.0037 - mse: 3.8429e-05 - val_loss: 7.3833e-06 - val_mae: 0.0020 - val_mse: 9.7182e-06\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 636us/step - loss: 2.0795e-05 - mae: 0.0031 - mse: 2.9326e-05 - val_loss: 3.6946e-06 - val_mae: 0.0018 - val_mse: 6.7980e-06\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 625us/step - loss: 2.2549e-05 - mae: 0.0032 - mse: 3.1865e-05 - val_loss: 3.0016e-06 - val_mae: 0.0018 - val_mse: 5.0515e-06\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 641us/step - loss: 3.0034e-05 - mae: 0.0034 - mse: 4.4240e-05 - val_loss: 1.2870e-06 - val_mae: 9.8441e-04 - val_mse: 2.1095e-06\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 638us/step - loss: 1.6571e-05 - mae: 0.0027 - mse: 2.4569e-05 - val_loss: 4.9057e-06 - val_mae: 0.0019 - val_mse: 6.7245e-06\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 631us/step - loss: 1.5140e-05 - mae: 0.0025 - mse: 2.2263e-05 - val_loss: 6.4415e-05 - val_mae: 0.0083 - val_mse: 9.7159e-05\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 649us/step - loss: 1.2473e-05 - mae: 0.0023 - mse: 1.7839e-05 - val_loss: 7.8828e-07 - val_mae: 7.7654e-04 - val_mse: 1.2010e-06\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 645us/step - loss: 1.1564e-05 - mae: 0.0024 - mse: 1.7416e-05 - val_loss: 7.1368e-06 - val_mae: 0.0030 - val_mse: 1.3162e-05\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 635us/step - loss: 7.8848e-06 - mae: 0.0018 - mse: 1.1296e-05 - val_loss: 4.5563e-07 - val_mae: 6.1941e-04 - val_mse: 6.8416e-07\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 639us/step - loss: 1.2791e-05 - mae: 0.0021 - mse: 1.8935e-05 - val_loss: 9.4881e-07 - val_mae: 7.8533e-04 - val_mse: 1.2777e-06\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 677us/step - loss: 1.0395e-05 - mae: 0.0019 - mse: 1.5446e-05 - val_loss: 8.5683e-07 - val_mae: 7.9193e-04 - val_mse: 1.2004e-06\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 669us/step - loss: 7.5616e-06 - mae: 0.0017 - mse: 1.1126e-05 - val_loss: 4.3122e-07 - val_mae: 5.7106e-04 - val_mse: 6.9876e-07\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 646us/step - loss: 7.6267e-06 - mae: 0.0019 - mse: 1.1369e-05 - val_loss: 1.3194e-06 - val_mae: 8.2563e-04 - val_mse: 1.9073e-06\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 660us/step - loss: 7.1691e-06 - mae: 0.0016 - mse: 1.0220e-05 - val_loss: 2.9610e-06 - val_mae: 0.0017 - val_mse: 3.9847e-06\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 632us/step - loss: 7.4708e-06 - mae: 0.0019 - mse: 1.1481e-05 - val_loss: 3.4753e-07 - val_mae: 5.2099e-04 - val_mse: 5.4871e-07\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 631us/step - loss: 7.8327e-06 - mae: 0.0017 - mse: 1.2347e-05 - val_loss: 1.0593e-04 - val_mae: 0.0082 - val_mse: 1.6802e-04\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 646us/step - loss: 1.1879e-05 - mae: 0.0018 - mse: 1.9223e-05 - val_loss: 5.3149e-07 - val_mae: 6.1942e-04 - val_mse: 7.7252e-07\n",
      "Iteration 4: delta = 0.7999999999999999, val_mae = 0.0006194203742779791, val_mse = 7.725198543084844e-07\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0094 - mae: 0.0233 - mse: 0.0170 - val_loss: 9.1621e-06 - val_mae: 0.0026 - val_mse: 1.4834e-05\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 631us/step - loss: 1.7295e-05 - mae: 0.0033 - mse: 2.4589e-05 - val_loss: 2.2366e-06 - val_mae: 0.0013 - val_mse: 3.7334e-06\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 639us/step - loss: 3.1780e-05 - mae: 0.0040 - mse: 4.4280e-05 - val_loss: 2.1543e-05 - val_mae: 0.0054 - val_mse: 3.8403e-05\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 634us/step - loss: 2.3967e-05 - mae: 0.0035 - mse: 3.4228e-05 - val_loss: 9.2511e-06 - val_mae: 0.0025 - val_mse: 1.2413e-05\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 625us/step - loss: 1.8222e-05 - mae: 0.0032 - mse: 2.5774e-05 - val_loss: 2.9797e-06 - val_mae: 0.0018 - val_mse: 5.0607e-06\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 627us/step - loss: 1.7921e-05 - mae: 0.0031 - mse: 2.5601e-05 - val_loss: 4.3217e-06 - val_mae: 0.0016 - val_mse: 5.5612e-06\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 629us/step - loss: 1.7527e-05 - mae: 0.0029 - mse: 2.5132e-05 - val_loss: 2.7767e-06 - val_mae: 0.0014 - val_mse: 4.2126e-06\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 635us/step - loss: 1.7112e-05 - mae: 0.0027 - mse: 2.5502e-05 - val_loss: 2.3195e-06 - val_mae: 0.0013 - val_mse: 3.2583e-06\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 631us/step - loss: 1.5373e-05 - mae: 0.0026 - mse: 2.1828e-05 - val_loss: 1.0478e-05 - val_mae: 0.0030 - val_mse: 1.5804e-05\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 618us/step - loss: 9.9594e-06 - mae: 0.0022 - mse: 1.4507e-05 - val_loss: 8.1346e-06 - val_mae: 0.0021 - val_mse: 1.1443e-05\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 622us/step - loss: 1.2516e-05 - mae: 0.0024 - mse: 1.8615e-05 - val_loss: 1.5107e-06 - val_mae: 0.0011 - val_mse: 2.3035e-06\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 617us/step - loss: 6.9304e-06 - mae: 0.0019 - mse: 1.0351e-05 - val_loss: 7.1215e-07 - val_mae: 7.4648e-04 - val_mse: 1.0166e-06\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 624us/step - loss: 8.1544e-06 - mae: 0.0019 - mse: 1.1794e-05 - val_loss: 4.4831e-06 - val_mae: 0.0016 - val_mse: 5.1392e-06\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 627us/step - loss: 6.8057e-06 - mae: 0.0015 - mse: 1.0725e-05 - val_loss: 2.1404e-06 - val_mae: 0.0017 - val_mse: 3.3280e-06\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 634us/step - loss: 1.0275e-05 - mae: 0.0015 - mse: 1.6536e-05 - val_loss: 2.1455e-06 - val_mae: 0.0016 - val_mse: 3.6533e-06\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 628us/step - loss: 7.8696e-06 - mae: 0.0017 - mse: 1.1985e-05 - val_loss: 3.9710e-07 - val_mae: 5.1680e-04 - val_mse: 5.6375e-07\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 624us/step - loss: 6.5681e-06 - mae: 0.0013 - mse: 9.4631e-06 - val_loss: 3.6415e-06 - val_mae: 0.0015 - val_mse: 4.7545e-06\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 629us/step - loss: 9.0676e-06 - mae: 0.0017 - mse: 1.3487e-05 - val_loss: 2.4732e-07 - val_mae: 4.4783e-04 - val_mse: 3.7546e-07\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 629us/step - loss: 4.7224e-06 - mae: 0.0013 - mse: 7.0127e-06 - val_loss: 9.8651e-07 - val_mae: 7.3456e-04 - val_mse: 1.2526e-06\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 629us/step - loss: 7.3006e-06 - mae: 0.0013 - mse: 1.2127e-05 - val_loss: 2.2178e-07 - val_mae: 4.7178e-04 - val_mse: 3.9916e-07\n",
      "Iteration 5: delta = 0.8999999999999999, val_mae = 0.0004717828123830259, val_mse = 3.9916008631735167e-07\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0049 - mae: 0.0150 - mse: 0.0086 - val_loss: 3.1786e-06 - val_mae: 0.0018 - val_mse: 5.8891e-06\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 634us/step - loss: 3.3143e-05 - mae: 0.0041 - mse: 4.7853e-05 - val_loss: 6.8335e-06 - val_mae: 0.0021 - val_mse: 9.5927e-06\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 636us/step - loss: 3.6825e-05 - mae: 0.0040 - mse: 5.3859e-05 - val_loss: 1.2225e-05 - val_mae: 0.0035 - val_mse: 2.0621e-05\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 650us/step - loss: 2.6763e-05 - mae: 0.0037 - mse: 3.7311e-05 - val_loss: 1.7299e-05 - val_mae: 0.0035 - val_mse: 2.3129e-05\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 651us/step - loss: 2.9591e-05 - mae: 0.0033 - mse: 4.3060e-05 - val_loss: 2.3522e-05 - val_mae: 0.0037 - val_mse: 2.8597e-05\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 684us/step - loss: 1.9843e-05 - mae: 0.0029 - mse: 2.7922e-05 - val_loss: 8.4767e-06 - val_mae: 0.0024 - val_mse: 1.1309e-05\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 770us/step - loss: 1.5217e-05 - mae: 0.0030 - mse: 2.2742e-05 - val_loss: 3.9139e-05 - val_mae: 0.0055 - val_mse: 4.7746e-05\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 694us/step - loss: 1.0118e-05 - mae: 0.0023 - mse: 1.4713e-05 - val_loss: 8.9067e-06 - val_mae: 0.0033 - val_mse: 1.5006e-05\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 701us/step - loss: 9.6053e-06 - mae: 0.0022 - mse: 1.4181e-05 - val_loss: 6.3123e-06 - val_mae: 0.0015 - val_mse: 7.6161e-06\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 626us/step - loss: 8.9915e-06 - mae: 0.0022 - mse: 1.3289e-05 - val_loss: 2.0710e-05 - val_mae: 0.0034 - val_mse: 3.0079e-05\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 689us/step - loss: 8.2565e-06 - mae: 0.0017 - mse: 1.2066e-05 - val_loss: 5.7023e-07 - val_mae: 7.8177e-04 - val_mse: 9.9656e-07\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 732us/step - loss: 7.8219e-06 - mae: 0.0016 - mse: 1.1571e-05 - val_loss: 1.0685e-05 - val_mae: 0.0030 - val_mse: 1.5808e-05\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 669us/step - loss: 6.5556e-06 - mae: 0.0016 - mse: 9.7817e-06 - val_loss: 3.4581e-07 - val_mae: 5.0319e-04 - val_mse: 5.4101e-07\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 646us/step - loss: 5.3832e-06 - mae: 0.0016 - mse: 7.9674e-06 - val_loss: 6.8317e-07 - val_mae: 6.9945e-04 - val_mse: 9.2126e-07\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 650us/step - loss: 6.0774e-06 - mae: 0.0014 - mse: 9.8921e-06 - val_loss: 1.2461e-04 - val_mae: 0.0085 - val_mse: 1.7989e-04\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 624us/step - loss: 5.0185e-06 - mae: 0.0011 - mse: 7.4109e-06 - val_loss: 1.1750e-05 - val_mae: 0.0035 - val_mse: 1.6047e-05\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 692us/step - loss: 6.4494e-06 - mae: 0.0014 - mse: 9.6171e-06 - val_loss: 1.7884e-07 - val_mae: 4.1881e-04 - val_mse: 2.7993e-07\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 700us/step - loss: 4.7994e-06 - mae: 0.0012 - mse: 7.5284e-06 - val_loss: 1.3330e-07 - val_mae: 4.0718e-04 - val_mse: 2.5829e-07\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 683us/step - loss: 4.2060e-06 - mae: 0.0012 - mse: 6.5327e-06 - val_loss: 2.6569e-07 - val_mae: 4.6943e-04 - val_mse: 4.6714e-07\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 640us/step - loss: 5.1085e-06 - mae: 0.0012 - mse: 7.6420e-06 - val_loss: 2.7248e-07 - val_mae: 4.6029e-04 - val_mse: 4.1699e-07\n",
      "Iteration 6: delta = 0.9999999999999999, val_mae = 0.0004602853150572628, val_mse = 4.1698990571603645e-07\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0060 - mae: 0.0173 - mse: 0.0091 - val_loss: 1.8454e-05 - val_mae: 0.0042 - val_mse: 2.6207e-05\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 626us/step - loss: 4.7575e-05 - mae: 0.0041 - mse: 6.4188e-05 - val_loss: 7.1176e-06 - val_mae: 0.0026 - val_mse: 1.1521e-05\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 657us/step - loss: 2.8914e-05 - mae: 0.0038 - mse: 4.0998e-05 - val_loss: 0.0012 - val_mae: 0.0276 - val_mse: 0.0019\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 632us/step - loss: 4.6173e-05 - mae: 0.0040 - mse: 6.3953e-05 - val_loss: 4.4037e-06 - val_mae: 0.0019 - val_mse: 6.8668e-06\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 648us/step - loss: 2.0796e-05 - mae: 0.0029 - mse: 2.9287e-05 - val_loss: 5.7505e-06 - val_mae: 0.0017 - val_mse: 7.5347e-06\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 655us/step - loss: 1.9023e-05 - mae: 0.0032 - mse: 2.7105e-05 - val_loss: 5.0001e-06 - val_mae: 0.0021 - val_mse: 8.6989e-06\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 630us/step - loss: 1.9648e-05 - mae: 0.0029 - mse: 2.9178e-05 - val_loss: 7.4326e-07 - val_mae: 8.5693e-04 - val_mse: 1.2991e-06\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 665us/step - loss: 1.4195e-05 - mae: 0.0027 - mse: 2.0320e-05 - val_loss: 1.5478e-06 - val_mae: 9.1599e-04 - val_mse: 1.9535e-06\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 638us/step - loss: 1.5069e-05 - mae: 0.0020 - mse: 2.3514e-05 - val_loss: 2.0101e-05 - val_mae: 0.0036 - val_mse: 2.4140e-05\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 638us/step - loss: 1.3965e-05 - mae: 0.0024 - mse: 2.0128e-05 - val_loss: 1.2143e-05 - val_mae: 0.0028 - val_mse: 1.8921e-05\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 674us/step - loss: 7.4522e-06 - mae: 0.0020 - mse: 1.1275e-05 - val_loss: 4.0924e-06 - val_mae: 0.0020 - val_mse: 5.9450e-06\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 661us/step - loss: 1.8042e-05 - mae: 0.0023 - mse: 2.8231e-05 - val_loss: 3.4212e-06 - val_mae: 0.0014 - val_mse: 4.0599e-06\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 673us/step - loss: 9.1135e-06 - mae: 0.0017 - mse: 1.3945e-05 - val_loss: 1.8975e-06 - val_mae: 0.0011 - val_mse: 2.6447e-06\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 678us/step - loss: 8.5053e-06 - mae: 0.0018 - mse: 1.3302e-05 - val_loss: 2.6199e-05 - val_mae: 0.0039 - val_mse: 3.0452e-05\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 654us/step - loss: 5.8091e-06 - mae: 0.0017 - mse: 8.5880e-06 - val_loss: 4.3124e-06 - val_mae: 0.0020 - val_mse: 5.8480e-06\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 641us/step - loss: 7.8545e-06 - mae: 0.0016 - mse: 1.1752e-05 - val_loss: 4.0700e-05 - val_mae: 0.0049 - val_mse: 6.0144e-05\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 670us/step - loss: 6.2571e-06 - mae: 0.0017 - mse: 9.5665e-06 - val_loss: 1.9680e-05 - val_mae: 0.0043 - val_mse: 2.6917e-05\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 645us/step - loss: 4.2398e-06 - mae: 0.0014 - mse: 6.2062e-06 - val_loss: 5.7443e-07 - val_mae: 5.8405e-04 - val_mse: 7.0144e-07\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 698us/step - loss: 5.7387e-06 - mae: 0.0014 - mse: 8.5989e-06 - val_loss: 1.1811e-06 - val_mae: 9.8399e-04 - val_mse: 1.5988e-06\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 646us/step - loss: 6.8908e-06 - mae: 0.0014 - mse: 1.0714e-05 - val_loss: 3.8706e-07 - val_mae: 5.2554e-04 - val_mse: 4.7801e-07\n",
      "Iteration 7: delta = 1.0999999999999999, val_mae = 0.0005255447467789054, val_mse = 4.780115432367893e-07\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0023 - mae: 0.0107 - mse: 0.0039 - val_loss: 1.9162e-05 - val_mae: 0.0034 - val_mse: 2.7757e-05\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 816us/step - loss: 5.0752e-05 - mae: 0.0036 - mse: 6.7440e-05 - val_loss: 1.5001e-05 - val_mae: 0.0031 - val_mse: 1.8508e-05\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 713us/step - loss: 3.2185e-05 - mae: 0.0037 - mse: 4.4638e-05 - val_loss: 3.1010e-06 - val_mae: 0.0016 - val_mse: 5.0872e-06\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 644us/step - loss: 2.8061e-05 - mae: 0.0033 - mse: 4.0491e-05 - val_loss: 1.8005e-05 - val_mae: 0.0038 - val_mse: 2.3775e-05\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 627us/step - loss: 2.6254e-05 - mae: 0.0033 - mse: 3.6295e-05 - val_loss: 1.8951e-04 - val_mae: 0.0126 - val_mse: 2.5943e-04\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 659us/step - loss: 2.0152e-05 - mae: 0.0031 - mse: 2.8924e-05 - val_loss: 2.3928e-05 - val_mae: 0.0046 - val_mse: 3.7360e-05\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 687us/step - loss: 1.1645e-05 - mae: 0.0024 - mse: 1.7335e-05 - val_loss: 6.8775e-06 - val_mae: 0.0019 - val_mse: 9.0789e-06\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 708us/step - loss: 1.0501e-05 - mae: 0.0024 - mse: 1.5452e-05 - val_loss: 3.4680e-06 - val_mae: 0.0018 - val_mse: 5.4390e-06\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 683us/step - loss: 1.3199e-05 - mae: 0.0024 - mse: 1.9866e-05 - val_loss: 2.4796e-06 - val_mae: 0.0014 - val_mse: 3.8689e-06\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 670us/step - loss: 8.5553e-06 - mae: 0.0020 - mse: 1.2856e-05 - val_loss: 9.5958e-07 - val_mae: 8.3455e-04 - val_mse: 1.3554e-06\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 649us/step - loss: 1.2896e-05 - mae: 0.0021 - mse: 2.0383e-05 - val_loss: 6.3429e-07 - val_mae: 5.8259e-04 - val_mse: 1.0005e-06\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 672us/step - loss: 6.7945e-06 - mae: 0.0012 - mse: 1.0640e-05 - val_loss: 4.7776e-07 - val_mae: 6.2472e-04 - val_mse: 7.1642e-07\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 647us/step - loss: 6.5701e-06 - mae: 0.0013 - mse: 1.0965e-05 - val_loss: 7.2444e-07 - val_mae: 9.0868e-04 - val_mse: 1.1168e-06\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 674us/step - loss: 5.6804e-06 - mae: 0.0013 - mse: 9.0882e-06 - val_loss: 1.4466e-05 - val_mae: 0.0049 - val_mse: 3.0669e-05\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 676us/step - loss: 5.0887e-06 - mae: 0.0012 - mse: 8.2207e-06 - val_loss: 5.7520e-07 - val_mae: 7.2133e-04 - val_mse: 9.4647e-07\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 655us/step - loss: 3.5787e-06 - mae: 0.0011 - mse: 5.7579e-06 - val_loss: 2.4051e-06 - val_mae: 0.0013 - val_mse: 3.3285e-06\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 669us/step - loss: 4.8575e-06 - mae: 0.0012 - mse: 7.7300e-06 - val_loss: 9.8440e-08 - val_mae: 2.4343e-04 - val_mse: 1.3194e-07\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 712us/step - loss: 3.2016e-06 - mae: 0.0011 - mse: 4.9420e-06 - val_loss: 1.2628e-06 - val_mae: 0.0013 - val_mse: 2.3204e-06\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 742us/step - loss: 7.4879e-06 - mae: 0.0012 - mse: 1.1074e-05 - val_loss: 1.2406e-07 - val_mae: 4.3231e-04 - val_mse: 2.3358e-07\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 807us/step - loss: 5.2250e-06 - mae: 9.5403e-04 - mse: 8.7822e-06 - val_loss: 1.3246e-07 - val_mae: 2.9090e-04 - val_mse: 1.8255e-07\n",
      "Iteration 8: delta = 1.2, val_mae = 0.00029089985764585435, val_mse = 1.8254846168019867e-07\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0052 - mae: 0.0152 - mse: 0.0085 - val_loss: 4.0907e-06 - val_mae: 0.0018 - val_mse: 7.4568e-06\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 667us/step - loss: 1.4189e-05 - mae: 0.0026 - mse: 2.0536e-05 - val_loss: 6.9040e-06 - val_mae: 0.0022 - val_mse: 8.7697e-06\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 684us/step - loss: 3.5851e-05 - mae: 0.0040 - mse: 4.9283e-05 - val_loss: 4.0108e-05 - val_mae: 0.0059 - val_mse: 6.5669e-05\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 656us/step - loss: 3.5376e-05 - mae: 0.0039 - mse: 4.9883e-05 - val_loss: 6.4975e-05 - val_mae: 0.0073 - val_mse: 9.2972e-05\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 657us/step - loss: 2.0058e-05 - mae: 0.0031 - mse: 2.8789e-05 - val_loss: 1.9773e-06 - val_mae: 0.0013 - val_mse: 3.2704e-06\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 741us/step - loss: 2.9180e-05 - mae: 0.0033 - mse: 4.3788e-05 - val_loss: 7.4259e-06 - val_mae: 0.0019 - val_mse: 1.0087e-05\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 717us/step - loss: 1.3876e-05 - mae: 0.0026 - mse: 2.0606e-05 - val_loss: 1.5580e-05 - val_mae: 0.0024 - val_mse: 1.9166e-05\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 678us/step - loss: 1.6210e-05 - mae: 0.0025 - mse: 2.3913e-05 - val_loss: 7.2921e-06 - val_mae: 0.0018 - val_mse: 8.4629e-06\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 701us/step - loss: 1.3571e-05 - mae: 0.0026 - mse: 2.0243e-05 - val_loss: 1.3994e-06 - val_mae: 9.7379e-04 - val_mse: 2.1720e-06\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 704us/step - loss: 1.1174e-05 - mae: 0.0021 - mse: 1.7537e-05 - val_loss: 5.5996e-06 - val_mae: 0.0021 - val_mse: 7.8011e-06\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 707us/step - loss: 6.6677e-06 - mae: 0.0020 - mse: 9.7356e-06 - val_loss: 9.9950e-06 - val_mae: 0.0023 - val_mse: 1.3141e-05\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 715us/step - loss: 8.6322e-06 - mae: 0.0016 - mse: 1.3311e-05 - val_loss: 2.3069e-07 - val_mae: 4.8346e-04 - val_mse: 4.0936e-07\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 684us/step - loss: 7.5355e-06 - mae: 0.0017 - mse: 1.1536e-05 - val_loss: 2.8519e-07 - val_mae: 4.4872e-04 - val_mse: 4.0083e-07\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 670us/step - loss: 6.4076e-06 - mae: 0.0015 - mse: 9.6866e-06 - val_loss: 1.1444e-05 - val_mae: 0.0029 - val_mse: 1.8268e-05\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 681us/step - loss: 3.5502e-06 - mae: 0.0012 - mse: 5.4180e-06 - val_loss: 5.6280e-06 - val_mae: 0.0018 - val_mse: 7.2452e-06\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 655us/step - loss: 1.1218e-05 - mae: 0.0017 - mse: 1.7937e-05 - val_loss: 2.4716e-07 - val_mae: 4.4069e-04 - val_mse: 3.9156e-07\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 635us/step - loss: 2.1262e-06 - mae: 9.4460e-04 - mse: 3.0168e-06 - val_loss: 1.3975e-06 - val_mae: 0.0011 - val_mse: 2.2492e-06\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 647us/step - loss: 7.0545e-06 - mae: 0.0013 - mse: 1.1152e-05 - val_loss: 4.1796e-08 - val_mae: 1.7880e-04 - val_mse: 7.9298e-08\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 727us/step - loss: 6.0444e-06 - mae: 0.0011 - mse: 9.8966e-06 - val_loss: 3.0457e-07 - val_mae: 5.5412e-04 - val_mse: 5.8135e-07\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 674us/step - loss: 6.8481e-06 - mae: 0.0012 - mse: 1.0991e-05 - val_loss: 1.1377e-07 - val_mae: 3.0395e-04 - val_mse: 1.8521e-07\n",
      "Iteration 9: delta = 1.3, val_mae = 0.0003039517905563116, val_mse = 1.8520830735724303e-07\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0047 - mae: 0.0147 - mse: 0.0068 - val_loss: 2.5311e-06 - val_mae: 0.0014 - val_mse: 3.8450e-06\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 696us/step - loss: 1.4947e-05 - mae: 0.0026 - mse: 1.9692e-05 - val_loss: 3.5998e-05 - val_mae: 0.0046 - val_mse: 4.5022e-05\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 717us/step - loss: 3.4299e-05 - mae: 0.0037 - mse: 4.6395e-05 - val_loss: 6.9355e-06 - val_mae: 0.0018 - val_mse: 9.4413e-06\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 666us/step - loss: 2.7448e-05 - mae: 0.0031 - mse: 3.7752e-05 - val_loss: 2.8829e-06 - val_mae: 0.0017 - val_mse: 4.4467e-06\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 698us/step - loss: 3.2498e-05 - mae: 0.0034 - mse: 4.4906e-05 - val_loss: 7.9692e-06 - val_mae: 0.0024 - val_mse: 1.1261e-05\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 680us/step - loss: 1.4980e-05 - mae: 0.0024 - mse: 2.1813e-05 - val_loss: 2.6207e-04 - val_mae: 0.0090 - val_mse: 3.4333e-04\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 698us/step - loss: 1.1673e-05 - mae: 0.0025 - mse: 1.6698e-05 - val_loss: 2.0828e-05 - val_mae: 0.0038 - val_mse: 2.7660e-05\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 662us/step - loss: 1.2733e-05 - mae: 0.0024 - mse: 1.8035e-05 - val_loss: 1.2431e-05 - val_mae: 0.0023 - val_mse: 1.4538e-05\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 650us/step - loss: 1.3410e-05 - mae: 0.0022 - mse: 1.8659e-05 - val_loss: 5.1590e-07 - val_mae: 7.8114e-04 - val_mse: 9.7726e-07\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 653us/step - loss: 1.1308e-05 - mae: 0.0019 - mse: 1.6418e-05 - val_loss: 8.5278e-06 - val_mae: 0.0030 - val_mse: 1.5555e-05\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 655us/step - loss: 8.6793e-06 - mae: 0.0019 - mse: 1.3334e-05 - val_loss: 6.5051e-07 - val_mae: 8.0596e-04 - val_mse: 1.2094e-06\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 644us/step - loss: 9.9015e-06 - mae: 0.0020 - mse: 1.4935e-05 - val_loss: 5.5663e-04 - val_mae: 0.0208 - val_mse: 9.2593e-04\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 645us/step - loss: 5.1448e-06 - mae: 0.0013 - mse: 7.9035e-06 - val_loss: 2.5331e-06 - val_mae: 0.0011 - val_mse: 3.4579e-06\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 638us/step - loss: 6.2815e-06 - mae: 0.0014 - mse: 9.8552e-06 - val_loss: 2.4834e-06 - val_mae: 0.0015 - val_mse: 3.8689e-06\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 627us/step - loss: 5.1572e-06 - mae: 0.0015 - mse: 7.5665e-06 - val_loss: 3.9416e-06 - val_mae: 0.0015 - val_mse: 5.7260e-06\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 694us/step - loss: 5.7700e-06 - mae: 0.0014 - mse: 8.6685e-06 - val_loss: 1.2098e-07 - val_mae: 3.1239e-04 - val_mse: 1.9199e-07\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 693us/step - loss: 6.3716e-06 - mae: 0.0014 - mse: 9.8841e-06 - val_loss: 5.0737e-07 - val_mae: 6.8271e-04 - val_mse: 7.3523e-07\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 668us/step - loss: 5.6172e-06 - mae: 0.0010 - mse: 8.2515e-06 - val_loss: 1.5329e-06 - val_mae: 8.8597e-04 - val_mse: 2.0738e-06\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 684us/step - loss: 7.8708e-06 - mae: 0.0016 - mse: 1.2967e-05 - val_loss: 6.4095e-08 - val_mae: 2.1822e-04 - val_mse: 1.2409e-07\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 664us/step - loss: 4.0887e-06 - mae: 9.8934e-04 - mse: 6.1735e-06 - val_loss: 2.0567e-07 - val_mae: 3.5866e-04 - val_mse: 2.8778e-07\n",
      "Iteration 10: delta = 1.4000000000000001, val_mae = 0.0003586648090276867, val_mse = 2.877764586628473e-07\n"
     ]
    }
   ],
   "source": [
    "# find best delta for loss function\n",
    "\n",
    "# function to train and evaluate model with a given delta\n",
    "def train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=delta), metrics=['mae', 'mse'], weighted_metrics=[])\n",
    "\n",
    "    history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    # get validation MAE, MSE and Huber loss\n",
    "    val_mae = history.history['val_mae'][-1]\n",
    "    val_mse = history.history['val_mse'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "    return val_mae, val_mse, val_loss\n",
    "\n",
    "# initialise delta value and step size\n",
    "delta = 0.5\n",
    "step_size = 0.1\n",
    "best_delta = delta\n",
    "best_val_mae = float('inf')\n",
    "best_val_mse = float('inf')\n",
    "\n",
    "# initialise arrays for plotting\n",
    "arr_val_mae = []\n",
    "arr_val_mse = []\n",
    "arr_val_loss = []\n",
    "\n",
    "# iterative search for optimal delta\n",
    "for i in range(10):\n",
    "    val_mae, val_mse, val_loss = train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval)\n",
    "\n",
    "    # print current results\n",
    "    print(f\"Iteration {i+1}: delta = {delta}, val_mae = {val_mae}, val_mse = {val_mse}\")\n",
    "\n",
    "    # update best delta if current results are better\n",
    "    if val_mae < best_val_mae and val_mse < best_val_mse: \n",
    "        best_delta = delta\n",
    "        best_val_mae = val_mae\n",
    "        best_val_mse = val_mse\n",
    "    \n",
    "    # store data\n",
    "    arr_val_mae.append(val_mae)\n",
    "    arr_val_mse.append(val_mse)\n",
    "    arr_val_loss.append(val_loss)\n",
    "    \n",
    "    # adjust delta for next iteration \n",
    "    delta += step_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best delta: 1.2\n",
      "Best validation MAE: 0.00029089985764585435\n",
      "Best validation MSE: 1.8254846168019867e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "deltas = np.arange(0.5, 1.5, 0.1)\n",
    "plt.plot(deltas, arr_val_mae, marker='o', linestyle='-', color='C0', label='MAE')\n",
    "plt.plot(deltas, arr_val_mse, marker='o', linestyle='-', color='C1', label='MSE')\n",
    "plt.plot(deltas, arr_val_loss, marker='o', linestyle='-', color='C2', label='Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Best delta: {best_delta}\")\n",
    "print(f\"Best validation MAE: {best_val_mae}\")\n",
    "print(f\"Best validation MSE: {best_val_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 3s 1ms/step - loss: 0.0052 - val_loss: 6.1077e-06\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 564us/step - loss: 1.2979e-05 - val_loss: 2.8237e-05\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 1s 561us/step - loss: 3.9175e-05 - val_loss: 9.9009e-06\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 570us/step - loss: 2.8132e-05 - val_loss: 2.3209e-05\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 563us/step - loss: 2.0126e-05 - val_loss: 5.8459e-06\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 568us/step - loss: 1.8011e-05 - val_loss: 1.3920e-05\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 569us/step - loss: 2.2421e-05 - val_loss: 1.0035e-05\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 563us/step - loss: 9.1726e-06 - val_loss: 5.9296e-05\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 563us/step - loss: 1.0892e-05 - val_loss: 8.4594e-06\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 570us/step - loss: 1.1122e-05 - val_loss: 2.2340e-06\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 565us/step - loss: 8.5739e-06 - val_loss: 7.0867e-07\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 1s 561us/step - loss: 9.6188e-06 - val_loss: 2.8681e-04\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 571us/step - loss: 8.9971e-06 - val_loss: 8.1033e-07\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 565us/step - loss: 6.8583e-06 - val_loss: 3.9558e-07\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 575us/step - loss: 8.0247e-06 - val_loss: 4.4088e-06\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 564us/step - loss: 3.9364e-06 - val_loss: 3.4393e-06\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 565us/step - loss: 4.6630e-06 - val_loss: 5.9572e-07\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 571us/step - loss: 4.3532e-06 - val_loss: 7.3358e-08\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 564us/step - loss: 4.3381e-06 - val_loss: 5.9679e-06\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 1s 560us/step - loss: 4.3988e-06 - val_loss: 3.9053e-06\n",
      "Training Huber Loss: 4.398849796416471e-06\n",
      "Validation Huber Loss: 3.90532704841462e-06\n"
     ]
    }
   ],
   "source": [
    "# evaluate base model \n",
    "base_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "base_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = base_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])\n",
    "\n",
    "# save model\n",
    "base_model.save(\"models/base_dnn_model_processed_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/base_dnn_model_processed_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "base_train_hl = history.history[\"loss\"][-1]\n",
    "base_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {base_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {base_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0476 - val_loss: 0.0066\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 583us/step - loss: 0.0053 - val_loss: 0.0045\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 582us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 589us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 583us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 582us/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 590us/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 579us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 583us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 590us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 580us/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 583us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 590us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 583us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 590us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 580us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 584us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 593us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 581us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 581us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Training Huber Loss: 0.0019612140022218227\n",
      "Validation Huber Loss: 0.0018785912543535233\n"
     ]
    }
   ],
   "source": [
    "# evaluate regularised model\n",
    "reg_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],), kernel_regularizer='l2'), \n",
    "        Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "reg_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "history = reg_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])\n",
    "\n",
    "# save model\n",
    "base_model.save(\"models/reg_dnn_model_processed_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/reg_dnn_model_processed_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "reg_train_hl = history.history[\"loss\"][-1]\n",
    "reg_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {reg_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {reg_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_8512/1497506516.py:28: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2728 - val_loss: 0.0211\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.0131 - val_loss: 0.0092\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 634us/step - loss: 0.0079 - val_loss: 0.0068\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 627us/step - loss: 0.0062 - val_loss: 0.0056\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 634us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 632us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 629us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 639us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 626us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0031 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 623ms/epoch - 584us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 676ms/epoch - 634us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 706ms/epoch - 662us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0031 - val_loss: 0.0029 - 675ms/epoch - 632us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 705ms/epoch - 661us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 741ms/epoch - 695us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 691ms/epoch - 647us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 749ms/epoch - 702us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 692ms/epoch - 648us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 660ms/epoch - 619us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 675ms/epoch - 633us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 640ms/epoch - 600us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 681ms/epoch - 638us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 670ms/epoch - 628us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 654ms/epoch - 613us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 644ms/epoch - 603us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 629ms/epoch - 589us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 624ms/epoch - 585us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 621ms/epoch - 582us/step\n",
      "267/267 - 0s - loss: 0.0028 - 147ms/epoch - 551us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  46.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2730 - val_loss: 0.0199\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 0.0134 - val_loss: 0.0099\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0084 - val_loss: 0.0072\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0045 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 656us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 663us/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 639us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 749us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 716us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 649us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 1ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 702us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0029 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 629ms/epoch - 589us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 1s/epoch - 1ms/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 1s/epoch - 1ms/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 1s/epoch - 1ms/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 2s - loss: 0.0029 - val_loss: 0.0028 - 2s/epoch - 2ms/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 2s - loss: 0.0029 - val_loss: 0.0029 - 2s/epoch - 2ms/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 2s - loss: 0.0029 - val_loss: 0.0029 - 2s/epoch - 2ms/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 1s/epoch - 1ms/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 729ms/epoch - 684us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0029 - 741ms/epoch - 694us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 661ms/epoch - 620us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 687ms/epoch - 644us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0027 - 713ms/epoch - 668us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0027 - 635ms/epoch - 595us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 663ms/epoch - 621us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0027 - 674ms/epoch - 631us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0027 - val_loss: 0.0027 - 662ms/epoch - 621us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0027 - 701ms/epoch - 657us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0027 - val_loss: 0.0027 - 696ms/epoch - 653us/step\n",
      "267/267 - 0s - loss: 0.0027 - 200ms/epoch - 751us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  54.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 7s 5ms/step - loss: 0.2745 - val_loss: 0.0219\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0150 - val_loss: 0.0109\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0089 - val_loss: 0.0076\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0066 - val_loss: 0.0059\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 3s 3ms/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 891us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 6s - loss: 0.0030 - val_loss: 0.0031 - 6s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0030 - 3s/epoch - 3ms/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0030 - 3s/epoch - 3ms/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0030 - 3s/epoch - 3ms/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 3s - loss: 0.0031 - val_loss: 0.0030 - 3s/epoch - 3ms/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 4s - loss: 0.0030 - val_loss: 0.0029 - 4s/epoch - 3ms/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0033 - 3s/epoch - 3ms/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0031 - 3s/epoch - 3ms/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0045 - 3s/epoch - 3ms/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0028 - 3s/epoch - 3ms/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0028 - 3s/epoch - 3ms/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0028 - 3s/epoch - 3ms/step\n",
      "267/267 - 0s - loss: 0.0028 - 484ms/epoch - 2ms/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 2.7min\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 8s 6ms/step - loss: 0.2708 - val_loss: 0.0167\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0115 - val_loss: 0.0086\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0073 - val_loss: 0.0064\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.0058 - val_loss: 0.0052\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 840us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 855us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 689us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 852us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 1/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0029 - 3s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 664ms/epoch - 622us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 641ms/epoch - 601us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 648ms/epoch - 608us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 645ms/epoch - 605us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 662ms/epoch - 620us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 632ms/epoch - 592us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 638ms/epoch - 598us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 639ms/epoch - 599us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 694ms/epoch - 651us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0058 - 659ms/epoch - 618us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 659ms/epoch - 617us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 642ms/epoch - 601us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0029 - 648ms/epoch - 608us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 631ms/epoch - 591us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 647ms/epoch - 606us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 637ms/epoch - 597us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 722ms/epoch - 677us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 641ms/epoch - 601us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 641ms/epoch - 601us/step\n",
      "267/267 - 0s - loss: 0.0028 - 160ms/epoch - 598us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2827 - val_loss: 0.0250\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.0165 - val_loss: 0.0115\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0093 - val_loss: 0.0076\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 659us/step - loss: 0.0067 - val_loss: 0.0061\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 763us/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 652us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 662us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 722us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 715us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 738us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 676us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 740us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 718us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 673ms/epoch - 630us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 629ms/epoch - 589us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 663ms/epoch - 622us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 640ms/epoch - 600us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 663ms/epoch - 621us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 630ms/epoch - 591us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 639ms/epoch - 599us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 643ms/epoch - 603us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 675ms/epoch - 633us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 641ms/epoch - 601us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 653ms/epoch - 612us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 665ms/epoch - 623us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 600ms/epoch - 562us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 675ms/epoch - 633us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 649ms/epoch - 609us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 612ms/epoch - 574us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 699ms/epoch - 655us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0029 - 660ms/epoch - 618us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 660ms/epoch - 618us/step\n",
      "267/267 - 0s - loss: 0.0028 - 151ms/epoch - 566us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  47.3s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3135 - val_loss: 0.4296\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.2534 - val_loss: 0.2805\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.3880 - val_loss: 0.2659\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.3401 - val_loss: 0.2589\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.3011 - val_loss: 0.3367\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.3070 - val_loss: 0.3784\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 624us/step - loss: 0.2695 - val_loss: 0.2847\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.2791 - val_loss: 0.2403\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.3477 - val_loss: 0.3069\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.2882 - val_loss: 0.2847\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.3797 - val_loss: 0.3057\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.2864 - val_loss: 0.2826\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 643us/step - loss: 0.2931 - val_loss: 0.2475\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 686us/step - loss: 0.3001 - val_loss: 0.3221\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 659us/step - loss: 0.2720 - val_loss: 0.2426\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.3779 - val_loss: 0.2971\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 691us/step - loss: 0.3253 - val_loss: 0.2714\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 677us/step - loss: 0.2916 - val_loss: 0.3312\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.4243 - val_loss: 0.2914\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.3076 - val_loss: 0.4380\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.3215 - val_loss: 0.5445 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.3328 - val_loss: 0.2973 - 666ms/epoch - 624us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.3046 - val_loss: 0.3039 - 628ms/epoch - 588us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.3009 - val_loss: 0.3118 - 617ms/epoch - 578us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.3138 - val_loss: 0.2618 - 652ms/epoch - 611us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.3007 - val_loss: 0.2954 - 621ms/epoch - 582us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.3017 - val_loss: 0.2300 - 636ms/epoch - 596us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.2852 - val_loss: 0.2934 - 653ms/epoch - 612us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.2868 - val_loss: 0.2568 - 681ms/epoch - 638us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.3943 - val_loss: 0.2629 - 725ms/epoch - 679us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.2802 - val_loss: 0.2515 - 622ms/epoch - 583us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.2964 - val_loss: 0.3109 - 627ms/epoch - 588us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.2910 - val_loss: 0.2606 - 647ms/epoch - 606us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.3059 - val_loss: 0.2646 - 623ms/epoch - 584us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.2909 - val_loss: 0.3793 - 645ms/epoch - 605us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.3147 - val_loss: 0.3514 - 634ms/epoch - 594us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.3482 - val_loss: 0.2715 - 635ms/epoch - 595us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.3059 - val_loss: 0.2596 - 667ms/epoch - 625us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.2764 - val_loss: 0.2456 - 648ms/epoch - 607us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.2824 - val_loss: 0.2847 - 658ms/epoch - 616us/step\n",
      "267/267 - 0s - loss: 0.2695 - 148ms/epoch - 556us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  45.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3029 - val_loss: 0.2329\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.3788 - val_loss: 0.2390\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 654us/step - loss: 0.2493 - val_loss: 0.2260\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 627us/step - loss: 0.3752 - val_loss: 0.2788\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.2595 - val_loss: 0.2620\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 639us/step - loss: 0.2721 - val_loss: 0.2346\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 0.3416 - val_loss: 0.2456\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 703us/step - loss: 0.3345 - val_loss: 0.2601\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.2823 - val_loss: 0.2727\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 629us/step - loss: 0.3345 - val_loss: 0.3070\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.2752 - val_loss: 0.2916\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 652us/step - loss: 0.2945 - val_loss: 0.2398\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.3551 - val_loss: 0.3155\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 632us/step - loss: 0.2769 - val_loss: 0.2566\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.3419 - val_loss: 0.4186\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 0.3041 - val_loss: 0.2384\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.2783 - val_loss: 0.3369\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 635us/step - loss: 0.2928 - val_loss: 0.3313\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 691us/step - loss: 0.3426 - val_loss: 0.2465\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 668us/step - loss: 0.2714 - val_loss: 0.3878\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.3873 - val_loss: 0.2391 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.2895 - val_loss: 0.2710 - 723ms/epoch - 677us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.2751 - val_loss: 0.3137 - 671ms/epoch - 629us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.2985 - val_loss: 0.3685 - 655ms/epoch - 614us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.3536 - val_loss: 0.2695 - 715ms/epoch - 670us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.3226 - val_loss: 0.2878 - 660ms/epoch - 619us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.2835 - val_loss: 0.2990 - 719ms/epoch - 674us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.3049 - val_loss: 0.5973 - 638ms/epoch - 598us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.3114 - val_loss: 0.9463 - 667ms/epoch - 626us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.3641 - val_loss: 0.3021 - 627ms/epoch - 588us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.3185 - val_loss: 0.2846 - 631ms/epoch - 591us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 2s - loss: 0.3153 - val_loss: 0.3338 - 2s/epoch - 2ms/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 3s - loss: 0.2864 - val_loss: 0.3752 - 3s/epoch - 2ms/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 2s - loss: 0.3279 - val_loss: 0.2687 - 2s/epoch - 2ms/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 2s - loss: 0.2906 - val_loss: 0.5176 - 2s/epoch - 2ms/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.3027 - val_loss: 0.3625 - 857ms/epoch - 804us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.3261 - val_loss: 0.2941 - 714ms/epoch - 669us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.3044 - val_loss: 0.2767 - 641ms/epoch - 601us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.2820 - val_loss: 0.2543 - 600ms/epoch - 563us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.3097 - val_loss: 0.3055 - 629ms/epoch - 590us/step\n",
      "267/267 - 0s - loss: 0.2890 - 143ms/epoch - 534us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  53.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3090 - val_loss: 0.2247\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3616 - val_loss: 0.6529\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.2800 - val_loss: 0.2593\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.4295 - val_loss: 0.3692\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.3751 - val_loss: 0.3111\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.3494 - val_loss: 0.2763\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.3076 - val_loss: 0.2852\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.3045 - val_loss: 0.3573\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.3395 - val_loss: 0.2611\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.3174 - val_loss: 0.3679\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.3100 - val_loss: 0.2813\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 831us/step - loss: 0.3256 - val_loss: 0.2737\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 790us/step - loss: 0.2932 - val_loss: 0.3170\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.3019 - val_loss: 0.2415\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 720us/step - loss: 0.3095 - val_loss: 0.2913\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 666us/step - loss: 0.2969 - val_loss: 0.3109\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 692us/step - loss: 0.3082 - val_loss: 0.2681\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 716us/step - loss: 0.3143 - val_loss: 0.2935\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.2960 - val_loss: 0.2517\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 686us/step - loss: 0.3654 - val_loss: 0.2891\n",
      "Epoch 1/20\n",
      "1067/1067 - 3s - loss: 0.2809 - val_loss: 0.2561 - 3s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.2839 - val_loss: 0.3417 - 1s/epoch - 1ms/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.3287 - val_loss: 0.2695 - 823ms/epoch - 771us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.2609 - val_loss: 0.2678 - 827ms/epoch - 775us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.3278 - val_loss: 0.2620 - 650ms/epoch - 609us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.2892 - val_loss: 0.2795 - 682ms/epoch - 639us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.2982 - val_loss: 0.3243 - 726ms/epoch - 680us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.3473 - val_loss: 0.3266 - 696ms/epoch - 652us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.2955 - val_loss: 0.2636 - 721ms/epoch - 676us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.2941 - val_loss: 0.3036 - 729ms/epoch - 683us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.3266 - val_loss: 0.4020 - 821ms/epoch - 769us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.3109 - val_loss: 0.2504 - 746ms/epoch - 699us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.2918 - val_loss: 0.2331 - 771ms/epoch - 723us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.2978 - val_loss: 0.3381 - 626ms/epoch - 586us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.3068 - val_loss: 0.2352 - 669ms/epoch - 627us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.3043 - val_loss: 0.2504 - 618ms/epoch - 580us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.3250 - val_loss: 0.3464 - 716ms/epoch - 671us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.2778 - val_loss: 0.2547 - 678ms/epoch - 635us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.3033 - val_loss: 0.3014 - 655ms/epoch - 614us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.2915 - val_loss: 0.2723 - 636ms/epoch - 596us/step\n",
      "267/267 - 0s - loss: 0.2677 - 151ms/epoch - 564us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3990 - val_loss: 1.6006\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 663us/step - loss: 0.3569 - val_loss: 0.2820\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 739us/step - loss: 0.2747 - val_loss: 0.3783\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.2749 - val_loss: 0.2357\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.4716 - val_loss: 0.3599\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.3216 - val_loss: 0.3135\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 761us/step - loss: 0.2824 - val_loss: 0.2750\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 737us/step - loss: 0.3579 - val_loss: 0.3345\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 795us/step - loss: 0.2834 - val_loss: 0.2711\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.3115 - val_loss: 0.2842\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 697us/step - loss: 0.3700 - val_loss: 0.4374\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 712us/step - loss: 0.3147 - val_loss: 0.3368\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 695us/step - loss: 0.3047 - val_loss: 0.3555\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.3270 - val_loss: 0.3021\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 699us/step - loss: 0.3101 - val_loss: 0.2790\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.3034 - val_loss: 0.2614\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 655us/step - loss: 0.3030 - val_loss: 0.2911\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 759us/step - loss: 0.2963 - val_loss: 0.2338\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 707us/step - loss: 0.2849 - val_loss: 0.2399\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 713us/step - loss: 0.3107 - val_loss: 0.2388\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.3229 - val_loss: 0.2639 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.2729 - val_loss: 0.2488 - 714ms/epoch - 670us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.3113 - val_loss: 0.2578 - 721ms/epoch - 676us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.2648 - val_loss: 0.3335 - 700ms/epoch - 656us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.3285 - val_loss: 0.2478 - 708ms/epoch - 664us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.2915 - val_loss: 0.2527 - 711ms/epoch - 667us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.2681 - val_loss: 0.2679 - 777ms/epoch - 728us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.3387 - val_loss: 0.2580 - 689ms/epoch - 646us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.2981 - val_loss: 0.2811 - 736ms/epoch - 689us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.2882 - val_loss: 0.3163 - 687ms/epoch - 644us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.3005 - val_loss: 0.3147 - 751ms/epoch - 704us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.2934 - val_loss: 0.2562 - 772ms/epoch - 724us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.3241 - val_loss: 0.7396 - 702ms/epoch - 658us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.2866 - val_loss: 0.2330 - 692ms/epoch - 649us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.2803 - val_loss: 0.3246 - 712ms/epoch - 667us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.3408 - val_loss: 0.3307 - 691ms/epoch - 648us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.2732 - val_loss: 0.2282 - 705ms/epoch - 661us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.2991 - val_loss: 0.2369 - 679ms/epoch - 636us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.3238 - val_loss: 0.2933 - 727ms/epoch - 682us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.3322 - val_loss: 0.2438 - 715ms/epoch - 670us/step\n",
      "267/267 - 0s - loss: 0.2423 - 157ms/epoch - 589us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  49.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3151 - val_loss: 0.2214\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 732us/step - loss: 0.3896 - val_loss: 0.6390\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.2960 - val_loss: 0.5644\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.2842 - val_loss: 0.2352\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 696us/step - loss: 0.3219 - val_loss: 0.2606\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 667us/step - loss: 0.4132 - val_loss: 0.4111\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.3104 - val_loss: 0.2668\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 651us/step - loss: 0.2844 - val_loss: 0.2518\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 647us/step - loss: 0.2858 - val_loss: 0.2686\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 676us/step - loss: 0.2981 - val_loss: 0.2435\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 654us/step - loss: 0.3335 - val_loss: 0.2532\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 659us/step - loss: 0.2653 - val_loss: 0.2826\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.4155 - val_loss: 0.3017\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 0.3104 - val_loss: 0.2827\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.3314 - val_loss: 0.3438\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.2938 - val_loss: 0.2466\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.2935 - val_loss: 0.2992\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 706us/step - loss: 0.3001 - val_loss: 0.2596\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 709us/step - loss: 0.2786 - val_loss: 0.2512\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.3312 - val_loss: 0.2412\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.2793 - val_loss: 0.2403 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.3240 - val_loss: 0.2448 - 703ms/epoch - 659us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.2746 - val_loss: 0.2734 - 677ms/epoch - 635us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.4815 - val_loss: 0.3269 - 698ms/epoch - 654us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.3418 - val_loss: 0.3362 - 670ms/epoch - 628us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.3018 - val_loss: 0.3060 - 672ms/epoch - 630us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.3564 - val_loss: 0.2568 - 686ms/epoch - 643us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.3420 - val_loss: 0.3076 - 704ms/epoch - 660us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.3001 - val_loss: 0.3670 - 689ms/epoch - 645us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.3069 - val_loss: 0.3139 - 648ms/epoch - 607us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.2929 - val_loss: 0.2806 - 649ms/epoch - 608us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.3031 - val_loss: 0.3887 - 758ms/epoch - 710us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.3043 - val_loss: 0.3212 - 679ms/epoch - 637us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.3523 - val_loss: 0.2424 - 711ms/epoch - 666us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.2818 - val_loss: 0.3041 - 701ms/epoch - 657us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.3341 - val_loss: 0.3729 - 670ms/epoch - 628us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.3182 - val_loss: 0.2864 - 653ms/epoch - 612us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.2878 - val_loss: 0.3641 - 668ms/epoch - 626us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.3513 - val_loss: 0.3125 - 681ms/epoch - 638us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.2987 - val_loss: 0.2601 - 696ms/epoch - 652us/step\n",
      "267/267 - 0s - loss: 0.2581 - 165ms/epoch - 617us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  48.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.5217 - val_loss: 0.6210\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.2357 - val_loss: 0.0662\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 877us/step - loss: 0.0507 - val_loss: 0.0412\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.0355 - val_loss: 0.0308\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0273 - val_loss: 0.0245\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0221 - val_loss: 0.0202\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0187 - val_loss: 0.0174\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0165 - val_loss: 0.0156\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0149 - val_loss: 0.0144\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0139 - val_loss: 0.0134\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0129 - val_loss: 0.0125\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0086 - val_loss: 0.0084 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0084 - val_loss: 0.0083 - 560ms/epoch - 524us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0082 - val_loss: 0.0081 - 636ms/epoch - 596us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0080 - val_loss: 0.0080 - 563ms/epoch - 528us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0079 - val_loss: 0.0078 - 613ms/epoch - 575us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0078 - val_loss: 0.0077 - 660ms/epoch - 618us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0076 - val_loss: 0.0075 - 574ms/epoch - 538us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0075 - val_loss: 0.0074 - 588ms/epoch - 551us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0074 - val_loss: 0.0073 - 538ms/epoch - 504us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0073 - val_loss: 0.0072 - 522ms/epoch - 490us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0071 - 549ms/epoch - 514us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0070 - 527ms/epoch - 494us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0069 - 660ms/epoch - 619us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0068 - 573ms/epoch - 537us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0067 - 546ms/epoch - 512us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0066 - 576ms/epoch - 540us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0065 - 600ms/epoch - 562us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0064 - 551ms/epoch - 516us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0063 - 538ms/epoch - 504us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0062 - 529ms/epoch - 495us/step\n",
      "267/267 - 0s - loss: 0.0062 - 147ms/epoch - 551us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  43.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.4762 - val_loss: 0.6051\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.2353 - val_loss: 0.0729\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0544 - val_loss: 0.0431\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0362 - val_loss: 0.0307\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0266 - val_loss: 0.0233\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0210 - val_loss: 0.0193\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0180 - val_loss: 0.0169\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0160 - val_loss: 0.0151\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 524us/step - loss: 0.0143 - val_loss: 0.0137\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 518us/step - loss: 0.0132 - val_loss: 0.0128\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 0.0124 - val_loss: 0.0121\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0118 - val_loss: 0.0115\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0084 - val_loss: 0.0083 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0083 - val_loss: 0.0082 - 613ms/epoch - 574us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0081 - val_loss: 0.0080 - 619ms/epoch - 580us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0080 - val_loss: 0.0079 - 548ms/epoch - 513us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0078 - val_loss: 0.0078 - 521ms/epoch - 488us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0077 - val_loss: 0.0077 - 527ms/epoch - 494us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0076 - val_loss: 0.0075 - 521ms/epoch - 488us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0075 - val_loss: 0.0074 - 523ms/epoch - 490us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0074 - val_loss: 0.0073 - 524ms/epoch - 491us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0072 - val_loss: 0.0072 - 554ms/epoch - 519us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0071 - 519ms/epoch - 486us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0070 - 522ms/epoch - 489us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0069 - 523ms/epoch - 490us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0068 - 524ms/epoch - 491us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0067 - 525ms/epoch - 492us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0067 - 521ms/epoch - 489us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0066 - 522ms/epoch - 489us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 523ms/epoch - 490us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0065 - 527ms/epoch - 493us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0064 - 522ms/epoch - 490us/step\n",
      "267/267 - 0s - loss: 0.0064 - 150ms/epoch - 560us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.4786 - val_loss: 0.5916\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.2214 - val_loss: 0.0631\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0486 - val_loss: 0.0383\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 495us/step - loss: 0.0318 - val_loss: 0.0268\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 514us/step - loss: 0.0236 - val_loss: 0.0208\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 503us/step - loss: 0.0186 - val_loss: 0.0167\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 492us/step - loss: 0.0153 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 492us/step - loss: 0.0134 - val_loss: 0.0127\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 513us/step - loss: 0.0123 - val_loss: 0.0118\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 516us/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0077 - val_loss: 0.0077 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0076 - val_loss: 0.0075 - 539ms/epoch - 505us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0074 - val_loss: 0.0074 - 576ms/epoch - 540us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0073 - val_loss: 0.0073 - 564ms/epoch - 529us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0072 - val_loss: 0.0072 - 544ms/epoch - 510us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0070 - 534ms/epoch - 500us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0069 - 545ms/epoch - 511us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0068 - 552ms/epoch - 518us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0067 - 554ms/epoch - 519us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 561ms/epoch - 526us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0066 - 535ms/epoch - 501us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0065 - 554ms/epoch - 519us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0064 - 535ms/epoch - 501us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0063 - 527ms/epoch - 494us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0062 - 539ms/epoch - 505us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0062 - 547ms/epoch - 513us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0061 - 525ms/epoch - 492us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0060 - 516ms/epoch - 484us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0060 - 531ms/epoch - 498us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0059 - 547ms/epoch - 512us/step\n",
      "267/267 - 0s - loss: 0.0059 - 151ms/epoch - 566us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.5666 - val_loss: 0.6449\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 0.2408 - val_loss: 0.0591\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 500us/step - loss: 0.0446 - val_loss: 0.0354\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0308 - val_loss: 0.0272\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0245 - val_loss: 0.0221\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 504us/step - loss: 0.0204 - val_loss: 0.0189\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 501us/step - loss: 0.0179 - val_loss: 0.0170\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 522us/step - loss: 0.0162 - val_loss: 0.0154\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0148 - val_loss: 0.0143\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0138 - val_loss: 0.0133\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0129 - val_loss: 0.0126\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0123 - val_loss: 0.0120\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0086 - val_loss: 0.0085 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0084 - val_loss: 0.0083 - 530ms/epoch - 497us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0082 - val_loss: 0.0082 - 549ms/epoch - 515us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0081 - val_loss: 0.0080 - 525ms/epoch - 492us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0079 - val_loss: 0.0078 - 525ms/epoch - 492us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0078 - val_loss: 0.0077 - 561ms/epoch - 526us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0076 - val_loss: 0.0076 - 558ms/epoch - 523us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0075 - val_loss: 0.0074 - 587ms/epoch - 550us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0074 - val_loss: 0.0073 - 533ms/epoch - 500us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0073 - val_loss: 0.0072 - 529ms/epoch - 496us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0071 - 513ms/epoch - 481us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0069 - 548ms/epoch - 514us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0068 - 531ms/epoch - 498us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0067 - 531ms/epoch - 498us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 538ms/epoch - 505us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 542ms/epoch - 508us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0064 - 522ms/epoch - 489us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0063 - 571ms/epoch - 535us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0063 - 590ms/epoch - 553us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0062 - 554ms/epoch - 519us/step\n",
      "267/267 - 0s - loss: 0.0062 - 151ms/epoch - 565us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.5085 - val_loss: 0.5994\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.2268 - val_loss: 0.0643\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0485 - val_loss: 0.0388\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0333 - val_loss: 0.0290\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0259 - val_loss: 0.0233\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0215 - val_loss: 0.0199\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0185 - val_loss: 0.0173\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0162 - val_loss: 0.0152\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0143 - val_loss: 0.0136\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0130 - val_loss: 0.0124\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0120 - val_loss: 0.0116\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0090 - val_loss: 0.0088\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0083 - val_loss: 0.0082 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0081 - val_loss: 0.0080 - 539ms/epoch - 505us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0079 - val_loss: 0.0079 - 548ms/epoch - 513us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0078 - val_loss: 0.0077 - 533ms/epoch - 500us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0076 - val_loss: 0.0076 - 558ms/epoch - 523us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0075 - val_loss: 0.0075 - 582ms/epoch - 546us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0074 - val_loss: 0.0073 - 536ms/epoch - 502us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0072 - val_loss: 0.0072 - 578ms/epoch - 541us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0071 - 531ms/epoch - 498us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0069 - 601ms/epoch - 563us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0068 - 547ms/epoch - 513us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0067 - 543ms/epoch - 509us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 575ms/epoch - 539us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 561ms/epoch - 526us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0064 - 529ms/epoch - 495us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0064 - 532ms/epoch - 498us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0063 - 535ms/epoch - 501us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0062 - 536ms/epoch - 502us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0061 - 560ms/epoch - 525us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0061 - 542ms/epoch - 508us/step\n",
      "267/267 - 0s - loss: 0.0061 - 148ms/epoch - 554us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  42.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "5335/5335 [==============================] - 5s 986us/step - loss: 0.4005 - val_loss: 0.4877\n",
      "Epoch 2/20\n",
      "5335/5335 [==============================] - 4s 692us/step - loss: 0.3907 - val_loss: 0.2727\n",
      "Epoch 3/20\n",
      "5335/5335 [==============================] - 3s 640us/step - loss: 0.4056 - val_loss: 0.1936\n",
      "Epoch 4/20\n",
      "5335/5335 [==============================] - 4s 707us/step - loss: 0.3558 - val_loss: 0.3037\n",
      "Epoch 5/20\n",
      "5335/5335 [==============================] - 3s 637us/step - loss: 0.3592 - val_loss: 0.2566\n",
      "Epoch 6/20\n",
      "5335/5335 [==============================] - 4s 663us/step - loss: 0.3625 - val_loss: 0.1606\n",
      "Epoch 7/20\n",
      "5335/5335 [==============================] - 4s 668us/step - loss: 0.3153 - val_loss: 0.2606\n",
      "Epoch 8/20\n",
      "5335/5335 [==============================] - 4s 677us/step - loss: 0.3039 - val_loss: 0.1889\n",
      "Epoch 9/20\n",
      "5335/5335 [==============================] - 3s 615us/step - loss: 0.3267 - val_loss: 0.3785\n",
      "Epoch 10/20\n",
      "5335/5335 [==============================] - 3s 623us/step - loss: 0.3225 - val_loss: 0.3563\n",
      "Epoch 11/20\n",
      "5335/5335 [==============================] - 3s 627us/step - loss: 0.3148 - val_loss: 0.8553\n",
      "Epoch 12/20\n",
      "5335/5335 [==============================] - 3s 652us/step - loss: 0.3146 - val_loss: 0.1783\n",
      "Epoch 13/20\n",
      "5335/5335 [==============================] - 3s 600us/step - loss: 0.2923 - val_loss: 0.4070\n",
      "Epoch 14/20\n",
      "5335/5335 [==============================] - 3s 589us/step - loss: 0.3099 - val_loss: 0.1591\n",
      "Epoch 15/20\n",
      "5335/5335 [==============================] - 3s 598us/step - loss: 0.2717 - val_loss: 0.2572\n",
      "Epoch 16/20\n",
      "5335/5335 [==============================] - 3s 589us/step - loss: 0.3134 - val_loss: 0.8471\n",
      "Epoch 17/20\n",
      "5335/5335 [==============================] - 3s 600us/step - loss: 0.3103 - val_loss: 0.3732\n",
      "Epoch 18/20\n",
      "5335/5335 [==============================] - 3s 593us/step - loss: 0.2833 - val_loss: 0.3130\n",
      "Epoch 19/20\n",
      "5335/5335 [==============================] - 3s 591us/step - loss: 0.2846 - val_loss: 0.1620\n",
      "Epoch 20/20\n",
      "5335/5335 [==============================] - 3s 586us/step - loss: 0.2719 - val_loss: 0.4311\n",
      "Epoch 1/20\n",
      "4268/4268 - 4s - loss: 0.2923 - val_loss: 0.1082 - 4s/epoch - 930us/step\n",
      "Epoch 2/20\n",
      "4268/4268 - 2s - loss: 0.3030 - val_loss: 0.2379 - 2s/epoch - 546us/step\n",
      "Epoch 3/20\n",
      "4268/4268 - 2s - loss: 0.3199 - val_loss: 0.2700 - 2s/epoch - 538us/step\n",
      "Epoch 4/20\n",
      "4268/4268 - 2s - loss: 0.2692 - val_loss: 0.1076 - 2s/epoch - 547us/step\n",
      "Epoch 5/20\n",
      "4268/4268 - 2s - loss: 0.2713 - val_loss: 0.2326 - 2s/epoch - 542us/step\n",
      "Epoch 6/20\n",
      "4268/4268 - 2s - loss: 0.2786 - val_loss: 0.1608 - 2s/epoch - 547us/step\n",
      "Epoch 7/20\n",
      "4268/4268 - 2s - loss: 0.2818 - val_loss: 0.2229 - 2s/epoch - 541us/step\n",
      "Epoch 8/20\n",
      "4268/4268 - 2s - loss: 0.2794 - val_loss: 0.2436 - 2s/epoch - 548us/step\n",
      "Epoch 9/20\n",
      "4268/4268 - 2s - loss: 0.2709 - val_loss: 0.3953 - 2s/epoch - 543us/step\n",
      "Epoch 10/20\n",
      "4268/4268 - 2s - loss: 0.2976 - val_loss: 0.1445 - 2s/epoch - 546us/step\n",
      "Epoch 11/20\n",
      "4268/4268 - 2s - loss: 0.2956 - val_loss: 0.2585 - 2s/epoch - 542us/step\n",
      "Epoch 12/20\n",
      "4268/4268 - 2s - loss: 0.2782 - val_loss: 0.3614 - 2s/epoch - 548us/step\n",
      "Epoch 13/20\n",
      "4268/4268 - 2s - loss: 0.2812 - val_loss: 0.2465 - 2s/epoch - 541us/step\n",
      "Epoch 14/20\n",
      "4268/4268 - 2s - loss: 0.2851 - val_loss: 0.3156 - 2s/epoch - 548us/step\n",
      "Epoch 15/20\n",
      "4268/4268 - 2s - loss: 0.2603 - val_loss: 0.1982 - 2s/epoch - 541us/step\n",
      "Epoch 16/20\n",
      "4268/4268 - 2s - loss: 0.2829 - val_loss: 0.1604 - 2s/epoch - 550us/step\n",
      "Epoch 17/20\n",
      "4268/4268 - 3s - loss: 0.2984 - val_loss: 0.2250 - 3s/epoch - 601us/step\n",
      "Epoch 18/20\n",
      "4268/4268 - 2s - loss: 0.2943 - val_loss: 0.1350 - 2s/epoch - 558us/step\n",
      "Epoch 19/20\n",
      "4268/4268 - 2s - loss: 0.2852 - val_loss: 0.2603 - 2s/epoch - 548us/step\n",
      "Epoch 20/20\n",
      "4268/4268 - 2s - loss: 0.2901 - val_loss: 0.1844 - 2s/epoch - 558us/step\n",
      "1067/1067 - 0s - loss: 0.1511 - 370ms/epoch - 347us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.2min\n",
      "Epoch 1/20\n",
      "5335/5335 [==============================] - 5s 920us/step - loss: 0.3435 - val_loss: 0.2719\n",
      "Epoch 2/20\n",
      "5335/5335 [==============================] - 3s 594us/step - loss: 0.3996 - val_loss: 0.6741\n",
      "Epoch 3/20\n",
      "5335/5335 [==============================] - 3s 612us/step - loss: 0.3244 - val_loss: 0.2444\n",
      "Epoch 4/20\n",
      "5335/5335 [==============================] - 3s 656us/step - loss: 0.3090 - val_loss: 0.2177\n",
      "Epoch 5/20\n",
      "5335/5335 [==============================] - 4s 666us/step - loss: 0.3321 - val_loss: 0.7220\n",
      "Epoch 6/20\n",
      "5335/5335 [==============================] - 4s 672us/step - loss: 0.2835 - val_loss: 0.2862\n",
      "Epoch 7/20\n",
      "5335/5335 [==============================] - 4s 680us/step - loss: 0.3294 - val_loss: 0.2477\n",
      "Epoch 8/20\n",
      "5335/5335 [==============================] - 3s 640us/step - loss: 0.3083 - val_loss: 0.2419\n",
      "Epoch 9/20\n",
      "5335/5335 [==============================] - 3s 620us/step - loss: 0.2785 - val_loss: 0.1681\n",
      "Epoch 10/20\n",
      "5335/5335 [==============================] - 4s 680us/step - loss: 0.2948 - val_loss: 0.1577\n",
      "Epoch 11/20\n",
      "5335/5335 [==============================] - 3s 631us/step - loss: 0.2737 - val_loss: 0.2978\n",
      "Epoch 12/20\n",
      "5335/5335 [==============================] - 3s 617us/step - loss: 0.2894 - val_loss: 0.3878\n",
      "Epoch 13/20\n",
      "5335/5335 [==============================] - 3s 639us/step - loss: 0.2932 - val_loss: 0.1863\n",
      "Epoch 14/20\n",
      "5335/5335 [==============================] - 3s 620us/step - loss: 0.3262 - val_loss: 0.1987\n",
      "Epoch 15/20\n",
      "5335/5335 [==============================] - 3s 648us/step - loss: 0.3216 - val_loss: 0.2618\n",
      "Epoch 16/20\n",
      "5335/5335 [==============================] - 3s 609us/step - loss: 0.3092 - val_loss: 0.1387\n",
      "Epoch 17/20\n",
      "5335/5335 [==============================] - 4s 660us/step - loss: 0.3006 - val_loss: 0.1668\n",
      "Epoch 18/20\n",
      "5335/5335 [==============================] - 4s 657us/step - loss: 0.3723 - val_loss: 0.2153\n",
      "Epoch 19/20\n",
      "5335/5335 [==============================] - 3s 608us/step - loss: 0.3583 - val_loss: 0.2665\n",
      "Epoch 20/20\n",
      "5335/5335 [==============================] - 3s 614us/step - loss: 0.3411 - val_loss: 0.1850\n",
      "Epoch 1/20\n",
      "4268/4268 - 4s - loss: 0.3337 - val_loss: 0.4036 - 4s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "4268/4268 - 2s - loss: 0.3155 - val_loss: 0.3267 - 2s/epoch - 581us/step\n",
      "Epoch 3/20\n",
      "4268/4268 - 3s - loss: 0.2727 - val_loss: 0.2967 - 3s/epoch - 619us/step\n",
      "Epoch 4/20\n",
      "4268/4268 - 3s - loss: 0.2922 - val_loss: 0.2640 - 3s/epoch - 602us/step\n",
      "Epoch 5/20\n",
      "4268/4268 - 3s - loss: 0.3152 - val_loss: 0.3445 - 3s/epoch - 604us/step\n",
      "Epoch 6/20\n",
      "4268/4268 - 3s - loss: 0.2671 - val_loss: 0.2319 - 3s/epoch - 601us/step\n",
      "Epoch 7/20\n",
      "4268/4268 - 3s - loss: 0.2940 - val_loss: 0.1428 - 3s/epoch - 609us/step\n",
      "Epoch 8/20\n",
      "4268/4268 - 3s - loss: 0.3067 - val_loss: 0.4697 - 3s/epoch - 587us/step\n",
      "Epoch 9/20\n",
      "4268/4268 - 2s - loss: 0.3084 - val_loss: 0.1915 - 2s/epoch - 557us/step\n",
      "Epoch 10/20\n",
      "4268/4268 - 2s - loss: 0.3258 - val_loss: 0.5209 - 2s/epoch - 556us/step\n",
      "Epoch 11/20\n",
      "4268/4268 - 3s - loss: 0.3025 - val_loss: 0.5993 - 3s/epoch - 586us/step\n",
      "Epoch 12/20\n",
      "4268/4268 - 2s - loss: 0.2753 - val_loss: 0.6049 - 2s/epoch - 555us/step\n",
      "Epoch 13/20\n",
      "4268/4268 - 2s - loss: 0.2883 - val_loss: 0.1786 - 2s/epoch - 582us/step\n",
      "Epoch 14/20\n",
      "4268/4268 - 3s - loss: 0.3224 - val_loss: 0.2990 - 3s/epoch - 590us/step\n",
      "Epoch 15/20\n",
      "4268/4268 - 3s - loss: 0.2801 - val_loss: 0.1543 - 3s/epoch - 616us/step\n",
      "Epoch 16/20\n",
      "4268/4268 - 3s - loss: 0.2870 - val_loss: 0.0802 - 3s/epoch - 620us/step\n",
      "Epoch 17/20\n",
      "4268/4268 - 2s - loss: 0.2768 - val_loss: 0.4077 - 2s/epoch - 562us/step\n",
      "Epoch 18/20\n",
      "4268/4268 - 2s - loss: 0.3245 - val_loss: 0.1934 - 2s/epoch - 568us/step\n",
      "Epoch 19/20\n",
      "4268/4268 - 3s - loss: 0.2898 - val_loss: 0.5856 - 3s/epoch - 598us/step\n",
      "Epoch 20/20\n",
      "4268/4268 - 2s - loss: 0.3192 - val_loss: 0.1794 - 2s/epoch - 566us/step\n",
      "1067/1067 - 0s - loss: 0.1439 - 348ms/epoch - 326us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.2min\n",
      "Epoch 1/20\n",
      "5335/5335 [==============================] - 5s 992us/step - loss: 0.3624 - val_loss: 0.3494\n",
      "Epoch 2/20\n",
      "5335/5335 [==============================] - 4s 659us/step - loss: 0.3385 - val_loss: 1.1616\n",
      "Epoch 3/20\n",
      "5335/5335 [==============================] - 3s 649us/step - loss: 0.3422 - val_loss: 0.1369\n",
      "Epoch 4/20\n",
      "5335/5335 [==============================] - 3s 625us/step - loss: 0.3148 - val_loss: 0.4281\n",
      "Epoch 5/20\n",
      "5335/5335 [==============================] - 3s 653us/step - loss: 0.3153 - val_loss: 0.2246\n",
      "Epoch 6/20\n",
      "5335/5335 [==============================] - 3s 656us/step - loss: 0.3060 - val_loss: 0.1709\n",
      "Epoch 7/20\n",
      "5335/5335 [==============================] - 3s 650us/step - loss: 0.2922 - val_loss: 0.3635\n",
      "Epoch 8/20\n",
      "5335/5335 [==============================] - 3s 653us/step - loss: 0.2930 - val_loss: 0.1638\n",
      "Epoch 9/20\n",
      "5335/5335 [==============================] - 4s 707us/step - loss: 0.3476 - val_loss: 0.2073\n",
      "Epoch 10/20\n",
      "5335/5335 [==============================] - 3s 643us/step - loss: 0.3005 - val_loss: 0.8481\n",
      "Epoch 11/20\n",
      "5335/5335 [==============================] - 3s 641us/step - loss: 0.3203 - val_loss: 0.2576\n",
      "Epoch 12/20\n",
      "5335/5335 [==============================] - 3s 654us/step - loss: 0.3329 - val_loss: 0.3266\n",
      "Epoch 13/20\n",
      "5335/5335 [==============================] - 3s 642us/step - loss: 0.2988 - val_loss: 0.3788\n",
      "Epoch 14/20\n",
      "5335/5335 [==============================] - 3s 635us/step - loss: 0.2933 - val_loss: 0.1421\n",
      "Epoch 15/20\n",
      "5335/5335 [==============================] - 3s 647us/step - loss: 0.2769 - val_loss: 0.2699\n",
      "Epoch 16/20\n",
      "5335/5335 [==============================] - 3s 655us/step - loss: 0.2919 - val_loss: 0.3496\n",
      "Epoch 17/20\n",
      "5335/5335 [==============================] - 4s 675us/step - loss: 0.2943 - val_loss: 0.1545\n",
      "Epoch 18/20\n",
      "5335/5335 [==============================] - 4s 668us/step - loss: 0.2986 - val_loss: 0.5638\n",
      "Epoch 19/20\n",
      "5335/5335 [==============================] - 3s 656us/step - loss: 0.3022 - val_loss: 0.2403\n",
      "Epoch 20/20\n",
      "5335/5335 [==============================] - 4s 659us/step - loss: 0.3223 - val_loss: 0.2180\n",
      "Epoch 1/20\n",
      "4268/4268 - 4s - loss: 0.3247 - val_loss: 0.5128 - 4s/epoch - 976us/step\n",
      "Epoch 2/20\n",
      "4268/4268 - 2s - loss: 0.3291 - val_loss: 0.3462 - 2s/epoch - 561us/step\n",
      "Epoch 3/20\n",
      "4268/4268 - 2s - loss: 0.3360 - val_loss: 0.5357 - 2s/epoch - 564us/step\n",
      "Epoch 4/20\n",
      "4268/4268 - 3s - loss: 0.3107 - val_loss: 0.1196 - 3s/epoch - 601us/step\n",
      "Epoch 5/20\n",
      "4268/4268 - 3s - loss: 0.3106 - val_loss: 0.1413 - 3s/epoch - 598us/step\n",
      "Epoch 6/20\n",
      "4268/4268 - 3s - loss: 0.2635 - val_loss: 0.1533 - 3s/epoch - 602us/step\n",
      "Epoch 7/20\n",
      "4268/4268 - 3s - loss: 0.2664 - val_loss: 0.3573 - 3s/epoch - 608us/step\n",
      "Epoch 8/20\n",
      "4268/4268 - 3s - loss: 0.2640 - val_loss: 0.2442 - 3s/epoch - 611us/step\n",
      "Epoch 9/20\n",
      "4268/4268 - 3s - loss: 0.3126 - val_loss: 0.4511 - 3s/epoch - 613us/step\n",
      "Epoch 10/20\n",
      "4268/4268 - 3s - loss: 0.3535 - val_loss: 0.2909 - 3s/epoch - 588us/step\n",
      "Epoch 11/20\n",
      "4268/4268 - 3s - loss: 0.3088 - val_loss: 0.3252 - 3s/epoch - 604us/step\n",
      "Epoch 12/20\n",
      "4268/4268 - 3s - loss: 0.3381 - val_loss: 0.2192 - 3s/epoch - 609us/step\n",
      "Epoch 13/20\n",
      "4268/4268 - 3s - loss: 0.2832 - val_loss: 0.1632 - 3s/epoch - 590us/step\n",
      "Epoch 14/20\n",
      "4268/4268 - 3s - loss: 0.3159 - val_loss: 0.3555 - 3s/epoch - 599us/step\n",
      "Epoch 15/20\n",
      "4268/4268 - 3s - loss: 0.3068 - val_loss: 0.1764 - 3s/epoch - 622us/step\n",
      "Epoch 16/20\n",
      "4268/4268 - 3s - loss: 0.2731 - val_loss: 0.3905 - 3s/epoch - 593us/step\n",
      "Epoch 17/20\n",
      "4268/4268 - 3s - loss: 0.3229 - val_loss: 0.1829 - 3s/epoch - 600us/step\n",
      "Epoch 18/20\n",
      "4268/4268 - 3s - loss: 0.2846 - val_loss: 0.4096 - 3s/epoch - 588us/step\n",
      "Epoch 19/20\n",
      "4268/4268 - 3s - loss: 0.2863 - val_loss: 0.1137 - 3s/epoch - 608us/step\n",
      "Epoch 20/20\n",
      "4268/4268 - 3s - loss: 0.3283 - val_loss: 0.2824 - 3s/epoch - 617us/step\n",
      "1067/1067 - 0s - loss: 0.2749 - 400ms/epoch - 375us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Epoch 1/20\n",
      "5335/5335 [==============================] - 5s 943us/step - loss: 0.3085 - val_loss: 0.7397\n",
      "Epoch 2/20\n",
      "5335/5335 [==============================] - 3s 630us/step - loss: 0.3866 - val_loss: 0.3753\n",
      "Epoch 3/20\n",
      "5335/5335 [==============================] - 3s 640us/step - loss: 0.3286 - val_loss: 0.1761\n",
      "Epoch 4/20\n",
      "5335/5335 [==============================] - 3s 630us/step - loss: 0.3131 - val_loss: 0.2437\n",
      "Epoch 5/20\n",
      "5335/5335 [==============================] - 3s 639us/step - loss: 0.2850 - val_loss: 0.3594\n",
      "Epoch 6/20\n",
      "5335/5335 [==============================] - 3s 638us/step - loss: 0.2968 - val_loss: 0.1682\n",
      "Epoch 7/20\n",
      "5335/5335 [==============================] - 3s 600us/step - loss: 0.2956 - val_loss: 0.7119\n",
      "Epoch 8/20\n",
      "5335/5335 [==============================] - 3s 605us/step - loss: 0.3179 - val_loss: 0.1702\n",
      "Epoch 9/20\n",
      "5335/5335 [==============================] - 3s 601us/step - loss: 0.3067 - val_loss: 0.1775\n",
      "Epoch 10/20\n",
      "5335/5335 [==============================] - 3s 607us/step - loss: 0.3178 - val_loss: 0.1899\n",
      "Epoch 11/20\n",
      "5335/5335 [==============================] - 3s 628us/step - loss: 0.2859 - val_loss: 0.2505\n",
      "Epoch 12/20\n",
      "5335/5335 [==============================] - 3s 645us/step - loss: 0.3046 - val_loss: 0.1672\n",
      "Epoch 13/20\n",
      "5335/5335 [==============================] - 4s 680us/step - loss: 0.2887 - val_loss: 0.1371\n",
      "Epoch 14/20\n",
      "5335/5335 [==============================] - 4s 658us/step - loss: 0.3302 - val_loss: 0.1845\n",
      "Epoch 15/20\n",
      "5335/5335 [==============================] - 3s 651us/step - loss: 0.2662 - val_loss: 0.4325\n",
      "Epoch 16/20\n",
      "5335/5335 [==============================] - 3s 587us/step - loss: 0.3177 - val_loss: 0.3856\n",
      "Epoch 17/20\n",
      "5335/5335 [==============================] - 4s 666us/step - loss: 0.2955 - val_loss: 0.4129\n",
      "Epoch 18/20\n",
      "5335/5335 [==============================] - 4s 660us/step - loss: 0.2948 - val_loss: 0.2380\n",
      "Epoch 19/20\n",
      "5335/5335 [==============================] - 4s 671us/step - loss: 0.3123 - val_loss: 0.4232\n",
      "Epoch 20/20\n",
      "5335/5335 [==============================] - 3s 637us/step - loss: 0.2639 - val_loss: 0.6904\n",
      "Epoch 1/20\n",
      "4268/4268 - 4s - loss: 0.3238 - val_loss: 0.6521 - 4s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "4268/4268 - 2s - loss: 0.3103 - val_loss: 0.3719 - 2s/epoch - 551us/step\n",
      "Epoch 3/20\n",
      "4268/4268 - 2s - loss: 0.3129 - val_loss: 0.2393 - 2s/epoch - 546us/step\n",
      "Epoch 4/20\n",
      "4268/4268 - 2s - loss: 0.3055 - val_loss: 0.2098 - 2s/epoch - 567us/step\n",
      "Epoch 5/20\n",
      "4268/4268 - 2s - loss: 0.2832 - val_loss: 0.1885 - 2s/epoch - 572us/step\n",
      "Epoch 6/20\n",
      "4268/4268 - 2s - loss: 0.3052 - val_loss: 0.1868 - 2s/epoch - 584us/step\n",
      "Epoch 7/20\n",
      "4268/4268 - 2s - loss: 0.2868 - val_loss: 0.3806 - 2s/epoch - 579us/step\n",
      "Epoch 8/20\n",
      "4268/4268 - 2s - loss: 0.3326 - val_loss: 0.1579 - 2s/epoch - 570us/step\n",
      "Epoch 9/20\n",
      "4268/4268 - 2s - loss: 0.3030 - val_loss: 1.1720 - 2s/epoch - 566us/step\n",
      "Epoch 10/20\n",
      "4268/4268 - 3s - loss: 0.2958 - val_loss: 0.3906 - 3s/epoch - 589us/step\n",
      "Epoch 11/20\n",
      "4268/4268 - 3s - loss: 0.3159 - val_loss: 0.3337 - 3s/epoch - 597us/step\n",
      "Epoch 12/20\n",
      "4268/4268 - 3s - loss: 0.3193 - val_loss: 0.4202 - 3s/epoch - 588us/step\n",
      "Epoch 13/20\n",
      "4268/4268 - 3s - loss: 0.3079 - val_loss: 0.0989 - 3s/epoch - 588us/step\n",
      "Epoch 14/20\n",
      "4268/4268 - 3s - loss: 0.2727 - val_loss: 0.1878 - 3s/epoch - 647us/step\n",
      "Epoch 15/20\n",
      "4268/4268 - 3s - loss: 0.3323 - val_loss: 0.2155 - 3s/epoch - 639us/step\n",
      "Epoch 16/20\n",
      "4268/4268 - 3s - loss: 0.2928 - val_loss: 0.1893 - 3s/epoch - 593us/step\n",
      "Epoch 17/20\n",
      "4268/4268 - 2s - loss: 0.3050 - val_loss: 0.4396 - 2s/epoch - 544us/step\n",
      "Epoch 18/20\n",
      "4268/4268 - 2s - loss: 0.2687 - val_loss: 0.3149 - 2s/epoch - 579us/step\n",
      "Epoch 19/20\n",
      "4268/4268 - 3s - loss: 0.2813 - val_loss: 0.4077 - 3s/epoch - 600us/step\n",
      "Epoch 20/20\n",
      "4268/4268 - 2s - loss: 0.3293 - val_loss: 0.3834 - 2s/epoch - 555us/step\n",
      "1067/1067 - 0s - loss: 0.3108 - 374ms/epoch - 351us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.2min\n",
      "Epoch 1/20\n",
      "5335/5335 [==============================] - 6s 1ms/step - loss: 0.3665 - val_loss: 0.6210\n",
      "Epoch 2/20\n",
      "5335/5335 [==============================] - 4s 670us/step - loss: 0.3598 - val_loss: 0.2912\n",
      "Epoch 3/20\n",
      "5335/5335 [==============================] - 3s 636us/step - loss: 0.3641 - val_loss: 0.1955\n",
      "Epoch 4/20\n",
      "5335/5335 [==============================] - 3s 647us/step - loss: 0.3285 - val_loss: 0.2718\n",
      "Epoch 5/20\n",
      "5335/5335 [==============================] - 3s 646us/step - loss: 0.3267 - val_loss: 0.2945\n",
      "Epoch 6/20\n",
      "5335/5335 [==============================] - 3s 636us/step - loss: 0.3402 - val_loss: 0.2478\n",
      "Epoch 7/20\n",
      "5335/5335 [==============================] - 3s 638us/step - loss: 0.3152 - val_loss: 0.2214\n",
      "Epoch 8/20\n",
      "5335/5335 [==============================] - 3s 585us/step - loss: 0.3056 - val_loss: 0.3033\n",
      "Epoch 9/20\n",
      "5335/5335 [==============================] - 3s 618us/step - loss: 0.3024 - val_loss: 0.1813\n",
      "Epoch 10/20\n",
      "5335/5335 [==============================] - 3s 641us/step - loss: 0.3423 - val_loss: 0.3030\n",
      "Epoch 11/20\n",
      "5335/5335 [==============================] - 4s 737us/step - loss: 0.2775 - val_loss: 0.2643\n",
      "Epoch 12/20\n",
      "5335/5335 [==============================] - 3s 624us/step - loss: 0.3240 - val_loss: 0.3432\n",
      "Epoch 13/20\n",
      "5335/5335 [==============================] - 3s 602us/step - loss: 0.2870 - val_loss: 0.3586\n",
      "Epoch 14/20\n",
      "5335/5335 [==============================] - 3s 594us/step - loss: 0.2956 - val_loss: 0.1611\n",
      "Epoch 15/20\n",
      "5335/5335 [==============================] - 3s 588us/step - loss: 0.2874 - val_loss: 0.1961\n",
      "Epoch 16/20\n",
      "5335/5335 [==============================] - 3s 592us/step - loss: 0.3379 - val_loss: 0.4310\n",
      "Epoch 17/20\n",
      "5335/5335 [==============================] - 3s 587us/step - loss: 0.3471 - val_loss: 0.3853\n",
      "Epoch 18/20\n",
      "5335/5335 [==============================] - 3s 589us/step - loss: 0.3006 - val_loss: 0.2111\n",
      "Epoch 19/20\n",
      "5335/5335 [==============================] - 3s 588us/step - loss: 0.3085 - val_loss: 0.2547\n",
      "Epoch 20/20\n",
      "5335/5335 [==============================] - 3s 597us/step - loss: 0.2877 - val_loss: 0.2782\n",
      "Epoch 1/20\n",
      "4268/4268 - 4s - loss: 0.2883 - val_loss: 1.3555 - 4s/epoch - 931us/step\n",
      "Epoch 2/20\n",
      "4268/4268 - 2s - loss: 0.2925 - val_loss: 0.1833 - 2s/epoch - 548us/step\n",
      "Epoch 3/20\n",
      "4268/4268 - 2s - loss: 0.2965 - val_loss: 0.5194 - 2s/epoch - 541us/step\n",
      "Epoch 4/20\n",
      "4268/4268 - 2s - loss: 0.2942 - val_loss: 0.3170 - 2s/epoch - 547us/step\n",
      "Epoch 5/20\n",
      "4268/4268 - 2s - loss: 0.2838 - val_loss: 0.1201 - 2s/epoch - 538us/step\n",
      "Epoch 6/20\n",
      "4268/4268 - 2s - loss: 0.2870 - val_loss: 0.1612 - 2s/epoch - 547us/step\n",
      "Epoch 7/20\n",
      "4268/4268 - 2s - loss: 0.3019 - val_loss: 0.1755 - 2s/epoch - 542us/step\n",
      "Epoch 8/20\n",
      "4268/4268 - 2s - loss: 0.3118 - val_loss: 0.2166 - 2s/epoch - 548us/step\n",
      "Epoch 9/20\n",
      "4268/4268 - 2s - loss: 0.3265 - val_loss: 0.2009 - 2s/epoch - 551us/step\n",
      "Epoch 10/20\n",
      "4268/4268 - 3s - loss: 0.3122 - val_loss: 0.2392 - 3s/epoch - 632us/step\n",
      "Epoch 11/20\n",
      "4268/4268 - 2s - loss: 0.2976 - val_loss: 0.1720 - 2s/epoch - 542us/step\n",
      "Epoch 12/20\n",
      "4268/4268 - 2s - loss: 0.2987 - val_loss: 0.1505 - 2s/epoch - 550us/step\n",
      "Epoch 13/20\n",
      "4268/4268 - 2s - loss: 0.3179 - val_loss: 0.3772 - 2s/epoch - 564us/step\n",
      "Epoch 14/20\n",
      "4268/4268 - 2s - loss: 0.2815 - val_loss: 0.1520 - 2s/epoch - 546us/step\n",
      "Epoch 15/20\n",
      "4268/4268 - 2s - loss: 0.3236 - val_loss: 0.3671 - 2s/epoch - 541us/step\n",
      "Epoch 16/20\n",
      "4268/4268 - 2s - loss: 0.2958 - val_loss: 0.2501 - 2s/epoch - 552us/step\n",
      "Epoch 17/20\n",
      "4268/4268 - 2s - loss: 0.2922 - val_loss: 0.2501 - 2s/epoch - 572us/step\n",
      "Epoch 18/20\n",
      "4268/4268 - 2s - loss: 0.2897 - val_loss: 0.3036 - 2s/epoch - 547us/step\n",
      "Epoch 19/20\n",
      "4268/4268 - 2s - loss: 0.2948 - val_loss: 0.3241 - 2s/epoch - 543us/step\n",
      "Epoch 20/20\n",
      "4268/4268 - 2s - loss: 0.3115 - val_loss: 0.1729 - 2s/epoch - 549us/step\n",
      "1067/1067 - 0s - loss: 0.1395 - 372ms/epoch - 348us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.2min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.1187 - val_loss: 0.1549\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 597us/step - loss: 0.1864 - val_loss: 0.5310\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 610us/step - loss: 0.2239 - val_loss: 0.0905\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 607us/step - loss: 0.1840 - val_loss: 0.1115\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 604us/step - loss: 0.1892 - val_loss: 0.1608\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 607us/step - loss: 0.2635 - val_loss: 0.1257\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 609us/step - loss: 0.1989 - val_loss: 0.0881\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 594us/step - loss: 0.2338 - val_loss: 0.2525\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 608us/step - loss: 0.2116 - val_loss: 0.0985\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 608us/step - loss: 0.2115 - val_loss: 0.2998\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 606us/step - loss: 0.2367 - val_loss: 0.1263\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 610us/step - loss: 0.1835 - val_loss: 0.1685\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 607us/step - loss: 0.1808 - val_loss: 0.1290\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 604us/step - loss: 0.2134 - val_loss: 0.3156\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 605us/step - loss: 0.2176 - val_loss: 0.1826\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 607us/step - loss: 0.1870 - val_loss: 0.3084\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 609us/step - loss: 0.2025 - val_loss: 0.2501\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 617us/step - loss: 0.1855 - val_loss: 0.1580\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 653us/step - loss: 0.1899 - val_loss: 0.1380\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 690us/step - loss: 0.1839 - val_loss: 0.2392\n",
      "Epoch 1/20\n",
      "2134/2134 - 3s - loss: 0.2202 - val_loss: 0.1128 - 3s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 0.1854 - val_loss: 0.1403 - 1s/epoch - 571us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 0.1849 - val_loss: 0.1525 - 1s/epoch - 583us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 0.1909 - val_loss: 0.4531 - 1s/epoch - 653us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.1988 - val_loss: 0.1372 - 1s/epoch - 660us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 0.1944 - val_loss: 0.1283 - 1s/epoch - 624us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 0.1768 - val_loss: 0.2256 - 1s/epoch - 598us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 0.2111 - val_loss: 0.1859 - 1s/epoch - 566us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 0.1918 - val_loss: 0.0903 - 1s/epoch - 579us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.1899 - val_loss: 0.1244 - 1s/epoch - 595us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 0.1904 - val_loss: 0.2594 - 1s/epoch - 572us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 0.1866 - val_loss: 0.0768 - 1s/epoch - 559us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 0.1833 - val_loss: 0.1053 - 1s/epoch - 566us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 0.1803 - val_loss: 0.3934 - 1s/epoch - 590us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 0.1886 - val_loss: 0.1814 - 1s/epoch - 592us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 0.2126 - val_loss: 0.5430 - 1s/epoch - 648us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 0.1807 - val_loss: 0.0721 - 1s/epoch - 610us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 0.1750 - val_loss: 0.2105 - 1s/epoch - 611us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.2244 - val_loss: 0.1456 - 1s/epoch - 613us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 0.1880 - val_loss: 0.0887 - 1s/epoch - 637us/step\n",
      "534/534 - 0s - loss: 0.0769 - 240ms/epoch - 450us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.2min\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.1568 - val_loss: 0.1649\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 664us/step - loss: 0.1696 - val_loss: 0.2586\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 691us/step - loss: 0.1822 - val_loss: 0.1732\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 667us/step - loss: 0.1973 - val_loss: 0.4681\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 665us/step - loss: 0.2675 - val_loss: 0.2064\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 695us/step - loss: 0.1695 - val_loss: 0.1206\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 740us/step - loss: 0.2643 - val_loss: 0.1386\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 681us/step - loss: 0.2138 - val_loss: 0.2943\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 638us/step - loss: 0.1918 - val_loss: 0.4226\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 671us/step - loss: 0.2111 - val_loss: 0.1579\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 691us/step - loss: 0.2136 - val_loss: 0.1740\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 695us/step - loss: 0.1766 - val_loss: 0.2957\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 682us/step - loss: 0.1753 - val_loss: 0.0755\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 676us/step - loss: 0.1583 - val_loss: 0.0799\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 679us/step - loss: 0.1995 - val_loss: 0.1795\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 702us/step - loss: 0.2018 - val_loss: 0.1393\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 663us/step - loss: 0.2149 - val_loss: 0.1026\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 689us/step - loss: 0.1584 - val_loss: 0.4371\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 688us/step - loss: 0.1984 - val_loss: 0.2166\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 651us/step - loss: 0.1826 - val_loss: 0.0926\n",
      "Epoch 1/20\n",
      "2134/2134 - 3s - loss: 0.2017 - val_loss: 0.3733 - 3s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 0.2320 - val_loss: 0.1700 - 1s/epoch - 623us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 0.1996 - val_loss: 0.5448 - 1s/epoch - 624us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 0.2079 - val_loss: 0.1320 - 1s/epoch - 634us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.2009 - val_loss: 0.1824 - 1s/epoch - 594us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 0.1836 - val_loss: 0.1227 - 1s/epoch - 594us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 0.1717 - val_loss: 0.1587 - 1s/epoch - 603us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 0.1740 - val_loss: 0.0981 - 1s/epoch - 637us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 0.1837 - val_loss: 0.5076 - 1s/epoch - 635us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.2193 - val_loss: 0.1747 - 1s/epoch - 604us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 0.1914 - val_loss: 0.1114 - 1s/epoch - 621us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 0.1818 - val_loss: 0.2113 - 1s/epoch - 616us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 0.1972 - val_loss: 0.2049 - 1s/epoch - 618us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 0.1956 - val_loss: 0.1317 - 1s/epoch - 605us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 0.2015 - val_loss: 0.1544 - 1s/epoch - 629us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 0.1658 - val_loss: 0.1705 - 1s/epoch - 668us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 0.1995 - val_loss: 0.0882 - 1s/epoch - 585us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 0.2071 - val_loss: 0.2316 - 1s/epoch - 600us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.1750 - val_loss: 0.0744 - 1s/epoch - 630us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 0.1776 - val_loss: 0.2050 - 1s/epoch - 608us/step\n",
      "534/534 - 0s - loss: 0.1656 - 245ms/epoch - 459us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0983 - val_loss: 0.0705\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 660us/step - loss: 0.2648 - val_loss: 0.1412\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 678us/step - loss: 0.1912 - val_loss: 0.1512\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 829us/step - loss: 0.2046 - val_loss: 0.4180\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 801us/step - loss: 0.2307 - val_loss: 0.0683\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 626us/step - loss: 0.1917 - val_loss: 0.4732\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 667us/step - loss: 0.2071 - val_loss: 0.3920\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 667us/step - loss: 0.2177 - val_loss: 0.2684\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 652us/step - loss: 0.1914 - val_loss: 0.1955\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 738us/step - loss: 0.1789 - val_loss: 0.1275\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 640us/step - loss: 0.1766 - val_loss: 0.2051\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 670us/step - loss: 0.2040 - val_loss: 0.1557\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 681us/step - loss: 0.1711 - val_loss: 0.1302\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 646us/step - loss: 0.1831 - val_loss: 0.1462\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 662us/step - loss: 0.2067 - val_loss: 0.1685\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 636us/step - loss: 0.2219 - val_loss: 0.2883\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 669us/step - loss: 0.1848 - val_loss: 0.3990\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 676us/step - loss: 0.2180 - val_loss: 0.3372\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 647us/step - loss: 0.1967 - val_loss: 0.0835\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 681us/step - loss: 0.2043 - val_loss: 0.3934\n",
      "Epoch 1/20\n",
      "2134/2134 - 3s - loss: 0.1981 - val_loss: 0.1894 - 3s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 0.2164 - val_loss: 0.1288 - 1s/epoch - 591us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 0.2141 - val_loss: 0.2802 - 1s/epoch - 587us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 0.1773 - val_loss: 0.2478 - 1s/epoch - 623us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.1567 - val_loss: 0.1050 - 1s/epoch - 610us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 2s - loss: 0.1742 - val_loss: 0.1400 - 2s/epoch - 746us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 0.1640 - val_loss: 0.0758 - 1s/epoch - 666us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 0.2090 - val_loss: 0.1443 - 1s/epoch - 605us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 0.1760 - val_loss: 0.0917 - 1s/epoch - 621us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.1998 - val_loss: 0.1474 - 1s/epoch - 612us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 0.2188 - val_loss: 0.1501 - 1s/epoch - 610us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 0.1774 - val_loss: 0.2087 - 1s/epoch - 633us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 0.1999 - val_loss: 0.2847 - 1s/epoch - 559us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 0.2139 - val_loss: 0.1297 - 1s/epoch - 570us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 0.1858 - val_loss: 0.1145 - 1s/epoch - 574us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 0.1849 - val_loss: 0.1640 - 1s/epoch - 607us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 0.2065 - val_loss: 0.2837 - 1s/epoch - 603us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 0.2226 - val_loss: 0.7963 - 1s/epoch - 648us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.1991 - val_loss: 0.1350 - 1s/epoch - 602us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 0.2153 - val_loss: 0.1926 - 1s/epoch - 634us/step\n",
      "534/534 - 0s - loss: 0.1483 - 244ms/epoch - 457us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.1696 - val_loss: 0.0989\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 660us/step - loss: 0.1282 - val_loss: 0.1342\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 673us/step - loss: 0.1248 - val_loss: 0.0730\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 667us/step - loss: 0.1343 - val_loss: 0.1175\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 644us/step - loss: 0.2091 - val_loss: 0.0853\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 709us/step - loss: 0.2004 - val_loss: 0.2125\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 633us/step - loss: 0.1979 - val_loss: 0.1619\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 635us/step - loss: 0.1948 - val_loss: 0.0863\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 640us/step - loss: 0.1821 - val_loss: 0.2360\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 657us/step - loss: 0.2122 - val_loss: 0.1069\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 624us/step - loss: 0.2348 - val_loss: 0.5104\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 669us/step - loss: 0.1802 - val_loss: 0.1617\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 641us/step - loss: 0.2085 - val_loss: 0.3182\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 655us/step - loss: 0.1768 - val_loss: 0.1583\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 626us/step - loss: 0.1780 - val_loss: 0.0544\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 626us/step - loss: 0.1746 - val_loss: 0.3028\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 626us/step - loss: 0.2009 - val_loss: 0.4489\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 615us/step - loss: 0.1922 - val_loss: 0.4516\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 627us/step - loss: 0.2224 - val_loss: 0.2564\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 737us/step - loss: 0.1847 - val_loss: 0.2477\n",
      "Epoch 1/20\n",
      "2134/2134 - 3s - loss: 0.1810 - val_loss: 0.4370 - 3s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 0.1919 - val_loss: 0.0866 - 1s/epoch - 631us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 0.1872 - val_loss: 0.3292 - 1s/epoch - 625us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 0.1723 - val_loss: 0.1138 - 1s/epoch - 607us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.1914 - val_loss: 0.0874 - 1s/epoch - 590us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 0.1965 - val_loss: 0.2296 - 1s/epoch - 587us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 0.1830 - val_loss: 0.2578 - 1s/epoch - 583us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 0.1866 - val_loss: 0.1352 - 1s/epoch - 612us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 0.1619 - val_loss: 0.0588 - 1s/epoch - 621us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.1772 - val_loss: 0.2026 - 1s/epoch - 617us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 0.1501 - val_loss: 0.2307 - 1s/epoch - 567us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 0.1830 - val_loss: 0.1900 - 1s/epoch - 669us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 0.2037 - val_loss: 0.1342 - 1s/epoch - 629us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 0.1589 - val_loss: 0.0976 - 1s/epoch - 568us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 0.1594 - val_loss: 0.3521 - 1s/epoch - 637us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 0.2216 - val_loss: 0.2522 - 1s/epoch - 596us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 0.2458 - val_loss: 0.0909 - 1s/epoch - 667us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 0.1915 - val_loss: 0.1029 - 1s/epoch - 612us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.1745 - val_loss: 0.2851 - 1s/epoch - 672us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 0.1815 - val_loss: 0.1313 - 1s/epoch - 615us/step\n",
      "534/534 - 0s - loss: 0.1107 - 232ms/epoch - 434us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.2034 - val_loss: 0.0897\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 590us/step - loss: 0.1639 - val_loss: 0.1845\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 608us/step - loss: 0.2035 - val_loss: 0.1037\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 604us/step - loss: 0.1683 - val_loss: 0.1934\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 603us/step - loss: 0.1939 - val_loss: 0.5960\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 623us/step - loss: 0.2076 - val_loss: 0.1151\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 623us/step - loss: 0.2199 - val_loss: 0.4115\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 631us/step - loss: 0.2019 - val_loss: 0.1032\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 640us/step - loss: 0.2254 - val_loss: 0.0682\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 676us/step - loss: 0.1982 - val_loss: 0.1299\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 601us/step - loss: 0.2013 - val_loss: 0.1496\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 591us/step - loss: 0.1828 - val_loss: 0.1841\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 606us/step - loss: 0.1797 - val_loss: 0.0878\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 602us/step - loss: 0.1973 - val_loss: 0.1595\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 605us/step - loss: 0.1943 - val_loss: 0.1942\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 608us/step - loss: 0.1833 - val_loss: 0.1193\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 621us/step - loss: 0.1871 - val_loss: 0.1768\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 632us/step - loss: 0.2149 - val_loss: 0.1286\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 697us/step - loss: 0.1777 - val_loss: 0.2351\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 670us/step - loss: 0.2218 - val_loss: 0.2585\n",
      "Epoch 1/20\n",
      "2134/2134 - 3s - loss: 0.1914 - val_loss: 0.0762 - 3s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 2s - loss: 0.1644 - val_loss: 0.1929 - 2s/epoch - 749us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 0.1935 - val_loss: 0.0926 - 1s/epoch - 613us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 0.1847 - val_loss: 0.1915 - 1s/epoch - 639us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.2100 - val_loss: 0.2492 - 1s/epoch - 639us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 0.1848 - val_loss: 0.1479 - 1s/epoch - 618us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 0.1981 - val_loss: 0.1058 - 1s/epoch - 600us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 0.1878 - val_loss: 0.1143 - 1s/epoch - 588us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 0.1825 - val_loss: 0.1987 - 1s/epoch - 629us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.1990 - val_loss: 0.1092 - 1s/epoch - 572us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 0.1834 - val_loss: 0.4309 - 1s/epoch - 558us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 0.1790 - val_loss: 0.1919 - 1s/epoch - 623us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 0.1715 - val_loss: 0.1578 - 1s/epoch - 653us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 0.2089 - val_loss: 0.0936 - 1s/epoch - 606us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 0.1746 - val_loss: 0.4068 - 1s/epoch - 621us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 0.1793 - val_loss: 0.0753 - 1s/epoch - 617us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 0.1774 - val_loss: 0.3430 - 1s/epoch - 610us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 0.1796 - val_loss: 0.2426 - 1s/epoch - 604us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.1937 - val_loss: 0.0634 - 1s/epoch - 583us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 0.1679 - val_loss: 0.4141 - 1s/epoch - 570us/step\n",
      "534/534 - 0s - loss: 0.4000 - 223ms/epoch - 417us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2737 - val_loss: 0.0451\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 696us/step - loss: 0.0372 - val_loss: 0.0364\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 677us/step - loss: 0.0301 - val_loss: 0.0274\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 667us/step - loss: 0.0253 - val_loss: 0.0226\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 692us/step - loss: 0.0204 - val_loss: 0.0179\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 693us/step - loss: 0.0167 - val_loss: 0.0164\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 669us/step - loss: 0.0149 - val_loss: 0.0142\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0136 - val_loss: 0.0132\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 644us/step - loss: 0.0127 - val_loss: 0.0121\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 678us/step - loss: 0.0118 - val_loss: 0.0114\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 662us/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 702us/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 664us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 677us/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 735us/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 701us/step - loss: 0.0080 - val_loss: 0.0078\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 690us/step - loss: 0.0078 - val_loss: 0.0079\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 717us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0074 - val_loss: 0.0073 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0072 - val_loss: 0.0072 - 646ms/epoch - 606us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0072 - 624ms/epoch - 585us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0069 - 658ms/epoch - 617us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0068 - 625ms/epoch - 586us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 623ms/epoch - 584us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0066 - 630ms/epoch - 590us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0064 - 644ms/epoch - 603us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0064 - 654ms/epoch - 613us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0062 - 667ms/epoch - 625us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0061 - 667ms/epoch - 625us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0060 - 631ms/epoch - 591us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0060 - 635ms/epoch - 595us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0058 - 634ms/epoch - 594us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0058 - 692ms/epoch - 649us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0056 - 648ms/epoch - 607us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0056 - 624ms/epoch - 585us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0056 - 632ms/epoch - 592us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0054 - 647ms/epoch - 606us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0055 - 647ms/epoch - 606us/step\n",
      "267/267 - 0s - loss: 0.0054 - 145ms/epoch - 543us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  46.9s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2851 - val_loss: 0.0447\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0380 - val_loss: 0.0336\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0303 - val_loss: 0.0268\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0257 - val_loss: 0.0232\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 659us/step - loss: 0.0212 - val_loss: 0.0181\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 630us/step - loss: 0.0168 - val_loss: 0.0151\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.0146 - val_loss: 0.0137\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0133 - val_loss: 0.0128\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 0.0123 - val_loss: 0.0118\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 668us/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 655us/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 670us/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 640us/step - loss: 0.0094 - val_loss: 0.0091\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0088 - val_loss: 0.0085\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 676us/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 673us/step - loss: 0.0080 - val_loss: 0.0077\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 880us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 723us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 638ms/epoch - 598us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0065 - 671ms/epoch - 629us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0063 - 682ms/epoch - 640us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0063 - 662ms/epoch - 620us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0062 - 683ms/epoch - 640us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0060 - 666ms/epoch - 624us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0059 - 660ms/epoch - 618us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0058 - 676ms/epoch - 634us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0059 - 638ms/epoch - 598us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0057 - 624ms/epoch - 585us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0056 - 630ms/epoch - 591us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0056 - 662ms/epoch - 620us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0055 - 670ms/epoch - 628us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0054 - 738ms/epoch - 692us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0056 - 642ms/epoch - 602us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0054 - 632ms/epoch - 592us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0053 - val_loss: 0.0053 - 651ms/epoch - 610us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0053 - val_loss: 0.0053 - 697ms/epoch - 653us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0052 - val_loss: 0.0052 - 679ms/epoch - 637us/step\n",
      "267/267 - 0s - loss: 0.0052 - 178ms/epoch - 666us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  47.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2611 - val_loss: 0.0445\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 714us/step - loss: 0.0379 - val_loss: 0.0320\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.0297 - val_loss: 0.0270\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 710us/step - loss: 0.0254 - val_loss: 0.0237\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 702us/step - loss: 0.0217 - val_loss: 0.0194\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 714us/step - loss: 0.0178 - val_loss: 0.0156\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 742us/step - loss: 0.0149 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 709us/step - loss: 0.0136 - val_loss: 0.0132\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 684us/step - loss: 0.0127 - val_loss: 0.0121\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0119 - val_loss: 0.0116\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 704us/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 719us/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 673us/step - loss: 0.0099 - val_loss: 0.0095\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 728us/step - loss: 0.0092 - val_loss: 0.0089\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 722us/step - loss: 0.0086 - val_loss: 0.0083\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 684us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 764us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 731us/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 704us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 722us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0070 - val_loss: 0.0068 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0068 - 727ms/epoch - 681us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0068 - 711ms/epoch - 666us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 666ms/epoch - 624us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0068 - 713ms/epoch - 668us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0063 - 638ms/epoch - 598us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0063 - 663ms/epoch - 621us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0060 - 632ms/epoch - 592us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0060 - 656ms/epoch - 615us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0060 - 639ms/epoch - 599us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0058 - 685ms/epoch - 642us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0059 - 738ms/epoch - 692us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0057 - 669ms/epoch - 627us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0056 - 701ms/epoch - 657us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0056 - 681ms/epoch - 638us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0055 - 714ms/epoch - 669us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0056 - 708ms/epoch - 664us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0054 - 691ms/epoch - 647us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0054 - 713ms/epoch - 668us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0053 - val_loss: 0.0054 - 707ms/epoch - 663us/step\n",
      "267/267 - 0s - loss: 0.0054 - 145ms/epoch - 542us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  48.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2565 - val_loss: 0.0448\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0380 - val_loss: 0.0319\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 706us/step - loss: 0.0297 - val_loss: 0.0271\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 707us/step - loss: 0.0248 - val_loss: 0.0224\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 738us/step - loss: 0.0196 - val_loss: 0.0172\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 764us/step - loss: 0.0163 - val_loss: 0.0151\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.0147 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 693us/step - loss: 0.0134 - val_loss: 0.0128\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 731us/step - loss: 0.0125 - val_loss: 0.0119\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 711us/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 669us/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 720us/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 779us/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 707us/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 687us/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 694us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 737us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 726us/step - loss: 0.0076 - val_loss: 0.0073\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 676us/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 752us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0069 - val_loss: 0.0067 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 664ms/epoch - 622us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0066 - 716ms/epoch - 671us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0063 - 685ms/epoch - 642us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0062 - 691ms/epoch - 648us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0062 - 624ms/epoch - 585us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0060 - 629ms/epoch - 590us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0059 - 650ms/epoch - 610us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0059 - 745ms/epoch - 698us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0059 - 690ms/epoch - 647us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0056 - 664ms/epoch - 622us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0057 - 630ms/epoch - 591us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0055 - 625ms/epoch - 585us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0055 - 649ms/epoch - 608us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0054 - 658ms/epoch - 617us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0054 - 713ms/epoch - 668us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0053 - 636ms/epoch - 596us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0053 - val_loss: 0.0053 - 636ms/epoch - 596us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0053 - val_loss: 0.0052 - 682ms/epoch - 639us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0052 - val_loss: 0.0051 - 656ms/epoch - 615us/step\n",
      "267/267 - 0s - loss: 0.0051 - 142ms/epoch - 533us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  48.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2539 - val_loss: 0.0439\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 749us/step - loss: 0.0370 - val_loss: 0.0319\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 717us/step - loss: 0.0299 - val_loss: 0.0271\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 724us/step - loss: 0.0255 - val_loss: 0.0250\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 735us/step - loss: 0.0228 - val_loss: 0.0233\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 719us/step - loss: 0.0204 - val_loss: 0.0192\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 754us/step - loss: 0.0182 - val_loss: 0.0182\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 739us/step - loss: 0.0151 - val_loss: 0.0137\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 777us/step - loss: 0.0131 - val_loss: 0.0125\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 660us/step - loss: 0.0121 - val_loss: 0.0121\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 669us/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 670us/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 630us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0093 - val_loss: 0.0093\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 666us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 664us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 647us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 656us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0072 - val_loss: 0.0070 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0070 - 729ms/epoch - 683us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0067 - 650ms/epoch - 609us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 643ms/epoch - 602us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 635ms/epoch - 595us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0064 - 635ms/epoch - 595us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0064 - 635ms/epoch - 595us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0063 - 673ms/epoch - 631us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0060 - 641ms/epoch - 600us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0061 - 641ms/epoch - 601us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0058 - 634ms/epoch - 594us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0060 - 677ms/epoch - 634us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0056 - 634ms/epoch - 594us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0057 - 670ms/epoch - 628us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0056 - 658ms/epoch - 616us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0057 - 631ms/epoch - 591us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0055 - 629ms/epoch - 590us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0054 - 626ms/epoch - 587us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0053 - 661ms/epoch - 619us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0053 - 650ms/epoch - 609us/step\n",
      "267/267 - 0s - loss: 0.0053 - 143ms/epoch - 536us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  48.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0525 - val_loss: 0.0071\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 632us/step - loss: 0.0979 - val_loss: 0.3381\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0989 - val_loss: 0.0204\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0374 - val_loss: 0.0337\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 691us/step - loss: 0.0407 - val_loss: 0.1199\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 704us/step - loss: 0.0887 - val_loss: 0.0528\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 748us/step - loss: 0.0566 - val_loss: 0.0431\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 720us/step - loss: 0.0360 - val_loss: 0.0746\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 704us/step - loss: 0.0530 - val_loss: 0.0148\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 708us/step - loss: 0.0547 - val_loss: 0.0450\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 722us/step - loss: 0.0523 - val_loss: 0.0255\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 632us/step - loss: 0.1999 - val_loss: 0.0340\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.0889 - val_loss: 0.0803\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 695us/step - loss: 0.0779 - val_loss: 0.1395\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 642us/step - loss: 0.0905 - val_loss: 0.0490\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 705us/step - loss: 0.0784 - val_loss: 0.0262\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 834us/step - loss: 0.0745 - val_loss: 0.0443\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 746us/step - loss: 0.0768 - val_loss: 0.1534\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 712us/step - loss: 0.0775 - val_loss: 0.1036\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 675us/step - loss: 0.0743 - val_loss: 0.0651\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0923 - val_loss: 0.0680 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.1021 - val_loss: 0.2053 - 629ms/epoch - 589us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.1432 - val_loss: 0.2783 - 619ms/epoch - 580us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.1383 - val_loss: 0.1565 - 668ms/epoch - 626us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0892 - val_loss: 0.0527 - 625ms/epoch - 585us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.1271 - val_loss: 0.0717 - 711ms/epoch - 666us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.1022 - val_loss: 0.0874 - 641ms/epoch - 600us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0910 - val_loss: 0.0714 - 674ms/epoch - 632us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0991 - val_loss: 0.0648 - 667ms/epoch - 625us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.1163 - val_loss: 0.1371 - 638ms/epoch - 598us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.1005 - val_loss: 0.1743 - 625ms/epoch - 586us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0981 - val_loss: 0.1846 - 615ms/epoch - 577us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0962 - val_loss: 0.0531 - 619ms/epoch - 580us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0906 - val_loss: 0.0444 - 621ms/epoch - 582us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.1253 - val_loss: 0.0852 - 637ms/epoch - 597us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.1151 - val_loss: 0.0835 - 648ms/epoch - 607us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.1227 - val_loss: 0.1914 - 625ms/epoch - 586us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.1007 - val_loss: 0.0767 - 645ms/epoch - 604us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.1022 - val_loss: 0.0357 - 714ms/epoch - 669us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.1661 - val_loss: 0.0782 - 754ms/epoch - 707us/step\n",
      "267/267 - 0s - loss: 0.0779 - 158ms/epoch - 591us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  47.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0539 - val_loss: 0.0087\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.1133 - val_loss: 0.7082\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 656us/step - loss: 0.1332 - val_loss: 0.0465\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 0.0588 - val_loss: 0.1442\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0413 - val_loss: 0.0180\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 654us/step - loss: 0.0872 - val_loss: 0.0829\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.0479 - val_loss: 0.0153\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 670us/step - loss: 0.0508 - val_loss: 0.0792\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.1598 - val_loss: 0.1008\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 756us/step - loss: 0.1310 - val_loss: 0.1718\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 718us/step - loss: 0.0891 - val_loss: 0.0642\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 701us/step - loss: 0.0910 - val_loss: 0.2253\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.1443 - val_loss: 0.3984\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 761us/step - loss: 0.1258 - val_loss: 0.1642\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 727us/step - loss: 0.0979 - val_loss: 0.0692\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 688us/step - loss: 0.0934 - val_loss: 0.0894\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 687us/step - loss: 0.1109 - val_loss: 0.0489\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 756us/step - loss: 0.1109 - val_loss: 0.2393\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 692us/step - loss: 0.1376 - val_loss: 0.0530\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 698us/step - loss: 0.1023 - val_loss: 0.1261\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.1064 - val_loss: 0.0721 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0998 - val_loss: 0.1669 - 617ms/epoch - 579us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.1231 - val_loss: 0.1724 - 617ms/epoch - 579us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.1068 - val_loss: 0.0752 - 621ms/epoch - 582us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.1074 - val_loss: 0.1442 - 619ms/epoch - 580us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0892 - val_loss: 0.0719 - 617ms/epoch - 578us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.1172 - val_loss: 0.1647 - 637ms/epoch - 597us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0977 - val_loss: 0.0621 - 622ms/epoch - 583us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0948 - val_loss: 0.2172 - 617ms/epoch - 578us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.1009 - val_loss: 0.2698 - 609ms/epoch - 571us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.1070 - val_loss: 0.0938 - 615ms/epoch - 576us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.1424 - val_loss: 0.0790 - 617ms/epoch - 578us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0977 - val_loss: 0.0373 - 617ms/epoch - 579us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.1232 - val_loss: 0.0653 - 626ms/epoch - 587us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.1124 - val_loss: 0.1517 - 620ms/epoch - 581us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0994 - val_loss: 0.0733 - 619ms/epoch - 580us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.1369 - val_loss: 0.3275 - 617ms/epoch - 578us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.1097 - val_loss: 0.0783 - 617ms/epoch - 578us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.1114 - val_loss: 0.0380 - 611ms/epoch - 573us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0911 - val_loss: 0.4377 - 616ms/epoch - 577us/step\n",
      "267/267 - 0s - loss: 0.3877 - 143ms/epoch - 535us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  47.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0717 - val_loss: 0.0268\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 743us/step - loss: 0.0764 - val_loss: 0.0182\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 677us/step - loss: 0.0448 - val_loss: 0.0210\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 694us/step - loss: 0.1467 - val_loss: 0.0651\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 660us/step - loss: 0.0394 - val_loss: 0.0336\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0419 - val_loss: 0.0254\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 655us/step - loss: 0.0779 - val_loss: 0.0158\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 634us/step - loss: 0.0918 - val_loss: 0.0733\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 666us/step - loss: 0.0403 - val_loss: 0.0179\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.1638 - val_loss: 0.1965\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.0759 - val_loss: 0.0159\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.1207 - val_loss: 0.0566\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 623us/step - loss: 0.0863 - val_loss: 0.0703\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 661us/step - loss: 0.0680 - val_loss: 0.1770\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0821 - val_loss: 0.0690\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.1010 - val_loss: 0.0176\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 0.0769 - val_loss: 0.0305\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 639us/step - loss: 0.0933 - val_loss: 0.1426\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 655us/step - loss: 0.0921 - val_loss: 0.0339\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 663us/step - loss: 0.0806 - val_loss: 0.0880\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0634 - val_loss: 0.0502 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.1325 - val_loss: 0.0266 - 621ms/epoch - 582us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0928 - val_loss: 0.0486 - 616ms/epoch - 578us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0930 - val_loss: 0.0595 - 616ms/epoch - 577us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0901 - val_loss: 0.0216 - 621ms/epoch - 582us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0797 - val_loss: 0.0548 - 626ms/epoch - 587us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.1033 - val_loss: 0.2459 - 622ms/epoch - 583us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0984 - val_loss: 0.0540 - 620ms/epoch - 581us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0879 - val_loss: 0.0663 - 617ms/epoch - 578us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.1030 - val_loss: 0.0964 - 617ms/epoch - 578us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.1017 - val_loss: 0.1256 - 622ms/epoch - 583us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0943 - val_loss: 0.0501 - 617ms/epoch - 579us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0929 - val_loss: 0.1294 - 619ms/epoch - 581us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.1071 - val_loss: 0.0461 - 622ms/epoch - 583us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0882 - val_loss: 0.0361 - 617ms/epoch - 578us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.1129 - val_loss: 0.1324 - 618ms/epoch - 579us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0889 - val_loss: 0.0895 - 623ms/epoch - 584us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.1033 - val_loss: 0.1037 - 643ms/epoch - 603us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.1049 - val_loss: 0.0652 - 622ms/epoch - 583us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0953 - val_loss: 0.0908 - 620ms/epoch - 581us/step\n",
      "267/267 - 0s - loss: 0.0836 - 142ms/epoch - 531us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  46.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0663 - val_loss: 0.0740\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 649us/step - loss: 0.0451 - val_loss: 0.0090\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 671us/step - loss: 0.2853 - val_loss: 0.1119\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 660us/step - loss: 0.0924 - val_loss: 0.0751\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0938 - val_loss: 0.0308\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.0649 - val_loss: 0.0250\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0905 - val_loss: 0.2086\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.0914 - val_loss: 0.1623\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0683 - val_loss: 0.1073\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 667us/step - loss: 0.0898 - val_loss: 0.0358\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0792 - val_loss: 0.0416\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 711us/step - loss: 0.0660 - val_loss: 0.0816\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 718us/step - loss: 0.0796 - val_loss: 0.1712\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 651us/step - loss: 0.0732 - val_loss: 0.0620\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 713us/step - loss: 0.0701 - val_loss: 0.0777\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 702us/step - loss: 0.0800 - val_loss: 0.0265\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 664us/step - loss: 0.1291 - val_loss: 0.0313\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 681us/step - loss: 0.0875 - val_loss: 0.0321\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 737us/step - loss: 0.0702 - val_loss: 0.0221\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.0913 - val_loss: 0.0523\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0918 - val_loss: 0.1409 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.1210 - val_loss: 0.0810 - 706ms/epoch - 662us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0793 - val_loss: 0.0589 - 654ms/epoch - 613us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0703 - val_loss: 0.0908 - 624ms/epoch - 585us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0745 - val_loss: 0.0348 - 621ms/epoch - 582us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0938 - val_loss: 0.0965 - 623ms/epoch - 584us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0988 - val_loss: 0.0413 - 622ms/epoch - 583us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0687 - val_loss: 0.0640 - 626ms/epoch - 587us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.1273 - val_loss: 0.0563 - 621ms/epoch - 582us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.1303 - val_loss: 0.0367 - 634ms/epoch - 594us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.1146 - val_loss: 0.0864 - 648ms/epoch - 607us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0990 - val_loss: 0.0977 - 625ms/epoch - 586us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.1000 - val_loss: 0.0664 - 627ms/epoch - 587us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0965 - val_loss: 0.1638 - 624ms/epoch - 585us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.1056 - val_loss: 0.0685 - 631ms/epoch - 592us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0931 - val_loss: 0.1248 - 624ms/epoch - 585us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0986 - val_loss: 0.0520 - 623ms/epoch - 584us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.1111 - val_loss: 0.0361 - 630ms/epoch - 590us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.1086 - val_loss: 0.0854 - 621ms/epoch - 582us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.1064 - val_loss: 0.0596 - 625ms/epoch - 586us/step\n",
      "267/267 - 0s - loss: 0.0503 - 142ms/epoch - 531us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  46.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0591 - val_loss: 0.0199\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 652us/step - loss: 0.0878 - val_loss: 0.0236\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.0371 - val_loss: 0.0733\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.1702 - val_loss: 0.1235\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 660us/step - loss: 0.0422 - val_loss: 0.0419\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 729us/step - loss: 0.0636 - val_loss: 0.0188\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 713us/step - loss: 0.0462 - val_loss: 0.1180\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 670us/step - loss: 0.0736 - val_loss: 0.0208\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 688us/step - loss: 0.0355 - val_loss: 0.0591\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0723 - val_loss: 0.0145\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 640us/step - loss: 0.0544 - val_loss: 0.1058\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.0407 - val_loss: 0.0185\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.1266 - val_loss: 0.0612\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 721us/step - loss: 0.0467 - val_loss: 0.0175\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 760us/step - loss: 0.0438 - val_loss: 0.2312\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 731us/step - loss: 0.1344 - val_loss: 0.0200\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 663us/step - loss: 0.0644 - val_loss: 0.0600\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 708us/step - loss: 0.0601 - val_loss: 0.0248\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 762us/step - loss: 0.1036 - val_loss: 0.0609\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 708us/step - loss: 0.0795 - val_loss: 0.0385\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0786 - val_loss: 0.0423 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.1018 - val_loss: 0.1499 - 681ms/epoch - 638us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0681 - val_loss: 0.1194 - 736ms/epoch - 690us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.1107 - val_loss: 0.0909 - 649ms/epoch - 608us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.1221 - val_loss: 0.1806 - 670ms/epoch - 628us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0950 - val_loss: 0.0429 - 689ms/epoch - 645us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0724 - val_loss: 0.0631 - 711ms/epoch - 666us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0935 - val_loss: 0.0488 - 697ms/epoch - 654us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0679 - val_loss: 0.0901 - 693ms/epoch - 649us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0765 - val_loss: 0.0711 - 717ms/epoch - 672us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0788 - val_loss: 0.1228 - 698ms/epoch - 654us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0817 - val_loss: 0.0988 - 681ms/epoch - 638us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0890 - val_loss: 0.0973 - 671ms/epoch - 629us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0718 - val_loss: 0.0709 - 720ms/epoch - 675us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0954 - val_loss: 0.0663 - 700ms/epoch - 656us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0690 - val_loss: 0.0265 - 701ms/epoch - 657us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.1552 - val_loss: 0.0559 - 710ms/epoch - 666us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0956 - val_loss: 0.0605 - 679ms/epoch - 636us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.1204 - val_loss: 0.2282 - 702ms/epoch - 658us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.1452 - val_loss: 0.1035 - 681ms/epoch - 638us/step\n",
      "267/267 - 0s - loss: 0.0845 - 180ms/epoch - 673us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  48.4s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3463 - val_loss: 0.2532\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.1966 - val_loss: 0.1496\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.1167 - val_loss: 0.0894\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0701 - val_loss: 0.0541\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0428 - val_loss: 0.0334\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0268 - val_loss: 0.0213\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0173 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0083 - val_loss: 0.0072\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0024 - val_loss: 0.0024 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 512ms/epoch - 479us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 558ms/epoch - 523us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 570ms/epoch - 534us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 591ms/epoch - 554us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 503ms/epoch - 471us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 507ms/epoch - 475us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 513ms/epoch - 480us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0020 - 547ms/epoch - 513us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 531ms/epoch - 497us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 514ms/epoch - 482us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0019 - 573ms/epoch - 537us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 544ms/epoch - 509us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 510ms/epoch - 478us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 557ms/epoch - 522us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 518ms/epoch - 486us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 566ms/epoch - 530us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 516ms/epoch - 484us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 512ms/epoch - 480us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0017 - val_loss: 0.0017 - 552ms/epoch - 518us/step\n",
      "267/267 - 0s - loss: 0.0016 - 154ms/epoch - 577us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  42.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3351 - val_loss: 0.2507\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.1947 - val_loss: 0.1481\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.1156 - val_loss: 0.0884\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0694 - val_loss: 0.0535\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0423 - val_loss: 0.0330\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0264 - val_loss: 0.0210\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0170 - val_loss: 0.0138\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0115 - val_loss: 0.0096\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0082 - val_loss: 0.0071\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0062 - val_loss: 0.0055\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 624us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0026 - val_loss: 0.0026 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0025 - 514ms/epoch - 482us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0025 - 501ms/epoch - 470us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0025 - 520ms/epoch - 487us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 531ms/epoch - 498us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0024 - 504ms/epoch - 472us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 501ms/epoch - 470us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 543ms/epoch - 509us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 516ms/epoch - 484us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 531ms/epoch - 498us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 524ms/epoch - 491us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 562ms/epoch - 526us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0022 - 534ms/epoch - 500us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 555ms/epoch - 520us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 560ms/epoch - 524us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 541ms/epoch - 507us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0021 - 519ms/epoch - 486us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0021 - 532ms/epoch - 499us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 542ms/epoch - 508us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 528ms/epoch - 495us/step\n",
      "267/267 - 0s - loss: 0.0019 - 149ms/epoch - 559us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  42.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3421 - val_loss: 0.2557\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 621us/step - loss: 0.1987 - val_loss: 0.1513\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.1180 - val_loss: 0.0904\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0709 - val_loss: 0.0547\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.0433 - val_loss: 0.0338\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0270 - val_loss: 0.0215\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0174 - val_loss: 0.0142\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0084 - val_loss: 0.0072\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 621us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0024 - val_loss: 0.0024 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 588ms/epoch - 551us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 533ms/epoch - 500us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 531ms/epoch - 498us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 515ms/epoch - 483us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 506ms/epoch - 474us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 507ms/epoch - 475us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 505ms/epoch - 473us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 505ms/epoch - 473us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 604ms/epoch - 566us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 548ms/epoch - 514us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 511ms/epoch - 479us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 546ms/epoch - 512us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0019 - 569ms/epoch - 533us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 546ms/epoch - 511us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 546ms/epoch - 512us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 520ms/epoch - 488us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0017 - val_loss: 0.0017 - 613ms/epoch - 574us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0017 - val_loss: 0.0017 - 549ms/epoch - 515us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0017 - val_loss: 0.0017 - 539ms/epoch - 505us/step\n",
      "267/267 - 0s - loss: 0.0016 - 144ms/epoch - 539us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  43.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3336 - val_loss: 0.2482\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.1927 - val_loss: 0.1465\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.1143 - val_loss: 0.0874\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0685 - val_loss: 0.0528\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0417 - val_loss: 0.0326\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0260 - val_loss: 0.0206\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0167 - val_loss: 0.0136\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0112 - val_loss: 0.0094\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0080 - val_loss: 0.0069\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0060 - val_loss: 0.0054\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0025 - val_loss: 0.0025 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0025 - 553ms/epoch - 518us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0024 - 533ms/epoch - 500us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 616ms/epoch - 577us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 580ms/epoch - 544us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 549ms/epoch - 514us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 542ms/epoch - 508us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 517ms/epoch - 485us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 613ms/epoch - 574us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 579ms/epoch - 543us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 545ms/epoch - 511us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 553ms/epoch - 518us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 545ms/epoch - 511us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 542ms/epoch - 508us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 538ms/epoch - 504us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 538ms/epoch - 504us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 544ms/epoch - 510us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 616ms/epoch - 578us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 577ms/epoch - 540us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 569ms/epoch - 533us/step\n",
      "267/267 - 0s - loss: 0.0018 - 154ms/epoch - 576us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  42.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3422 - val_loss: 0.2553\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.1984 - val_loss: 0.1510\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.1178 - val_loss: 0.0902\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0707 - val_loss: 0.0546\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0431 - val_loss: 0.0337\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0269 - val_loss: 0.0214\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0174 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0083 - val_loss: 0.0072\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 635us/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0026 - val_loss: 0.0026 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0025 - 558ms/epoch - 523us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0025 - 510ms/epoch - 478us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 572ms/epoch - 536us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 509ms/epoch - 477us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0024 - 509ms/epoch - 477us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 507ms/epoch - 476us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 519ms/epoch - 486us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 505ms/epoch - 473us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 540ms/epoch - 506us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0022 - 579ms/epoch - 542us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 596ms/epoch - 559us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 563ms/epoch - 528us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0021 - 527ms/epoch - 494us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 516ms/epoch - 484us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 514ms/epoch - 481us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 511ms/epoch - 479us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0020 - 513ms/epoch - 481us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 516ms/epoch - 484us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 510ms/epoch - 478us/step\n",
      "267/267 - 0s - loss: 0.0018 - 143ms/epoch - 536us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  42.6s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.4293 - val_loss: 0.3482\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 0.1321 - val_loss: 0.0740\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.0642 - val_loss: 0.0583\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 640us/step - loss: 0.0536 - val_loss: 0.0507\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 621us/step - loss: 0.0479 - val_loss: 0.0461\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0441 - val_loss: 0.0430\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.0413 - val_loss: 0.0409\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0390 - val_loss: 0.0391\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0372 - val_loss: 0.0367\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.0356 - val_loss: 0.0358\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.0343 - val_loss: 0.0346\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0332 - val_loss: 0.0330\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 0.0321 - val_loss: 0.0319\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0311 - val_loss: 0.0313\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0303 - val_loss: 0.0312\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0296 - val_loss: 0.0293\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.0289 - val_loss: 0.0294\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 0.0283 - val_loss: 0.0280\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0278 - val_loss: 0.0279\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 0.0274 - val_loss: 0.0280\n",
      "Epoch 1/20\n",
      "1067/1067 - 3s - loss: 0.0269 - val_loss: 0.0271 - 3s/epoch - 3ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0266 - val_loss: 0.0261 - 726ms/epoch - 680us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0263 - val_loss: 0.0258 - 585ms/epoch - 548us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0261 - val_loss: 0.0261 - 547ms/epoch - 513us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0258 - val_loss: 0.0259 - 555ms/epoch - 520us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0255 - val_loss: 0.0253 - 566ms/epoch - 530us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0251 - val_loss: 0.0246 - 565ms/epoch - 529us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0249 - val_loss: 0.0267 - 546ms/epoch - 512us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0248 - val_loss: 0.0241 - 547ms/epoch - 513us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0245 - val_loss: 0.0239 - 566ms/epoch - 530us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0243 - val_loss: 0.0237 - 599ms/epoch - 561us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0240 - val_loss: 0.0235 - 548ms/epoch - 513us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0239 - val_loss: 0.0233 - 541ms/epoch - 507us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 0s - loss: 0.0237 - val_loss: 0.0234 - 471ms/epoch - 442us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 0s - loss: 0.0234 - val_loss: 0.0228 - 488ms/epoch - 457us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 0s - loss: 0.0233 - val_loss: 0.0228 - 490ms/epoch - 459us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0231 - val_loss: 0.0226 - 506ms/epoch - 475us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0229 - val_loss: 0.0228 - 576ms/epoch - 540us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0227 - val_loss: 0.0226 - 580ms/epoch - 544us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0225 - val_loss: 0.0219 - 565ms/epoch - 530us/step\n",
      "267/267 - 0s - loss: 0.0207 - 138ms/epoch - 517us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  44.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 1.4036 - val_loss: 0.3449\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.1290 - val_loss: 0.0691\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 500us/step - loss: 0.0594 - val_loss: 0.0534\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0492 - val_loss: 0.0463\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 485us/step - loss: 0.0438 - val_loss: 0.0422\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0405 - val_loss: 0.0394\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 501us/step - loss: 0.0380 - val_loss: 0.0378\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 496us/step - loss: 0.0361 - val_loss: 0.0355\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0345 - val_loss: 0.0339\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 490us/step - loss: 0.0332 - val_loss: 0.0326\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 489us/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 446us/step - loss: 0.0310 - val_loss: 0.0307\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0302 - val_loss: 0.0300\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 474us/step - loss: 0.0294 - val_loss: 0.0289\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0287 - val_loss: 0.0282\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 488us/step - loss: 0.0281 - val_loss: 0.0299\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 482us/step - loss: 0.0274 - val_loss: 0.0272\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 484us/step - loss: 0.0269 - val_loss: 0.0265\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 494us/step - loss: 0.0264 - val_loss: 0.0261\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 516us/step - loss: 0.0259 - val_loss: 0.0258\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0255 - val_loss: 0.0260 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 0s - loss: 0.0251 - val_loss: 0.0257 - 462ms/epoch - 433us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 0s - loss: 0.0249 - val_loss: 0.0265 - 419ms/epoch - 393us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 0s - loss: 0.0246 - val_loss: 0.0243 - 453ms/epoch - 425us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 0s - loss: 0.0243 - val_loss: 0.0241 - 478ms/epoch - 448us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 0s - loss: 0.0240 - val_loss: 0.0258 - 455ms/epoch - 426us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 0s - loss: 0.0238 - val_loss: 0.0239 - 491ms/epoch - 460us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 0s - loss: 0.0236 - val_loss: 0.0234 - 466ms/epoch - 436us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 0s - loss: 0.0234 - val_loss: 0.0231 - 438ms/epoch - 411us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 0s - loss: 0.0232 - val_loss: 0.0229 - 458ms/epoch - 430us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0229 - val_loss: 0.0225 - 525ms/epoch - 492us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 0s - loss: 0.0228 - val_loss: 0.0244 - 474ms/epoch - 445us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0225 - val_loss: 0.0222 - 513ms/epoch - 481us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 0s - loss: 0.0223 - val_loss: 0.0222 - 473ms/epoch - 443us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 0s - loss: 0.0222 - val_loss: 0.0229 - 457ms/epoch - 428us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 0s - loss: 0.0221 - val_loss: 0.0217 - 447ms/epoch - 419us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 0s - loss: 0.0219 - val_loss: 0.0218 - 488ms/epoch - 458us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 0s - loss: 0.0218 - val_loss: 0.0223 - 444ms/epoch - 416us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 0s - loss: 0.0216 - val_loss: 0.0217 - 457ms/epoch - 428us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 0s - loss: 0.0214 - val_loss: 0.0212 - 448ms/epoch - 420us/step\n",
      "267/267 - 0s - loss: 0.0205 - 124ms/epoch - 465us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  35.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 1.4384 - val_loss: 0.3577\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 499us/step - loss: 0.1302 - val_loss: 0.0706\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0610 - val_loss: 0.0554\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 509us/step - loss: 0.0507 - val_loss: 0.0487\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 515us/step - loss: 0.0452 - val_loss: 0.0438\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 512us/step - loss: 0.0415 - val_loss: 0.0402\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 439us/step - loss: 0.0388 - val_loss: 0.0376\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 473us/step - loss: 0.0366 - val_loss: 0.0366\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 460us/step - loss: 0.0349 - val_loss: 0.0344\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 478us/step - loss: 0.0334 - val_loss: 0.0334\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 437us/step - loss: 0.0321 - val_loss: 0.0320\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 471us/step - loss: 0.0310 - val_loss: 0.0304\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 442us/step - loss: 0.0300 - val_loss: 0.0312\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 512us/step - loss: 0.0292 - val_loss: 0.0287\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 471us/step - loss: 0.0284 - val_loss: 0.0288\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 501us/step - loss: 0.0277 - val_loss: 0.0272\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0272 - val_loss: 0.0271\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 486us/step - loss: 0.0267 - val_loss: 0.0264\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 518us/step - loss: 0.0262 - val_loss: 0.0258\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 474us/step - loss: 0.0257 - val_loss: 0.0253\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0253 - val_loss: 0.0249 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 0s - loss: 0.0250 - val_loss: 0.0248 - 494ms/epoch - 463us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 0s - loss: 0.0247 - val_loss: 0.0244 - 424ms/epoch - 397us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 0s - loss: 0.0246 - val_loss: 0.0241 - 430ms/epoch - 403us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 0s - loss: 0.0242 - val_loss: 0.0246 - 427ms/epoch - 400us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 0s - loss: 0.0240 - val_loss: 0.0235 - 450ms/epoch - 422us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 0s - loss: 0.0237 - val_loss: 0.0232 - 427ms/epoch - 400us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 0s - loss: 0.0236 - val_loss: 0.0239 - 428ms/epoch - 401us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 0s - loss: 0.0234 - val_loss: 0.0235 - 427ms/epoch - 400us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 0s - loss: 0.0232 - val_loss: 0.0228 - 424ms/epoch - 398us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 0s - loss: 0.0230 - val_loss: 0.0245 - 429ms/epoch - 402us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 0s - loss: 0.0228 - val_loss: 0.0226 - 434ms/epoch - 407us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 0s - loss: 0.0226 - val_loss: 0.0225 - 427ms/epoch - 400us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 0s - loss: 0.0225 - val_loss: 0.0224 - 427ms/epoch - 400us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 0s - loss: 0.0223 - val_loss: 0.0225 - 428ms/epoch - 402us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 0s - loss: 0.0222 - val_loss: 0.0222 - 426ms/epoch - 399us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 0s - loss: 0.0221 - val_loss: 0.0218 - 428ms/epoch - 401us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 0s - loss: 0.0219 - val_loss: 0.0217 - 425ms/epoch - 398us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 0s - loss: 0.0217 - val_loss: 0.0225 - 424ms/epoch - 398us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 0s - loss: 0.0217 - val_loss: 0.0216 - 425ms/epoch - 399us/step\n",
      "267/267 - 0s - loss: 0.0204 - 125ms/epoch - 470us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  34.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 1.4155 - val_loss: 0.3452\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 452us/step - loss: 0.1276 - val_loss: 0.0705\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 477us/step - loss: 0.0621 - val_loss: 0.0569\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 509us/step - loss: 0.0531 - val_loss: 0.0505\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 503us/step - loss: 0.0482 - val_loss: 0.0477\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0447 - val_loss: 0.0439\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 503us/step - loss: 0.0420 - val_loss: 0.0409\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 458us/step - loss: 0.0399 - val_loss: 0.0404\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 501us/step - loss: 0.0381 - val_loss: 0.0377\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0367 - val_loss: 0.0373\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 0.0355 - val_loss: 0.0348\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 521us/step - loss: 0.0343 - val_loss: 0.0338\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0334 - val_loss: 0.0329\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 473us/step - loss: 0.0325 - val_loss: 0.0322\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0316 - val_loss: 0.0318\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0309 - val_loss: 0.0307\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 460us/step - loss: 0.0302 - val_loss: 0.0298\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 479us/step - loss: 0.0296 - val_loss: 0.0291\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 510us/step - loss: 0.0291 - val_loss: 0.0285\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 453us/step - loss: 0.0285 - val_loss: 0.0287\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0282 - val_loss: 0.0277 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 0s - loss: 0.0279 - val_loss: 0.0273 - 447ms/epoch - 419us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 0s - loss: 0.0275 - val_loss: 0.0271 - 440ms/epoch - 413us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 0s - loss: 0.0272 - val_loss: 0.0266 - 446ms/epoch - 418us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 0s - loss: 0.0268 - val_loss: 0.0265 - 446ms/epoch - 418us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 0s - loss: 0.0266 - val_loss: 0.0261 - 432ms/epoch - 405us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 0s - loss: 0.0262 - val_loss: 0.0256 - 442ms/epoch - 414us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 0s - loss: 0.0259 - val_loss: 0.0252 - 447ms/epoch - 419us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 0s - loss: 0.0256 - val_loss: 0.0250 - 451ms/epoch - 423us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 0s - loss: 0.0253 - val_loss: 0.0248 - 450ms/epoch - 421us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 0s - loss: 0.0251 - val_loss: 0.0243 - 484ms/epoch - 453us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 0s - loss: 0.0248 - val_loss: 0.0243 - 468ms/epoch - 439us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 0s - loss: 0.0245 - val_loss: 0.0239 - 451ms/epoch - 423us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 0s - loss: 0.0243 - val_loss: 0.0240 - 432ms/epoch - 405us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 0s - loss: 0.0240 - val_loss: 0.0242 - 428ms/epoch - 401us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 0s - loss: 0.0239 - val_loss: 0.0250 - 425ms/epoch - 399us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0236 - val_loss: 0.0228 - 502ms/epoch - 471us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 0s - loss: 0.0235 - val_loss: 0.0231 - 499ms/epoch - 468us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 0s - loss: 0.0232 - val_loss: 0.0241 - 487ms/epoch - 457us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 0s - loss: 0.0231 - val_loss: 0.0225 - 424ms/epoch - 397us/step\n",
      "267/267 - 0s - loss: 0.0211 - 124ms/epoch - 465us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  34.9s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 1.4027 - val_loss: 0.3300\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.1282 - val_loss: 0.0752\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 500us/step - loss: 0.0654 - val_loss: 0.0593\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 506us/step - loss: 0.0552 - val_loss: 0.0525\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 0.0497 - val_loss: 0.0479\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 450us/step - loss: 0.0458 - val_loss: 0.0448\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0428 - val_loss: 0.0417\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 513us/step - loss: 0.0405 - val_loss: 0.0397\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 473us/step - loss: 0.0385 - val_loss: 0.0376\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0369 - val_loss: 0.0366\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 510us/step - loss: 0.0355 - val_loss: 0.0375\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 514us/step - loss: 0.0344 - val_loss: 0.0360\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0333 - val_loss: 0.0386\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0324 - val_loss: 0.0318\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 477us/step - loss: 0.0316 - val_loss: 0.0313\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 485us/step - loss: 0.0307 - val_loss: 0.0312\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 484us/step - loss: 0.0301 - val_loss: 0.0297\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 497us/step - loss: 0.0294 - val_loss: 0.0292\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 517us/step - loss: 0.0288 - val_loss: 0.0286\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 516us/step - loss: 0.0283 - val_loss: 0.0278\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0277 - val_loss: 0.0275 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 0s - loss: 0.0274 - val_loss: 0.0276 - 434ms/epoch - 407us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 0s - loss: 0.0271 - val_loss: 0.0277 - 492ms/epoch - 461us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0266 - val_loss: 0.0268 - 540ms/epoch - 506us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 0s - loss: 0.0264 - val_loss: 0.0260 - 450ms/epoch - 421us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 0s - loss: 0.0261 - val_loss: 0.0256 - 489ms/epoch - 458us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 0s - loss: 0.0257 - val_loss: 0.0260 - 444ms/epoch - 416us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 0s - loss: 0.0254 - val_loss: 0.0253 - 422ms/epoch - 395us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 0s - loss: 0.0252 - val_loss: 0.0247 - 420ms/epoch - 394us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 0s - loss: 0.0248 - val_loss: 0.0248 - 424ms/epoch - 397us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 0s - loss: 0.0247 - val_loss: 0.0240 - 424ms/epoch - 398us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 0s - loss: 0.0244 - val_loss: 0.0246 - 453ms/epoch - 425us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 0s - loss: 0.0242 - val_loss: 0.0237 - 443ms/epoch - 415us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 0s - loss: 0.0239 - val_loss: 0.0234 - 423ms/epoch - 396us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 0s - loss: 0.0238 - val_loss: 0.0249 - 421ms/epoch - 394us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 0s - loss: 0.0235 - val_loss: 0.0231 - 422ms/epoch - 395us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 0s - loss: 0.0234 - val_loss: 0.0227 - 418ms/epoch - 392us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 0s - loss: 0.0233 - val_loss: 0.0244 - 427ms/epoch - 400us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 0s - loss: 0.0231 - val_loss: 0.0227 - 431ms/epoch - 404us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0229 - val_loss: 0.0222 - 510ms/epoch - 478us/step\n",
      "267/267 - 0s - loss: 0.0214 - 135ms/epoch - 506us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  35.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2741 - val_loss: 0.0180\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0126 - val_loss: 0.0095\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 0.0081 - val_loss: 0.0070\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0063 - val_loss: 0.0057\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 625us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 630us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 592ms/epoch - 555us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 624ms/epoch - 584us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 620ms/epoch - 582us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 571ms/epoch - 535us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 583ms/epoch - 546us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0031 - 579ms/epoch - 543us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 571ms/epoch - 535us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 658ms/epoch - 617us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 557ms/epoch - 522us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 619ms/epoch - 580us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0031 - 590ms/epoch - 553us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 539ms/epoch - 505us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 539ms/epoch - 505us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 550ms/epoch - 515us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 544ms/epoch - 510us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 541ms/epoch - 507us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 542ms/epoch - 508us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 538ms/epoch - 504us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0039 - 546ms/epoch - 512us/step\n",
      "267/267 - 0s - loss: 0.0036 - 121ms/epoch - 453us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  40.3s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2841 - val_loss: 0.0193\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0136 - val_loss: 0.0103\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0087 - val_loss: 0.0074\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 712us/step - loss: 0.0065 - val_loss: 0.0057\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 635us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 649us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 627us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0031 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0031 - 544ms/epoch - 509us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 550ms/epoch - 516us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 547ms/epoch - 513us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 569ms/epoch - 533us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 549ms/epoch - 514us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 552ms/epoch - 518us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 545ms/epoch - 511us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 554ms/epoch - 520us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 547ms/epoch - 513us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 545ms/epoch - 511us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 547ms/epoch - 512us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 544ms/epoch - 510us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 544ms/epoch - 510us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 540ms/epoch - 506us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 543ms/epoch - 509us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 542ms/epoch - 508us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 546ms/epoch - 511us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 549ms/epoch - 515us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0038 - 625ms/epoch - 586us/step\n",
      "267/267 - 0s - loss: 0.0034 - 127ms/epoch - 474us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  39.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2827 - val_loss: 0.0190\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0129 - val_loss: 0.0094\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0079 - val_loss: 0.0069\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0062 - val_loss: 0.0057\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 626us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 635us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 823us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 689us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0031 - val_loss: 0.0033\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 544ms/epoch - 510us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 575ms/epoch - 539us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 635ms/epoch - 595us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 584ms/epoch - 548us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 543ms/epoch - 509us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 552ms/epoch - 517us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 540ms/epoch - 506us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 543ms/epoch - 509us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 543ms/epoch - 509us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 573ms/epoch - 537us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0028 - 576ms/epoch - 539us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 573ms/epoch - 537us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 552ms/epoch - 517us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 541ms/epoch - 507us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 543ms/epoch - 509us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 541ms/epoch - 507us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 540ms/epoch - 506us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0029 - 553ms/epoch - 519us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 540ms/epoch - 506us/step\n",
      "267/267 - 0s - loss: 0.0028 - 121ms/epoch - 452us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  39.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2871 - val_loss: 0.0221\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0152 - val_loss: 0.0106\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0087 - val_loss: 0.0072\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0064 - val_loss: 0.0058\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 643us/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 642us/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 627us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 672us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 695us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 701us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 723us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 675us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 554ms/epoch - 519us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 545ms/epoch - 511us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0035 - 544ms/epoch - 510us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0031 - val_loss: 0.0029 - 562ms/epoch - 527us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 555ms/epoch - 520us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 544ms/epoch - 510us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 566ms/epoch - 530us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 545ms/epoch - 511us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 542ms/epoch - 508us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 558ms/epoch - 523us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 547ms/epoch - 512us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 610ms/epoch - 572us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 553ms/epoch - 518us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 552ms/epoch - 517us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 544ms/epoch - 510us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 550ms/epoch - 515us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 555ms/epoch - 521us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 552ms/epoch - 517us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 555ms/epoch - 520us/step\n",
      "267/267 - 0s - loss: 0.0028 - 122ms/epoch - 456us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  40.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2831 - val_loss: 0.0192\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.0137 - val_loss: 0.0101\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 654us/step - loss: 0.0084 - val_loss: 0.0070\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 711us/step - loss: 0.0062 - val_loss: 0.0056\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 670us/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 728us/step - loss: 0.0046 - val_loss: 0.0043\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 667us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 623us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 625us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 662us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 623us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 534ms/epoch - 500us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 582ms/epoch - 545us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0030 - 560ms/epoch - 525us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 571ms/epoch - 535us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 573ms/epoch - 537us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 562ms/epoch - 526us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 569ms/epoch - 533us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 556ms/epoch - 521us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 561ms/epoch - 526us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 556ms/epoch - 521us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 553ms/epoch - 518us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 549ms/epoch - 515us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 555ms/epoch - 521us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 551ms/epoch - 517us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 552ms/epoch - 518us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0029 - 562ms/epoch - 527us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 560ms/epoch - 525us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0039 - 555ms/epoch - 520us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 566ms/epoch - 530us/step\n",
      "267/267 - 0s - loss: 0.0028 - 126ms/epoch - 472us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  41.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3459 - val_loss: 0.2569\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 482us/step - loss: 0.1996 - val_loss: 0.1519\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 503us/step - loss: 0.1185 - val_loss: 0.0908\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 504us/step - loss: 0.0712 - val_loss: 0.0550\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 514us/step - loss: 0.0435 - val_loss: 0.0340\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 473us/step - loss: 0.0272 - val_loss: 0.0217\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 487us/step - loss: 0.0176 - val_loss: 0.0144\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 494us/step - loss: 0.0119 - val_loss: 0.0100\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 0.0085 - val_loss: 0.0074\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 464us/step - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 474us/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 482us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 445us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 447us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 478us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 481us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 458us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 469us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 1/20\n",
      "1334/1334 - 2s - loss: 0.0027 - val_loss: 0.0027 - 2s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "1334/1334 - 1s - loss: 0.0026 - val_loss: 0.0026 - 600ms/epoch - 449us/step\n",
      "Epoch 3/20\n",
      "1334/1334 - 1s - loss: 0.0026 - val_loss: 0.0026 - 580ms/epoch - 434us/step\n",
      "Epoch 4/20\n",
      "1334/1334 - 1s - loss: 0.0025 - val_loss: 0.0025 - 571ms/epoch - 428us/step\n",
      "Epoch 5/20\n",
      "1334/1334 - 1s - loss: 0.0025 - val_loss: 0.0025 - 530ms/epoch - 398us/step\n",
      "Epoch 6/20\n",
      "1334/1334 - 1s - loss: 0.0024 - val_loss: 0.0024 - 522ms/epoch - 392us/step\n",
      "Epoch 7/20\n",
      "1334/1334 - 1s - loss: 0.0024 - val_loss: 0.0024 - 525ms/epoch - 394us/step\n",
      "Epoch 8/20\n",
      "1334/1334 - 1s - loss: 0.0024 - val_loss: 0.0024 - 543ms/epoch - 407us/step\n",
      "Epoch 9/20\n",
      "1334/1334 - 1s - loss: 0.0023 - val_loss: 0.0023 - 567ms/epoch - 425us/step\n",
      "Epoch 10/20\n",
      "1334/1334 - 1s - loss: 0.0023 - val_loss: 0.0023 - 547ms/epoch - 410us/step\n",
      "Epoch 11/20\n",
      "1334/1334 - 1s - loss: 0.0022 - val_loss: 0.0022 - 553ms/epoch - 414us/step\n",
      "Epoch 12/20\n",
      "1334/1334 - 1s - loss: 0.0022 - val_loss: 0.0022 - 578ms/epoch - 433us/step\n",
      "Epoch 13/20\n",
      "1334/1334 - 1s - loss: 0.0022 - val_loss: 0.0022 - 612ms/epoch - 459us/step\n",
      "Epoch 14/20\n",
      "1334/1334 - 1s - loss: 0.0021 - val_loss: 0.0022 - 562ms/epoch - 421us/step\n",
      "Epoch 15/20\n",
      "1334/1334 - 1s - loss: 0.0021 - val_loss: 0.0021 - 599ms/epoch - 449us/step\n",
      "Epoch 16/20\n",
      "1334/1334 - 1s - loss: 0.0021 - val_loss: 0.0021 - 602ms/epoch - 451us/step\n",
      "Epoch 17/20\n",
      "1334/1334 - 1s - loss: 0.0021 - val_loss: 0.0021 - 528ms/epoch - 396us/step\n",
      "Epoch 18/20\n",
      "1334/1334 - 1s - loss: 0.0020 - val_loss: 0.0021 - 525ms/epoch - 393us/step\n",
      "Epoch 19/20\n",
      "1334/1334 - 1s - loss: 0.0020 - val_loss: 0.0020 - 519ms/epoch - 389us/step\n",
      "Epoch 20/20\n",
      "1334/1334 - 1s - loss: 0.0020 - val_loss: 0.0020 - 546ms/epoch - 409us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x317289cd0&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x317289cd0&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x317289cd0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x317289cd0&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x317289cd0>,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={'activation': ['relu', 'tanh'],\n",
       "                             'batch_size': [16, 32, 64],\n",
       "                             'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
       "                             'learning_rate': [0.001, 0.01, 0.1],\n",
       "                             'optimizer': ['sgd', 'adam']},\n",
       "              verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define base model function for bayes search\n",
    "def create_model(activation='relu', optimizer='adam', learning_rate=0.001, kernel_regularizer=None, batch_size=32):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'optimizer': ['sgd', 'adam'],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform random search hyperparameter tuning\n",
    "bayes_search = BayesSearchCV(estimator=keras_reg, search_spaces=param_dist, n_iter=10, cv=5, verbose=2, random_state=42, error_score='raise')\n",
    "bayes_search.fit(Xtrain, ytrain, sample_weight=wtrain, validation_data=(Xval, yval, wval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.505213</td>\n",
       "      <td>45.772704</td>\n",
       "      <td>0.254974</td>\n",
       "      <td>0.142270</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.002796</td>\n",
       "      <td>-0.002730</td>\n",
       "      <td>-0.002799</td>\n",
       "      <td>-0.002793</td>\n",
       "      <td>-0.002795</td>\n",
       "      <td>-0.002783</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.452278</td>\n",
       "      <td>11.530733</td>\n",
       "      <td>0.171502</td>\n",
       "      <td>0.008555</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.269539</td>\n",
       "      <td>-0.289035</td>\n",
       "      <td>-0.267694</td>\n",
       "      <td>-0.242272</td>\n",
       "      <td>-0.258110</td>\n",
       "      <td>-0.265330</td>\n",
       "      <td>0.015291</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.991496</td>\n",
       "      <td>0.848519</td>\n",
       "      <td>0.167091</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.006232</td>\n",
       "      <td>-0.006389</td>\n",
       "      <td>-0.005917</td>\n",
       "      <td>-0.006209</td>\n",
       "      <td>-0.006068</td>\n",
       "      <td>-0.006163</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132.749105</td>\n",
       "      <td>2.708723</td>\n",
       "      <td>0.390171</td>\n",
       "      <td>0.018474</td>\n",
       "      <td>tanh</td>\n",
       "      <td>16</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 16, 'kern...</td>\n",
       "      <td>-0.151107</td>\n",
       "      <td>-0.143950</td>\n",
       "      <td>-0.274928</td>\n",
       "      <td>-0.310833</td>\n",
       "      <td>-0.139528</td>\n",
       "      <td>-0.204069</td>\n",
       "      <td>0.073491</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.800706</td>\n",
       "      <td>1.794140</td>\n",
       "      <td>0.255131</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 32, 'kern...</td>\n",
       "      <td>-0.076903</td>\n",
       "      <td>-0.165621</td>\n",
       "      <td>-0.148347</td>\n",
       "      <td>-0.110676</td>\n",
       "      <td>-0.400030</td>\n",
       "      <td>-0.180315</td>\n",
       "      <td>0.114067</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>47.685086</td>\n",
       "      <td>0.784581</td>\n",
       "      <td>0.168622</td>\n",
       "      <td>0.013626</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.005406</td>\n",
       "      <td>-0.005187</td>\n",
       "      <td>-0.005390</td>\n",
       "      <td>-0.005070</td>\n",
       "      <td>-0.005279</td>\n",
       "      <td>-0.005267</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>47.084572</td>\n",
       "      <td>0.752071</td>\n",
       "      <td>0.170252</td>\n",
       "      <td>0.014930</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.077937</td>\n",
       "      <td>-0.387723</td>\n",
       "      <td>-0.083635</td>\n",
       "      <td>-0.050334</td>\n",
       "      <td>-0.084537</td>\n",
       "      <td>-0.136833</td>\n",
       "      <td>0.126065</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42.273023</td>\n",
       "      <td>0.395488</td>\n",
       "      <td>0.167234</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.001617</td>\n",
       "      <td>-0.001884</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>-0.001792</td>\n",
       "      <td>-0.001818</td>\n",
       "      <td>-0.001741</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36.761418</td>\n",
       "      <td>3.757783</td>\n",
       "      <td>0.144581</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.020660</td>\n",
       "      <td>-0.020468</td>\n",
       "      <td>-0.020449</td>\n",
       "      <td>-0.021147</td>\n",
       "      <td>-0.021387</td>\n",
       "      <td>-0.020822</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40.125172</td>\n",
       "      <td>0.551569</td>\n",
       "      <td>0.137422</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.003582</td>\n",
       "      <td>-0.003393</td>\n",
       "      <td>-0.002792</td>\n",
       "      <td>-0.002813</td>\n",
       "      <td>-0.002806</td>\n",
       "      <td>-0.003077</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      83.505213     45.772704         0.254974        0.142270   \n",
       "1      54.452278     11.530733         0.171502        0.008555   \n",
       "2      41.991496      0.848519         0.167091        0.001235   \n",
       "3     132.749105      2.708723         0.390171        0.018474   \n",
       "4      76.800706      1.794140         0.255131        0.009214   \n",
       "5      47.685086      0.784581         0.168622        0.013626   \n",
       "6      47.084572      0.752071         0.170252        0.014930   \n",
       "7      42.273023      0.395488         0.167234        0.005081   \n",
       "8      36.761418      3.757783         0.144581        0.006500   \n",
       "9      40.125172      0.551569         0.137422        0.003076   \n",
       "\n",
       "  param_activation param_batch_size param_kernel_regularizer  \\\n",
       "0             relu               64                    l1_l2   \n",
       "1             tanh               64                       l1   \n",
       "2             relu               64                       l1   \n",
       "3             tanh               16                       l2   \n",
       "4             tanh               32                       l2   \n",
       "5             tanh               64                       l1   \n",
       "6             tanh               64                       l2   \n",
       "7             tanh               64                       l2   \n",
       "8             tanh               64                    l1_l2   \n",
       "9             relu               64                    l1_l2   \n",
       "\n",
       "  param_learning_rate param_optimizer  \\\n",
       "0               0.001            adam   \n",
       "1                 0.1            adam   \n",
       "2                0.01             sgd   \n",
       "3                 0.1            adam   \n",
       "4                 0.1            adam   \n",
       "5               0.001            adam   \n",
       "6                 0.1            adam   \n",
       "7                 0.1             sgd   \n",
       "8                0.01             sgd   \n",
       "9               0.001            adam   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.002796   \n",
       "1  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.269539   \n",
       "2  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.006232   \n",
       "3  {'activation': 'tanh', 'batch_size': 16, 'kern...          -0.151107   \n",
       "4  {'activation': 'tanh', 'batch_size': 32, 'kern...          -0.076903   \n",
       "5  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.005406   \n",
       "6  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.077937   \n",
       "7  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.001617   \n",
       "8  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.020660   \n",
       "9  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.003582   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0          -0.002730          -0.002799          -0.002793          -0.002795   \n",
       "1          -0.289035          -0.267694          -0.242272          -0.258110   \n",
       "2          -0.006389          -0.005917          -0.006209          -0.006068   \n",
       "3          -0.143950          -0.274928          -0.310833          -0.139528   \n",
       "4          -0.165621          -0.148347          -0.110676          -0.400030   \n",
       "5          -0.005187          -0.005390          -0.005070          -0.005279   \n",
       "6          -0.387723          -0.083635          -0.050334          -0.084537   \n",
       "7          -0.001884          -0.001595          -0.001792          -0.001818   \n",
       "8          -0.020468          -0.020449          -0.021147          -0.021387   \n",
       "9          -0.003393          -0.002792          -0.002813          -0.002806   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0        -0.002783        0.000026                2  \n",
       "1        -0.265330        0.015291               10  \n",
       "2        -0.006163        0.000160                5  \n",
       "3        -0.204069        0.073491                9  \n",
       "4        -0.180315        0.114067                8  \n",
       "5        -0.005267        0.000126                4  \n",
       "6        -0.136833        0.126065                7  \n",
       "7        -0.001741        0.000114                1  \n",
       "8        -0.020822        0.000378                6  \n",
       "9        -0.003077        0.000340                3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bayes_search_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "\n",
    "display(bayes_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_8512/3282579545.py:23: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 28452\n",
      "max_resources_: 85357\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 6\n",
      "n_resources: 28452\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2702 - val_loss: 0.0429\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0366 - val_loss: 0.0324\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0294 - val_loss: 0.0283\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0253 - val_loss: 0.0239\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0223 - val_loss: 0.0206\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0194 - val_loss: 0.0174\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0157 - val_loss: 0.0144\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.0136 - val_loss: 0.0131\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0126 - val_loss: 0.0119\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0118 - val_loss: 0.0113\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0091 - val_loss: 0.0088\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 0.0078 - val_loss: 0.0076\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 621us/step - loss: 0.0073 - val_loss: 0.0075\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0071 - 299ms/epoch - 420us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0070 - 309ms/epoch - 434us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0069 - 301ms/epoch - 422us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0068 - 288ms/epoch - 404us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0067 - 297ms/epoch - 418us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0067 - 322ms/epoch - 453us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0066 - 292ms/epoch - 410us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0065 - 303ms/epoch - 426us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0065 - 300ms/epoch - 422us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0064 - 334ms/epoch - 470us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0064 - 304ms/epoch - 427us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0063 - 292ms/epoch - 410us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0062 - 291ms/epoch - 409us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0062 - 295ms/epoch - 415us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0062 - 316ms/epoch - 443us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0061 - 298ms/epoch - 419us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0060 - 295ms/epoch - 415us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0060 - 292ms/epoch - 410us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0060 - 285ms/epoch - 400us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0059 - 300ms/epoch - 422us/step\n",
      "178/178 - 0s - loss: 0.0061 - 100ms/epoch - 563us/step\n",
      "712/712 - 0s - loss: 0.0061 - 219ms/epoch - 307us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  30.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2987 - val_loss: 0.0456\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0392 - val_loss: 0.0390\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 0.0310 - val_loss: 0.0281\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0259 - val_loss: 0.0249\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0218 - val_loss: 0.0189\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0171 - val_loss: 0.0161\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0150 - val_loss: 0.0142\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0137 - val_loss: 0.0130\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0127 - val_loss: 0.0124\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0117 - val_loss: 0.0113\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 520us/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0090 - val_loss: 0.0086\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 738us/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0071 - 345ms/epoch - 485us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0069 - 331ms/epoch - 465us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0069 - 332ms/epoch - 466us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0068 - 290ms/epoch - 407us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0068 - 288ms/epoch - 405us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0066 - 321ms/epoch - 451us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0066 - 295ms/epoch - 415us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0065 - 313ms/epoch - 440us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0065 - 298ms/epoch - 419us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0064 - 296ms/epoch - 416us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0064 - 293ms/epoch - 412us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0063 - 296ms/epoch - 416us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0063 - 300ms/epoch - 421us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0062 - 303ms/epoch - 425us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0062 - 289ms/epoch - 406us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0061 - 288ms/epoch - 405us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0061 - 296ms/epoch - 416us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0060 - 300ms/epoch - 421us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0060 - 305ms/epoch - 428us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0060 - 307ms/epoch - 431us/step\n",
      "178/178 - 0s - loss: 0.0058 - 107ms/epoch - 601us/step\n",
      "712/712 - 0s - loss: 0.0057 - 236ms/epoch - 331us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  29.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2455 - val_loss: 0.0414\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0356 - val_loss: 0.0310\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 0.0290 - val_loss: 0.0265\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0248 - val_loss: 0.0223\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0202 - val_loss: 0.0178\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0165 - val_loss: 0.0156\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 0.0148 - val_loss: 0.0140\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0137 - val_loss: 0.0129\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0127 - val_loss: 0.0123\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 0.0098 - val_loss: 0.0093\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0088 - val_loss: 0.0085\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0072 - 288ms/epoch - 404us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0070 - 286ms/epoch - 402us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0069 - 285ms/epoch - 401us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0068 - 288ms/epoch - 405us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0067 - 284ms/epoch - 399us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0066 - 287ms/epoch - 403us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0066 - 283ms/epoch - 397us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0066 - 288ms/epoch - 404us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0065 - 286ms/epoch - 401us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0064 - 285ms/epoch - 400us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0063 - 285ms/epoch - 401us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0063 - 287ms/epoch - 404us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0063 - 308ms/epoch - 432us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0062 - 286ms/epoch - 402us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0061 - 284ms/epoch - 399us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0061 - 285ms/epoch - 400us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0061 - 283ms/epoch - 397us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0060 - 285ms/epoch - 400us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0060 - 289ms/epoch - 405us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0059 - 288ms/epoch - 404us/step\n",
      "178/178 - 0s - loss: 0.0059 - 93ms/epoch - 525us/step\n",
      "712/712 - 0s - loss: 0.0058 - 188ms/epoch - 263us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  29.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2322 - val_loss: 0.0410\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0364 - val_loss: 0.0318\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0297 - val_loss: 0.0271\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0257 - val_loss: 0.0254\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0208 - val_loss: 0.0183\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 0.0169 - val_loss: 0.0154\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0151 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0138 - val_loss: 0.0137\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 522us/step - loss: 0.0127 - val_loss: 0.0124\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0118 - val_loss: 0.0114\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 521us/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0090 - val_loss: 0.0087\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 524us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0072 - 286ms/epoch - 402us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0071 - 284ms/epoch - 398us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0070 - 283ms/epoch - 397us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0069 - 286ms/epoch - 401us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0068 - 283ms/epoch - 398us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0067 - 284ms/epoch - 399us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0067 - 284ms/epoch - 399us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0067 - 283ms/epoch - 398us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0065 - 284ms/epoch - 399us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0065 - 285ms/epoch - 401us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0064 - 326ms/epoch - 458us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0064 - 289ms/epoch - 406us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0063 - 284ms/epoch - 399us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0063 - 284ms/epoch - 399us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0063 - 283ms/epoch - 398us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0062 - 285ms/epoch - 400us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0062 - 283ms/epoch - 398us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0061 - 310ms/epoch - 435us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0061 - 314ms/epoch - 442us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0060 - 283ms/epoch - 397us/step\n",
      "178/178 - 0s - loss: 0.0059 - 95ms/epoch - 536us/step\n",
      "712/712 - 0s - loss: 0.0060 - 188ms/epoch - 264us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  29.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3049 - val_loss: 0.0483\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0398 - val_loss: 0.0339\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0306 - val_loss: 0.0282\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0259 - val_loss: 0.0236\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 0.0227 - val_loss: 0.0209\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0197 - val_loss: 0.0175\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0161 - val_loss: 0.0144\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0137 - val_loss: 0.0133\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0126 - val_loss: 0.0119\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0117 - val_loss: 0.0119\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0089 - val_loss: 0.0085\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0085 - val_loss: 0.0082\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0072 - 295ms/epoch - 414us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0071 - 481ms/epoch - 676us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0071 - 324ms/epoch - 455us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0070 - 322ms/epoch - 452us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0069 - 287ms/epoch - 402us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0069 - 289ms/epoch - 406us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0068 - 293ms/epoch - 411us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0067 - 319ms/epoch - 448us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0066 - 302ms/epoch - 425us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0066 - 343ms/epoch - 482us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0066 - 311ms/epoch - 437us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0065 - 289ms/epoch - 406us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0064 - 287ms/epoch - 404us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0064 - 315ms/epoch - 442us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0063 - 325ms/epoch - 456us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0063 - 324ms/epoch - 455us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0062 - 344ms/epoch - 483us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0062 - 306ms/epoch - 429us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0062 - 325ms/epoch - 457us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0062 - 317ms/epoch - 445us/step\n",
      "178/178 - 0s - loss: 0.0060 - 96ms/epoch - 541us/step\n",
      "712/712 - 0s - loss: 0.0060 - 187ms/epoch - 263us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  29.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0636 - val_loss: 0.0041\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0012 - val_loss: 9.1677e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 9.9316e-04 - val_loss: 7.8574e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 9.6023e-04 - val_loss: 0.0010\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 9.1996e-04 - val_loss: 6.9983e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 8.7277e-04 - val_loss: 0.0020\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 9.2581e-04 - val_loss: 7.4659e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 8.5349e-04 - val_loss: 7.3798e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 8.1265e-04 - val_loss: 6.2289e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 8.8748e-04 - val_loss: 6.0924e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 7.7065e-04 - val_loss: 6.8096e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 8.5081e-04 - val_loss: 6.2276e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 7.8043e-04 - val_loss: 5.5898e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 9.1696e-04 - 301ms/epoch - 423us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0011 - 307ms/epoch - 431us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0011 - 289ms/epoch - 405us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 7.8322e-04 - 294ms/epoch - 413us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0010 - 300ms/epoch - 422us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0011 - 301ms/epoch - 422us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 9.6353e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0012 - 290ms/epoch - 407us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 8.7459e-04 - 310ms/epoch - 435us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 9.7411e-04 - 290ms/epoch - 407us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 9.4979e-04 - 287ms/epoch - 403us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 8.7803e-04 - 302ms/epoch - 423us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 8.7515e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 9.6249e-04 - 300ms/epoch - 421us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 8.3450e-04 - 303ms/epoch - 425us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0011 - 308ms/epoch - 433us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0010 - 317ms/epoch - 445us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 8.0046e-04 - 293ms/epoch - 412us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 9.5286e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 9.6723e-04 - 292ms/epoch - 411us/step\n",
      "178/178 - 0s - loss: 5.3872e-04 - 100ms/epoch - 564us/step\n",
      "712/712 - 0s - loss: 5.4062e-04 - 203ms/epoch - 285us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0736 - val_loss: 0.0040\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0029 - val_loss: 0.0021\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0011 - val_loss: 9.0283e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0010 - val_loss: 7.8236e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 8.9758e-04 - val_loss: 7.2605e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 8.9630e-04 - val_loss: 6.4442e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 8.1002e-04 - val_loss: 6.7754e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 8.1330e-04 - val_loss: 6.1533e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 7.5225e-04 - val_loss: 6.3136e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 7.0115e-04 - val_loss: 9.3229e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 524us/step - loss: 7.2214e-04 - val_loss: 6.4005e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 6.8400e-04 - val_loss: 0.0011\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 6.8444e-04 - val_loss: 6.1563e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 6.2951e-04 - val_loss: 0.0012\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 521us/step - loss: 6.8112e-04 - val_loss: 7.8356e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 6.1777e-04 - val_loss: 0.0010\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 6.1372e-04 - val_loss: 8.0141e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 8.8636e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 8.1884e-04 - 283ms/epoch - 398us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 9.0465e-04 - 283ms/epoch - 397us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 8.3738e-04 - 294ms/epoch - 413us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 7.0057e-04 - 375ms/epoch - 527us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 7.7151e-04 - 349ms/epoch - 490us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 8.8219e-04 - 315ms/epoch - 442us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 7.3520e-04 - 302ms/epoch - 424us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 7.4424e-04 - 342ms/epoch - 480us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 8.0089e-04 - 335ms/epoch - 470us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 7.4510e-04 - 309ms/epoch - 434us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 8.4922e-04 - 289ms/epoch - 407us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 8.3310e-04 - 328ms/epoch - 461us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 7.7766e-04 - 336ms/epoch - 473us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 6.8905e-04 - 370ms/epoch - 520us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0010 - 331ms/epoch - 464us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 5.8204e-04 - 312ms/epoch - 438us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 8.1894e-04 - 321ms/epoch - 451us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 8.0951e-04 - 323ms/epoch - 454us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 7.8586e-04 - 310ms/epoch - 435us/step\n",
      "178/178 - 0s - loss: 6.0381e-04 - 95ms/epoch - 536us/step\n",
      "712/712 - 0s - loss: 5.9075e-04 - 188ms/epoch - 263us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0676 - val_loss: 0.0033\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0026 - val_loss: 0.0019\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0011 - val_loss: 9.2976e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0010 - val_loss: 9.3339e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 9.3098e-04 - val_loss: 7.3364e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 8.6409e-04 - val_loss: 0.0012\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 790us/step - loss: 8.2639e-04 - val_loss: 9.6681e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 672us/step - loss: 7.8378e-04 - val_loss: 6.3112e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 7.9126e-04 - val_loss: 5.4812e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 7.2436e-04 - val_loss: 5.6053e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 7.2356e-04 - val_loss: 5.5284e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 6.9959e-04 - val_loss: 5.0838e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 6.1950e-04 - val_loss: 4.8499e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 6.5612e-04 - val_loss: 9.3581e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 6.6063e-04 - val_loss: 5.5049e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 6.2308e-04 - val_loss: 5.3285e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 6.4126e-04 - val_loss: 4.3894e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 5.6007e-04 - val_loss: 4.7128e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0011 - 290ms/epoch - 407us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 7.3618e-04 - 285ms/epoch - 401us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 6.4423e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 7.1120e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 7.8853e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 6.2724e-04 - 271ms/epoch - 380us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 8.3135e-04 - 252ms/epoch - 354us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 6.0406e-04 - 288ms/epoch - 405us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 8.4770e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 7.3026e-04 - 323ms/epoch - 454us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 7.9899e-04 - 291ms/epoch - 409us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 7.2161e-04 - 291ms/epoch - 409us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 8.8931e-04 - 286ms/epoch - 402us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 6.3998e-04 - 282ms/epoch - 397us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 9.7998e-04 - 283ms/epoch - 398us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 6.2698e-04 - 287ms/epoch - 403us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 9.4142e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 6.4851e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 7.8842e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 7.8052e-04 - 281ms/epoch - 395us/step\n",
      "178/178 - 0s - loss: 5.7786e-04 - 94ms/epoch - 528us/step\n",
      "712/712 - 0s - loss: 5.7385e-04 - 186ms/epoch - 261us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0736 - val_loss: 0.0038\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0012 - val_loss: 9.3500e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 0.0011 - val_loss: 9.1200e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0010 - val_loss: 0.0024\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 9.3994e-04 - val_loss: 9.3041e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 9.5869e-04 - val_loss: 7.1454e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 8.7084e-04 - val_loss: 0.0012\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 9.1470e-04 - val_loss: 9.0216e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 8.7564e-04 - val_loss: 6.9273e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 8.7835e-04 - val_loss: 6.4247e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 8.0217e-04 - val_loss: 0.0013\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 8.6199e-04 - val_loss: 5.8411e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 8.0979e-04 - val_loss: 8.5778e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 8.0882e-04 - val_loss: 5.9530e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0014 - 287ms/epoch - 403us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 9.0090e-04 - 285ms/epoch - 401us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 9.3735e-04 - 292ms/epoch - 411us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 9.1295e-04 - 290ms/epoch - 407us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 8.9804e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 8.8573e-04 - 305ms/epoch - 428us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0011 - 309ms/epoch - 434us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0011 - 331ms/epoch - 465us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 9.4730e-04 - 321ms/epoch - 451us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 8.5011e-04 - 317ms/epoch - 445us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 9.9331e-04 - 322ms/epoch - 452us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 8.9836e-04 - 342ms/epoch - 481us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 9.5030e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0012 - 301ms/epoch - 423us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 7.6186e-04 - 304ms/epoch - 427us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0010 - 306ms/epoch - 430us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 8.9986e-04 - 306ms/epoch - 430us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0011 - 301ms/epoch - 423us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 9.0481e-04 - 301ms/epoch - 422us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 8.3757e-04 - 330ms/epoch - 463us/step\n",
      "178/178 - 0s - loss: 5.0953e-04 - 158ms/epoch - 887us/step\n",
      "712/712 - 0s - loss: 5.1653e-04 - 231ms/epoch - 324us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.3s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0616 - val_loss: 0.0035\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0013 - val_loss: 9.9422e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 9.3339e-04 - val_loss: 7.9821e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 9.5204e-04 - val_loss: 7.2052e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 9.2679e-04 - val_loss: 6.8772e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 8.7810e-04 - val_loss: 0.0024\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 8.7902e-04 - val_loss: 7.1993e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 7.9799e-04 - val_loss: 6.2939e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 7.9586e-04 - val_loss: 5.8022e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 8.3272e-04 - val_loss: 0.0012\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 7.5730e-04 - val_loss: 0.0013\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 7.5863e-04 - val_loss: 5.5145e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 7.6443e-04 - val_loss: 7.1899e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0011 - 298ms/epoch - 418us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 9.8922e-04 - 395ms/epoch - 555us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 9.6770e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 8.9430e-04 - 326ms/epoch - 458us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 9.5282e-04 - 316ms/epoch - 443us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 8.7747e-04 - 302ms/epoch - 424us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 8.5198e-04 - 290ms/epoch - 407us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 8.8333e-04 - 377ms/epoch - 530us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0010 - 292ms/epoch - 410us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 7.8991e-04 - 297ms/epoch - 417us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 9.9667e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 8.6676e-04 - 300ms/epoch - 421us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 8.8890e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 8.6809e-04 - 332ms/epoch - 466us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0010 - 291ms/epoch - 409us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 8.7239e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 9.9363e-04 - 382ms/epoch - 537us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 8.5742e-04 - 305ms/epoch - 429us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 8.4572e-04 - 367ms/epoch - 515us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 8.0631e-04 - 312ms/epoch - 438us/step\n",
      "178/178 - 0s - loss: 4.7523e-04 - 103ms/epoch - 581us/step\n",
      "712/712 - 0s - loss: 4.7434e-04 - 200ms/epoch - 281us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.9s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2983 - val_loss: 0.0457\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0385 - val_loss: 0.0349\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0301 - val_loss: 0.0304\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0257 - val_loss: 0.0242\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0225 - val_loss: 0.0218\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0191 - val_loss: 0.0169\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0157 - val_loss: 0.0150\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0140 - val_loss: 0.0135\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0128 - val_loss: 0.0129\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0120 - val_loss: 0.0113\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0098 - val_loss: 0.0095\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 625us/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0074 - 305ms/epoch - 429us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0073 - 320ms/epoch - 449us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0071 - 299ms/epoch - 419us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0071 - 300ms/epoch - 421us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0070 - 299ms/epoch - 420us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0070 - 312ms/epoch - 438us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0069 - 315ms/epoch - 442us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0069 - 320ms/epoch - 449us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0068 - 306ms/epoch - 430us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0067 - 309ms/epoch - 433us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0066 - 305ms/epoch - 428us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0066 - 294ms/epoch - 413us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0065 - 303ms/epoch - 425us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0065 - 301ms/epoch - 423us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0065 - 301ms/epoch - 422us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0064 - 313ms/epoch - 440us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0064 - 304ms/epoch - 426us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0063 - 324ms/epoch - 455us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0063 - 296ms/epoch - 416us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0063 - 321ms/epoch - 451us/step\n",
      "178/178 - 0s - loss: 0.0068 - 140ms/epoch - 787us/step\n",
      "712/712 - 0s - loss: 0.0068 - 200ms/epoch - 281us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  30.3s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2975 - val_loss: 0.0474\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0388 - val_loss: 0.0331\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 687us/step - loss: 0.0305 - val_loss: 0.0271\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 625us/step - loss: 0.0261 - val_loss: 0.0235\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 642us/step - loss: 0.0225 - val_loss: 0.0202\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0189 - val_loss: 0.0167\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0159 - val_loss: 0.0161\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0144 - val_loss: 0.0134\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 642us/step - loss: 0.0132 - val_loss: 0.0134\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 0.0122 - val_loss: 0.0115\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0094 - val_loss: 0.0094\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0090 - val_loss: 0.0087\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 667us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0078 - val_loss: 0.0075\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0076 - 343ms/epoch - 482us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0074 - 315ms/epoch - 442us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0073 - 317ms/epoch - 445us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0073 - 315ms/epoch - 443us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0071 - 316ms/epoch - 444us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0071 - 329ms/epoch - 462us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0071 - 337ms/epoch - 473us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0069 - 291ms/epoch - 408us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0070 - 334ms/epoch - 469us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0069 - 367ms/epoch - 515us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0068 - 293ms/epoch - 411us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0068 - 315ms/epoch - 442us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0067 - 310ms/epoch - 436us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0068 - 303ms/epoch - 425us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0067 - 310ms/epoch - 436us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0066 - 332ms/epoch - 466us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0066 - 326ms/epoch - 459us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0066 - 308ms/epoch - 432us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0065 - 298ms/epoch - 419us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0064 - 309ms/epoch - 434us/step\n",
      "178/178 - 0s - loss: 0.0063 - 110ms/epoch - 616us/step\n",
      "712/712 - 0s - loss: 0.0062 - 203ms/epoch - 285us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  31.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3023 - val_loss: 0.0452\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0376 - val_loss: 0.0317\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0295 - val_loss: 0.0291\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.0250 - val_loss: 0.0233\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 640us/step - loss: 0.0214 - val_loss: 0.0194\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0174 - val_loss: 0.0159\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0150 - val_loss: 0.0138\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0136 - val_loss: 0.0128\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0126 - val_loss: 0.0122\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0096 - val_loss: 0.0092\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0091 - val_loss: 0.0087\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0087 - val_loss: 0.0084\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0075 - 309ms/epoch - 433us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0074 - 293ms/epoch - 411us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0072 - 289ms/epoch - 405us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0072 - 288ms/epoch - 405us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0071 - 291ms/epoch - 409us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0071 - 289ms/epoch - 406us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0070 - 289ms/epoch - 405us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0069 - 289ms/epoch - 406us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0069 - 290ms/epoch - 407us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0069 - 289ms/epoch - 406us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0068 - 288ms/epoch - 404us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0068 - 290ms/epoch - 408us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0067 - 290ms/epoch - 407us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0066 - 289ms/epoch - 406us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0066 - 291ms/epoch - 409us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0065 - 288ms/epoch - 405us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0065 - 291ms/epoch - 408us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0064 - 289ms/epoch - 405us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0064 - 297ms/epoch - 417us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0064 - 289ms/epoch - 407us/step\n",
      "178/178 - 0s - loss: 0.0061 - 97ms/epoch - 546us/step\n",
      "712/712 - 0s - loss: 0.0060 - 190ms/epoch - 267us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  29.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3010 - val_loss: 0.0466\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0388 - val_loss: 0.0339\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0305 - val_loss: 0.0274\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0263 - val_loss: 0.0250\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0228 - val_loss: 0.0213\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0189 - val_loss: 0.0168\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0160 - val_loss: 0.0176\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0132 - val_loss: 0.0143\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0123 - val_loss: 0.0125\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0096 - val_loss: 0.0091\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0091 - val_loss: 0.0088\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0087 - val_loss: 0.0091\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0078 - 292ms/epoch - 410us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0076 - 289ms/epoch - 406us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0075 - 290ms/epoch - 407us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0074 - 292ms/epoch - 410us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0074 - 291ms/epoch - 408us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0072 - 291ms/epoch - 408us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0072 - 293ms/epoch - 411us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0071 - 293ms/epoch - 411us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0071 - 288ms/epoch - 404us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0069 - 295ms/epoch - 414us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0070 - 288ms/epoch - 404us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0068 - 299ms/epoch - 420us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0069 - 290ms/epoch - 407us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0068 - 285ms/epoch - 400us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0068 - 305ms/epoch - 429us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0067 - 285ms/epoch - 401us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0067 - 284ms/epoch - 398us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0066 - 289ms/epoch - 405us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0066 - 284ms/epoch - 400us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0065 - 303ms/epoch - 425us/step\n",
      "178/178 - 0s - loss: 0.0063 - 101ms/epoch - 565us/step\n",
      "712/712 - 0s - loss: 0.0064 - 189ms/epoch - 266us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  29.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2946 - val_loss: 0.0457\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 623us/step - loss: 0.0370 - val_loss: 0.0325\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0293 - val_loss: 0.0262\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.0250 - val_loss: 0.0274\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0219 - val_loss: 0.0203\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0180 - val_loss: 0.0160\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0155 - val_loss: 0.0147\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0140 - val_loss: 0.0135\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0130 - val_loss: 0.0127\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 698us/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 669us/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 624us/step - loss: 0.0085 - val_loss: 0.0087\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 0.0079 - val_loss: 0.0087\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0076 - val_loss: 0.0084\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0074 - 317ms/epoch - 445us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0073 - 316ms/epoch - 443us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0072 - 310ms/epoch - 436us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0071 - 310ms/epoch - 435us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0071 - 318ms/epoch - 447us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0070 - 308ms/epoch - 432us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0070 - 316ms/epoch - 443us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0069 - 320ms/epoch - 450us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0069 - 312ms/epoch - 438us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0068 - 330ms/epoch - 464us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0067 - 303ms/epoch - 425us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0067 - 318ms/epoch - 447us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0067 - 319ms/epoch - 448us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0066 - 315ms/epoch - 443us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0065 - 314ms/epoch - 442us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0066 - 374ms/epoch - 525us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0064 - 314ms/epoch - 441us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0064 - 328ms/epoch - 460us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0064 - 307ms/epoch - 431us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0063 - 306ms/epoch - 429us/step\n",
      "178/178 - 0s - loss: 0.0061 - 107ms/epoch - 600us/step\n",
      "712/712 - 0s - loss: 0.0061 - 199ms/epoch - 279us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  31.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2476 - val_loss: 0.0219\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 0.0136 - val_loss: 0.0092\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0077 - val_loss: 0.0064\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 699us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 629us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0029 - val_loss: 0.0043\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0031 - 291ms/epoch - 409us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0029 - 291ms/epoch - 409us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0029 - 321ms/epoch - 451us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 302ms/epoch - 424us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0028 - 318ms/epoch - 446us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0029 - 284ms/epoch - 399us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 283ms/epoch - 398us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0028 - 289ms/epoch - 406us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 312ms/epoch - 438us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0028 - 303ms/epoch - 425us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 307ms/epoch - 431us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0032 - 303ms/epoch - 425us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0028 - 304ms/epoch - 427us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0028 - 327ms/epoch - 459us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 302ms/epoch - 424us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0028 - 304ms/epoch - 427us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0030 - 305ms/epoch - 428us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0029 - 304ms/epoch - 427us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0028 - 310ms/epoch - 435us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0028 - 298ms/epoch - 418us/step\n",
      "178/178 - 0s - loss: 0.0028 - 102ms/epoch - 573us/step\n",
      "712/712 - 0s - loss: 0.0028 - 206ms/epoch - 289us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  30.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2686 - val_loss: 0.0217\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0143 - val_loss: 0.0102\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0085 - val_loss: 0.0072\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0064 - val_loss: 0.0058\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0047 - val_loss: 0.0044\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 676us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 654us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0033 - 288ms/epoch - 405us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0029 - 284ms/epoch - 398us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 284ms/epoch - 399us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 281ms/epoch - 394us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0030 - 283ms/epoch - 397us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0029 - 301ms/epoch - 422us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 296ms/epoch - 416us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0029 - 303ms/epoch - 426us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 296ms/epoch - 416us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0030 - 289ms/epoch - 406us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 301ms/epoch - 422us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0030 - 282ms/epoch - 397us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0028 - 281ms/epoch - 394us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0029 - 294ms/epoch - 413us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0030 - 290ms/epoch - 408us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0028 - 281ms/epoch - 395us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0028 - 282ms/epoch - 396us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0029 - 286ms/epoch - 402us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 297ms/epoch - 417us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0028 - 292ms/epoch - 410us/step\n",
      "178/178 - 0s - loss: 0.0028 - 101ms/epoch - 567us/step\n",
      "712/712 - 0s - loss: 0.0028 - 195ms/epoch - 273us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  31.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2477 - val_loss: 0.0224\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0142 - val_loss: 0.0098\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0083 - val_loss: 0.0072\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 0.0064 - val_loss: 0.0059\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0044 - val_loss: 0.0048\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0032 - 305ms/epoch - 428us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 297ms/epoch - 417us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 321ms/epoch - 451us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 309ms/epoch - 434us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0030 - 298ms/epoch - 418us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 289ms/epoch - 406us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0031 - 301ms/epoch - 422us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0029 - 323ms/epoch - 454us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 307ms/epoch - 431us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0031 - 318ms/epoch - 446us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 308ms/epoch - 432us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0029 - 305ms/epoch - 428us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0030 - 292ms/epoch - 410us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0029 - 285ms/epoch - 400us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 305ms/epoch - 428us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0029 - 319ms/epoch - 448us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0033 - 296ms/epoch - 416us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0028 - 294ms/epoch - 413us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 310ms/epoch - 435us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0030 - 303ms/epoch - 425us/step\n",
      "178/178 - 0s - loss: 0.0029 - 96ms/epoch - 542us/step\n",
      "712/712 - 0s - loss: 0.0029 - 227ms/epoch - 318us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  30.3s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2470 - val_loss: 0.0166\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 639us/step - loss: 0.0116 - val_loss: 0.0084\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0073 - val_loss: 0.0063\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 524us/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0049 - val_loss: 0.0045\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 522us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0031 - val_loss: 0.0033\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 521us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0033 - 284ms/epoch - 399us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0029 - 302ms/epoch - 425us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 298ms/epoch - 419us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0029 - 318ms/epoch - 447us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0030 - 317ms/epoch - 445us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 407us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 287ms/epoch - 403us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0029 - 289ms/epoch - 406us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 283ms/epoch - 398us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0029 - 291ms/epoch - 409us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0030 - 285ms/epoch - 400us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0029 - 283ms/epoch - 397us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0029 - 283ms/epoch - 397us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0030 - 283ms/epoch - 398us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 281ms/epoch - 395us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0031 - 292ms/epoch - 410us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 407us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0030 - 287ms/epoch - 403us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0028 - 288ms/epoch - 404us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0029 - 284ms/epoch - 399us/step\n",
      "178/178 - 0s - loss: 0.0028 - 94ms/epoch - 527us/step\n",
      "712/712 - 0s - loss: 0.0028 - 232ms/epoch - 325us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  29.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2571 - val_loss: 0.0229\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 524us/step - loss: 0.0153 - val_loss: 0.0110\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0088 - val_loss: 0.0074\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0066 - val_loss: 0.0059\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 518us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 520us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0030 - val_loss: 0.0056\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0031 - 322ms/epoch - 453us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 294ms/epoch - 413us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 308ms/epoch - 433us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0031 - 307ms/epoch - 431us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0030 - 309ms/epoch - 434us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 366ms/epoch - 513us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 286ms/epoch - 401us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 407us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 316ms/epoch - 443us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0030 - 329ms/epoch - 463us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0030 - 310ms/epoch - 435us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0029 - 384ms/epoch - 539us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0029 - 309ms/epoch - 433us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0030 - 303ms/epoch - 426us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0031 - 367ms/epoch - 515us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0029 - 293ms/epoch - 411us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 407us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0030 - 288ms/epoch - 404us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 286ms/epoch - 402us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0030 - 289ms/epoch - 405us/step\n",
      "178/178 - 0s - loss: 0.0028 - 98ms/epoch - 552us/step\n",
      "712/712 - 0s - loss: 0.0028 - 191ms/epoch - 268us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  29.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0635 - val_loss: 0.0023\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0015 - val_loss: 9.9710e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 8.8471e-04 - val_loss: 6.2602e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 6.7155e-04 - val_loss: 4.8345e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 5.3223e-04 - val_loss: 3.7894e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 4.7853e-04 - val_loss: 3.0975e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 4.3801e-04 - val_loss: 2.7641e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 4.1851e-04 - val_loss: 3.3645e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 3.2140e-04 - val_loss: 2.8607e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 4.1444e-04 - val_loss: 2.2163e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 3.4879e-04 - val_loss: 2.0446e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 3.5056e-04 - val_loss: 2.0562e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 3.2330e-04 - val_loss: 1.8803e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 3.4069e-04 - val_loss: 2.0894e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 3.4158e-04 - val_loss: 1.8026e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 2.3990e-04 - val_loss: 1.7315e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 2.9932e-04 - val_loss: 1.6471e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 2.8010e-04 - val_loss: 2.0883e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 3.2401e-04 - val_loss: 3.0947e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 2.8358e-04 - val_loss: 1.9942e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 7.3190e-04 - 310ms/epoch - 436us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 4.6710e-04 - 306ms/epoch - 429us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 6.8766e-04 - 311ms/epoch - 437us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 2.8799e-04 - 287ms/epoch - 403us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 3.4799e-04 - 315ms/epoch - 443us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 3.8437e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 5.2544e-04 - 364ms/epoch - 511us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 1.9738e-04 - 336ms/epoch - 472us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 3.6380e-04 - 311ms/epoch - 437us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 8.0577e-04 - 335ms/epoch - 470us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 2.2026e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 3.1208e-04 - 350ms/epoch - 491us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 2.6126e-04 - 304ms/epoch - 427us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 6.0984e-04 - 339ms/epoch - 476us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 2.0672e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 3.4767e-04 - 301ms/epoch - 423us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 3.6408e-04 - 294ms/epoch - 412us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 2.1989e-04 - 297ms/epoch - 418us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 5.4372e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 3.6143e-04 - 349ms/epoch - 490us/step\n",
      "178/178 - 0s - loss: 2.0673e-04 - 110ms/epoch - 620us/step\n",
      "712/712 - 0s - loss: 2.0755e-04 - 216ms/epoch - 303us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  30.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0590 - val_loss: 0.0023\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 0.0015 - val_loss: 9.2614e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 518us/step - loss: 8.5254e-04 - val_loss: 5.9684e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 6.3707e-04 - val_loss: 9.7767e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 5.1165e-04 - val_loss: 4.5212e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 4.8219e-04 - val_loss: 3.1162e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 4.3738e-04 - val_loss: 2.8554e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 6.8020e-04 - val_loss: 2.7594e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 3.7879e-04 - val_loss: 2.4805e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 627us/step - loss: 3.4863e-04 - val_loss: 2.6135e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 3.8723e-04 - val_loss: 0.0131\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 514us/step - loss: 5.3301e-04 - val_loss: 2.2023e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 2.7518e-04 - val_loss: 2.1818e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 3.6225e-04 - val_loss: 1.9772e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 2.8339e-04 - val_loss: 6.0861e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 3.6697e-04 - val_loss: 1.8612e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 2.1495e-04 - val_loss: 1.6482e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 3.2455e-04 - val_loss: 1.6830e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 2.8390e-04 - val_loss: 1.9821e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 2.5910e-04 - val_loss: 1.5881e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 8.2717e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 1.7680e-04 - 282ms/epoch - 397us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 5.6338e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 2.2230e-04 - 295ms/epoch - 415us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 4.8100e-04 - 282ms/epoch - 397us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 1.8284e-04 - 290ms/epoch - 407us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 4.7554e-04 - 311ms/epoch - 436us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 3.6429e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 5.5401e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 4.2139e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 1.8775e-04 - 317ms/epoch - 445us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 3.2982e-04 - 318ms/epoch - 446us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 2.5101e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 3.6718e-04 - 285ms/epoch - 400us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 6.1465e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 2.7808e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 5.8696e-04 - 316ms/epoch - 444us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 1.8378e-04 - 286ms/epoch - 401us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 2.8730e-04 - 283ms/epoch - 397us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 7.2139e-04 - 280ms/epoch - 393us/step\n",
      "178/178 - 0s - loss: 1.8080e-04 - 95ms/epoch - 531us/step\n",
      "712/712 - 0s - loss: 1.8028e-04 - 188ms/epoch - 264us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  29.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0706 - val_loss: 0.0027\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 9.4495e-04 - val_loss: 6.5930e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 6.7335e-04 - val_loss: 7.5913e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 6.2611e-04 - val_loss: 4.1450e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 520us/step - loss: 5.0243e-04 - val_loss: 4.3316e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 4.4557e-04 - val_loss: 4.2530e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 4.0781e-04 - val_loss: 2.6219e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 3.5883e-04 - val_loss: 2.3518e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 522us/step - loss: 4.2707e-04 - val_loss: 3.2680e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 2.4570e-04 - val_loss: 3.5712e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 3.1709e-04 - val_loss: 3.0610e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 522us/step - loss: 3.3305e-04 - val_loss: 1.9147e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 3.1136e-04 - val_loss: 3.0777e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 2.9362e-04 - val_loss: 1.7172e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 520us/step - loss: 2.6760e-04 - val_loss: 2.2408e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 2.6068e-04 - val_loss: 1.7545e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 2.6300e-04 - val_loss: 1.8720e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 3.9587e-04 - val_loss: 1.6867e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 2.0672e-04 - val_loss: 1.6554e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 6.5819e-04 - 294ms/epoch - 412us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 3.4444e-04 - 290ms/epoch - 407us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 3.6064e-04 - 285ms/epoch - 400us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 3.6529e-04 - 285ms/epoch - 401us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 2.9284e-04 - 286ms/epoch - 401us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 3.9554e-04 - 308ms/epoch - 432us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 6.2729e-04 - 329ms/epoch - 462us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 1.8025e-04 - 315ms/epoch - 442us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 2.2062e-04 - 286ms/epoch - 402us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 4.3638e-04 - 288ms/epoch - 404us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 2.9314e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 4.2294e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 3.3557e-04 - 288ms/epoch - 405us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 2.1056e-04 - 288ms/epoch - 405us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 2.7976e-04 - 292ms/epoch - 410us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 9.3347e-04 - 287ms/epoch - 402us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 2.0192e-04 - 291ms/epoch - 408us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 1.9357e-04 - 297ms/epoch - 418us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 2.7621e-04 - 314ms/epoch - 441us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 2.7081e-04 - 305ms/epoch - 428us/step\n",
      "178/178 - 0s - loss: 0.0012 - 100ms/epoch - 561us/step\n",
      "712/712 - 0s - loss: 0.0011 - 210ms/epoch - 295us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  28.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0581 - val_loss: 0.0026\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0016 - val_loss: 9.7937e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 9.5362e-04 - val_loss: 6.5791e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 6.5523e-04 - val_loss: 6.8319e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 5.9265e-04 - val_loss: 4.4199e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 6.8885e-04 - val_loss: 3.7758e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 4.3177e-04 - val_loss: 3.1596e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 6.9498e-04 - val_loss: 3.7719e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 4.7381e-04 - val_loss: 3.5421e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 5.3667e-04 - val_loss: 0.0018\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 3.5391e-04 - val_loss: 0.0010\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 4.6642e-04 - val_loss: 2.8395e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 3.6474e-04 - val_loss: 2.4805e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 4.0575e-04 - val_loss: 2.1450e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 3.5202e-04 - val_loss: 2.0237e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 5.0414e-04 - val_loss: 2.6931e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 2.9822e-04 - val_loss: 0.0015\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 3.7718e-04 - val_loss: 2.1998e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 3.1639e-04 - val_loss: 1.9238e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 4.5717e-04 - val_loss: 1.9682e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 6.4195e-04 - 310ms/epoch - 435us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 4.8144e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 5.0602e-04 - 319ms/epoch - 448us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 5.4384e-04 - 326ms/epoch - 458us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 8.2059e-04 - 286ms/epoch - 402us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 2.3999e-04 - 286ms/epoch - 402us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 3.5050e-04 - 291ms/epoch - 409us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 5.8879e-04 - 287ms/epoch - 403us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 3.6408e-04 - 295ms/epoch - 415us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 7.5512e-04 - 299ms/epoch - 420us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 2.6233e-04 - 295ms/epoch - 415us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 3.4655e-04 - 305ms/epoch - 429us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 4.2696e-04 - 307ms/epoch - 431us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 4.4573e-04 - 297ms/epoch - 417us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 3.9361e-04 - 294ms/epoch - 413us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 5.2029e-04 - 312ms/epoch - 438us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 3.7009e-04 - 297ms/epoch - 418us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 3.2891e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 4.4368e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 3.9567e-04 - 320ms/epoch - 450us/step\n",
      "178/178 - 0s - loss: 4.9541e-04 - 98ms/epoch - 550us/step\n",
      "712/712 - 0s - loss: 5.0457e-04 - 199ms/epoch - 279us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  30.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0673 - val_loss: 0.0027\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 8.7562e-04 - val_loss: 7.0433e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 8.1820e-04 - val_loss: 4.9956e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 5.4431e-04 - val_loss: 4.0213e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 5.3957e-04 - val_loss: 3.5315e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 3.9873e-04 - val_loss: 2.8655e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 3.9886e-04 - val_loss: 3.9135e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 4.0027e-04 - val_loss: 2.7350e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 642us/step - loss: 3.4521e-04 - val_loss: 2.1812e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 644us/step - loss: 4.2866e-04 - val_loss: 2.2321e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 8.7457e-04 - val_loss: 2.7490e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 2.8182e-04 - val_loss: 2.1896e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 623us/step - loss: 2.9153e-04 - val_loss: 2.3536e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 2.9859e-04 - val_loss: 1.9990e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 3.6462e-04 - val_loss: 6.4342e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 2.9530e-04 - val_loss: 1.7956e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 2.6828e-04 - val_loss: 3.1756e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 4.5817e-04 - val_loss: 1.7168e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 3.6588e-04 - val_loss: 1.7197e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 4.3494e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 5.7133e-04 - 342ms/epoch - 480us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 5.3472e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 2.3165e-04 - 301ms/epoch - 423us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 4.0002e-04 - 311ms/epoch - 436us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 3.8710e-04 - 357ms/epoch - 501us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 5.1700e-04 - 310ms/epoch - 435us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 2.5371e-04 - 283ms/epoch - 398us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 7.1625e-04 - 285ms/epoch - 400us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 2.2031e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 2.6844e-04 - 324ms/epoch - 455us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 4.8718e-04 - 299ms/epoch - 419us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 5.1118e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 2.4593e-04 - 299ms/epoch - 419us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 4.3338e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 3.2745e-04 - 285ms/epoch - 400us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 3.4585e-04 - 285ms/epoch - 401us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 2.9476e-04 - 294ms/epoch - 413us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 5.7308e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 2.2239e-04 - 286ms/epoch - 402us/step\n",
      "178/178 - 0s - loss: 1.8123e-04 - 100ms/epoch - 562us/step\n",
      "712/712 - 0s - loss: 1.8167e-04 - 207ms/epoch - 291us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  30.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2876 - val_loss: 0.0201\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0139 - val_loss: 0.0102\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0085 - val_loss: 0.0073\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 605us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0032 - 305ms/epoch - 428us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 296ms/epoch - 416us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0031 - 334ms/epoch - 470us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 299ms/epoch - 420us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0031 - 295ms/epoch - 415us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 284ms/epoch - 398us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0031 - 309ms/epoch - 434us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0030 - 300ms/epoch - 422us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0032 - 297ms/epoch - 417us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0030 - 299ms/epoch - 421us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 310ms/epoch - 435us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0029 - 305ms/epoch - 429us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0032 - 285ms/epoch - 400us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0029 - 286ms/epoch - 402us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0034 - 288ms/epoch - 405us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0029 - 297ms/epoch - 418us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 285ms/epoch - 400us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0029 - 284ms/epoch - 398us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0030 - 290ms/epoch - 408us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0029 - 301ms/epoch - 422us/step\n",
      "178/178 - 0s - loss: 0.0029 - 98ms/epoch - 550us/step\n",
      "712/712 - 0s - loss: 0.0029 - 186ms/epoch - 261us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  30.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2865 - val_loss: 0.0183\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 520us/step - loss: 0.0124 - val_loss: 0.0090\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0076 - val_loss: 0.0066\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0060 - val_loss: 0.0055\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0046 - val_loss: 0.0043\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 517us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0031 - val_loss: 0.0033\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0036 - 289ms/epoch - 406us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 283ms/epoch - 398us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 296ms/epoch - 415us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0032 - 297ms/epoch - 417us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0031 - 301ms/epoch - 423us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 297ms/epoch - 417us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0032 - 298ms/epoch - 419us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0030 - 312ms/epoch - 439us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 330ms/epoch - 463us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0030 - 326ms/epoch - 458us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0032 - 300ms/epoch - 421us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0030 - 314ms/epoch - 442us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0030 - 360ms/epoch - 506us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0030 - 380ms/epoch - 533us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0030 - 333ms/epoch - 468us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0031 - 343ms/epoch - 481us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 304ms/epoch - 427us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0031 - 317ms/epoch - 445us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 303ms/epoch - 426us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0032 - 296ms/epoch - 415us/step\n",
      "178/178 - 0s - loss: 0.0029 - 103ms/epoch - 577us/step\n",
      "712/712 - 0s - loss: 0.0029 - 196ms/epoch - 275us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  29.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2776 - val_loss: 0.0184\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0130 - val_loss: 0.0099\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0083 - val_loss: 0.0071\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 754us/step - loss: 0.0047 - val_loss: 0.0044\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0032 - 301ms/epoch - 423us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 314ms/epoch - 441us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 314ms/epoch - 442us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 311ms/epoch - 437us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0031 - 301ms/epoch - 423us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 297ms/epoch - 417us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 304ms/epoch - 427us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0030 - 302ms/epoch - 424us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0031 - 308ms/epoch - 433us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0031 - 303ms/epoch - 425us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 303ms/epoch - 426us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0030 - 307ms/epoch - 432us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0034 - 305ms/epoch - 428us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0029 - 300ms/epoch - 421us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 323ms/epoch - 453us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0031 - 301ms/epoch - 423us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 302ms/epoch - 424us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0030 - 295ms/epoch - 415us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 408us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0029 - 287ms/epoch - 403us/step\n",
      "178/178 - 0s - loss: 0.0029 - 97ms/epoch - 547us/step\n",
      "712/712 - 0s - loss: 0.0029 - 191ms/epoch - 269us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  29.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2681 - val_loss: 0.0175\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0124 - val_loss: 0.0093\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0079 - val_loss: 0.0067\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0062 - val_loss: 0.0056\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0031 - 308ms/epoch - 432us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0034 - 365ms/epoch - 512us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 316ms/epoch - 443us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 313ms/epoch - 440us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0031 - 301ms/epoch - 422us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0032 - 298ms/epoch - 418us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 290ms/epoch - 407us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0030 - 296ms/epoch - 416us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0031 - 300ms/epoch - 421us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0029 - 309ms/epoch - 433us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0030 - 288ms/epoch - 404us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0032 - 291ms/epoch - 409us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0029 - 295ms/epoch - 415us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0036 - 294ms/epoch - 414us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 321ms/epoch - 451us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0029 - 295ms/epoch - 414us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 297ms/epoch - 417us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0029 - 295ms/epoch - 415us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0030 - 294ms/epoch - 414us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 407us/step\n",
      "178/178 - 0s - loss: 0.0052 - 102ms/epoch - 573us/step\n",
      "712/712 - 0s - loss: 0.0052 - 193ms/epoch - 271us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  29.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2930 - val_loss: 0.0192\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0132 - val_loss: 0.0097\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0080 - val_loss: 0.0068\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 521us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0046 - val_loss: 0.0043\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0032 - val_loss: 0.0036\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0036 - 302ms/epoch - 424us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 297ms/epoch - 417us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 298ms/epoch - 419us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 295ms/epoch - 414us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0030 - 304ms/epoch - 427us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 310ms/epoch - 435us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 316ms/epoch - 444us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0030 - 284ms/epoch - 399us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 288ms/epoch - 404us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0032 - 287ms/epoch - 403us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 302ms/epoch - 424us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0030 - 304ms/epoch - 428us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0030 - 299ms/epoch - 420us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0032 - 311ms/epoch - 437us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 350ms/epoch - 492us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0032 - 302ms/epoch - 425us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 307ms/epoch - 432us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0030 - 306ms/epoch - 429us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 312ms/epoch - 438us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0034 - 293ms/epoch - 412us/step\n",
      "178/178 - 0s - loss: 0.0029 - 115ms/epoch - 648us/step\n",
      "712/712 - 0s - loss: 0.0029 - 227ms/epoch - 318us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  29.5s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 2\n",
      "n_resources: 85356\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0726 - val_loss: 0.0045\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 0.0028 - val_loss: 0.0020\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0010 - val_loss: 9.9659e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 9.9130e-04 - val_loss: 7.7674e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 9.0221e-04 - val_loss: 6.9492e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 8.3015e-04 - val_loss: 6.7069e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 8.2299e-04 - val_loss: 8.1617e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 7.6831e-04 - val_loss: 0.0013\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 7.7200e-04 - val_loss: 7.9166e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 7.1736e-04 - val_loss: 6.2282e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 6.9906e-04 - val_loss: 0.0016\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 6.9297e-04 - val_loss: 8.1574e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 6.7799e-04 - val_loss: 4.9092e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 634us/step - loss: 7.4912e-04 - val_loss: 5.0908e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 7.1508e-04 - val_loss: 4.5536e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 7.3288e-04 - val_loss: 5.5948e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 6.3516e-04 - val_loss: 4.7729e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 8.2130e-04 - 852ms/epoch - 399us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 7.4987e-04 - 999ms/epoch - 468us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 8.5827e-04 - 927ms/epoch - 435us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 7.8940e-04 - 897ms/epoch - 420us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 9.3069e-04 - 858ms/epoch - 402us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 9.1992e-04 - 875ms/epoch - 410us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 7.9214e-04 - 892ms/epoch - 418us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 9.6470e-04 - 948ms/epoch - 444us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 9.1302e-04 - 930ms/epoch - 436us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 8.3057e-04 - 899ms/epoch - 421us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 8.0294e-04 - 843ms/epoch - 395us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 9.3294e-04 - 877ms/epoch - 411us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 9.5415e-04 - 914ms/epoch - 428us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 7.4319e-04 - 862ms/epoch - 404us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 7.6113e-04 - 886ms/epoch - 415us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 8.6584e-04 - 871ms/epoch - 408us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 8.7599e-04 - 874ms/epoch - 409us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 8.1325e-04 - 886ms/epoch - 415us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 7.7191e-04 - 887ms/epoch - 415us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 7.6710e-04 - 855ms/epoch - 401us/step\n",
      "534/534 - 0s - loss: 4.2933e-04 - 181ms/epoch - 338us/step\n",
      "2134/2134 - 1s - loss: 4.3201e-04 - 551ms/epoch - 258us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  44.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0805 - val_loss: 0.0042\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0031 - val_loss: 0.0021\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0011 - val_loss: 9.5888e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 649us/step - loss: 9.6502e-04 - val_loss: 0.0012\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 9.7051e-04 - val_loss: 9.8390e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 8.2513e-04 - val_loss: 7.4489e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 8.4500e-04 - val_loss: 7.0151e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 8.1574e-04 - val_loss: 0.0038\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 7.7701e-04 - val_loss: 0.0037\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 7.8479e-04 - val_loss: 7.0383e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 821us/step - loss: 7.2531e-04 - val_loss: 5.7180e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 6.7415e-04 - val_loss: 6.6713e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 6.8927e-04 - val_loss: 4.7412e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 6.1046e-04 - val_loss: 4.4133e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 6.7307e-04 - val_loss: 4.8580e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 7.5012e-04 - val_loss: 5.5223e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 5.8843e-04 - val_loss: 4.7156e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 7.7921e-04 - 900ms/epoch - 422us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 8.4564e-04 - 907ms/epoch - 425us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 9.4033e-04 - 892ms/epoch - 418us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 8.0078e-04 - 931ms/epoch - 436us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 8.2422e-04 - 949ms/epoch - 445us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 7.7536e-04 - 873ms/epoch - 409us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 8.9120e-04 - 870ms/epoch - 408us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 7.2565e-04 - 880ms/epoch - 413us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 7.7819e-04 - 890ms/epoch - 417us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 8.1418e-04 - 837ms/epoch - 392us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 8.6149e-04 - 844ms/epoch - 396us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 8.2850e-04 - 874ms/epoch - 410us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 8.2078e-04 - 843ms/epoch - 395us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 7.4607e-04 - 917ms/epoch - 430us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 8.3624e-04 - 895ms/epoch - 420us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 7.5024e-04 - 915ms/epoch - 429us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 7.3144e-04 - 899ms/epoch - 421us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 8.6473e-04 - 899ms/epoch - 421us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 8.3448e-04 - 953ms/epoch - 447us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 8.0525e-04 - 850ms/epoch - 398us/step\n",
      "534/534 - 0s - loss: 4.6565e-04 - 187ms/epoch - 351us/step\n",
      "2134/2134 - 1s - loss: 4.6570e-04 - 574ms/epoch - 269us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  44.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0682 - val_loss: 0.0039\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 0.0031 - val_loss: 0.0025\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 9.7256e-04 - val_loss: 7.7798e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 9.2413e-04 - val_loss: 6.9307e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 8.0228e-04 - val_loss: 6.1307e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 7.9912e-04 - val_loss: 6.7406e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 7.9032e-04 - val_loss: 6.2856e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 7.4254e-04 - val_loss: 5.7353e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 7.2606e-04 - val_loss: 5.9115e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 6.5763e-04 - val_loss: 4.8385e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 6.8254e-04 - val_loss: 5.4037e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 6.9964e-04 - val_loss: 5.0401e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 6.3048e-04 - val_loss: 6.0125e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 6.3742e-04 - val_loss: 6.2055e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 6.9332e-04 - val_loss: 6.5845e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 5.8723e-04 - val_loss: 0.0018\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 8.6347e-04 - 956ms/epoch - 448us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 8.3795e-04 - 932ms/epoch - 437us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 8.9964e-04 - 929ms/epoch - 436us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 7.7017e-04 - 892ms/epoch - 418us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.0010 - 913ms/epoch - 428us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 8.4334e-04 - 867ms/epoch - 406us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 7.4514e-04 - 897ms/epoch - 420us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 7.9949e-04 - 871ms/epoch - 408us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 8.0692e-04 - 850ms/epoch - 398us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 7.9209e-04 - 850ms/epoch - 398us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 8.1380e-04 - 859ms/epoch - 402us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 8.8211e-04 - 870ms/epoch - 408us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 8.7669e-04 - 855ms/epoch - 401us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 8.0693e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 7.7326e-04 - 868ms/epoch - 407us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 7.4097e-04 - 881ms/epoch - 413us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 8.1149e-04 - 911ms/epoch - 427us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 9.0293e-04 - 882ms/epoch - 413us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 6.8666e-04 - 895ms/epoch - 419us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 8.1083e-04 - 869ms/epoch - 407us/step\n",
      "534/534 - 0s - loss: 6.8295e-04 - 180ms/epoch - 338us/step\n",
      "2134/2134 - 1s - loss: 6.8793e-04 - 557ms/epoch - 261us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  44.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0647 - val_loss: 0.0039\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0012 - val_loss: 9.4086e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0010 - val_loss: 9.2040e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0010 - val_loss: 9.5112e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 8.8323e-04 - val_loss: 8.3514e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 8.9851e-04 - val_loss: 6.4008e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 7.7979e-04 - val_loss: 5.7666e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 7.6414e-04 - val_loss: 6.0143e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 7.7412e-04 - val_loss: 8.2900e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 6.9606e-04 - val_loss: 6.2399e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 6.8390e-04 - val_loss: 4.8479e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 888us/step - loss: 6.7648e-04 - val_loss: 5.8168e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 6.8771e-04 - val_loss: 4.6704e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 710us/step - loss: 6.9455e-04 - val_loss: 5.1577e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 6.3855e-04 - val_loss: 5.0395e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 6.0650e-04 - val_loss: 5.8178e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 6.1360e-04 - val_loss: 4.5393e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 8.7805e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 7.9218e-04 - 908ms/epoch - 426us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 8.5060e-04 - 903ms/epoch - 423us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 7.4913e-04 - 882ms/epoch - 413us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 8.1204e-04 - 923ms/epoch - 432us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 7.9553e-04 - 941ms/epoch - 441us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 7.5461e-04 - 902ms/epoch - 423us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 8.6089e-04 - 946ms/epoch - 443us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 7.5737e-04 - 912ms/epoch - 427us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 8.6013e-04 - 915ms/epoch - 429us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 7.4190e-04 - 908ms/epoch - 426us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 8.3689e-04 - 938ms/epoch - 439us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 8.8207e-04 - 975ms/epoch - 457us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 7.4972e-04 - 898ms/epoch - 421us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 7.5690e-04 - 864ms/epoch - 405us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 7.4434e-04 - 921ms/epoch - 432us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 8.5740e-04 - 864ms/epoch - 405us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 6.7297e-04 - 940ms/epoch - 441us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 6.8552e-04 - 902ms/epoch - 422us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 7.3472e-04 - 907ms/epoch - 425us/step\n",
      "534/534 - 0s - loss: 5.5818e-04 - 183ms/epoch - 343us/step\n",
      "2134/2134 - 1s - loss: 5.6126e-04 - 567ms/epoch - 266us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  45.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0911 - val_loss: 0.0043\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0032 - val_loss: 0.0025\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0010 - val_loss: 8.7964e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 9.9939e-04 - val_loss: 8.9065e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 9.5875e-04 - val_loss: 7.5794e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 9.4149e-04 - val_loss: 7.5286e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 8.5425e-04 - val_loss: 6.8206e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 9.0083e-04 - val_loss: 6.3825e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 8.4490e-04 - val_loss: 6.0751e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 7.9612e-04 - val_loss: 0.0019\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 8.5403e-04 - val_loss: 6.9381e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 7.9171e-04 - val_loss: 5.5681e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 8.2550e-04 - val_loss: 7.2325e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 9.5743e-04 - 953ms/epoch - 446us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 9.1389e-04 - 954ms/epoch - 447us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 9.7658e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 9.5798e-04 - 878ms/epoch - 411us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 9.4893e-04 - 928ms/epoch - 435us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 9.1920e-04 - 914ms/epoch - 428us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 9.5255e-04 - 919ms/epoch - 431us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 9.1757e-04 - 1s/epoch - 534us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 9.2613e-04 - 1s/epoch - 545us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.0010 - 1s/epoch - 484us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 9.5337e-04 - 1s/epoch - 480us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 8.8785e-04 - 947ms/epoch - 444us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 9.7836e-04 - 889ms/epoch - 417us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 9.0225e-04 - 865ms/epoch - 406us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 8.9508e-04 - 960ms/epoch - 450us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 8.1324e-04 - 894ms/epoch - 419us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 8.3027e-04 - 889ms/epoch - 416us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 8.3851e-04 - 884ms/epoch - 414us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.0011 - 954ms/epoch - 447us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 9.5435e-04 - 864ms/epoch - 405us/step\n",
      "534/534 - 0s - loss: 0.0012 - 180ms/epoch - 337us/step\n",
      "2134/2134 - 1s - loss: 0.0012 - 534ms/epoch - 250us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  45.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0657 - val_loss: 0.0025\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0010 - val_loss: 6.6875e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 6.6056e-04 - val_loss: 9.0635e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 5.4027e-04 - val_loss: 4.8309e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 6.4373e-04 - val_loss: 3.5592e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 4.2147e-04 - val_loss: 3.2303e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 4.0736e-04 - val_loss: 2.9534e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 5.4373e-04 - val_loss: 5.7285e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 3.1473e-04 - val_loss: 2.4461e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 3.6637e-04 - val_loss: 6.3995e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 3.3695e-04 - val_loss: 2.3380e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 3.9622e-04 - val_loss: 2.0874e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 4.2813e-04 - val_loss: 1.9620e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 3.0878e-04 - val_loss: 1.8422e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 4.3434e-04 - val_loss: 0.0010\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 3.8225e-04 - val_loss: 1.9662e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 2.4957e-04 - val_loss: 1.7538e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 3.2421e-04 - val_loss: 7.7619e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 2.6105e-04 - val_loss: 2.4642e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 4.4148e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 3.9824e-04 - 884ms/epoch - 414us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 5.1510e-04 - 937ms/epoch - 439us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 2.9700e-04 - 925ms/epoch - 433us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 5.5779e-04 - 997ms/epoch - 467us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 2.6719e-04 - 963ms/epoch - 451us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 3.8647e-04 - 946ms/epoch - 443us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 3.3207e-04 - 914ms/epoch - 428us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 3.5364e-04 - 906ms/epoch - 424us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 3.6014e-04 - 907ms/epoch - 425us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 3.3666e-04 - 918ms/epoch - 430us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 4.0838e-04 - 924ms/epoch - 433us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 3.9715e-04 - 981ms/epoch - 460us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 3.2157e-04 - 933ms/epoch - 437us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 2.9408e-04 - 927ms/epoch - 434us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 4.3336e-04 - 980ms/epoch - 459us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 2.5929e-04 - 893ms/epoch - 419us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 2.9271e-04 - 903ms/epoch - 423us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 2.4967e-04 - 915ms/epoch - 429us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 3.0396e-04 - 882ms/epoch - 413us/step\n",
      "534/534 - 0s - loss: 1.5108e-04 - 200ms/epoch - 374us/step\n",
      "2134/2134 - 1s - loss: 1.5100e-04 - 614ms/epoch - 288us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  44.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0565 - val_loss: 0.0023\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0015 - val_loss: 9.2305e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 8.5337e-04 - val_loss: 6.1522e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 6.6450e-04 - val_loss: 5.2322e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 5.7473e-04 - val_loss: 8.7556e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 4.5861e-04 - val_loss: 3.4993e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 4.5401e-04 - val_loss: 2.8823e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 4.0671e-04 - val_loss: 2.5680e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 3.7431e-04 - val_loss: 2.5606e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 3.8734e-04 - val_loss: 2.1793e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 3.1395e-04 - val_loss: 2.4284e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 4.5022e-04 - val_loss: 2.0708e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 3.4519e-04 - val_loss: 1.9626e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 2.9763e-04 - val_loss: 1.8630e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 3.0716e-04 - val_loss: 2.1653e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 2.7893e-04 - val_loss: 2.8508e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 3.6557e-04 - val_loss: 1.6785e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 3.6336e-04 - val_loss: 1.8453e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 2.8016e-04 - val_loss: 1.7408e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 2.6071e-04 - val_loss: 1.6643e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 4.6098e-04 - 892ms/epoch - 418us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 4.2152e-04 - 902ms/epoch - 423us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 5.1158e-04 - 876ms/epoch - 410us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 3.5872e-04 - 1s/epoch - 469us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 3.5640e-04 - 1s/epoch - 471us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 4.2389e-04 - 881ms/epoch - 413us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 2.9846e-04 - 912ms/epoch - 427us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 5.9419e-04 - 922ms/epoch - 432us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 2.3042e-04 - 872ms/epoch - 409us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 4.2004e-04 - 880ms/epoch - 412us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 4.0161e-04 - 891ms/epoch - 418us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 2.7257e-04 - 933ms/epoch - 437us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 3.0420e-04 - 874ms/epoch - 409us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 2.7568e-04 - 857ms/epoch - 402us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 3.9932e-04 - 861ms/epoch - 404us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 3.0599e-04 - 848ms/epoch - 397us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 2.1519e-04 - 865ms/epoch - 405us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 3.1010e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 2.1139e-04 - 999ms/epoch - 468us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 2.6313e-04 - 933ms/epoch - 437us/step\n",
      "534/534 - 1s - loss: 1.7689e-04 - 551ms/epoch - 1ms/step\n",
      "2134/2134 - 1s - loss: 1.7611e-04 - 615ms/epoch - 288us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  44.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0606 - val_loss: 0.0021\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0014 - val_loss: 9.0106e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 8.7634e-04 - val_loss: 6.2237e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 6.6373e-04 - val_loss: 4.4551e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 5.6227e-04 - val_loss: 0.0026\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 6.4631e-04 - val_loss: 0.0014\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 3.5705e-04 - val_loss: 3.0402e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 3.8205e-04 - val_loss: 3.2737e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 4.2719e-04 - val_loss: 2.7583e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 3.9103e-04 - val_loss: 2.2817e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 4.2862e-04 - val_loss: 2.2057e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 635us/step - loss: 3.2203e-04 - val_loss: 2.0165e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 3.8837e-04 - val_loss: 2.0201e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 3.4428e-04 - val_loss: 1.9705e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 4.4056e-04 - val_loss: 2.0266e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 3.3849e-04 - val_loss: 0.0022\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 2.6626e-04 - val_loss: 6.7088e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 3.2450e-04 - val_loss: 1.8845e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 3.1605e-04 - val_loss: 1.8030e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 3.1496e-04 - val_loss: 2.2016e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 5.5278e-04 - 946ms/epoch - 444us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 5.5057e-04 - 940ms/epoch - 440us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 3.5301e-04 - 975ms/epoch - 457us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 3.5305e-04 - 868ms/epoch - 407us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 3.1857e-04 - 953ms/epoch - 447us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 3.3645e-04 - 883ms/epoch - 414us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 3.5475e-04 - 861ms/epoch - 403us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 2.8204e-04 - 931ms/epoch - 436us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 4.0868e-04 - 906ms/epoch - 425us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 3.9037e-04 - 861ms/epoch - 403us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 2.6342e-04 - 858ms/epoch - 402us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 2.9688e-04 - 880ms/epoch - 412us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 2.6822e-04 - 979ms/epoch - 459us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 3.1335e-04 - 994ms/epoch - 466us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 3.0586e-04 - 994ms/epoch - 466us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 3.2647e-04 - 904ms/epoch - 424us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 2.5333e-04 - 957ms/epoch - 448us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 2.9268e-04 - 918ms/epoch - 430us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 3.1050e-04 - 917ms/epoch - 430us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 2.9782e-04 - 927ms/epoch - 434us/step\n",
      "534/534 - 0s - loss: 1.4696e-04 - 202ms/epoch - 379us/step\n",
      "2134/2134 - 1s - loss: 1.4712e-04 - 562ms/epoch - 263us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  45.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0537 - val_loss: 0.0022\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.0014 - val_loss: 9.1673e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 8.5795e-04 - val_loss: 5.9369e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 5.9864e-04 - val_loss: 4.9743e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 5.6942e-04 - val_loss: 4.1564e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 4.3646e-04 - val_loss: 5.0207e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 4.7344e-04 - val_loss: 2.9106e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 3.5044e-04 - val_loss: 2.3986e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 3.6499e-04 - val_loss: 2.4320e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 3.3331e-04 - val_loss: 0.0071\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 3.0877e-04 - val_loss: 1.9523e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 3.4918e-04 - val_loss: 3.5109e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 643us/step - loss: 3.3684e-04 - val_loss: 1.7999e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 644us/step - loss: 3.0581e-04 - val_loss: 1.8314e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 2.9481e-04 - val_loss: 1.8751e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 2.6883e-04 - val_loss: 1.6025e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 3.6881e-04 - val_loss: 1.7075e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 2.2197e-04 - val_loss: 1.5977e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 4.7297e-04 - val_loss: 3.6249e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 2.3738e-04 - val_loss: 1.5934e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 7.4324e-04 - 880ms/epoch - 412us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 2.5256e-04 - 887ms/epoch - 416us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 3.4687e-04 - 903ms/epoch - 423us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 4.7802e-04 - 882ms/epoch - 413us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 3.3626e-04 - 981ms/epoch - 460us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 2.9293e-04 - 983ms/epoch - 461us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 2.6853e-04 - 913ms/epoch - 428us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 3.1688e-04 - 951ms/epoch - 446us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 2.6996e-04 - 965ms/epoch - 452us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 3.8983e-04 - 932ms/epoch - 437us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 2.3255e-04 - 931ms/epoch - 436us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 3.1397e-04 - 868ms/epoch - 407us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 3.4293e-04 - 978ms/epoch - 459us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 2.6541e-04 - 890ms/epoch - 417us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 2.5122e-04 - 863ms/epoch - 404us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 2.5879e-04 - 865ms/epoch - 405us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 3.1325e-04 - 881ms/epoch - 413us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 2.2145e-04 - 915ms/epoch - 429us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 2.4795e-04 - 906ms/epoch - 425us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 2.8972e-04 - 901ms/epoch - 422us/step\n",
      "534/534 - 0s - loss: 1.2991e-04 - 182ms/epoch - 341us/step\n",
      "2134/2134 - 1s - loss: 1.3021e-04 - 588ms/epoch - 275us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  45.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0585 - val_loss: 0.0029\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 9.5884e-04 - val_loss: 6.8322e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 6.7225e-04 - val_loss: 4.6639e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 5.5250e-04 - val_loss: 3.6630e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 5.2967e-04 - val_loss: 3.2313e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 3.9880e-04 - val_loss: 2.7478e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 4.3887e-04 - val_loss: 4.5525e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 4.1200e-04 - val_loss: 3.0510e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 4.7191e-04 - val_loss: 2.2873e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 2.6633e-04 - val_loss: 2.8261e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 3.5447e-04 - val_loss: 1.9909e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 4.7807e-04 - val_loss: 2.1517e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 2.8834e-04 - val_loss: 1.8699e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 3.9360e-04 - val_loss: 1.8869e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 3.3915e-04 - val_loss: 1.8013e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 3.3765e-04 - val_loss: 4.9443e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 3.1487e-04 - val_loss: 1.7688e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 4.6200e-04 - val_loss: 1.9159e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 3.8841e-04 - val_loss: 1.7862e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 5.3457e-04 - 930ms/epoch - 436us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 6.7494e-04 - 866ms/epoch - 406us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 2.4911e-04 - 872ms/epoch - 409us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 4.6864e-04 - 917ms/epoch - 430us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 5.1216e-04 - 945ms/epoch - 443us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 4.3064e-04 - 871ms/epoch - 408us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 3.0724e-04 - 909ms/epoch - 426us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 4.6509e-04 - 912ms/epoch - 427us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 3.1482e-04 - 924ms/epoch - 433us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 3.5992e-04 - 977ms/epoch - 458us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 3.5467e-04 - 874ms/epoch - 410us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 3.8087e-04 - 901ms/epoch - 422us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 2.4953e-04 - 949ms/epoch - 445us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 4.4264e-04 - 916ms/epoch - 429us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 2.8235e-04 - 963ms/epoch - 451us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 3.2328e-04 - 932ms/epoch - 437us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 2.5865e-04 - 948ms/epoch - 444us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 4.3405e-04 - 943ms/epoch - 442us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 2.6000e-04 - 934ms/epoch - 438us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 2.7976e-04 - 878ms/epoch - 412us/step\n",
      "534/534 - 0s - loss: 1.6443e-04 - 192ms/epoch - 360us/step\n",
      "2134/2134 - 1s - loss: 1.6489e-04 - 622ms/epoch - 292us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  44.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0561 - val_loss: 0.0021\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 0.0014 - val_loss: 9.2383e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 9.7716e-04 - val_loss: 6.6481e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 675us/step - loss: 6.7259e-04 - val_loss: 5.3679e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 649us/step - loss: 5.8071e-04 - val_loss: 4.2788e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 837us/step - loss: 4.6934e-04 - val_loss: 3.3424e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 5.4553e-04 - val_loss: 3.1576e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 4.5306e-04 - val_loss: 2.8795e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 3.6620e-04 - val_loss: 4.4593e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 4.0128e-04 - val_loss: 2.6293e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 3.1721e-04 - val_loss: 2.1372e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 4.9296e-04 - val_loss: 4.0549e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 3.1576e-04 - val_loss: 2.1868e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 2.8523e-04 - val_loss: 3.3181e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 3.4791e-04 - val_loss: 5.9747e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 3.3262e-04 - val_loss: 1.8322e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 629us/step - loss: 3.4467e-04 - val_loss: 1.7439e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 4.3113e-04 - val_loss: 1.8619e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 625us/step - loss: 2.6136e-04 - val_loss: 1.7173e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 2.9119e-04 - val_loss: 1.6241e-04\n",
      "Epoch 1/20\n",
      "2668/2668 - 1s - loss: 5.8920e-04 - 1s/epoch - 436us/step\n",
      "Epoch 2/20\n",
      "2668/2668 - 1s - loss: 4.1278e-04 - 1s/epoch - 410us/step\n",
      "Epoch 3/20\n",
      "2668/2668 - 1s - loss: 4.7269e-04 - 1s/epoch - 411us/step\n",
      "Epoch 4/20\n",
      "2668/2668 - 1s - loss: 4.9087e-04 - 1s/epoch - 429us/step\n",
      "Epoch 5/20\n",
      "2668/2668 - 1s - loss: 3.7035e-04 - 1s/epoch - 421us/step\n",
      "Epoch 6/20\n",
      "2668/2668 - 1s - loss: 4.4278e-04 - 1s/epoch - 420us/step\n",
      "Epoch 7/20\n",
      "2668/2668 - 1s - loss: 3.0055e-04 - 1s/epoch - 407us/step\n",
      "Epoch 8/20\n",
      "2668/2668 - 1s - loss: 3.9533e-04 - 1s/epoch - 420us/step\n",
      "Epoch 9/20\n",
      "2668/2668 - 1s - loss: 3.1738e-04 - 1s/epoch - 419us/step\n",
      "Epoch 10/20\n",
      "2668/2668 - 1s - loss: 3.2711e-04 - 1s/epoch - 432us/step\n",
      "Epoch 11/20\n",
      "2668/2668 - 1s - loss: 3.2944e-04 - 1s/epoch - 471us/step\n",
      "Epoch 12/20\n",
      "2668/2668 - 1s - loss: 2.7291e-04 - 1s/epoch - 450us/step\n",
      "Epoch 13/20\n",
      "2668/2668 - 1s - loss: 3.1980e-04 - 1s/epoch - 446us/step\n",
      "Epoch 14/20\n",
      "2668/2668 - 1s - loss: 2.5195e-04 - 1s/epoch - 445us/step\n",
      "Epoch 15/20\n",
      "2668/2668 - 1s - loss: 3.2511e-04 - 1s/epoch - 431us/step\n",
      "Epoch 16/20\n",
      "2668/2668 - 1s - loss: 2.2790e-04 - 1s/epoch - 460us/step\n",
      "Epoch 17/20\n",
      "2668/2668 - 1s - loss: 2.8532e-04 - 1s/epoch - 425us/step\n",
      "Epoch 18/20\n",
      "2668/2668 - 1s - loss: 4.2994e-04 - 1s/epoch - 410us/step\n",
      "Epoch 19/20\n",
      "2668/2668 - 1s - loss: 2.8164e-04 - 1s/epoch - 420us/step\n",
      "Epoch 20/20\n",
      "2668/2668 - 1s - loss: 2.6543e-04 - 1s/epoch - 409us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31451edc0&gt;,\n",
       "                    param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;]},\n",
       "                    verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31451edc0&gt;,\n",
       "                    param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;]},\n",
       "                    verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31451edc0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31451edc0&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(error_score='raise',\n",
       "                    estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x31451edc0>,\n",
       "                    param_grid={'activation': ['tanh', 'relu'],\n",
       "                                'kernel_regularizer': ['l1', 'l2', 'l1_l2']},\n",
       "                    verbose=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search selected hyperparameters\n",
    "\n",
    "# define base model function for grid search\n",
    "def create_model(activation='relu', kernel_regularizer='l2'):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=64, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2']\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform grid search hyperparameter tuning\n",
    "grid_search = HalvingGridSearchCV(estimator=keras_reg, param_grid=param_dist, cv=5, verbose=2, error_score='raise')\n",
    "grid_search.fit(Xtrain, ytrain, sample_weight=wtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>n_resources</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>29.277782</td>\n",
       "      <td>0.438441</td>\n",
       "      <td>0.112907</td>\n",
       "      <td>0.005524</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.006056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005936</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.006106</td>\n",
       "      <td>-0.005748</td>\n",
       "      <td>-0.005817</td>\n",
       "      <td>-0.006003</td>\n",
       "      <td>-0.005998</td>\n",
       "      <td>-0.005934</td>\n",
       "      <td>0.000132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>29.347707</td>\n",
       "      <td>0.258205</td>\n",
       "      <td>0.127457</td>\n",
       "      <td>0.028411</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>-0.000591</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.000517</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>30.181596</td>\n",
       "      <td>0.923040</td>\n",
       "      <td>0.129607</td>\n",
       "      <td>0.016089</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.006757</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006316</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.006789</td>\n",
       "      <td>-0.006201</td>\n",
       "      <td>-0.006036</td>\n",
       "      <td>-0.006361</td>\n",
       "      <td>-0.006121</td>\n",
       "      <td>-0.006302</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>29.927985</td>\n",
       "      <td>0.787953</td>\n",
       "      <td>0.179043</td>\n",
       "      <td>0.132378</td>\n",
       "      <td>relu</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.002785</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002810</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.002786</td>\n",
       "      <td>-0.002781</td>\n",
       "      <td>-0.002864</td>\n",
       "      <td>-0.002800</td>\n",
       "      <td>-0.002822</td>\n",
       "      <td>-0.002811</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>29.578543</td>\n",
       "      <td>0.730748</td>\n",
       "      <td>0.116243</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000452</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.001140</td>\n",
       "      <td>-0.000505</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>-0.000443</td>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>29.643308</td>\n",
       "      <td>0.384341</td>\n",
       "      <td>0.117571</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>relu</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.002896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003368</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.002898</td>\n",
       "      <td>-0.002908</td>\n",
       "      <td>-0.002881</td>\n",
       "      <td>-0.005189</td>\n",
       "      <td>-0.002923</td>\n",
       "      <td>-0.003360</td>\n",
       "      <td>0.000915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>85356</td>\n",
       "      <td>44.846986</td>\n",
       "      <td>0.544574</td>\n",
       "      <td>0.197627</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000674</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.000432</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.000688</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>-0.001232</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>85356</td>\n",
       "      <td>44.786772</td>\n",
       "      <td>0.520556</td>\n",
       "      <td>0.281276</td>\n",
       "      <td>0.143043</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
       "0     0        28452      29.277782      0.438441         0.112907   \n",
       "1     0        28452      29.347707      0.258205         0.127457   \n",
       "2     0        28452      30.181596      0.923040         0.129607   \n",
       "3     0        28452      29.927985      0.787953         0.179043   \n",
       "4     0        28452      29.578543      0.730748         0.116243   \n",
       "5     0        28452      29.643308      0.384341         0.117571   \n",
       "6     1        85356      44.846986      0.544574         0.197627   \n",
       "7     1        85356      44.786772      0.520556         0.281276   \n",
       "\n",
       "   std_score_time param_activation param_kernel_regularizer  \\\n",
       "0        0.005524             tanh                       l1   \n",
       "1        0.028411             tanh                       l2   \n",
       "2        0.016089             tanh                    l1_l2   \n",
       "3        0.132378             relu                       l1   \n",
       "4        0.006994             relu                       l2   \n",
       "5        0.007166             relu                    l1_l2   \n",
       "6        0.002991             tanh                       l2   \n",
       "7        0.143043             relu                       l2   \n",
       "\n",
       "                                              params  split0_test_score  ...  \\\n",
       "0  {'activation': 'tanh', 'kernel_regularizer': '...          -0.006056  ...   \n",
       "1  {'activation': 'tanh', 'kernel_regularizer': '...          -0.000539  ...   \n",
       "2  {'activation': 'tanh', 'kernel_regularizer': '...          -0.006757  ...   \n",
       "3  {'activation': 'relu', 'kernel_regularizer': '...          -0.002785  ...   \n",
       "4  {'activation': 'relu', 'kernel_regularizer': '...          -0.000207  ...   \n",
       "5  {'activation': 'relu', 'kernel_regularizer': '...          -0.002896  ...   \n",
       "6  {'activation': 'tanh', 'kernel_regularizer': '...          -0.000429  ...   \n",
       "7  {'activation': 'relu', 'kernel_regularizer': '...          -0.000151  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0        -0.005936        0.000083                7           -0.006106   \n",
       "1        -0.000541        0.000046                3           -0.000541   \n",
       "2        -0.006316        0.000230                8           -0.006789   \n",
       "3        -0.002810        0.000031                5           -0.002786   \n",
       "4        -0.000452        0.000390                2           -0.000208   \n",
       "5        -0.003368        0.000933                6           -0.002898   \n",
       "6        -0.000674        0.000293                4           -0.000432   \n",
       "7        -0.000154        0.000016                1           -0.000151   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0           -0.005748           -0.005817           -0.006003   \n",
       "1           -0.000591           -0.000574           -0.000517   \n",
       "2           -0.006201           -0.006036           -0.006361   \n",
       "3           -0.002781           -0.002864           -0.002800   \n",
       "4           -0.000180           -0.001140           -0.000505   \n",
       "5           -0.002908           -0.002881           -0.005189   \n",
       "6           -0.000466           -0.000688           -0.000561   \n",
       "7           -0.000176           -0.000147           -0.000130   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0           -0.005998         -0.005934         0.000132  \n",
       "1           -0.000474         -0.000539         0.000041  \n",
       "2           -0.006121         -0.006302         0.000266  \n",
       "3           -0.002822         -0.002811         0.000030  \n",
       "4           -0.000182         -0.000443         0.000369  \n",
       "5           -0.002923         -0.003360         0.000915  \n",
       "6           -0.001232         -0.000676         0.000292  \n",
       "7           -0.000165         -0.000154         0.000016  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "display(grid_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'activation': 'relu', 'kernel_regularizer': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0519 - val_loss: 0.0023\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0015 - val_loss: 9.1489e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 8.3574e-04 - val_loss: 6.1465e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 6.6634e-04 - val_loss: 4.6885e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 5.7144e-04 - val_loss: 3.6956e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 4.8092e-04 - val_loss: 3.2419e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 4.7032e-04 - val_loss: 2.7739e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 4.3496e-04 - val_loss: 2.5733e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 3.3716e-04 - val_loss: 2.9419e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 4.1987e-04 - val_loss: 8.3258e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 3.7042e-04 - val_loss: 2.1454e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 3.4379e-04 - val_loss: 2.2366e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 4.1834e-04 - val_loss: 2.4765e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 2.7459e-04 - val_loss: 1.9947e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 4.2787e-04 - val_loss: 2.0451e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 2.5853e-04 - val_loss: 2.8759e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 3.3572e-04 - val_loss: 1.7864e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 4.7710e-04 - val_loss: 1.8278e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 3.3296e-04 - val_loss: 1.9861e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 4.8454e-04 - val_loss: 1.9839e-04\n"
     ]
    }
   ],
   "source": [
    "# evaluate best model\n",
    "best_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "    Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "best_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = best_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=64, validation_data=(Xval, yval, wval))\n",
    "\n",
    "# save the best model\n",
    "best_model.save(\"models/best_dnn_model_processed_pd.h5\")\n",
    "\n",
    "# save model history in models\n",
    "with open('models/best_dnn_model_processed_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Huber Loss: 0.0004845440562348813\n",
      "Best Validation Huber Loss: 0.0001983915426535532\n"
     ]
    }
   ],
   "source": [
    "best_train_hl = best_model.history.history[\"loss\"][-1]\n",
    "best_val_hl = best_model.history.history[\"val_loss\"][-1]\n",
    "print(f\"Best Training Huber Loss: {best_train_hl}\")\n",
    "print(f\"Best Validation Huber Loss: {best_val_hl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 64-bit ('tf-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff044f01a3488bc48e7d3679a79037bfca5d9a382741a26c6ac0d6891b8e9545"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
