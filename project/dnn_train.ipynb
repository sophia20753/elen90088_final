{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_halving_search_cv # required for HalvingGridSearchCV\n",
    "from sklearn.model_selection import train_test_split, HalvingGridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.callbacks import EarlyStopping, History\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw PD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steering_angle</th>\n",
       "      <th>distance_to_road_center</th>\n",
       "      <th>angle_from_straight_in_rads</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124995</th>\n",
       "      <td>0.144081</td>\n",
       "      <td>0.147693</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>-3.835282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124996</th>\n",
       "      <td>0.142465</td>\n",
       "      <td>0.148484</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>-3.813875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124997</th>\n",
       "      <td>0.140811</td>\n",
       "      <td>0.149215</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>-3.792289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124998</th>\n",
       "      <td>0.139120</td>\n",
       "      <td>0.149889</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>-3.770526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124999</th>\n",
       "      <td>0.137394</td>\n",
       "      <td>0.150505</td>\n",
       "      <td>-0.006668</td>\n",
       "      <td>-0.201773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        steering_angle  distance_to_road_center  angle_from_straight_in_rads  \\\n",
       "0             2.588463                 1.026888                     2.956561   \n",
       "1             2.588463                 1.026888                     2.956561   \n",
       "2             2.588463                 1.026888                     2.956561   \n",
       "3             2.588463                 1.026888                     2.956561   \n",
       "4             2.588463                 1.026888                     2.956561   \n",
       "...                ...                      ...                          ...   \n",
       "124995        0.144081                 0.147693                     0.010285   \n",
       "124996        0.142465                 0.148484                     0.006000   \n",
       "124997        0.140811                 0.149215                     0.001745   \n",
       "124998        0.139120                 0.149889                    -0.002478   \n",
       "124999        0.137394                 0.150505                    -0.006668   \n",
       "\n",
       "          reward  \n",
       "0      -0.012300  \n",
       "1      -0.012300  \n",
       "2      -0.012300  \n",
       "3      -0.012300  \n",
       "4      -0.012614  \n",
       "...          ...  \n",
       "124995 -3.835282  \n",
       "124996 -3.813875  \n",
       "124997 -3.792289  \n",
       "124998 -3.770526  \n",
       "124999 -0.201773  \n",
       "\n",
       "[125000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/2548153325.py:15: RuntimeWarning: divide by zero encountered in divide\n",
      "  weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n"
     ]
    }
   ],
   "source": [
    "# import data from csv to dataframe\n",
    "filename = \"train_data/raw_data_pd.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "display(df)\n",
    "\n",
    "# split into input and target features, and weights\n",
    "X = df[['distance_to_road_center', 'angle_from_straight_in_rads']].values\n",
    "y = df['steering_angle'].values\n",
    "r = df['reward'].values\n",
    "\n",
    "# normalize rewards to [0, 1] range\n",
    "norm_r = (r - np.min(r)) / (np.max(r) - np.min(r))\n",
    "\n",
    "# calculate weights based on normalized rewards\n",
    "weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n",
    "\n",
    "# split into training and validation sets\n",
    "Xtrain, Xval, ytrain, yval, wtrain, wval = train_test_split(X, y, weights, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0311 - val_loss: 4.9459e-04\n",
      "Epoch 2/100\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 5.7531e-04 - val_loss: 7.1825e-04\n",
      "Epoch 3/100\n",
      "3125/3125 [==============================] - 2s 507us/step - loss: 4.4264e-04 - val_loss: 5.8055e-04\n",
      "Epoch 4/100\n",
      "3125/3125 [==============================] - 2s 522us/step - loss: 4.8484e-04 - val_loss: 3.8641e-05\n",
      "Epoch 5/100\n",
      "3125/3125 [==============================] - 2s 508us/step - loss: 2.5327e-04 - val_loss: 4.7775e-05\n",
      "Epoch 6/100\n",
      "3125/3125 [==============================] - 2s 521us/step - loss: 2.6589e-04 - val_loss: 2.4218e-04\n",
      "Epoch 7/100\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 2.0023e-04 - val_loss: 5.0007e-06\n",
      "Epoch 8/100\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 2.1890e-04 - val_loss: 6.4376e-06\n",
      "Epoch 9/100\n",
      "3125/3125 [==============================] - 2s 509us/step - loss: 1.4528e-04 - val_loss: 1.8688e-05\n",
      "Epoch 10/100\n",
      "3125/3125 [==============================] - 2s 510us/step - loss: 1.7536e-04 - val_loss: 1.3059e-05\n",
      "Epoch 11/100\n",
      "3125/3125 [==============================] - 2s 505us/step - loss: 1.1187e-04 - val_loss: 1.0243e-04\n",
      "Epoch 12/100\n",
      "3125/3125 [==============================] - 2s 515us/step - loss: 1.3933e-04 - val_loss: 2.0056e-05\n"
     ]
    }
   ],
   "source": [
    "# find optimal number of epochs using early stopping\n",
    "\n",
    "# define base model with default Huber loss delta = 1.0\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for steering angle prediction\n",
    "])\n",
    "\n",
    "model.compile(loss=Huber(delta=1.0), optimizer=Adam(learning_rate=0.001), weighted_metrics=[])\n",
    "\n",
    "# define early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=100, batch_size=32, validation_data = (Xval, yval, wval), callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0202 - mae: 0.0251 - mse: 0.0104 - val_loss: 1.5993e-04 - val_mae: 0.0052 - val_mse: 6.3100e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 7.1293e-04 - mae: 0.0093 - mse: 2.6307e-04 - val_loss: 0.0031 - val_mae: 0.0230 - val_mse: 9.9338e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 5.7935e-04 - mae: 0.0097 - mse: 2.3994e-04 - val_loss: 6.5062e-04 - val_mae: 0.0123 - val_mse: 3.5072e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 4.6699e-04 - mae: 0.0089 - mse: 2.1845e-04 - val_loss: 7.2402e-05 - val_mae: 0.0046 - val_mse: 4.5898e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 2.8915e-04 - mae: 0.0068 - mse: 1.2366e-04 - val_loss: 2.7315e-05 - val_mae: 0.0033 - val_mse: 1.6124e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 2.4844e-04 - mae: 0.0066 - mse: 1.1776e-04 - val_loss: 1.2982e-04 - val_mae: 0.0078 - val_mse: 9.2997e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 556us/step - loss: 2.2921e-04 - mae: 0.0064 - mse: 1.0785e-04 - val_loss: 1.6559e-04 - val_mae: 0.0049 - val_mse: 4.8297e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 1.6793e-04 - mae: 0.0057 - mse: 8.9933e-05 - val_loss: 3.0036e-04 - val_mae: 0.0115 - val_mse: 2.4888e-04\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.5048e-04 - mae: 0.0047 - mse: 7.5915e-05 - val_loss: 2.9277e-05 - val_mae: 0.0036 - val_mse: 1.9212e-05\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.4594e-04 - mae: 0.0044 - mse: 7.4576e-05 - val_loss: 1.1561e-05 - val_mae: 0.0019 - val_mse: 6.4723e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.2166e-04 - mae: 0.0040 - mse: 6.1894e-05 - val_loss: 2.1074e-05 - val_mae: 0.0029 - val_mse: 1.1959e-05\n",
      "Iteration 1: delta = 0.5, val_mae = 0.002943823579698801, val_mse = 1.1959174116782378e-05\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0179 - mae: 0.0315 - mse: 0.0141 - val_loss: 5.0269e-04 - val_mae: 0.0119 - val_mse: 2.3428e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 9.1817e-04 - mae: 0.0118 - mse: 3.6821e-04 - val_loss: 5.4259e-04 - val_mae: 0.0144 - val_mse: 3.1531e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 6.1241e-04 - mae: 0.0095 - mse: 2.3283e-04 - val_loss: 2.8834e-04 - val_mae: 0.0075 - val_mse: 1.3596e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 4.6370e-04 - mae: 0.0079 - mse: 1.8891e-04 - val_loss: 5.0387e-05 - val_mae: 0.0041 - val_mse: 3.0028e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 4.4266e-04 - mae: 0.0075 - mse: 1.6384e-04 - val_loss: 1.2292e-04 - val_mae: 0.0065 - val_mse: 8.0425e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 2.8406e-04 - mae: 0.0061 - mse: 1.2867e-04 - val_loss: 0.0011 - val_mae: 0.0171 - val_mse: 7.6907e-04\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 2.7603e-04 - mae: 0.0060 - mse: 1.2185e-04 - val_loss: 1.0649e-04 - val_mae: 0.0081 - val_mse: 8.3981e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 1.9524e-04 - mae: 0.0055 - mse: 1.0114e-04 - val_loss: 7.5181e-06 - val_mae: 0.0019 - val_mse: 6.3815e-06\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 585us/step - loss: 1.9535e-04 - mae: 0.0051 - mse: 9.1940e-05 - val_loss: 1.6227e-05 - val_mae: 0.0025 - val_mse: 9.9240e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 595us/step - loss: 1.8053e-04 - mae: 0.0049 - mse: 8.5892e-05 - val_loss: 1.2958e-04 - val_mae: 0.0055 - val_mse: 4.4025e-05\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 590us/step - loss: 1.3691e-04 - mae: 0.0039 - mse: 6.3916e-05 - val_loss: 0.0014 - val_mae: 0.0249 - val_mse: 0.0011\n",
      "Iteration 2: delta = 0.6, val_mae = 0.024870606139302254, val_mse = 0.0010972677264362574\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0114 - mae: 0.0257 - mse: 0.0094 - val_loss: 5.3391e-05 - val_mae: 0.0044 - val_mse: 3.7553e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 9.6109e-04 - mae: 0.0101 - mse: 3.9020e-04 - val_loss: 0.0014 - val_mae: 0.0215 - val_mse: 9.1424e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 4.7032e-04 - mae: 0.0080 - mse: 1.8341e-04 - val_loss: 2.6630e-04 - val_mae: 0.0081 - val_mse: 1.5600e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 6.6518e-04 - mae: 0.0071 - mse: 2.1967e-04 - val_loss: 1.0308e-04 - val_mae: 0.0078 - val_mse: 9.6577e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 4.7084e-04 - mae: 0.0070 - mse: 1.8236e-04 - val_loss: 2.1098e-05 - val_mae: 0.0024 - val_mse: 1.1410e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 2.2838e-04 - mae: 0.0055 - mse: 9.9337e-05 - val_loss: 5.1811e-04 - val_mae: 0.0174 - val_mse: 4.4689e-04\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 2.0463e-04 - mae: 0.0057 - mse: 9.5618e-05 - val_loss: 1.9682e-05 - val_mae: 0.0030 - val_mse: 1.3833e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.6610e-04 - mae: 0.0044 - mse: 6.7608e-05 - val_loss: 9.1411e-06 - val_mae: 0.0018 - val_mse: 7.1671e-06\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 2.3054e-04 - mae: 0.0049 - mse: 1.0475e-04 - val_loss: 0.0012 - val_mae: 0.0150 - val_mse: 3.6073e-04\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.6315e-04 - mae: 0.0043 - mse: 9.5132e-05 - val_loss: 1.8588e-05 - val_mae: 0.0019 - val_mse: 6.7877e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 1.9439e-04 - mae: 0.0043 - mse: 1.0390e-04 - val_loss: 1.7810e-06 - val_mae: 8.9890e-04 - val_mse: 1.7142e-06\n",
      "Iteration 3: delta = 0.7, val_mae = 0.0008989019552245736, val_mse = 1.7142334627351374e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0223 - mae: 0.0271 - mse: 0.0115 - val_loss: 0.0015 - val_mae: 0.0321 - val_mse: 0.0015\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 6.2845e-04 - mae: 0.0095 - mse: 2.5019e-04 - val_loss: 2.4619e-04 - val_mae: 0.0106 - val_mse: 2.0971e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 6.0896e-04 - mae: 0.0094 - mse: 3.1653e-04 - val_loss: 3.8566e-04 - val_mae: 0.0070 - val_mse: 1.2440e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 5.6446e-04 - mae: 0.0095 - mse: 2.9370e-04 - val_loss: 3.7102e-04 - val_mae: 0.0118 - val_mse: 2.7360e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 2.4134e-04 - mae: 0.0059 - mse: 9.6261e-05 - val_loss: 1.5849e-04 - val_mae: 0.0047 - val_mse: 5.4793e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 3.4626e-04 - mae: 0.0072 - mse: 1.6621e-04 - val_loss: 4.5183e-05 - val_mae: 0.0032 - val_mse: 2.0729e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 1.5767e-04 - mae: 0.0053 - mse: 7.7483e-05 - val_loss: 2.5716e-04 - val_mae: 0.0099 - val_mse: 2.1014e-04\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.8267e-04 - mae: 0.0056 - mse: 9.4568e-05 - val_loss: 6.7420e-05 - val_mae: 0.0032 - val_mse: 2.1570e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 556us/step - loss: 1.9465e-04 - mae: 0.0047 - mse: 8.9738e-05 - val_loss: 1.2042e-04 - val_mae: 0.0095 - val_mse: 1.3092e-04\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.6518e-04 - mae: 0.0050 - mse: 8.5304e-05 - val_loss: 2.0021e-05 - val_mae: 0.0023 - val_mse: 9.3320e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.1535e-04 - mae: 0.0037 - mse: 4.7635e-05 - val_loss: 3.0474e-06 - val_mae: 0.0010 - val_mse: 2.0951e-06\n",
      "Iteration 4: delta = 0.7999999999999999, val_mae = 0.0010287829209119081, val_mse = 2.0950956240994856e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0255 - mae: 0.0301 - mse: 0.0121 - val_loss: 3.8480e-04 - val_mae: 0.0080 - val_mse: 1.4563e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 8.5414e-04 - mae: 0.0105 - mse: 3.2599e-04 - val_loss: 2.2098e-05 - val_mae: 0.0032 - val_mse: 2.0943e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 556us/step - loss: 4.6625e-04 - mae: 0.0078 - mse: 1.8403e-04 - val_loss: 3.0350e-05 - val_mae: 0.0029 - val_mse: 1.6471e-05\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 5.0720e-04 - mae: 0.0073 - mse: 2.3415e-04 - val_loss: 2.1361e-05 - val_mae: 0.0024 - val_mse: 1.3373e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 3.0236e-04 - mae: 0.0061 - mse: 1.3501e-04 - val_loss: 1.2982e-05 - val_mae: 0.0018 - val_mse: 5.9104e-06\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 1.7702e-04 - mae: 0.0051 - mse: 7.9110e-05 - val_loss: 2.9651e-05 - val_mae: 0.0035 - val_mse: 2.0781e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 528us/step - loss: 1.7276e-04 - mae: 0.0047 - mse: 9.3099e-05 - val_loss: 0.0023 - val_mae: 0.0280 - val_mse: 0.0018\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 2.4888e-04 - mae: 0.0043 - mse: 1.0232e-04 - val_loss: 2.2918e-05 - val_mae: 0.0042 - val_mse: 2.6106e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 1.1057e-04 - mae: 0.0037 - mse: 5.9276e-05 - val_loss: 5.0996e-06 - val_mae: 0.0016 - val_mse: 4.2748e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 1.4172e-04 - mae: 0.0046 - mse: 6.8312e-05 - val_loss: 4.0365e-06 - val_mae: 0.0013 - val_mse: 3.3686e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 1.1190e-04 - mae: 0.0033 - mse: 5.2435e-05 - val_loss: 2.0810e-06 - val_mae: 0.0011 - val_mse: 2.5243e-06\n",
      "Iteration 5: delta = 0.8999999999999999, val_mae = 0.0011450344463810325, val_mse = 2.5242668471037177e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0221 - mae: 0.0245 - mse: 0.0082 - val_loss: 0.0014 - val_mae: 0.0218 - val_mse: 7.6380e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 7.3161e-04 - mae: 0.0098 - mse: 2.7063e-04 - val_loss: 1.0519e-04 - val_mae: 0.0048 - val_mse: 4.6909e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 6.0558e-04 - mae: 0.0089 - mse: 2.3874e-04 - val_loss: 1.4503e-04 - val_mae: 0.0058 - val_mse: 6.3119e-05\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 523us/step - loss: 3.1684e-04 - mae: 0.0072 - mse: 1.4621e-04 - val_loss: 6.0222e-04 - val_mae: 0.0152 - val_mse: 3.2410e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 521us/step - loss: 2.8636e-04 - mae: 0.0066 - mse: 1.2217e-04 - val_loss: 1.0486e-04 - val_mae: 0.0048 - val_mse: 4.2946e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 522us/step - loss: 1.8973e-04 - mae: 0.0056 - mse: 8.7885e-05 - val_loss: 1.7900e-04 - val_mae: 0.0047 - val_mse: 4.8652e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 2.5495e-04 - mae: 0.0048 - mse: 9.8091e-05 - val_loss: 4.8226e-06 - val_mae: 0.0014 - val_mse: 3.9607e-06\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 535us/step - loss: 1.8153e-04 - mae: 0.0037 - mse: 7.3064e-05 - val_loss: 0.0053 - val_mae: 0.0303 - val_mse: 0.0016\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 520us/step - loss: 1.3014e-04 - mae: 0.0039 - mse: 6.9618e-05 - val_loss: 1.8095e-06 - val_mae: 8.7120e-04 - val_mse: 1.7165e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 523us/step - loss: 9.9326e-05 - mae: 0.0032 - mse: 4.2085e-05 - val_loss: 2.3668e-06 - val_mae: 9.2389e-04 - val_mse: 1.6628e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 508us/step - loss: 1.0992e-04 - mae: 0.0039 - mse: 5.5732e-05 - val_loss: 2.7451e-06 - val_mae: 9.7746e-04 - val_mse: 1.7462e-06\n",
      "Iteration 6: delta = 0.9999999999999999, val_mae = 0.0009774580830708146, val_mse = 1.7462235746279475e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0248 - mae: 0.0229 - mse: 0.0078 - val_loss: 4.8171e-05 - val_mae: 0.0045 - val_mse: 4.0733e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 9.2655e-04 - mae: 0.0121 - mse: 4.1391e-04 - val_loss: 1.1910e-04 - val_mae: 0.0052 - val_mse: 5.1821e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 526us/step - loss: 5.7925e-04 - mae: 0.0096 - mse: 2.3423e-04 - val_loss: 6.7896e-04 - val_mae: 0.0105 - val_mse: 2.2890e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 524us/step - loss: 5.0893e-04 - mae: 0.0074 - mse: 1.9574e-04 - val_loss: 8.1922e-04 - val_mae: 0.0162 - val_mse: 3.5495e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 617us/step - loss: 3.1946e-04 - mae: 0.0070 - mse: 1.7556e-04 - val_loss: 3.5842e-04 - val_mae: 0.0160 - val_mse: 3.2989e-04\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 588us/step - loss: 2.2603e-04 - mae: 0.0061 - mse: 1.0574e-04 - val_loss: 7.4329e-04 - val_mae: 0.0159 - val_mse: 5.7037e-04\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 558us/step - loss: 2.8874e-04 - mae: 0.0066 - mse: 1.2610e-04 - val_loss: 2.8004e-04 - val_mae: 0.0120 - val_mse: 2.3289e-04\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 2.0672e-04 - mae: 0.0047 - mse: 9.4651e-05 - val_loss: 3.2528e-05 - val_mae: 0.0043 - val_mse: 2.2604e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 2.0818e-04 - mae: 0.0049 - mse: 9.5721e-05 - val_loss: 2.3110e-05 - val_mae: 0.0022 - val_mse: 9.1263e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.7532e-04 - mae: 0.0043 - mse: 8.3759e-05 - val_loss: 3.2591e-05 - val_mae: 0.0039 - val_mse: 2.3659e-05\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.0985e-04 - mae: 0.0045 - mse: 5.3047e-05 - val_loss: 1.7614e-04 - val_mae: 0.0078 - val_mse: 1.0241e-04\n",
      "Iteration 7: delta = 1.0999999999999999, val_mae = 0.007791235577315092, val_mse = 0.00010241157724522054\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0222 - mae: 0.0238 - mse: 0.0100 - val_loss: 2.2203e-04 - val_mae: 0.0057 - val_mse: 6.9195e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 8.8828e-04 - mae: 0.0098 - mse: 3.4518e-04 - val_loss: 1.8628e-05 - val_mae: 0.0029 - val_mse: 1.5384e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 6.4063e-04 - mae: 0.0091 - mse: 2.8052e-04 - val_loss: 4.4558e-04 - val_mae: 0.0091 - val_mse: 1.7246e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 3.9207e-04 - mae: 0.0080 - mse: 1.7642e-04 - val_loss: 1.1273e-04 - val_mae: 0.0048 - val_mse: 4.7311e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 3.3197e-04 - mae: 0.0076 - mse: 1.6882e-04 - val_loss: 4.8506e-05 - val_mae: 0.0029 - val_mse: 1.7219e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 2.3450e-04 - mae: 0.0053 - mse: 9.7111e-05 - val_loss: 3.2150e-04 - val_mae: 0.0063 - val_mse: 9.9308e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 557us/step - loss: 3.5725e-04 - mae: 0.0055 - mse: 1.5663e-04 - val_loss: 2.9813e-05 - val_mae: 0.0028 - val_mse: 1.3478e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.8142e-04 - mae: 0.0044 - mse: 8.9416e-05 - val_loss: 1.4668e-04 - val_mae: 0.0070 - val_mse: 1.0808e-04\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 555us/step - loss: 1.0711e-04 - mae: 0.0042 - mse: 5.3082e-05 - val_loss: 6.3507e-06 - val_mae: 0.0024 - val_mse: 7.9813e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 1.4143e-04 - mae: 0.0039 - mse: 6.4006e-05 - val_loss: 0.0011 - val_mae: 0.0212 - val_mse: 9.5096e-04\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.4094e-04 - mae: 0.0034 - mse: 5.3753e-05 - val_loss: 4.8846e-06 - val_mae: 0.0011 - val_mse: 2.5810e-06\n",
      "Iteration 8: delta = 1.2, val_mae = 0.001096259569749236, val_mse = 2.5810188617469976e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0304 - mae: 0.0230 - mse: 0.0107 - val_loss: 1.5114e-04 - val_mae: 0.0054 - val_mse: 5.8559e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 8.7749e-04 - mae: 0.0106 - mse: 3.3684e-04 - val_loss: 2.5765e-05 - val_mae: 0.0035 - val_mse: 2.1725e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 535us/step - loss: 5.9876e-04 - mae: 0.0084 - mse: 2.8471e-04 - val_loss: 5.2961e-05 - val_mae: 0.0054 - val_mse: 5.3889e-05\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 4.7338e-04 - mae: 0.0073 - mse: 1.8480e-04 - val_loss: 6.4867e-05 - val_mae: 0.0051 - val_mse: 4.2587e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 2.5211e-04 - mae: 0.0064 - mse: 1.2908e-04 - val_loss: 0.0027 - val_mae: 0.0139 - val_mse: 5.6304e-04\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 528us/step - loss: 2.6622e-04 - mae: 0.0062 - mse: 1.1705e-04 - val_loss: 1.0077e-05 - val_mae: 0.0022 - val_mse: 1.0210e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 1.5176e-04 - mae: 0.0048 - mse: 7.4001e-05 - val_loss: 3.1539e-05 - val_mae: 0.0036 - val_mse: 2.3778e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 532us/step - loss: 1.5780e-04 - mae: 0.0048 - mse: 9.1301e-05 - val_loss: 6.0171e-05 - val_mae: 0.0022 - val_mse: 1.4143e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 2.1998e-04 - mae: 0.0052 - mse: 1.4376e-04 - val_loss: 7.8193e-05 - val_mae: 0.0055 - val_mse: 3.8699e-05\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 9.8217e-05 - mae: 0.0035 - mse: 4.5323e-05 - val_loss: 1.8920e-05 - val_mae: 0.0020 - val_mse: 8.4045e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 528us/step - loss: 1.3668e-04 - mae: 0.0038 - mse: 5.4840e-05 - val_loss: 7.8829e-06 - val_mae: 0.0013 - val_mse: 4.0456e-06\n",
      "Iteration 9: delta = 1.3, val_mae = 0.0013006088556721807, val_mse = 4.045555670018075e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0286 - mae: 0.0258 - mse: 0.0099 - val_loss: 8.9270e-04 - val_mae: 0.0148 - val_mse: 4.5197e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 9.5435e-04 - mae: 0.0120 - mse: 4.5623e-04 - val_loss: 1.2477e-04 - val_mae: 0.0046 - val_mse: 4.3036e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 9.4698e-04 - mae: 0.0104 - mse: 3.7462e-04 - val_loss: 0.0092 - val_mae: 0.0475 - val_mse: 0.0033\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 586us/step - loss: 5.2611e-04 - mae: 0.0085 - mse: 2.0991e-04 - val_loss: 7.3887e-04 - val_mae: 0.0086 - val_mse: 1.9458e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 579us/step - loss: 4.0725e-04 - mae: 0.0074 - mse: 1.8848e-04 - val_loss: 1.3648e-04 - val_mae: 0.0050 - val_mse: 5.5901e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 2.1604e-04 - mae: 0.0063 - mse: 1.0755e-04 - val_loss: 1.4588e-05 - val_mae: 0.0020 - val_mse: 8.5403e-06\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 2.4824e-04 - mae: 0.0061 - mse: 1.0683e-04 - val_loss: 4.5263e-05 - val_mae: 0.0029 - val_mse: 1.7558e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 567us/step - loss: 1.6628e-04 - mae: 0.0052 - mse: 8.6930e-05 - val_loss: 2.3331e-04 - val_mae: 0.0042 - val_mse: 6.1065e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 603us/step - loss: 1.6646e-04 - mae: 0.0051 - mse: 8.4189e-05 - val_loss: 2.4806e-05 - val_mae: 0.0026 - val_mse: 1.0330e-05\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 594us/step - loss: 1.3747e-04 - mae: 0.0045 - mse: 8.1643e-05 - val_loss: 6.2283e-05 - val_mae: 0.0039 - val_mse: 2.4124e-05\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 659us/step - loss: 1.3672e-04 - mae: 0.0042 - mse: 6.9716e-05 - val_loss: 1.6083e-05 - val_mae: 0.0020 - val_mse: 8.3948e-06\n",
      "Iteration 10: delta = 1.4000000000000001, val_mae = 0.0019502261420711875, val_mse = 8.39479162095813e-06\n"
     ]
    }
   ],
   "source": [
    "# find best delta for loss function\n",
    "\n",
    "# function to train and evaluate model with a given delta\n",
    "def train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=delta), metrics=['mae', 'mse'], weighted_metrics=[])\n",
    "\n",
    "    history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    # get validation MAE, MSE and Huber loss\n",
    "    val_mae = history.history['val_mae'][-1]\n",
    "    val_mse = history.history['val_mse'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "    return val_mae, val_mse, val_loss\n",
    "\n",
    "# initialise delta value and step size\n",
    "delta = 0.5\n",
    "step_size = 0.1\n",
    "best_delta = delta\n",
    "best_val_mae = float('inf')\n",
    "best_val_mse = float('inf')\n",
    "\n",
    "# initialise arrays for plotting\n",
    "arr_val_mae = []\n",
    "arr_val_mse = []\n",
    "arr_val_loss = []\n",
    "\n",
    "# iterative search for optimal delta\n",
    "for i in range(10):\n",
    "    val_mae, val_mse, val_loss = train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval)\n",
    "\n",
    "    # print current results\n",
    "    print(f\"Iteration {i+1}: delta = {delta}, val_mae = {val_mae}, val_mse = {val_mse}\")\n",
    "\n",
    "    # update best delta if current results are better\n",
    "    if val_mae < best_val_mae and val_mse < best_val_mse: \n",
    "        best_delta = delta\n",
    "        best_val_mae = val_mae\n",
    "        best_val_mse = val_mse\n",
    "    \n",
    "    # store data\n",
    "    arr_val_mae.append(val_mae)\n",
    "    arr_val_mse.append(val_mse)\n",
    "    arr_val_loss.append(val_loss)\n",
    "    \n",
    "    # adjust delta for next iteration \n",
    "    delta += step_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best delta: 0.7\n",
      "Best validation MAE: 0.0008989019552245736\n",
      "Best validation MSE: 1.7142334627351374e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "deltas = np.arange(0.5, 1.5, 0.1)\n",
    "plt.plot(deltas, arr_val_mae, marker='o', linestyle='-', color='C0', label='MAE')\n",
    "plt.plot(deltas, arr_val_mse, marker='o', linestyle='-', color='C1', label='MSE')\n",
    "plt.plot(deltas, arr_val_loss, marker='o', linestyle='-', color='C2', label='Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Best delta: {best_delta}\")\n",
    "print(f\"Best validation MAE: {best_val_mae}\")\n",
    "print(f\"Best validation MSE: {best_val_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "3119/3125 [============================>.] - ETA: 0s - loss: 0.0124WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0124 - val_loss: 1.4695e-04\n",
      "Epoch 2/21\n",
      "3032/3125 [============================>.] - ETA: 0s - loss: 7.2352e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 503us/step - loss: 7.0489e-04 - val_loss: 3.3615e-04\n",
      "Epoch 3/21\n",
      "3085/3125 [============================>.] - ETA: 0s - loss: 4.9224e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 4.8774e-04 - val_loss: 1.1937e-04\n",
      "Epoch 4/21\n",
      "3032/3125 [============================>.] - ETA: 0s - loss: 3.0539e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 503us/step - loss: 2.9723e-04 - val_loss: 4.5692e-05\n",
      "Epoch 5/21\n",
      "3092/3125 [============================>.] - ETA: 0s - loss: 2.7563e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 2.7682e-04 - val_loss: 2.9324e-04\n",
      "Epoch 6/21\n",
      "3085/3125 [============================>.] - ETA: 0s - loss: 2.5836e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 2.5597e-04 - val_loss: 8.6236e-05\n",
      "Epoch 7/21\n",
      "3019/3125 [===========================>..] - ETA: 0s - loss: 1.7472e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 507us/step - loss: 1.9329e-04 - val_loss: 5.5759e-05\n",
      "Epoch 8/21\n",
      "3059/3125 [============================>.] - ETA: 0s - loss: 1.1668e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 517us/step - loss: 1.1497e-04 - val_loss: 2.0861e-04\n",
      "Epoch 9/21\n",
      "3111/3125 [============================>.] - ETA: 0s - loss: 1.6754e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 1.6720e-04 - val_loss: 1.1083e-04\n",
      "Epoch 10/21\n",
      "3078/3125 [============================>.] - ETA: 0s - loss: 1.4426e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 515us/step - loss: 1.4221e-04 - val_loss: 4.7471e-06\n",
      "Epoch 11/21\n",
      "3018/3125 [===========================>..] - ETA: 0s - loss: 1.1278e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 506us/step - loss: 1.1056e-04 - val_loss: 1.0144e-05\n",
      "Epoch 12/21\n",
      "3084/3125 [============================>.] - ETA: 0s - loss: 1.7072e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 1.6850e-04 - val_loss: 1.2806e-06\n",
      "Epoch 13/21\n",
      "3104/3125 [============================>.] - ETA: 0s - loss: 9.1941e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 9.1351e-05 - val_loss: 1.4919e-05\n",
      "Epoch 14/21\n",
      "3019/3125 [===========================>..] - ETA: 0s - loss: 1.0733e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 507us/step - loss: 1.0450e-04 - val_loss: 5.0571e-05\n",
      "Epoch 15/21\n",
      "3089/3125 [============================>.] - ETA: 0s - loss: 1.2246e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 1.2123e-04 - val_loss: 1.0353e-05\n",
      "Epoch 16/21\n",
      "3033/3125 [============================>.] - ETA: 0s - loss: 7.1371e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 504us/step - loss: 6.9639e-05 - val_loss: 1.1107e-05\n",
      "Epoch 17/21\n",
      "3074/3125 [============================>.] - ETA: 0s - loss: 9.0365e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 516us/step - loss: 8.9453e-05 - val_loss: 1.1710e-05\n",
      "Epoch 18/21\n",
      "3100/3125 [============================>.] - ETA: 0s - loss: 8.5348e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 8.5123e-05 - val_loss: 2.5125e-04\n",
      "Epoch 19/21\n",
      "3024/3125 [============================>.] - ETA: 0s - loss: 1.2449e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 505us/step - loss: 1.2051e-04 - val_loss: 1.3043e-06\n",
      "Epoch 20/21\n",
      "3089/3125 [============================>.] - ETA: 0s - loss: 7.9355e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 7.8451e-05 - val_loss: 5.0948e-07\n",
      "Epoch 21/21\n",
      "3030/3125 [============================>.] - ETA: 0s - loss: 8.6113e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 510us/step - loss: 8.3932e-05 - val_loss: 4.8413e-06\n",
      "Training Huber Loss: 8.393226016778499e-05\n",
      "Validation Huber Loss: 4.84133943245979e-06\n"
     ]
    }
   ],
   "source": [
    "# evaluate base model \n",
    "base_model = Sequential([\n",
    "                    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "                    Dense(32, activation='relu'),\n",
    "                    Dense(1)\n",
    "])\n",
    "\n",
    "base_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = base_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])\n",
    "\n",
    "# create folder for models\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# save model\n",
    "base_model.save(\"models/base_dnn_model_raw_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/base_dnn_model_raw_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "base_train_hl = history.history[\"loss\"][-1]\n",
    "base_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {base_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {base_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "3116/3125 [============================>.] - ETA: 0s - loss: 0.1236WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.1233 - val_loss: 0.0321\n",
      "Epoch 2/21\n",
      "3060/3125 [============================>.] - ETA: 0s - loss: 0.0195WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 0.0195 - val_loss: 0.0155\n",
      "Epoch 3/21\n",
      "3107/3125 [============================>.] - ETA: 0s - loss: 0.0136WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 4/21\n",
      "3100/3125 [============================>.] - ETA: 0s - loss: 0.0106WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 0.0106 - val_loss: 0.0088\n",
      "Epoch 5/21\n",
      "3047/3125 [============================>.] - ETA: 0s - loss: 0.0085WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 0.0085 - val_loss: 0.0072\n",
      "Epoch 6/21\n",
      "3121/3125 [============================>.] - ETA: 0s - loss: 0.0073WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 563us/step - loss: 0.0073 - val_loss: 0.0059\n",
      "Epoch 7/21\n",
      "3098/3125 [============================>.] - ETA: 0s - loss: 0.0082WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 0.0085 - val_loss: 0.0135\n",
      "Epoch 8/21\n",
      "3016/3125 [===========================>..] - ETA: 0s - loss: 0.0057WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 0.0057 - val_loss: 0.0080\n",
      "Epoch 9/21\n",
      "3067/3125 [============================>.] - ETA: 0s - loss: 0.0069WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 0.0068 - val_loss: 0.0050\n",
      "Epoch 10/21\n",
      "3019/3125 [===========================>..] - ETA: 0s - loss: 0.0055WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 0.0055 - val_loss: 0.0059\n",
      "Epoch 11/21\n",
      "3043/3125 [============================>.] - ETA: 0s - loss: 0.0055WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 555us/step - loss: 0.0055 - val_loss: 0.0046\n",
      "Epoch 12/21\n",
      "3050/3125 [============================>.] - ETA: 0s - loss: 0.0054WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 0.0054 - val_loss: 0.0044\n",
      "Epoch 13/21\n",
      "3076/3125 [============================>.] - ETA: 0s - loss: 0.0060WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 0.0060 - val_loss: 0.0044\n",
      "Epoch 14/21\n",
      "3020/3125 [===========================>..] - ETA: 0s - loss: 0.0048WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 15/21\n",
      "3048/3125 [============================>.] - ETA: 0s - loss: 0.0054WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 0.0054 - val_loss: 0.0042\n",
      "Epoch 16/21\n",
      "3025/3125 [============================>.] - ETA: 0s - loss: 0.0050WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 562us/step - loss: 0.0049 - val_loss: 0.0040\n",
      "Epoch 17/21\n",
      "3079/3125 [============================>.] - ETA: 0s - loss: 0.0058WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 0.0058 - val_loss: 0.0040\n",
      "Epoch 18/21\n",
      "3119/3125 [============================>.] - ETA: 0s - loss: 0.0045WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 594us/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 19/21\n",
      "3066/3125 [============================>.] - ETA: 0s - loss: 0.0049WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 0.0049 - val_loss: 0.0039\n",
      "Epoch 20/21\n",
      "3030/3125 [============================>.] - ETA: 0s - loss: 0.0048WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 0.0049 - val_loss: 0.0114\n",
      "Epoch 21/21\n",
      "3049/3125 [============================>.] - ETA: 0s - loss: 0.0045WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 519us/step - loss: 0.0045 - val_loss: 0.0037\n"
     ]
    }
   ],
   "source": [
    "# evaluate regularised model\n",
    "reg_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],), kernel_regularizer='l2'), \n",
    "        Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "reg_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = reg_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Huber Loss: 0.00447514234110713\n",
      "Validation Huber Loss: 0.003721917513757944\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "base_model.save(\"models/reg_dnn_model_raw_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/reg_dnn_model_raw_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "reg_train_hl = history.history[\"loss\"][-1]\n",
    "reg_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {reg_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {reg_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_95306/2299525531.py:28: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=20, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3913 - val_loss: 0.0504\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0354 - val_loss: 0.0248\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0214 - val_loss: 0.0172\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0180 - val_loss: 0.0160\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0132 - val_loss: 0.0119\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0102 - val_loss: 0.0092\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0085 - val_loss: 0.0077\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0079 - val_loss: 0.0072\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0071 - val_loss: 0.0065\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0065 - val_loss: 0.0062\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0067 - val_loss: 0.0061\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0057 - val_loss: 0.0051\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0051 - val_loss: 0.0049\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  47.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4063 - val_loss: 0.0449\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0310 - val_loss: 0.0262\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0198 - val_loss: 0.0165\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0155 - val_loss: 0.0141\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0125 - val_loss: 0.0111\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0094 - val_loss: 0.0086\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 638us/step - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0058 - val_loss: 0.0053\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0049 - val_loss: 0.0047\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  49.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3926 - val_loss: 0.0517\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0350 - val_loss: 0.0245\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0208 - val_loss: 0.0169\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0165 - val_loss: 0.0140\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0133 - val_loss: 0.0114\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0098 - val_loss: 0.0106\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0089 - val_loss: 0.0086\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0080 - val_loss: 0.0074\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0068 - val_loss: 0.0063\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0067 - val_loss: 0.0062\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 640us/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0057 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0053 - val_loss: 0.0048\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  49.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3740 - val_loss: 0.0547\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 537us/step - loss: 0.0360 - val_loss: 0.0254\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 523us/step - loss: 0.0214 - val_loss: 0.0179\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0173 - val_loss: 0.0184\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0138 - val_loss: 0.0121\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0116 - val_loss: 0.0103\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0090 - val_loss: 0.0085\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0077 - val_loss: 0.0072\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0071 - val_loss: 0.0064\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0065 - val_loss: 0.0063\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  48.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4054 - val_loss: 0.0512\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0364 - val_loss: 0.0256\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0220 - val_loss: 0.0176\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0169 - val_loss: 0.0141\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.0137 - val_loss: 0.0115\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0087 - val_loss: 0.0084\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0085 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 724us/step - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0069 - val_loss: 0.0060\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 705us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0064 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4548 - val_loss: 1.5654\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.4229 - val_loss: 1.5156\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.2551 - val_loss: 1.6405\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 1.4295 - val_loss: 1.2983\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 1.1793 - val_loss: 1.1226\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 1.1613 - val_loss: 2.0160\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2354 - val_loss: 1.1302\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 636us/step - loss: 1.1498 - val_loss: 0.7400\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 1.1035 - val_loss: 0.9781\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 1.0542 - val_loss: 1.7383\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 1.1506 - val_loss: 1.5261\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.2901 - val_loss: 1.2529\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0950 - val_loss: 0.9223\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 1.0057 - val_loss: 1.2830\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 1.1760 - val_loss: 1.9218\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.1963 - val_loss: 1.1139\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0766 - val_loss: 1.0442\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 1.0828 - val_loss: 0.9278\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 1.0241 - val_loss: 1.2778\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.0451 - val_loss: 1.1896\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 1.1658 - val_loss: 0.8618\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  47.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4894 - val_loss: 1.5536\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 681us/step - loss: 1.3949 - val_loss: 0.8424\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 1.2348 - val_loss: 2.4273\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 686us/step - loss: 1.3126 - val_loss: 1.0281\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 1.1545 - val_loss: 1.8219\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 1.2431 - val_loss: 1.7817\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 1.1064 - val_loss: 0.9372\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 1.1494 - val_loss: 1.1165\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 1.0696 - val_loss: 1.1350\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 1.2778 - val_loss: 1.2750\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.9945 - val_loss: 1.0512\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.9584 - val_loss: 0.8012\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 1.0732 - val_loss: 1.2680\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 1.0877 - val_loss: 0.7701\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.0771 - val_loss: 1.2036\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.1390 - val_loss: 1.3090\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 1.0615 - val_loss: 1.4114\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 1.1238 - val_loss: 0.6854\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.1122 - val_loss: 1.5386\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 1.0594 - val_loss: 1.5226\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 1.0355 - val_loss: 0.7678\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  48.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3110 - val_loss: 1.8916\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 1.5598 - val_loss: 0.9401\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2016 - val_loss: 3.6472\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 1.3655 - val_loss: 1.1979\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.1794 - val_loss: 0.9721\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 1.0479 - val_loss: 0.7877\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.1509 - val_loss: 1.0790\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.1672 - val_loss: 1.0041\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.1202 - val_loss: 1.1706\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.0972 - val_loss: 1.0665\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.1506 - val_loss: 0.9168\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 1.0471 - val_loss: 0.7631\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 1.0863 - val_loss: 1.4450\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 1.0688 - val_loss: 0.6892\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.0769 - val_loss: 1.9591\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 1.0388 - val_loss: 0.6801\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0011 - val_loss: 0.8488\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 1.0507 - val_loss: 1.0638\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 1.2632 - val_loss: 1.5597\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 1.0690 - val_loss: 1.5742\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 1.0258 - val_loss: 1.2597\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  47.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4352 - val_loss: 2.3524\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 1.4732 - val_loss: 1.8195\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 1.3513 - val_loss: 1.6046\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2815 - val_loss: 1.0628\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 1.2549 - val_loss: 0.8947\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 1.3595 - val_loss: 1.0425\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 1.2575 - val_loss: 1.1783\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 1.1457 - val_loss: 1.1053\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2386 - val_loss: 0.8328\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 1.1121 - val_loss: 0.7683\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 1.0800 - val_loss: 0.8122\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 1.1334 - val_loss: 1.0681\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.1360 - val_loss: 1.7227\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 1.1523 - val_loss: 0.8457\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 1.0846 - val_loss: 1.0112\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 661us/step - loss: 1.1795 - val_loss: 1.2654\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 1.1770 - val_loss: 2.9915\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 1.1655 - val_loss: 1.0430\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 1.1196 - val_loss: 0.6694\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 1.1168 - val_loss: 0.9777\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.9897 - val_loss: 1.5852\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  48.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2536 - val_loss: 1.4058\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.4792 - val_loss: 1.9845\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 1.3039 - val_loss: 1.2030\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 1.3195 - val_loss: 2.4667\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 1.2003 - val_loss: 0.9571\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 1.1969 - val_loss: 1.6163\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.0961 - val_loss: 1.3730\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0934 - val_loss: 1.9173\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 1.1045 - val_loss: 1.4569\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.9799 - val_loss: 0.7403\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.3255 - val_loss: 1.5843\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.2985 - val_loss: 1.4799\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 1.0522 - val_loss: 1.1813\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 1.0945 - val_loss: 1.1196\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 1.2515 - val_loss: 1.6401\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.1444 - val_loss: 1.0283\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 1.2275 - val_loss: 1.8738\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 1.0204 - val_loss: 0.6723\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 642us/step - loss: 1.1011 - val_loss: 0.8072\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 1.0705 - val_loss: 1.9125\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 1.0199 - val_loss: 1.0806\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  47.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3781 - val_loss: 0.4339\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.1522 - val_loss: 0.0624\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0513 - val_loss: 0.0401\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0392 - val_loss: 0.0306\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.0310 - val_loss: 0.0303\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0244 - val_loss: 0.0202\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0201 - val_loss: 0.0183\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0199 - val_loss: 0.0159\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0174 - val_loss: 0.0228\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 450us/step - loss: 0.0156 - val_loss: 0.0133\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0140 - val_loss: 0.0125\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0127 - val_loss: 0.0115\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 450us/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0114 - val_loss: 0.0098\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0106 - val_loss: 0.0093\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 448us/step - loss: 0.0103 - val_loss: 0.0094\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 512us/step - loss: 0.0098 - val_loss: 0.0091\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0092 - val_loss: 0.0087\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0086 - val_loss: 0.0082\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.6s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3577 - val_loss: 0.4205\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.1412 - val_loss: 0.0505\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 448us/step - loss: 0.0395 - val_loss: 0.0309\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0283 - val_loss: 0.0222\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0219 - val_loss: 0.0179\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0205 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0170 - val_loss: 0.0140\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0155 - val_loss: 0.0136\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0147 - val_loss: 0.0117\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 458us/step - loss: 0.0128 - val_loss: 0.0108\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0098 - val_loss: 0.0093\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0095 - val_loss: 0.0089\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 453us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3143 - val_loss: 0.3964\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.1338 - val_loss: 0.0543\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0444 - val_loss: 0.0361\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0357 - val_loss: 0.0506\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0293 - val_loss: 0.0239\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0274 - val_loss: 0.0221\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0238 - val_loss: 0.0209\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0212 - val_loss: 0.0182\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0176 - val_loss: 0.0222\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0164 - val_loss: 0.0135\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0154 - val_loss: 0.0128\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0158 - val_loss: 0.0131\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0169 - val_loss: 0.0113\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0136 - val_loss: 0.0106\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0147 - val_loss: 0.0102\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 442us/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0101 - val_loss: 0.0091\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 439us/step - loss: 0.0089 - val_loss: 0.0084\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 442us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.3639 - val_loss: 0.4114\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.1400 - val_loss: 0.0602\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0439 - val_loss: 0.0363\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 464us/step - loss: 0.0306 - val_loss: 0.0254\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 443us/step - loss: 0.0254 - val_loss: 0.0205\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0222 - val_loss: 0.0173\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0196 - val_loss: 0.0166\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0163 - val_loss: 0.0142\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0142 - val_loss: 0.0130\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0128 - val_loss: 0.0119\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 435us/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 439us/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 440us/step - loss: 0.0107 - val_loss: 0.0090\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0096 - val_loss: 0.0088\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 438us/step - loss: 0.0099 - val_loss: 0.0086\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 447us/step - loss: 0.0102 - val_loss: 0.0085\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0091 - val_loss: 0.0083\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  43.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3247 - val_loss: 0.4125\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.1398 - val_loss: 0.0532\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0414 - val_loss: 0.0324\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0292 - val_loss: 0.0250\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0263 - val_loss: 0.0200\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0218 - val_loss: 0.0180\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0196 - val_loss: 0.0212\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0181 - val_loss: 0.0150\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0166 - val_loss: 0.0907\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0165 - val_loss: 0.0143\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0138 - val_loss: 0.0122\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0123 - val_loss: 0.0116\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 464us/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 466us/step - loss: 0.0121 - val_loss: 0.0095\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 486us/step - loss: 0.0101 - val_loss: 0.0090\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 517us/step - loss: 0.0112 - val_loss: 0.0088\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0111 - val_loss: 0.0084\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 0.0088 - val_loss: 0.0082\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0082 - val_loss: 0.0082\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  44.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 811us/step - loss: 2.0495 - val_loss: 1.8333\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 556us/step - loss: 1.9853 - val_loss: 1.0758\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 537us/step - loss: 2.0177 - val_loss: 1.7394\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 4s 571us/step - loss: 1.8155 - val_loss: 0.6018\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.5840 - val_loss: 1.1553\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 539us/step - loss: 1.7037 - val_loss: 1.7619\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.6147 - val_loss: 1.4786\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 3s 558us/step - loss: 1.6448 - val_loss: 1.1432\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 542us/step - loss: 1.7160 - val_loss: 6.7320\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 559us/step - loss: 1.5923 - val_loss: 1.7710\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6038 - val_loss: 0.8006\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 539us/step - loss: 1.5625 - val_loss: 3.3897\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 4s 584us/step - loss: 1.6752 - val_loss: 3.7497\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 538us/step - loss: 1.5930 - val_loss: 1.3925\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 527us/step - loss: 1.7788 - val_loss: 1.2418\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.5695 - val_loss: 1.1778\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.4755 - val_loss: 1.0965\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6045 - val_loss: 0.8042\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 533us/step - loss: 1.6140 - val_loss: 2.1645\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 536us/step - loss: 1.6665 - val_loss: 3.2180\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 4s 566us/step - loss: 1.5921 - val_loss: 1.6398\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 795us/step - loss: 2.0826 - val_loss: 1.6291\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.8595 - val_loss: 2.8289\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 551us/step - loss: 1.7470 - val_loss: 1.6122\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 3s 526us/step - loss: 1.7286 - val_loss: 1.8451\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 532us/step - loss: 1.6521 - val_loss: 0.9254\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.7053 - val_loss: 2.2272\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.5833 - val_loss: 1.3344\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 4s 572us/step - loss: 1.7034 - val_loss: 1.0786\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 539us/step - loss: 1.6402 - val_loss: 2.0677\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.6700 - val_loss: 0.8380\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 4s 576us/step - loss: 1.5911 - val_loss: 0.9440\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.6941 - val_loss: 1.8238\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6054 - val_loss: 1.3997\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 4s 563us/step - loss: 1.6051 - val_loss: 1.0353\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 541us/step - loss: 1.5952 - val_loss: 0.9604\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 538us/step - loss: 1.6235 - val_loss: 1.1089\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 4s 573us/step - loss: 1.8128 - val_loss: 0.7699\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.7668 - val_loss: 0.8111\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.6631 - val_loss: 2.6912\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 4s 562us/step - loss: 1.7038 - val_loss: 0.7816\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.7381 - val_loss: 1.8475\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 824us/step - loss: 2.0848 - val_loss: 1.8964\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 537us/step - loss: 1.9604 - val_loss: 0.9799\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 538us/step - loss: 1.7706 - val_loss: 2.7968\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 4s 581us/step - loss: 1.8178 - val_loss: 0.8405\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.8514 - val_loss: 3.7617\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 542us/step - loss: 1.6464 - val_loss: 1.2769\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 4s 572us/step - loss: 1.7299 - val_loss: 2.0901\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6110 - val_loss: 3.6727\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.5695 - val_loss: 0.9895\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 546us/step - loss: 1.6302 - val_loss: 2.2467\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 3s 528us/step - loss: 1.7181 - val_loss: 2.0060\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.5945 - val_loss: 1.1329\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 4s 570us/step - loss: 1.7250 - val_loss: 0.9861\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.6462 - val_loss: 1.3101\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 536us/step - loss: 1.8206 - val_loss: 1.6681\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 553us/step - loss: 1.6675 - val_loss: 0.9190\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.5987 - val_loss: 0.9040\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 540us/step - loss: 1.6117 - val_loss: 0.9613\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 558us/step - loss: 1.7704 - val_loss: 0.7324\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6189 - val_loss: 3.5304\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 4s 574us/step - loss: 1.5071 - val_loss: 3.6898\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 791us/step - loss: 2.2296 - val_loss: 2.6894\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 554us/step - loss: 1.8376 - val_loss: 1.2147\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 541us/step - loss: 1.7435 - val_loss: 0.9304\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 3s 545us/step - loss: 1.8264 - val_loss: 1.1540\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 558us/step - loss: 1.8282 - val_loss: 1.8848\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.7473 - val_loss: 2.3212\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.8021 - val_loss: 2.3163\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 4s 563us/step - loss: 1.6223 - val_loss: 2.0090\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 549us/step - loss: 1.6435 - val_loss: 0.8118\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.7447 - val_loss: 4.3669\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 4s 574us/step - loss: 1.6616 - val_loss: 1.0755\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 541us/step - loss: 1.7779 - val_loss: 1.2148\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 3s 540us/step - loss: 1.6262 - val_loss: 3.5810\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 554us/step - loss: 1.6404 - val_loss: 1.0313\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.7011 - val_loss: 1.3704\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 532us/step - loss: 1.6487 - val_loss: 3.3312\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 4s 569us/step - loss: 1.7005 - val_loss: 0.7953\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 533us/step - loss: 1.6248 - val_loss: 1.2721\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 4s 560us/step - loss: 1.7068 - val_loss: 1.7680\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 557us/step - loss: 1.7333 - val_loss: 0.5884\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 3s 522us/step - loss: 1.6675 - val_loss: 0.9908\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.2min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 774us/step - loss: 2.1779 - val_loss: 1.4063\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 525us/step - loss: 1.8682 - val_loss: 3.0732\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 532us/step - loss: 1.8061 - val_loss: 0.7397\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 3s 536us/step - loss: 1.6944 - val_loss: 1.5711\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 545us/step - loss: 1.7009 - val_loss: 3.3226\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 4s 575us/step - loss: 1.6455 - val_loss: 1.0860\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 560us/step - loss: 1.8057 - val_loss: 1.3950\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 3s 527us/step - loss: 1.6595 - val_loss: 3.0940\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 554us/step - loss: 1.8185 - val_loss: 1.0772\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 4s 567us/step - loss: 1.7835 - val_loss: 2.3305\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 3s 522us/step - loss: 1.6996 - val_loss: 2.7528\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 556us/step - loss: 1.8290 - val_loss: 1.8732\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 3s 548us/step - loss: 1.7584 - val_loss: 2.3031\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.6866 - val_loss: 1.2577\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 553us/step - loss: 1.7467 - val_loss: 2.4673\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 552us/step - loss: 1.6664 - val_loss: 1.1721\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 3s 533us/step - loss: 1.8591 - val_loss: 3.9664\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 4s 570us/step - loss: 1.9158 - val_loss: 3.0424\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 546us/step - loss: 1.6148 - val_loss: 0.9112\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 529us/step - loss: 1.5801 - val_loss: 0.9372\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.6065 - val_loss: 0.7728\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 1.3138 - val_loss: 0.4532\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.2757 - val_loss: 0.5570\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.2025 - val_loss: 1.0355\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 1.1667 - val_loss: 1.0184\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 1.2145 - val_loss: 0.8794\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 1.0625 - val_loss: 0.8471\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 1.1149 - val_loss: 2.7763\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 1.1695 - val_loss: 1.3112\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 567us/step - loss: 1.1349 - val_loss: 0.7711\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.2292 - val_loss: 0.6008\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 596us/step - loss: 1.1854 - val_loss: 0.4148\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.1794 - val_loss: 1.3229\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.1348 - val_loss: 0.4279\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 564us/step - loss: 1.1109 - val_loss: 0.9632\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 574us/step - loss: 1.1120 - val_loss: 0.6256\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 574us/step - loss: 1.1666 - val_loss: 0.7041\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.0740 - val_loss: 2.1306\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 1.1004 - val_loss: 1.3319\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1979 - val_loss: 1.4518\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 559us/step - loss: 1.0777 - val_loss: 1.4715\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 573us/step - loss: 1.0911 - val_loss: 0.9016\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 1.4827 - val_loss: 1.4923\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.4227 - val_loss: 1.4094\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.3079 - val_loss: 1.0970\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.3872 - val_loss: 0.9075\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 595us/step - loss: 1.2271 - val_loss: 1.0691\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 572us/step - loss: 1.2792 - val_loss: 0.4303\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 565us/step - loss: 1.1838 - val_loss: 1.6258\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.2704 - val_loss: 2.6877\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1844 - val_loss: 0.9091\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.2034 - val_loss: 1.2839\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.3089 - val_loss: 1.0650\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 606us/step - loss: 1.2428 - val_loss: 1.8647\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 557us/step - loss: 1.1691 - val_loss: 1.6116\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 1.2043 - val_loss: 0.8117\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.0830 - val_loss: 0.8571\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.1902 - val_loss: 1.5128\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 570us/step - loss: 1.1331 - val_loss: 0.8823\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 1.1470 - val_loss: 1.1137\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 1.0958 - val_loss: 1.7578\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1036 - val_loss: 0.7895\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.0959 - val_loss: 2.5830\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 1.5119 - val_loss: 1.0155\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 579us/step - loss: 1.2930 - val_loss: 0.8148\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 581us/step - loss: 1.2643 - val_loss: 0.4344\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 1.3187 - val_loss: 2.0839\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.1392 - val_loss: 1.6559\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.2644 - val_loss: 0.8539\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.2365 - val_loss: 0.7371\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.1408 - val_loss: 0.6687\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.0993 - val_loss: 0.8836\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 1.0337 - val_loss: 0.7442\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.2246 - val_loss: 1.0844\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 0.9990 - val_loss: 1.1175\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.2760 - val_loss: 2.3433\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.1260 - val_loss: 0.6654\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.0991 - val_loss: 0.8295\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 0.9976 - val_loss: 1.2872\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 0.9738 - val_loss: 0.5575\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 0.9822 - val_loss: 0.5591\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.1437 - val_loss: 2.8904\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.1838 - val_loss: 0.6201\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.0390 - val_loss: 1.1287\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 1.3537 - val_loss: 3.2304\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 557us/step - loss: 1.2241 - val_loss: 0.7114\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.2891 - val_loss: 0.8875\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.2299 - val_loss: 1.2056\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.2072 - val_loss: 1.4694\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 565us/step - loss: 1.2006 - val_loss: 0.9376\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.2708 - val_loss: 0.5127\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 573us/step - loss: 1.2289 - val_loss: 0.4724\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 576us/step - loss: 1.1163 - val_loss: 1.9452\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 608us/step - loss: 1.1768 - val_loss: 2.2652\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.2898 - val_loss: 0.9334\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.1127 - val_loss: 0.5052\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.0420 - val_loss: 1.0299\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 574us/step - loss: 0.9830 - val_loss: 1.3125\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 572us/step - loss: 1.1674 - val_loss: 0.4857\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.1020 - val_loss: 0.7583\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.0609 - val_loss: 0.6556\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1073 - val_loss: 0.6437\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.1356 - val_loss: 1.4740\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.0726 - val_loss: 0.5107\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 583us/step - loss: 0.9805 - val_loss: 0.7234\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 1.5645 - val_loss: 0.9579\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.3933 - val_loss: 2.0165\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.2286 - val_loss: 0.7878\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.0514 - val_loss: 1.3322\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 570us/step - loss: 1.2913 - val_loss: 3.2380\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 576us/step - loss: 1.1876 - val_loss: 0.8611\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 1.1269 - val_loss: 1.5577\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 0.9962 - val_loss: 0.6705\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.2040 - val_loss: 1.1424\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 577us/step - loss: 1.2394 - val_loss: 1.0566\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 605us/step - loss: 1.1684 - val_loss: 1.4895\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 603us/step - loss: 1.1026 - val_loss: 0.7058\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.1220 - val_loss: 1.2121\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.1655 - val_loss: 1.4523\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.2646 - val_loss: 0.8966\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 567us/step - loss: 1.0991 - val_loss: 0.4141\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.0723 - val_loss: 1.9051\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 572us/step - loss: 1.0062 - val_loss: 1.5180\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.1790 - val_loss: 1.8682\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 558us/step - loss: 1.0427 - val_loss: 0.5334\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 590us/step - loss: 1.1291 - val_loss: 1.4961\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.5009 - val_loss: 0.1091\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.1017 - val_loss: 0.0809\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0805 - val_loss: 0.0724\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0688 - val_loss: 0.0596\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0604 - val_loss: 0.0537\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 704us/step - loss: 0.0544 - val_loss: 0.0579\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 864us/step - loss: 0.0499 - val_loss: 0.0574\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 864us/step - loss: 0.0440 - val_loss: 0.0389\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 698us/step - loss: 0.0364 - val_loss: 0.0321\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 667us/step - loss: 0.0315 - val_loss: 0.0298\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0292 - val_loss: 0.0270\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0276 - val_loss: 0.0246\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0261 - val_loss: 0.0240\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0253 - val_loss: 0.0297\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0238 - val_loss: 0.0218\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0230 - val_loss: 0.0213\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 696us/step - loss: 0.0223 - val_loss: 0.0222\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 675us/step - loss: 0.0217 - val_loss: 0.0201\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 720us/step - loss: 0.0209 - val_loss: 0.0205\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.0203 - val_loss: 0.0189\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 683us/step - loss: 0.0198 - val_loss: 0.0212\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  51.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4908 - val_loss: 0.1065\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0989 - val_loss: 0.0824\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 682us/step - loss: 0.0800 - val_loss: 0.0764\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0679 - val_loss: 0.0597\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 629us/step - loss: 0.0601 - val_loss: 0.0615\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0543 - val_loss: 0.0469\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0482 - val_loss: 0.0461\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0409 - val_loss: 0.0342\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0335 - val_loss: 0.0304\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 658us/step - loss: 0.0304 - val_loss: 0.0283\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0283 - val_loss: 0.0256\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0264 - val_loss: 0.0246\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0250 - val_loss: 0.0231\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0239 - val_loss: 0.0221\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 666us/step - loss: 0.0230 - val_loss: 0.0210\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0218 - val_loss: 0.0217\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0208 - val_loss: 0.0202\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 666us/step - loss: 0.0202 - val_loss: 0.0197\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0194 - val_loss: 0.0184\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 645us/step - loss: 0.0190 - val_loss: 0.0209\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 626us/step - loss: 0.0181 - val_loss: 0.0171\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  50.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4765 - val_loss: 0.1089\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.1041 - val_loss: 0.0823\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0826 - val_loss: 0.0745\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0701 - val_loss: 0.0706\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0614 - val_loss: 0.0549\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0554 - val_loss: 0.0477\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0499 - val_loss: 0.0434\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0441 - val_loss: 0.0420\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0370 - val_loss: 0.0333\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 637us/step - loss: 0.0319 - val_loss: 0.0290\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0290 - val_loss: 0.0263\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0274 - val_loss: 0.0249\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0260 - val_loss: 0.0242\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0250 - val_loss: 0.0231\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0240 - val_loss: 0.0228\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0229 - val_loss: 0.0211\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0222 - val_loss: 0.0232\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0215 - val_loss: 0.0200\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0209 - val_loss: 0.0196\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 692us/step - loss: 0.0200 - val_loss: 0.0209\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 740us/step - loss: 0.0196 - val_loss: 0.0180\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  49.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4379 - val_loss: 0.1000\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0946 - val_loss: 0.0831\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0760 - val_loss: 0.0678\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0650 - val_loss: 0.0618\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0583 - val_loss: 0.0513\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0532 - val_loss: 0.0471\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0491 - val_loss: 0.0448\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0456 - val_loss: 0.0413\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.0438 - val_loss: 0.0424\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0415 - val_loss: 0.0402\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0383 - val_loss: 0.0332\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0348 - val_loss: 0.0381\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0303 - val_loss: 0.0290\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0263 - val_loss: 0.0246\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0243 - val_loss: 0.0220\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0232 - val_loss: 0.0251\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0221 - val_loss: 0.0214\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 700us/step - loss: 0.0219 - val_loss: 0.0196\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 626us/step - loss: 0.0207 - val_loss: 0.0191\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0202 - val_loss: 0.0192\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0196 - val_loss: 0.0184\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  49.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4648 - val_loss: 0.1101\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.1003 - val_loss: 0.0799\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0791 - val_loss: 0.0689\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0678 - val_loss: 0.0584\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0600 - val_loss: 0.0537\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0543 - val_loss: 0.0600\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0501 - val_loss: 0.0457\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0464 - val_loss: 0.0417\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0429 - val_loss: 0.0384\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0397 - val_loss: 0.0369\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0363 - val_loss: 0.0344\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0319 - val_loss: 0.0279\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0276 - val_loss: 0.0249\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0254 - val_loss: 0.0237\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0240 - val_loss: 0.0219\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0228 - val_loss: 0.0230\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0219 - val_loss: 0.0201\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0214 - val_loss: 0.0200\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0209 - val_loss: 0.0193\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0203 - val_loss: 0.0191\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0198 - val_loss: 0.0182\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  48.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8257 - val_loss: 1.0841\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.7961 - val_loss: 1.8390\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.9293 - val_loss: 0.7291\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.7981 - val_loss: 0.4040\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.7961 - val_loss: 0.6338\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.7606 - val_loss: 0.5805\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.8357 - val_loss: 0.6535\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7379 - val_loss: 1.3158\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.7925 - val_loss: 0.8457\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.9728 - val_loss: 0.4094\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.8004 - val_loss: 0.7565\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.7480 - val_loss: 1.6881\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.6774 - val_loss: 0.7345\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.7003 - val_loss: 0.9248\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.8209 - val_loss: 0.5067\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.7357 - val_loss: 0.8436\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.7064 - val_loss: 0.7397\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.6986 - val_loss: 0.4369\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.6661 - val_loss: 0.7733\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.7657 - val_loss: 0.5325\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.8372 - val_loss: 0.6942\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  48.6s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7540 - val_loss: 0.5935\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 661us/step - loss: 0.9600 - val_loss: 0.5619\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.8767 - val_loss: 1.4013\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.7201 - val_loss: 0.8441\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.7841 - val_loss: 1.2540\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.9837 - val_loss: 0.5844\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.7907 - val_loss: 0.8751\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.8221 - val_loss: 0.4917\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.6586 - val_loss: 0.6788\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.8244 - val_loss: 0.7687\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.8070 - val_loss: 1.2177\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.7881 - val_loss: 0.3000\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.7595 - val_loss: 1.0335\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.7706 - val_loss: 1.2513\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.7115 - val_loss: 1.6043\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.8340 - val_loss: 0.4869\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.7587 - val_loss: 0.4795\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7453 - val_loss: 0.5054\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.8020 - val_loss: 1.2062\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.6782 - val_loss: 1.0712\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7217 - val_loss: 0.6947\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  49.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8259 - val_loss: 0.7914\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.8503 - val_loss: 1.4953\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 1.0301 - val_loss: 0.4811\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 695us/step - loss: 0.8100 - val_loss: 0.6207\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.8781 - val_loss: 0.6775\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.7969 - val_loss: 0.7816\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.7632 - val_loss: 0.5114\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.7226 - val_loss: 0.9786\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7845 - val_loss: 0.4548\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.6876 - val_loss: 1.2931\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.8842 - val_loss: 0.7475\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.7147 - val_loss: 0.8473\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.7138 - val_loss: 0.4855\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.8563 - val_loss: 0.6942\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.7323 - val_loss: 0.5321\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.7344 - val_loss: 0.4130\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.6635 - val_loss: 0.4413\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.7459 - val_loss: 0.5090\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7696 - val_loss: 0.6121\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.6943 - val_loss: 0.3167\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.7159 - val_loss: 0.4275\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  48.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8108 - val_loss: 0.6632\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 1.0144 - val_loss: 1.3635\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.9622 - val_loss: 0.7523\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.7466 - val_loss: 0.7003\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.8471 - val_loss: 2.2345\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 669us/step - loss: 0.7158 - val_loss: 0.6101\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.7460 - val_loss: 0.9688\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.8945 - val_loss: 2.0750\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.8393 - val_loss: 0.7837\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.7145 - val_loss: 1.3448\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.8506 - val_loss: 0.8265\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.7636 - val_loss: 0.4143\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.7345 - val_loss: 0.7574\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.9312 - val_loss: 0.8654\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.7928 - val_loss: 1.1743\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.8166 - val_loss: 0.6067\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.7407 - val_loss: 0.6288\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.8150 - val_loss: 1.3362\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.7802 - val_loss: 0.6273\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.6838 - val_loss: 0.8952\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.6871 - val_loss: 0.7546\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  49.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8026 - val_loss: 1.1283\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.9399 - val_loss: 1.0160\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.8103 - val_loss: 0.8321\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.8769 - val_loss: 1.0033\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 658us/step - loss: 0.6961 - val_loss: 0.8674\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.6724 - val_loss: 0.9293\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.8434 - val_loss: 0.4928\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 692us/step - loss: 0.7881 - val_loss: 0.8932\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.6938 - val_loss: 0.3938\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.8286 - val_loss: 0.4843\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 662us/step - loss: 0.6383 - val_loss: 0.5622\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.6835 - val_loss: 0.4465\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.8587 - val_loss: 0.9712\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.7464 - val_loss: 0.6541\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.6609 - val_loss: 0.6444\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.7408 - val_loss: 0.3988\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.7057 - val_loss: 0.6988\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.7602 - val_loss: 1.0903\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.7159 - val_loss: 1.4128\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.6291 - val_loss: 0.8044\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.6761 - val_loss: 0.3876\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  50.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3594 - val_loss: 0.2418\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.1824 - val_loss: 0.1319\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.1013 - val_loss: 0.0742\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0582 - val_loss: 0.0431\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0349 - val_loss: 0.0266\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0222 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 501us/step - loss: 0.0152 - val_loss: 0.0120\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0113 - val_loss: 0.0094\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0090 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0076 - val_loss: 0.0064\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0067 - val_loss: 0.0058\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0062 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0059 - val_loss: 0.0050\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 466us/step - loss: 0.0050 - val_loss: 0.0042\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 499us/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0044 - val_loss: 0.0041\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  42.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3683 - val_loss: 0.2460\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.1846 - val_loss: 0.1333\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.1021 - val_loss: 0.0790\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 537us/step - loss: 0.0586 - val_loss: 0.0434\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0350 - val_loss: 0.0276\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0222 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 506us/step - loss: 0.0151 - val_loss: 0.0120\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0111 - val_loss: 0.0091\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0088 - val_loss: 0.0074\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.0075 - val_loss: 0.0063\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0066 - val_loss: 0.0057\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0060 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0051 - val_loss: 0.0044\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0046 - val_loss: 0.0040\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  44.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3678 - val_loss: 0.2445\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.1842 - val_loss: 0.1327\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 497us/step - loss: 0.1018 - val_loss: 0.0746\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0584 - val_loss: 0.0435\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0350 - val_loss: 0.0267\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0222 - val_loss: 0.0172\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0152 - val_loss: 0.0124\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0113 - val_loss: 0.0092\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0090 - val_loss: 0.0079\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0061 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0050 - val_loss: 0.0043\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 492us/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0046 - val_loss: 0.0040\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  43.8s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3613 - val_loss: 0.2416\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.1823 - val_loss: 0.1308\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.1005 - val_loss: 0.0748\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0575 - val_loss: 0.0427\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0344 - val_loss: 0.0261\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0218 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0148 - val_loss: 0.0214\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0109 - val_loss: 0.0089\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0073 - val_loss: 0.0089\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0059 - val_loss: 0.0051\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 497us/step - loss: 0.0050 - val_loss: 0.0043\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 486us/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  44.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3676 - val_loss: 0.2454\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 506us/step - loss: 0.1854 - val_loss: 0.1341\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.1029 - val_loss: 0.0762\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0588 - val_loss: 0.0436\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0351 - val_loss: 0.0265\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0221 - val_loss: 0.0176\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0150 - val_loss: 0.0119\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0110 - val_loss: 0.0089\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0087 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0073 - val_loss: 0.0063\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0065 - val_loss: 0.0057\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0050 - val_loss: 0.0045\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 531us/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0043 - val_loss: 0.0037\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0041 - val_loss: 0.0055\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  44.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2769 - val_loss: 0.2397\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.1453 - val_loss: 0.1188\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.1091 - val_loss: 0.0981\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0972 - val_loss: 0.0922\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0899 - val_loss: 0.0761\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0847 - val_loss: 0.0765\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0800 - val_loss: 0.1352\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 499us/step - loss: 0.0759 - val_loss: 0.0872\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0728 - val_loss: 0.0686\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0702 - val_loss: 0.0866\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0674 - val_loss: 0.0599\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 514us/step - loss: 0.0640 - val_loss: 0.0720\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0635 - val_loss: 0.0534\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0602 - val_loss: 0.0559\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.0584 - val_loss: 0.0525\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 465us/step - loss: 0.0573 - val_loss: 0.0563\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0564 - val_loss: 0.0494\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0557 - val_loss: 0.0490\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0548 - val_loss: 0.0563\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0526 - val_loss: 0.0657\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0521 - val_loss: 0.0447\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  44.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2681 - val_loss: 0.2390\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.1428 - val_loss: 0.1090\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.1058 - val_loss: 0.0928\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0947 - val_loss: 0.0967\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0871 - val_loss: 0.0765\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0816 - val_loss: 0.0699\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 517us/step - loss: 0.0775 - val_loss: 0.0949\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0730 - val_loss: 0.0666\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0698 - val_loss: 0.0621\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0667 - val_loss: 0.0601\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0642 - val_loss: 0.0535\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.0614 - val_loss: 0.0562\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0605 - val_loss: 0.0663\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.0575 - val_loss: 0.0528\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0562 - val_loss: 0.0489\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0556 - val_loss: 0.1579\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0539 - val_loss: 0.0491\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0528 - val_loss: 0.0480\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0525 - val_loss: 0.0819\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0511 - val_loss: 0.0474\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0501 - val_loss: 0.0466\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  44.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3061 - val_loss: 0.2437\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.1441 - val_loss: 0.1102\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.1083 - val_loss: 0.0982\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0971 - val_loss: 0.0875\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 515us/step - loss: 0.0890 - val_loss: 0.1368\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0838 - val_loss: 0.0713\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0791 - val_loss: 0.0713\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 488us/step - loss: 0.0747 - val_loss: 0.0696\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0725 - val_loss: 0.0610\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0689 - val_loss: 0.0759\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0668 - val_loss: 0.0687\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0641 - val_loss: 0.0774\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0621 - val_loss: 0.0567\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0598 - val_loss: 0.0794\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0588 - val_loss: 0.0523\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 497us/step - loss: 0.0574 - val_loss: 0.0530\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 492us/step - loss: 0.0561 - val_loss: 0.0527\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 521us/step - loss: 0.0549 - val_loss: 0.0510\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0535 - val_loss: 0.0495\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0532 - val_loss: 0.0458\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0523 - val_loss: 0.0474\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  43.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2884 - val_loss: 0.2423\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.1446 - val_loss: 0.1139\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.1081 - val_loss: 0.0931\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0969 - val_loss: 0.0828\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0887 - val_loss: 0.0761\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0833 - val_loss: 0.0905\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0790 - val_loss: 0.0874\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0765 - val_loss: 0.0652\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0713 - val_loss: 0.0870\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0694 - val_loss: 0.0638\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 501us/step - loss: 0.0658 - val_loss: 0.0615\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0650 - val_loss: 0.0570\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0624 - val_loss: 0.1057\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 499us/step - loss: 0.0611 - val_loss: 0.0652\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.0585 - val_loss: 0.0502\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0583 - val_loss: 0.0559\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0575 - val_loss: 0.2498\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0557 - val_loss: 0.0494\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0549 - val_loss: 0.0465\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0541 - val_loss: 0.0468\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0525 - val_loss: 0.0620\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  43.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3054 - val_loss: 0.2460\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.1494 - val_loss: 0.1231\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.1100 - val_loss: 0.1390\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0962 - val_loss: 0.1000\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0885 - val_loss: 0.0773\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0813 - val_loss: 0.0716\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0769 - val_loss: 0.0676\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0726 - val_loss: 0.0648\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0693 - val_loss: 0.0599\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0667 - val_loss: 0.1222\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0645 - val_loss: 0.0580\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0622 - val_loss: 0.0556\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0600 - val_loss: 0.0534\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0586 - val_loss: 0.0526\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0568 - val_loss: 0.0568\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0560 - val_loss: 0.0509\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0540 - val_loss: 0.0488\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0534 - val_loss: 0.0505\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 506us/step - loss: 0.0532 - val_loss: 0.0480\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 510us/step - loss: 0.0510 - val_loss: 0.0433\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  44.3s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3760 - val_loss: 0.0559\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0393 - val_loss: 0.0275\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0236 - val_loss: 0.0183\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 661us/step - loss: 0.0168 - val_loss: 0.0143\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0136 - val_loss: 0.0118\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 647us/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 727us/step - loss: 0.0103 - val_loss: 0.0095\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0095 - val_loss: 0.0084\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 652us/step - loss: 0.0085 - val_loss: 0.0096\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0079 - val_loss: 0.0073\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 669us/step - loss: 0.0069 - val_loss: 0.0062\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0062 - val_loss: 0.0078\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0058 - val_loss: 0.0064\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 660us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 728us/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0053 - val_loss: 0.0052\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3673 - val_loss: 0.0431\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0298 - val_loss: 0.0211\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.0189 - val_loss: 0.0153\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0146 - val_loss: 0.0125\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0127 - val_loss: 0.0112\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0102 - val_loss: 0.0087\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0085 - val_loss: 0.0094\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0079 - val_loss: 0.0076\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0069 - val_loss: 0.0063\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 692us/step - loss: 0.0060 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0052 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0051 - val_loss: 0.0049\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4291 - val_loss: 0.0434\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0306 - val_loss: 0.0227\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0196 - val_loss: 0.0160\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0151 - val_loss: 0.0132\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 659us/step - loss: 0.0124 - val_loss: 0.0109\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0101 - val_loss: 0.0094\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 655us/step - loss: 0.0088 - val_loss: 0.0080\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 631us/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0068 - val_loss: 0.0060\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0059 - val_loss: 0.0056\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0055 - val_loss: 0.0064\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 651us/step - loss: 0.0054 - val_loss: 0.0056\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0050 - val_loss: 0.0067\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3826 - val_loss: 0.0523\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0364 - val_loss: 0.0265\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0220 - val_loss: 0.0188\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0161 - val_loss: 0.0136\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0135 - val_loss: 0.0117\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0097 - val_loss: 0.0086\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0086 - val_loss: 0.0082\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0061 - val_loss: 0.0059\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 755us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 668us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 711us/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 630us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0046 - val_loss: 0.0045\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.9s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3885 - val_loss: 0.0525\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0369 - val_loss: 0.0272\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 637us/step - loss: 0.0231 - val_loss: 0.0185\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 724us/step - loss: 0.0169 - val_loss: 0.0140\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 838us/step - loss: 0.0132 - val_loss: 0.0122\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 2s 990us/step - loss: 0.0100 - val_loss: 0.0090\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 625us/step - loss: 0.0087 - val_loss: 0.0083\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 712us/step - loss: 0.0085 - val_loss: 0.0112\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0077 - val_loss: 0.0071\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0057 - val_loss: 0.0061\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 636us/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 662us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  52.6s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3564 - val_loss: 0.2425\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.1828 - val_loss: 0.1314\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.1011 - val_loss: 0.0740\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0580 - val_loss: 0.0430\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0348 - val_loss: 0.0263\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0221 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0151 - val_loss: 0.0120\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0112 - val_loss: 0.0091\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 507us/step - loss: 0.0089 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0075 - val_loss: 0.0064\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0067 - val_loss: 0.0057\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0061 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0049 - val_loss: 0.0044\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 509us/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 514us/step - loss: 0.0042 - val_loss: 0.0037\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20>,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={'activation': ['relu', 'tanh'],\n",
       "                             'batch_size': [16, 32, 64],\n",
       "                             'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
       "                             'learning_rate': [0.001, 0.01, 0.1],\n",
       "                             'optimizer': ['sgd', 'adam']},\n",
       "              verbose=2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define base model function for bayes search\n",
    "def create_model(activation='relu', optimizer='adam', learning_rate=0.001, kernel_regularizer=None, batch_size=32):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'optimizer': ['sgd', 'adam'],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epochs, verbose=2)\n",
    "\n",
    "# perform random search hyperparameter tuning\n",
    "bayes_search = BayesSearchCV(estimator=keras_reg, search_spaces=param_dist, n_iter=10, cv=5, verbose=2, random_state=42, error_score='raise')\n",
    "bayes_search.fit(Xtrain, ytrain, sample_weight=wtrain, validation_data=(Xval, yval, wval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.897756</td>\n",
       "      <td>0.870543</td>\n",
       "      <td>0.156242</td>\n",
       "      <td>0.011116</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.004013</td>\n",
       "      <td>-0.003887</td>\n",
       "      <td>-0.003985</td>\n",
       "      <td>-0.004065</td>\n",
       "      <td>-0.004182</td>\n",
       "      <td>-0.004026</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.712921</td>\n",
       "      <td>0.397587</td>\n",
       "      <td>0.144377</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.920400</td>\n",
       "      <td>-0.372206</td>\n",
       "      <td>-0.663099</td>\n",
       "      <td>-0.555351</td>\n",
       "      <td>-0.749872</td>\n",
       "      <td>-0.652185</td>\n",
       "      <td>0.184047</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.110752</td>\n",
       "      <td>1.256021</td>\n",
       "      <td>0.151572</td>\n",
       "      <td>0.010048</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.006722</td>\n",
       "      <td>-0.005950</td>\n",
       "      <td>-0.007479</td>\n",
       "      <td>-0.006512</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>-0.006538</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135.101206</td>\n",
       "      <td>0.380900</td>\n",
       "      <td>0.378550</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>tanh</td>\n",
       "      <td>16</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 16, 'kern...</td>\n",
       "      <td>-1.221039</td>\n",
       "      <td>-2.241018</td>\n",
       "      <td>-2.258673</td>\n",
       "      <td>-1.308517</td>\n",
       "      <td>-1.526313</td>\n",
       "      <td>-1.711112</td>\n",
       "      <td>0.451005</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.599411</td>\n",
       "      <td>0.461664</td>\n",
       "      <td>0.228074</td>\n",
       "      <td>0.008765</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 32, 'kern...</td>\n",
       "      <td>-0.551183</td>\n",
       "      <td>-0.671337</td>\n",
       "      <td>-0.275650</td>\n",
       "      <td>-0.343786</td>\n",
       "      <td>-0.761923</td>\n",
       "      <td>-0.520776</td>\n",
       "      <td>0.186095</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49.627714</td>\n",
       "      <td>0.804522</td>\n",
       "      <td>0.150416</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.011203</td>\n",
       "      <td>-0.010994</td>\n",
       "      <td>-0.012591</td>\n",
       "      <td>-0.012352</td>\n",
       "      <td>-0.013580</td>\n",
       "      <td>-0.012144</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49.005421</td>\n",
       "      <td>0.492690</td>\n",
       "      <td>0.149297</td>\n",
       "      <td>0.007677</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.624877</td>\n",
       "      <td>-0.266007</td>\n",
       "      <td>-0.635726</td>\n",
       "      <td>-0.305873</td>\n",
       "      <td>-1.075089</td>\n",
       "      <td>-0.581515</td>\n",
       "      <td>0.291190</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43.598086</td>\n",
       "      <td>0.795214</td>\n",
       "      <td>0.150025</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.002285</td>\n",
       "      <td>-0.002225</td>\n",
       "      <td>-0.002348</td>\n",
       "      <td>-0.002100</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.002230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43.600693</td>\n",
       "      <td>0.447184</td>\n",
       "      <td>0.152037</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.041846</td>\n",
       "      <td>-0.029398</td>\n",
       "      <td>-0.051953</td>\n",
       "      <td>-0.034058</td>\n",
       "      <td>-0.031372</td>\n",
       "      <td>-0.037725</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50.836121</td>\n",
       "      <td>0.821611</td>\n",
       "      <td>0.152331</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.004155</td>\n",
       "      <td>-0.004024</td>\n",
       "      <td>-0.004070</td>\n",
       "      <td>-0.003794</td>\n",
       "      <td>-0.004121</td>\n",
       "      <td>-0.004033</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      48.897756      0.870543         0.156242        0.011116   \n",
       "1      47.712921      0.397587         0.144377        0.003170   \n",
       "2      42.110752      1.256021         0.151572        0.010048   \n",
       "3     135.101206      0.380900         0.378550        0.007319   \n",
       "4      76.599411      0.461664         0.228074        0.008765   \n",
       "5      49.627714      0.804522         0.150416        0.005282   \n",
       "6      49.005421      0.492690         0.149297        0.007677   \n",
       "7      43.598086      0.795214         0.150025        0.006603   \n",
       "8      43.600693      0.447184         0.152037        0.005516   \n",
       "9      50.836121      0.821611         0.152331        0.004190   \n",
       "\n",
       "  param_activation param_batch_size param_kernel_regularizer  \\\n",
       "0             relu               64                    l1_l2   \n",
       "1             tanh               64                       l1   \n",
       "2             relu               64                       l1   \n",
       "3             tanh               16                       l2   \n",
       "4             tanh               32                       l2   \n",
       "5             tanh               64                       l1   \n",
       "6             tanh               64                       l2   \n",
       "7             tanh               64                       l2   \n",
       "8             tanh               64                    l1_l2   \n",
       "9             relu               64                    l1_l2   \n",
       "\n",
       "  param_learning_rate param_optimizer  \\\n",
       "0               0.001            adam   \n",
       "1                 0.1            adam   \n",
       "2                0.01             sgd   \n",
       "3                 0.1            adam   \n",
       "4                 0.1            adam   \n",
       "5               0.001            adam   \n",
       "6                 0.1            adam   \n",
       "7                 0.1             sgd   \n",
       "8                0.01             sgd   \n",
       "9               0.001            adam   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.004013   \n",
       "1  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.920400   \n",
       "2  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.006722   \n",
       "3  {'activation': 'tanh', 'batch_size': 16, 'kern...          -1.221039   \n",
       "4  {'activation': 'tanh', 'batch_size': 32, 'kern...          -0.551183   \n",
       "5  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.011203   \n",
       "6  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.624877   \n",
       "7  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.002285   \n",
       "8  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.041846   \n",
       "9  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.004155   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0          -0.003887          -0.003985          -0.004065          -0.004182   \n",
       "1          -0.372206          -0.663099          -0.555351          -0.749872   \n",
       "2          -0.005950          -0.007479          -0.006512          -0.006029   \n",
       "3          -2.241018          -2.258673          -1.308517          -1.526313   \n",
       "4          -0.671337          -0.275650          -0.343786          -0.761923   \n",
       "5          -0.010994          -0.012591          -0.012352          -0.013580   \n",
       "6          -0.266007          -0.635726          -0.305873          -1.075089   \n",
       "7          -0.002225          -0.002348          -0.002100          -0.002193   \n",
       "8          -0.029398          -0.051953          -0.034058          -0.031372   \n",
       "9          -0.004024          -0.004070          -0.003794          -0.004121   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0        -0.004026        0.000097                2  \n",
       "1        -0.652185        0.184047                9  \n",
       "2        -0.006538        0.000552                4  \n",
       "3        -1.711112        0.451005               10  \n",
       "4        -0.520776        0.186095                7  \n",
       "5        -0.012144        0.000950                5  \n",
       "6        -0.581515        0.291190                8  \n",
       "7        -0.002230        0.000084                1  \n",
       "8        -0.037725        0.008277                6  \n",
       "9        -0.004033        0.000127                3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bayes_search_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "\n",
    "display(bayes_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/3143193033.py:24: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 11111\n",
      "max_resources_: 100000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 12\n",
      "n_resources: 11111\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 1ms/step - loss: 1.3465 - val_loss: 0.4055\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 435us/step - loss: 0.1402 - val_loss: 0.0562\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0448 - val_loss: 0.0366\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0348 - val_loss: 0.0322\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0338 - val_loss: 0.0500\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0288 - val_loss: 0.0334\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0286 - val_loss: 0.0230\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 466us/step - loss: 0.0245 - val_loss: 0.0188\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0235 - val_loss: 0.0199\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 445us/step - loss: 0.0237 - val_loss: 0.0168\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0213 - val_loss: 0.0153\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0208 - 88ms/epoch - 317us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0396 - 90ms/epoch - 324us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0186 - 90ms/epoch - 323us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0177 - 86ms/epoch - 311us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0195 - 93ms/epoch - 333us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0194 - 89ms/epoch - 319us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0162 - 86ms/epoch - 311us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0179 - 85ms/epoch - 307us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0256 - 86ms/epoch - 310us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0148 - 87ms/epoch - 313us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0187 - 86ms/epoch - 308us/step\n",
      "70/70 - 0s - loss: 0.0142 - 65ms/epoch - 929us/step\n",
      "278/278 - 0s - loss: 0.0142 - 82ms/epoch - 296us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  17.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3651 - val_loss: 0.4194\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 441us/step - loss: 0.1461 - val_loss: 0.0542\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 440us/step - loss: 0.0406 - val_loss: 0.0334\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 424us/step - loss: 0.0324 - val_loss: 0.0320\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0252 - val_loss: 0.0195\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 438us/step - loss: 0.0239 - val_loss: 0.0236\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0190 - val_loss: 0.0152\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 435us/step - loss: 0.0163 - val_loss: 0.0162\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 423us/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 431us/step - loss: 0.0163 - val_loss: 0.0134\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 426us/step - loss: 0.0142 - val_loss: 0.0132\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0174 - 88ms/epoch - 315us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0170 - 85ms/epoch - 305us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0153 - 85ms/epoch - 307us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0137 - 86ms/epoch - 308us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0216 - 85ms/epoch - 304us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0121 - 86ms/epoch - 308us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0148 - 85ms/epoch - 307us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0125 - 85ms/epoch - 306us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0124 - 85ms/epoch - 306us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0145 - 85ms/epoch - 307us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0172 - 85ms/epoch - 305us/step\n",
      "70/70 - 0s - loss: 0.0136 - 66ms/epoch - 945us/step\n",
      "278/278 - 0s - loss: 0.0136 - 81ms/epoch - 292us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  16.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3610 - val_loss: 0.3997\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 445us/step - loss: 0.1283 - val_loss: 0.0460\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 436us/step - loss: 0.0364 - val_loss: 0.0298\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 429us/step - loss: 0.0257 - val_loss: 0.0228\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0211 - val_loss: 0.0193\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 439us/step - loss: 0.0203 - val_loss: 0.0165\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0180 - val_loss: 0.0154\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 433us/step - loss: 0.0177 - val_loss: 0.0181\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0182 - val_loss: 0.0136\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0165 - val_loss: 0.0124\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0143 - val_loss: 0.0117\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0160 - 90ms/epoch - 322us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0149 - 88ms/epoch - 318us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0193 - 86ms/epoch - 309us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0144 - 86ms/epoch - 310us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0157 - 87ms/epoch - 313us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0123 - 89ms/epoch - 321us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0135 - 88ms/epoch - 317us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0132 - 91ms/epoch - 326us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0170 - 93ms/epoch - 333us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0137 - 93ms/epoch - 334us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0110 - 93ms/epoch - 334us/step\n",
      "70/70 - 0s - loss: 0.0105 - 68ms/epoch - 965us/step\n",
      "278/278 - 0s - loss: 0.0105 - 93ms/epoch - 336us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  17.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3553 - val_loss: 0.4157\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.1376 - val_loss: 0.0526\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 424us/step - loss: 0.0418 - val_loss: 0.0327\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0318 - val_loss: 0.0273\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 423us/step - loss: 0.0272 - val_loss: 0.0229\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 431us/step - loss: 0.0272 - val_loss: 0.3417\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0219 - val_loss: 0.0328\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 421us/step - loss: 0.0180 - val_loss: 0.0157\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 421us/step - loss: 0.0159 - val_loss: 0.0150\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 438us/step - loss: 0.0159 - val_loss: 0.0133\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 422us/step - loss: 0.0133 - val_loss: 0.0118\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0124 - 88ms/epoch - 317us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0117 - 87ms/epoch - 313us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0117 - 88ms/epoch - 315us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0115 - 87ms/epoch - 312us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0119 - 87ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0121 - 87ms/epoch - 311us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0121 - 87ms/epoch - 313us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0114 - 87ms/epoch - 314us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0141 - 87ms/epoch - 315us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0111 - 88ms/epoch - 316us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0110 - 87ms/epoch - 312us/step\n",
      "70/70 - 0s - loss: 0.0104 - 65ms/epoch - 923us/step\n",
      "278/278 - 0s - loss: 0.0104 - 81ms/epoch - 292us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  17.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3507 - val_loss: 0.4093\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 430us/step - loss: 0.1345 - val_loss: 0.0495\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 426us/step - loss: 0.0388 - val_loss: 0.0318\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 423us/step - loss: 0.0311 - val_loss: 0.0300\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 437us/step - loss: 0.0287 - val_loss: 0.0213\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 424us/step - loss: 0.0239 - val_loss: 0.0198\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0188 - val_loss: 0.0164\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 430us/step - loss: 0.0170 - val_loss: 0.0150\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0149 - val_loss: 0.0141\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0166 - val_loss: 0.0128\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 429us/step - loss: 0.0133 - val_loss: 0.0121\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0234 - 100ms/epoch - 358us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0244 - 87ms/epoch - 313us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0153 - 86ms/epoch - 310us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0148 - 87ms/epoch - 314us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0207 - 87ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0186 - 87ms/epoch - 312us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0124 - 99ms/epoch - 357us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0194 - 86ms/epoch - 311us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0161 - 86ms/epoch - 311us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0146 - 86ms/epoch - 311us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0146 - 87ms/epoch - 315us/step\n",
      "70/70 - 0s - loss: 0.0113 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.0113 - 83ms/epoch - 297us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  16.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3960 - val_loss: 0.0494\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0342 - val_loss: 0.0250\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0205 - val_loss: 0.0212\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0150 - val_loss: 0.0130\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0125 - val_loss: 0.0107\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0102 - val_loss: 0.0094\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0090 - val_loss: 0.0083\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0080 - val_loss: 0.0075\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0070 - val_loss: 0.0064\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0063 - 117ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0067 - 114ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0060 - 115ms/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0069 - 115ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0063 - 125ms/epoch - 448us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0059 - 122ms/epoch - 440us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0059 - 119ms/epoch - 428us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0061 - 119ms/epoch - 427us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0063 - 122ms/epoch - 440us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0058 - 123ms/epoch - 442us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0059 - 119ms/epoch - 428us/step\n",
      "70/70 - 0s - loss: 0.0057 - 68ms/epoch - 976us/step\n",
      "278/278 - 0s - loss: 0.0057 - 84ms/epoch - 302us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3854 - val_loss: 0.0480\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0310 - val_loss: 0.0213\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0177 - val_loss: 0.0149\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0147 - val_loss: 0.0121\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0100 - val_loss: 0.0089\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 0.0066 - val_loss: 0.0062\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0090 - 115ms/epoch - 413us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0058 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0058 - 114ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0058 - 114ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0057 - 113ms/epoch - 405us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0057 - 114ms/epoch - 409us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0062 - 114ms/epoch - 409us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0060 - 113ms/epoch - 408us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0056 - 114ms/epoch - 409us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0062 - 113ms/epoch - 406us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0056 - 113ms/epoch - 406us/step\n",
      "70/70 - 0s - loss: 0.0055 - 67ms/epoch - 961us/step\n",
      "278/278 - 0s - loss: 0.0055 - 81ms/epoch - 291us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  18.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3846 - val_loss: 0.0478\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0327 - val_loss: 0.0284\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0189 - val_loss: 0.0150\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0139 - val_loss: 0.0121\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0113 - val_loss: 0.0101\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0096 - val_loss: 0.0086\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0078 - val_loss: 0.0072\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 536us/step - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0060 - val_loss: 0.0056\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0077 - 117ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0057 - 115ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0056 - 114ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0056 - 115ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0058 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0056 - 116ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0055 - 115ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0055 - 115ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0055 - 116ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0056 - 115ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0059 - 113ms/epoch - 407us/step\n",
      "70/70 - 0s - loss: 0.0054 - 72ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0054 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3621 - val_loss: 0.0486\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0342 - val_loss: 0.0245\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0209 - val_loss: 0.0177\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 536us/step - loss: 0.0156 - val_loss: 0.0132\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0126 - val_loss: 0.0114\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0084 - val_loss: 0.0076\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0068 - val_loss: 0.0063\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0096 - 114ms/epoch - 412us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0063 - 113ms/epoch - 407us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0062 - 116ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0064 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0064 - 114ms/epoch - 409us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0088 - 114ms/epoch - 410us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0061 - 115ms/epoch - 412us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0061 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0061 - 115ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0066 - 116ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0060 - 116ms/epoch - 416us/step\n",
      "70/70 - 0s - loss: 0.0060 - 65ms/epoch - 924us/step\n",
      "278/278 - 0s - loss: 0.0060 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3908 - val_loss: 0.0493\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0328 - val_loss: 0.0224\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0191 - val_loss: 0.0156\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0140 - val_loss: 0.0121\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0115 - val_loss: 0.0100\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0096 - val_loss: 0.0090\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0087 - val_loss: 0.0078\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0070 - val_loss: 0.0065\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0078 - 118ms/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0056 - 123ms/epoch - 443us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0060 - 122ms/epoch - 437us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0056 - 119ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0055 - 120ms/epoch - 432us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0055 - 126ms/epoch - 452us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0056 - 117ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0063 - 118ms/epoch - 423us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0075 - 128ms/epoch - 459us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0054 - 122ms/epoch - 437us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0054 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.0054 - 65ms/epoch - 927us/step\n",
      "278/278 - 0s - loss: 0.0054 - 83ms/epoch - 299us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1026 - val_loss: 0.0409\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0401 - val_loss: 0.0304\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0293 - val_loss: 0.0273\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0295 - val_loss: 0.0323\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0276 - val_loss: 0.0268\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0277 - val_loss: 0.0266\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0286 - val_loss: 0.0279\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0282 - val_loss: 0.0261\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0277 - val_loss: 0.0262\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0275 - val_loss: 0.0254\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0287 - val_loss: 0.0260\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0483 - 117ms/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0281 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0317 - 115ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0271 - 113ms/epoch - 406us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0266 - 127ms/epoch - 458us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0262 - 125ms/epoch - 449us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0324 - 114ms/epoch - 411us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0299 - 120ms/epoch - 430us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0264 - 115ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0402 - 115ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0270 - 114ms/epoch - 409us/step\n",
      "70/70 - 0s - loss: 0.0267 - 66ms/epoch - 938us/step\n",
      "278/278 - 0s - loss: 0.0267 - 83ms/epoch - 297us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0995 - val_loss: 0.0342\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0355 - val_loss: 0.0484\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0305 - val_loss: 0.0325\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0276 - val_loss: 0.0263\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0306 - val_loss: 0.0280\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0307 - val_loss: 0.0276\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0269 - val_loss: 0.0261\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0282 - val_loss: 0.0266\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0266 - val_loss: 0.0258\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0295 - val_loss: 0.0274\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 651us/step - loss: 0.0265 - val_loss: 0.0285\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0566 - 117ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0273 - 113ms/epoch - 407us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0282 - 114ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0274 - 114ms/epoch - 410us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0267 - 114ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0266 - 115ms/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0265 - 116ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0285 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0429 - 115ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0285 - 116ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0275 - 122ms/epoch - 439us/step\n",
      "70/70 - 0s - loss: 0.0270 - 70ms/epoch - 996us/step\n",
      "278/278 - 0s - loss: 0.0270 - 82ms/epoch - 296us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0978 - val_loss: 0.0353\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0337 - val_loss: 0.0345\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0290 - val_loss: 0.0268\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0281 - val_loss: 0.0262\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0280 - val_loss: 0.0265\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0286 - val_loss: 0.0303\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0277 - val_loss: 0.0271\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0282 - val_loss: 0.0262\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0278 - val_loss: 0.0259\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0275 - val_loss: 0.0279\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0287 - val_loss: 0.0268\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0333 - 123ms/epoch - 444us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0267 - 121ms/epoch - 436us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0324 - 125ms/epoch - 449us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0307 - 122ms/epoch - 438us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0265 - 119ms/epoch - 430us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0352 - 119ms/epoch - 429us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0266 - 118ms/epoch - 424us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0293 - 118ms/epoch - 425us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0277 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0430 - 117ms/epoch - 422us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0274 - 110ms/epoch - 395us/step\n",
      "70/70 - 0s - loss: 0.0264 - 68ms/epoch - 977us/step\n",
      "278/278 - 0s - loss: 0.0264 - 88ms/epoch - 316us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1056 - val_loss: 0.0613\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0358 - val_loss: 0.0303\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0298 - val_loss: 0.0277\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0291 - val_loss: 0.0272\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0288 - val_loss: 0.0272\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0283 - val_loss: 0.0270\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0280 - val_loss: 0.0276\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0284 - val_loss: 0.0263\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0288 - val_loss: 0.0272\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0269 - val_loss: 0.0280\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0294 - val_loss: 0.0267\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0288 - 124ms/epoch - 447us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0526 - 121ms/epoch - 435us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0275 - 125ms/epoch - 450us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0279 - 117ms/epoch - 420us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0290 - 118ms/epoch - 426us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0275 - 131ms/epoch - 470us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0301 - 127ms/epoch - 456us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0273 - 116ms/epoch - 417us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0352 - 114ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0285 - 119ms/epoch - 428us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0337 - 142ms/epoch - 512us/step\n",
      "70/70 - 0s - loss: 0.0287 - 66ms/epoch - 936us/step\n",
      "278/278 - 0s - loss: 0.0287 - 85ms/epoch - 305us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1011 - val_loss: 0.0319\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0308 - val_loss: 0.0288\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0324 - val_loss: 0.0287\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0279 - val_loss: 0.0273\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0291 - val_loss: 0.0306\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0289 - val_loss: 0.0281\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0271 - val_loss: 0.0262\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0300 - val_loss: 0.0275\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0291 - val_loss: 0.0263\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0285 - val_loss: 0.0322\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0277 - val_loss: 0.0263\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0323 - 124ms/epoch - 447us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0305 - 115ms/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0282 - 118ms/epoch - 423us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0283 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0423 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0309 - 117ms/epoch - 420us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0267 - 117ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0272 - 109ms/epoch - 393us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0274 - 128ms/epoch - 460us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0273 - 104ms/epoch - 374us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0276 - 103ms/epoch - 370us/step\n",
      "70/70 - 0s - loss: 0.0269 - 69ms/epoch - 979us/step\n",
      "278/278 - 0s - loss: 0.0269 - 84ms/epoch - 303us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3300 - val_loss: 0.2675\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 531us/step - loss: 0.2860 - val_loss: 0.2474\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 525us/step - loss: 0.2707 - val_loss: 0.3840\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.2758 - val_loss: 0.2520\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.2681 - val_loss: 0.2504\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.3030 - val_loss: 0.2759\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.2590 - val_loss: 0.2493\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.2685 - val_loss: 0.4090\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.2641 - val_loss: 0.2842\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2743 - val_loss: 0.2543\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.2898 - val_loss: 0.2599\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3654 - 118ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.2581 - 115ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3261 - 112ms/epoch - 404us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2542 - 117ms/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2930 - 114ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2542 - 115ms/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.3043 - 117ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2996 - 117ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3365 - 116ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2554 - 114ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.3258 - 113ms/epoch - 407us/step\n",
      "70/70 - 0s - loss: 0.2511 - 70ms/epoch - 994us/step\n",
      "278/278 - 0s - loss: 0.2510 - 86ms/epoch - 311us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3370 - val_loss: 0.2700\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.2954 - val_loss: 0.2630\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.2865 - val_loss: 0.2740\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.2765 - val_loss: 0.2524\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.2759 - val_loss: 0.2738\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.2765 - val_loss: 0.2599\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.2716 - val_loss: 0.2615\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.2705 - val_loss: 0.2531\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.2752 - val_loss: 0.2520\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.2780 - val_loss: 0.2560\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2632 - val_loss: 0.2596\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4385 - 119ms/epoch - 429us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.4000 - 115ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.2614 - 118ms/epoch - 424us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2551 - 117ms/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3828 - 133ms/epoch - 478us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2856 - 120ms/epoch - 433us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.2560 - 115ms/epoch - 412us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3851 - 116ms/epoch - 419us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2599 - 117ms/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.4005 - 119ms/epoch - 429us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2574 - 117ms/epoch - 421us/step\n",
      "70/70 - 0s - loss: 0.2491 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.2490 - 82ms/epoch - 294us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3473 - val_loss: 0.2886\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.2792 - val_loss: 0.2545\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.2848 - val_loss: 0.2629\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.2880 - val_loss: 0.3089\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.2769 - val_loss: 0.2625\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2739 - val_loss: 0.2643\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.2778 - val_loss: 0.2504\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.2621 - val_loss: 0.2485\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.2746 - val_loss: 0.2556\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.2697 - val_loss: 0.2559\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.2861 - val_loss: 0.2588\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4729 - 115ms/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.2995 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3738 - 113ms/epoch - 406us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2598 - 114ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2608 - 114ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2660 - 113ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.2864 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.4161 - 113ms/epoch - 405us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2603 - 113ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3387 - 115ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2565 - 126ms/epoch - 452us/step\n",
      "70/70 - 0s - loss: 0.2542 - 64ms/epoch - 914us/step\n",
      "278/278 - 0s - loss: 0.2541 - 85ms/epoch - 306us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3284 - val_loss: 0.2611\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.2845 - val_loss: 0.2483\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.2944 - val_loss: 0.2620\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.2854 - val_loss: 0.2783\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.2849 - val_loss: 0.2550\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2712 - val_loss: 0.2555\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.2731 - val_loss: 0.2508\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.2727 - val_loss: 0.2477\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.2684 - val_loss: 0.2540\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.2748 - val_loss: 0.7859\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.2805 - val_loss: 0.2492\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4792 - 114ms/epoch - 411us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3824 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.2546 - 114ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2534 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2529 - 126ms/epoch - 453us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.3175 - 114ms/epoch - 411us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4005 - 113ms/epoch - 405us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2540 - 114ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2625 - 114ms/epoch - 409us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2538 - 114ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2534 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.2455 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.2456 - 82ms/epoch - 293us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3266 - val_loss: 0.2568\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.2785 - val_loss: 0.2578\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.2913 - val_loss: 0.2531\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.2769 - val_loss: 0.2475\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.2669 - val_loss: 0.2601\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.2981 - val_loss: 0.2510\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.2715 - val_loss: 0.2700\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.2651 - val_loss: 0.2603\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.2743 - val_loss: 0.2537\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.2679 - val_loss: 0.6151\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.2706 - val_loss: 0.2791\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3185 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3209 - 114ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3510 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2949 - 131ms/epoch - 471us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2534 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2539 - 114ms/epoch - 410us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.3196 - 115ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3097 - 114ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.4370 - 114ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2578 - 115ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2545 - 117ms/epoch - 419us/step\n",
      "70/70 - 0s - loss: 0.2485 - 65ms/epoch - 930us/step\n",
      "278/278 - 0s - loss: 0.2486 - 84ms/epoch - 302us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3513 - val_loss: 0.2317\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 450us/step - loss: 0.1737 - val_loss: 0.1265\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0955 - val_loss: 0.0702\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0534 - val_loss: 0.0398\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0307 - val_loss: 0.0233\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0184 - val_loss: 0.0143\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 458us/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0078 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0034 - 90ms/epoch - 325us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0033 - 90ms/epoch - 323us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 89ms/epoch - 321us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 316us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 316us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 87ms/epoch - 313us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0030 - 88ms/epoch - 315us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 90ms/epoch - 323us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 90ms/epoch - 323us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 89ms/epoch - 320us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 317us/step\n",
      "70/70 - 0s - loss: 0.0027 - 67ms/epoch - 960us/step\n",
      "278/278 - 0s - loss: 0.0027 - 87ms/epoch - 312us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3425 - val_loss: 0.2290\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 465us/step - loss: 0.1716 - val_loss: 0.1249\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0941 - val_loss: 0.0691\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0526 - val_loss: 0.0390\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 443us/step - loss: 0.0301 - val_loss: 0.0228\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0179 - val_loss: 0.0140\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 442us/step - loss: 0.0112 - val_loss: 0.0090\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 446us/step - loss: 0.0075 - val_loss: 0.0063\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0032 - 89ms/epoch - 318us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0031 - 87ms/epoch - 314us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0030 - 88ms/epoch - 317us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0030 - 88ms/epoch - 316us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 313us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 313us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0028 - 87ms/epoch - 312us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 316us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 316us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0027 - 87ms/epoch - 312us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 316us/step\n",
      "70/70 - 0s - loss: 0.0025 - 67ms/epoch - 953us/step\n",
      "278/278 - 0s - loss: 0.0025 - 85ms/epoch - 307us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3490 - val_loss: 0.2337\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.1752 - val_loss: 0.1276\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 482us/step - loss: 0.0962 - val_loss: 0.0706\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0538 - val_loss: 0.0400\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0309 - val_loss: 0.0234\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0185 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0078 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 447us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0033 - 104ms/epoch - 375us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0033 - 94ms/epoch - 337us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 93ms/epoch - 336us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 317us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0030 - 89ms/epoch - 320us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 89ms/epoch - 318us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0029 - 90ms/epoch - 323us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 314us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 89ms/epoch - 321us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 90ms/epoch - 326us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 90ms/epoch - 325us/step\n",
      "70/70 - 0s - loss: 0.0026 - 66ms/epoch - 947us/step\n",
      "278/278 - 0s - loss: 0.0026 - 84ms/epoch - 301us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3657 - val_loss: 0.2352\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.1762 - val_loss: 0.1282\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0966 - val_loss: 0.0709\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0539 - val_loss: 0.0401\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0309 - val_loss: 0.0234\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0184 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0116 - val_loss: 0.0093\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0034 - 93ms/epoch - 336us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0032 - 91ms/epoch - 326us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 102ms/epoch - 365us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 318us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0030 - 92ms/epoch - 332us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 94ms/epoch - 339us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0029 - 94ms/epoch - 338us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 94ms/epoch - 338us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 95ms/epoch - 341us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 89ms/epoch - 319us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 101ms/epoch - 364us/step\n",
      "70/70 - 0s - loss: 0.0027 - 66ms/epoch - 941us/step\n",
      "278/278 - 0s - loss: 0.0027 - 86ms/epoch - 311us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  18.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3586 - val_loss: 0.2408\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.1804 - val_loss: 0.1313\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.0990 - val_loss: 0.0727\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0553 - val_loss: 0.0412\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 449us/step - loss: 0.0317 - val_loss: 0.0240\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0189 - val_loss: 0.0148\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0119 - val_loss: 0.0096\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0080 - val_loss: 0.0069\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0058 - val_loss: 0.0050\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0033 - 90ms/epoch - 324us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0033 - 88ms/epoch - 318us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 87ms/epoch - 313us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 87ms/epoch - 314us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0030 - 87ms/epoch - 313us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 90ms/epoch - 324us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0029 - 89ms/epoch - 321us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 313us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 87ms/epoch - 314us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 90ms/epoch - 323us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 87ms/epoch - 314us/step\n",
      "70/70 - 0s - loss: 0.0027 - 66ms/epoch - 940us/step\n",
      "278/278 - 0s - loss: 0.0027 - 85ms/epoch - 304us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1058 - val_loss: 0.0093\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0054 - val_loss: 0.0032\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0038 - val_loss: 0.0025\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0028 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0026 - val_loss: 0.0015\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0017 - val_loss: 0.0058\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0030 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 9.5735e-04 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0128 - 116ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0070 - 114ms/epoch - 410us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0018 - 116ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0012 - 113ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0011 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0010 - 113ms/epoch - 405us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0015 - 115ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0012 - 116ms/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0086 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.0012 - 64ms/epoch - 910us/step\n",
      "278/278 - 0s - loss: 0.0012 - 82ms/epoch - 294us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1016 - val_loss: 0.0084\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0060 - val_loss: 0.0095\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0039 - val_loss: 0.0024\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0029 - val_loss: 0.0018\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0084 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0015 - 114ms/epoch - 411us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0014 - 117ms/epoch - 422us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0014 - 116ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0052 - 115ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0057 - 115ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0019 - 115ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0021 - 121ms/epoch - 436us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0013 - 117ms/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0013 - 117ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0057 - 112ms/epoch - 404us/step\n",
      "70/70 - 0s - loss: 0.0023 - 65ms/epoch - 933us/step\n",
      "278/278 - 0s - loss: 0.0022 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1327 - val_loss: 0.0125\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0065 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0046 - val_loss: 0.0030\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0032 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0028 - val_loss: 0.0057\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0022 - val_loss: 0.0237\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0015 - val_loss: 0.0027\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0076 - 118ms/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0012 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0021 - 113ms/epoch - 407us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0018 - 117ms/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0042 - 133ms/epoch - 477us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0011 - 116ms/epoch - 417us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0048 - 116ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0016 - 114ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0045 - 114ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0015 - 116ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0015 - 115ms/epoch - 415us/step\n",
      "70/70 - 0s - loss: 0.0014 - 65ms/epoch - 926us/step\n",
      "278/278 - 0s - loss: 0.0014 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0827 - val_loss: 0.0064\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0050 - val_loss: 0.0030\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0040 - val_loss: 0.0023\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0025 - val_loss: 0.0031\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0013 - val_loss: 9.8234e-04\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0032 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0085 - 114ms/epoch - 410us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0014 - 113ms/epoch - 406us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0020 - 113ms/epoch - 408us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0011 - 112ms/epoch - 404us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0011 - 114ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0016 - 114ms/epoch - 410us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0022 - 113ms/epoch - 407us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0045 - 113ms/epoch - 406us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0064 - 114ms/epoch - 408us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0013 - 114ms/epoch - 409us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0011 - 113ms/epoch - 408us/step\n",
      "70/70 - 0s - loss: 0.0011 - 67ms/epoch - 960us/step\n",
      "278/278 - 0s - loss: 0.0011 - 82ms/epoch - 293us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1294 - val_loss: 0.0102\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0064 - val_loss: 0.0056\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0040 - val_loss: 0.0025\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0039 - val_loss: 0.0023\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0028 - val_loss: 0.0020\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0027 - val_loss: 0.0015\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0102 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0026 - 117ms/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0018 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0013 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0040 - 122ms/epoch - 440us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0057 - 115ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0015 - 116ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0016 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0017 - 115ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0023 - 114ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0012 - 117ms/epoch - 422us/step\n",
      "70/70 - 0s - loss: 0.0011 - 65ms/epoch - 932us/step\n",
      "278/278 - 0s - loss: 0.0011 - 82ms/epoch - 296us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0322 - val_loss: 0.0051\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0106 - val_loss: 0.0178\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0089 - val_loss: 0.0029\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0065 - val_loss: 0.0134\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0037 - val_loss: 0.0060\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0066 - val_loss: 0.0028\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0027 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0034 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0037 - val_loss: 0.0466\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0599 - 125ms/epoch - 449us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0036 - 114ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0026 - 118ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0025 - 115ms/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0021 - 114ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0038 - 116ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0040 - 116ms/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0020 - 120ms/epoch - 431us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0015 - 129ms/epoch - 462us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0019 - 117ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0038 - 117ms/epoch - 420us/step\n",
      "70/70 - 0s - loss: 0.0013 - 65ms/epoch - 926us/step\n",
      "278/278 - 0s - loss: 0.0013 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0361 - val_loss: 0.0084\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0095 - val_loss: 0.0098\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0069 - val_loss: 0.0023\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0064 - val_loss: 0.0026\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0043 - val_loss: 0.0028\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0035 - val_loss: 0.0174\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0048 - val_loss: 0.0019\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0038 - val_loss: 0.0018\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0052 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0060 - 115ms/epoch - 415us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0016 - 114ms/epoch - 411us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0053 - 117ms/epoch - 422us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0014 - 119ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0018 - 119ms/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0061 - 115ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0017 - 116ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0080 - 135ms/epoch - 485us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0033 - 123ms/epoch - 443us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0019 - 116ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0087 - 116ms/epoch - 418us/step\n",
      "70/70 - 0s - loss: 0.0034 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.0034 - 94ms/epoch - 337us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0338 - val_loss: 0.0053\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0155 - val_loss: 0.0129\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0087 - val_loss: 0.0040\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0062 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0057 - val_loss: 0.0067\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0048 - val_loss: 0.0041\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0034 - val_loss: 0.0023\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0039 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0035 - val_loss: 0.0015\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0237 - 127ms/epoch - 455us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0018 - 124ms/epoch - 445us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0017 - 125ms/epoch - 451us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0016 - 120ms/epoch - 432us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0084 - 119ms/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0018 - 120ms/epoch - 430us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0030 - 120ms/epoch - 432us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0099 - 120ms/epoch - 432us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0017 - 121ms/epoch - 434us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0082 - 120ms/epoch - 430us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0022 - 122ms/epoch - 438us/step\n",
      "70/70 - 0s - loss: 0.0016 - 73ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0015 - 90ms/epoch - 322us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0385 - val_loss: 0.0204\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0093 - val_loss: 0.0036\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0064 - val_loss: 0.0029\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0068 - val_loss: 0.0022\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0042 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0041 - val_loss: 0.0067\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0069 - val_loss: 0.0027\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0051 - val_loss: 0.0018\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0194 - 118ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0019 - 125ms/epoch - 449us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0018 - 114ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0016 - 115ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0014 - 116ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0014 - 116ms/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0102 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0014 - 115ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0043 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0014 - 115ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0016 - 116ms/epoch - 419us/step\n",
      "70/70 - 0s - loss: 0.0071 - 65ms/epoch - 931us/step\n",
      "278/278 - 0s - loss: 0.0073 - 83ms/epoch - 298us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0353 - val_loss: 0.1669\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0122 - val_loss: 0.0094\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0068 - val_loss: 0.0024\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0051 - val_loss: 0.0022\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0065 - val_loss: 0.0037\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0034 - val_loss: 0.0021\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0053 - val_loss: 0.0368\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0024 - val_loss: 0.0036\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0337 - 117ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0086 - 138ms/epoch - 496us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0023 - 115ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0018 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0016 - 115ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0015 - 116ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0015 - 121ms/epoch - 435us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0036 - 118ms/epoch - 424us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0013 - 117ms/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0019 - 117ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0086 - 115ms/epoch - 412us/step\n",
      "70/70 - 0s - loss: 0.0019 - 65ms/epoch - 933us/step\n",
      "278/278 - 0s - loss: 0.0019 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0817 - val_loss: 0.0719\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0353 - val_loss: 0.0108\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0420 - val_loss: 0.0113\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0245 - val_loss: 0.0061\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0225 - val_loss: 0.0195\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0251 - val_loss: 0.0983\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0231 - val_loss: 0.0048\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0328 - val_loss: 0.0070\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0175 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0196 - val_loss: 0.0082\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0482 - val_loss: 0.0049\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0521 - 120ms/epoch - 431us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0598 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0818 - 115ms/epoch - 413us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0135 - 113ms/epoch - 408us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2159 - 113ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0829 - 113ms/epoch - 408us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0225 - 115ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0093 - 114ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0182 - 115ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2546 - 115ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0247 - 116ms/epoch - 415us/step\n",
      "70/70 - 0s - loss: 0.0053 - 64ms/epoch - 912us/step\n",
      "278/278 - 0s - loss: 0.0053 - 84ms/epoch - 302us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0878 - val_loss: 0.0192\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0237 - val_loss: 0.0060\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0388 - val_loss: 0.0034\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0459 - val_loss: 0.0518\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0217 - val_loss: 0.0048\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0228 - val_loss: 0.0159\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0335 - val_loss: 0.0099\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0216 - val_loss: 0.0160\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0268 - val_loss: 0.1032\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0221 - val_loss: 0.0091\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0214 - val_loss: 0.0045\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0828 - 117ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0220 - 116ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3516 - 114ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0303 - 113ms/epoch - 407us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0235 - 113ms/epoch - 407us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0064 - 113ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0081 - 113ms/epoch - 408us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.1599 - 113ms/epoch - 407us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0151 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0095 - 113ms/epoch - 407us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0903 - 112ms/epoch - 403us/step\n",
      "70/70 - 0s - loss: 0.0060 - 65ms/epoch - 925us/step\n",
      "278/278 - 0s - loss: 0.0059 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0877 - val_loss: 0.0093\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0334 - val_loss: 0.0140\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0528 - val_loss: 0.0086\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0287 - val_loss: 0.0060\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0217 - val_loss: 0.0053\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0349 - val_loss: 0.0100\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0271 - val_loss: 0.0046\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0251 - val_loss: 0.0037\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0172 - val_loss: 0.0033\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0357 - val_loss: 0.0203\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0447 - val_loss: 0.0059\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0229 - 125ms/epoch - 451us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0526 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0649 - 114ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0931 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0098 - 114ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0061 - 120ms/epoch - 433us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0743 - 117ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0225 - 119ms/epoch - 429us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.1106 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.1751 - 117ms/epoch - 419us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0106 - 115ms/epoch - 415us/step\n",
      "70/70 - 0s - loss: 0.0053 - 65ms/epoch - 933us/step\n",
      "278/278 - 0s - loss: 0.0052 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0882 - val_loss: 0.0172\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0399 - val_loss: 0.0151\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 635us/step - loss: 0.0260 - val_loss: 0.0048\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0364 - val_loss: 0.0051\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0262 - val_loss: 0.3343\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0240 - val_loss: 0.0113\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0266 - val_loss: 0.0071\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0250 - val_loss: 0.0041\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0323 - val_loss: 0.0075\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0234 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0325 - val_loss: 0.0572\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0198 - 123ms/epoch - 443us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.2494 - 115ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0991 - 115ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0154 - 115ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0154 - 116ms/epoch - 416us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.1370 - 115ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0135 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0518 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0127 - 126ms/epoch - 452us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0781 - 119ms/epoch - 427us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0349 - 137ms/epoch - 492us/step\n",
      "70/70 - 0s - loss: 0.0134 - 71ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0127 - 92ms/epoch - 330us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0691 - val_loss: 0.0070\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0339 - val_loss: 0.0592\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0340 - val_loss: 0.1339\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0350 - val_loss: 0.0085\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0499 - val_loss: 0.0041\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0263 - val_loss: 0.0104\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 718us/step - loss: 0.0294 - val_loss: 0.0107\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0311 - val_loss: 0.0084\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0200 - val_loss: 0.0118\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0191 - val_loss: 0.0047\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0217 - val_loss: 0.0036\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.1301 - 118ms/epoch - 426us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0081 - 133ms/epoch - 479us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0370 - 119ms/epoch - 429us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.1167 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0240 - 114ms/epoch - 409us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.1504 - 114ms/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0156 - 111ms/epoch - 398us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0087 - 117ms/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0406 - 116ms/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0828 - 117ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0310 - 121ms/epoch - 436us/step\n",
      "70/70 - 0s - loss: 0.0158 - 64ms/epoch - 918us/step\n",
      "278/278 - 0s - loss: 0.0153 - 84ms/epoch - 301us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.2734 - val_loss: 0.1981\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0732 - val_loss: 0.0428\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.0380 - val_loss: 0.0382\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 464us/step - loss: 0.0342 - val_loss: 0.0256\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0299 - val_loss: 0.0222\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0250 - val_loss: 0.0268\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 443us/step - loss: 0.0225 - val_loss: 0.0200\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0195 - val_loss: 0.0161\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 461us/step - loss: 0.0166 - val_loss: 0.0148\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0153 - val_loss: 0.0137\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0143 - val_loss: 0.0136\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0237 - 90ms/epoch - 322us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0154 - 84ms/epoch - 304us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0135 - 85ms/epoch - 305us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0197 - 84ms/epoch - 302us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0129 - 83ms/epoch - 299us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0122 - 83ms/epoch - 300us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0130 - 82ms/epoch - 294us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0124 - 84ms/epoch - 302us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0119 - 84ms/epoch - 301us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0124 - 92ms/epoch - 331us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0118 - 98ms/epoch - 352us/step\n",
      "70/70 - 0s - loss: 0.0114 - 69ms/epoch - 984us/step\n",
      "278/278 - 0s - loss: 0.0114 - 86ms/epoch - 310us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  17.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3252 - val_loss: 0.2172\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0797 - val_loss: 0.0475\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0443 - val_loss: 0.0407\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0380 - val_loss: 0.0327\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0325 - val_loss: 0.0257\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0257 - val_loss: 0.0217\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 461us/step - loss: 0.0220 - val_loss: 0.0191\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0203 - val_loss: 0.0173\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 484us/step - loss: 0.0226 - val_loss: 0.0162\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0184 - val_loss: 0.0160\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0170 - val_loss: 0.0142\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0205 - 84ms/epoch - 303us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0188 - 83ms/epoch - 299us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0218 - 83ms/epoch - 298us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0147 - 83ms/epoch - 298us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0172 - 83ms/epoch - 300us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0223 - 96ms/epoch - 346us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0152 - 82ms/epoch - 296us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0152 - 84ms/epoch - 302us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0146 - 84ms/epoch - 302us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0255 - 83ms/epoch - 299us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0138 - 84ms/epoch - 304us/step\n",
      "70/70 - 0s - loss: 0.0126 - 70ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0126 - 86ms/epoch - 310us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  17.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.2958 - val_loss: 0.2223\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0857 - val_loss: 0.0480\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 482us/step - loss: 0.0444 - val_loss: 0.0355\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0390 - val_loss: 0.0581\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0325 - val_loss: 0.0306\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0248 - val_loss: 0.0212\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0224 - val_loss: 0.0177\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0203 - val_loss: 0.0159\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0180 - val_loss: 0.0146\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0161 - val_loss: 0.0155\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0143 - val_loss: 0.0123\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0143 - 109ms/epoch - 392us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0156 - 95ms/epoch - 342us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0171 - 84ms/epoch - 303us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0132 - 82ms/epoch - 295us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0137 - 84ms/epoch - 301us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0145 - 89ms/epoch - 321us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0128 - 88ms/epoch - 316us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0139 - 86ms/epoch - 309us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0126 - 85ms/epoch - 304us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0129 - 82ms/epoch - 296us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0178 - 81ms/epoch - 293us/step\n",
      "70/70 - 0s - loss: 0.0111 - 67ms/epoch - 964us/step\n",
      "278/278 - 0s - loss: 0.0111 - 82ms/epoch - 296us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  17.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2390 - val_loss: 0.2058\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0847 - val_loss: 0.0552\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0486 - val_loss: 0.0368\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 486us/step - loss: 0.0365 - val_loss: 0.0277\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 482us/step - loss: 0.0295 - val_loss: 0.0229\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 488us/step - loss: 0.0265 - val_loss: 0.0210\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0207 - val_loss: 0.0187\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0192 - val_loss: 0.0156\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0174 - val_loss: 0.0149\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0166 - val_loss: 0.0135\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0137 - val_loss: 0.0140\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0189 - 94ms/epoch - 338us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0324 - 92ms/epoch - 332us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0128 - 86ms/epoch - 309us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0205 - 86ms/epoch - 308us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0169 - 87ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0150 - 86ms/epoch - 310us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0260 - 86ms/epoch - 309us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0233 - 86ms/epoch - 308us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0175 - 87ms/epoch - 314us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0160 - 86ms/epoch - 309us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0138 - 86ms/epoch - 308us/step\n",
      "70/70 - 0s - loss: 0.0115 - 73ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0115 - 84ms/epoch - 301us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  18.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2859 - val_loss: 0.2070\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0795 - val_loss: 0.0464\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0418 - val_loss: 0.2256\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0332 - val_loss: 0.0442\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0276 - val_loss: 0.0220\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0240 - val_loss: 0.0195\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0208 - val_loss: 0.0173\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0183 - val_loss: 0.0160\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0185 - val_loss: 0.0143\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 465us/step - loss: 0.0158 - val_loss: 0.0140\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0139 - val_loss: 0.0122\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0349 - 93ms/epoch - 333us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0385 - 85ms/epoch - 305us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0296 - 84ms/epoch - 301us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0230 - 84ms/epoch - 301us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0309 - 82ms/epoch - 293us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0208 - 91ms/epoch - 326us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0144 - 84ms/epoch - 302us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0137 - 85ms/epoch - 305us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0131 - 84ms/epoch - 300us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0285 - 81ms/epoch - 291us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0184 - 81ms/epoch - 290us/step\n",
      "70/70 - 0s - loss: 0.0118 - 69ms/epoch - 991us/step\n",
      "278/278 - 0s - loss: 0.0118 - 92ms/epoch - 330us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  18.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3932 - val_loss: 0.0532\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.0365 - val_loss: 0.0276\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0224 - val_loss: 0.0267\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0173 - val_loss: 0.0156\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0140 - val_loss: 0.0120\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0099 - val_loss: 0.0090\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0093 - val_loss: 0.0081\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0080 - val_loss: 0.0076\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0085 - val_loss: 0.0073\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0077 - val_loss: 0.0068\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0073 - 142ms/epoch - 509us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0073 - 116ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0109 - 119ms/epoch - 427us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0076 - 120ms/epoch - 433us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0066 - 115ms/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0066 - 114ms/epoch - 409us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0066 - 115ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0067 - 115ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0076 - 122ms/epoch - 440us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0067 - 117ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0074 - 116ms/epoch - 418us/step\n",
      "70/70 - 0s - loss: 0.0070 - 67ms/epoch - 958us/step\n",
      "278/278 - 0s - loss: 0.0070 - 83ms/epoch - 298us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4212 - val_loss: 0.0576\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0376 - val_loss: 0.0280\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0225 - val_loss: 0.0180\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0165 - val_loss: 0.0142\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0133 - val_loss: 0.0147\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0114 - val_loss: 0.0099\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0095 - val_loss: 0.0091\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0089 - val_loss: 0.0080\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0082 - val_loss: 0.0078\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0070 - val_loss: 0.0077\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0137 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0066 - 118ms/epoch - 426us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0065 - 115ms/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0065 - 116ms/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0064 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0082 - 122ms/epoch - 439us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0070 - 116ms/epoch - 419us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0067 - 118ms/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0065 - 121ms/epoch - 436us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0066 - 123ms/epoch - 441us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0098 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.0062 - 67ms/epoch - 963us/step\n",
      "278/278 - 0s - loss: 0.0062 - 80ms/epoch - 287us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3702 - val_loss: 0.0449\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.0314 - val_loss: 0.0240\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.0195 - val_loss: 0.0166\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0146 - val_loss: 0.0128\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0123 - val_loss: 0.0106\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0103 - val_loss: 0.0093\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 657us/step - loss: 0.0088 - val_loss: 0.0076\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0172 - 119ms/epoch - 430us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0065 - 118ms/epoch - 424us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0062 - 117ms/epoch - 421us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0062 - 117ms/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0061 - 119ms/epoch - 427us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0062 - 118ms/epoch - 425us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0065 - 116ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0063 - 118ms/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0061 - 116ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0064 - 116ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0061 - 131ms/epoch - 470us/step\n",
      "70/70 - 0s - loss: 0.0059 - 71ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0059 - 86ms/epoch - 309us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3909 - val_loss: 0.0514\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0353 - val_loss: 0.0250\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0216 - val_loss: 0.0170\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0167 - val_loss: 0.0136\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0134 - val_loss: 0.0116\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0115 - val_loss: 0.0102\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0100 - val_loss: 0.0091\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0090 - val_loss: 0.0084\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0083 - val_loss: 0.0076\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0078 - val_loss: 0.0072\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0114 - 121ms/epoch - 435us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0067 - 120ms/epoch - 432us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0067 - 118ms/epoch - 425us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0068 - 119ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0079 - 134ms/epoch - 481us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0066 - 119ms/epoch - 427us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0101 - 117ms/epoch - 422us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0065 - 117ms/epoch - 422us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0066 - 117ms/epoch - 422us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0069 - 119ms/epoch - 428us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0064 - 118ms/epoch - 426us/step\n",
      "70/70 - 0s - loss: 0.0064 - 70ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0064 - 84ms/epoch - 301us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3897 - val_loss: 0.0476\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0340 - val_loss: 0.0279\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0165 - val_loss: 0.0138\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0141 - val_loss: 0.0117\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0106 - val_loss: 0.0095\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0094 - val_loss: 0.0086\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0081 - val_loss: 0.0074\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0079 - val_loss: 0.0070\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0100 - 119ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0069 - 117ms/epoch - 422us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0070 - 120ms/epoch - 433us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0069 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0071 - 131ms/epoch - 470us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0078 - 116ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0087 - 118ms/epoch - 425us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0068 - 117ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0072 - 125ms/epoch - 450us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0073 - 117ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0075 - 118ms/epoch - 426us/step\n",
      "70/70 - 0s - loss: 0.0065 - 70ms/epoch - 999us/step\n",
      "278/278 - 0s - loss: 0.0065 - 85ms/epoch - 307us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1149 - val_loss: 0.0355\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0374 - val_loss: 0.0884\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0321 - val_loss: 0.0288\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0319 - val_loss: 0.0290\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0292 - val_loss: 0.0269\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0302 - val_loss: 0.0279\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0290 - val_loss: 0.0266\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0288 - val_loss: 0.0269\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0284 - val_loss: 0.0267\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0282 - val_loss: 0.0281\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0295 - val_loss: 0.0280\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0366 - 120ms/epoch - 430us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0283 - 117ms/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0270 - 118ms/epoch - 425us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0348 - 118ms/epoch - 425us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0282 - 116ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0301 - 118ms/epoch - 423us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0271 - 131ms/epoch - 471us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0449 - 118ms/epoch - 424us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0288 - 116ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0315 - 115ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0287 - 117ms/epoch - 421us/step\n",
      "70/70 - 0s - loss: 0.0287 - 70ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0287 - 86ms/epoch - 309us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1176 - val_loss: 0.0386\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0349 - val_loss: 0.0302\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0315 - val_loss: 0.0284\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0308 - val_loss: 0.0279\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0288 - val_loss: 0.0272\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0281 - val_loss: 0.0265\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0289 - val_loss: 0.0262\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0295 - val_loss: 0.0265\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0277 - val_loss: 0.0267\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0280 - val_loss: 0.0282\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0281 - val_loss: 0.0267\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0473 - 119ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0286 - 118ms/epoch - 426us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0267 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0278 - 118ms/epoch - 424us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0405 - 118ms/epoch - 425us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0270 - 117ms/epoch - 422us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0270 - 116ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0282 - 117ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0289 - 118ms/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0307 - 120ms/epoch - 433us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0851 - 120ms/epoch - 431us/step\n",
      "70/70 - 0s - loss: 0.0284 - 90ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0284 - 86ms/epoch - 309us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0966 - val_loss: 0.0323\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0338 - val_loss: 0.0286\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0311 - val_loss: 0.0276\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0289 - val_loss: 0.0269\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0288 - val_loss: 0.0261\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0295 - val_loss: 0.0263\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0283 - val_loss: 0.0283\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0284 - val_loss: 0.0257\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0275 - val_loss: 0.0278\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0286 - val_loss: 0.0261\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0269 - val_loss: 0.0370\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0373 - 120ms/epoch - 433us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0298 - 119ms/epoch - 426us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0272 - 117ms/epoch - 420us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0593 - 116ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0278 - 118ms/epoch - 423us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0268 - 117ms/epoch - 422us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0265 - 118ms/epoch - 424us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0274 - 117ms/epoch - 423us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0292 - 119ms/epoch - 429us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0280 - 118ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0269 - 118ms/epoch - 424us/step\n",
      "70/70 - 0s - loss: 0.0279 - 69ms/epoch - 987us/step\n",
      "278/278 - 0s - loss: 0.0279 - 86ms/epoch - 308us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1051 - val_loss: 0.0344\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0361 - val_loss: 0.0313\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0323 - val_loss: 0.0284\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0298 - val_loss: 0.0277\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0290 - val_loss: 0.0271\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 688us/step - loss: 0.0287 - val_loss: 0.0744\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0292 - val_loss: 0.0274\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0280 - val_loss: 0.0271\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0288 - val_loss: 0.0264\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0274 - val_loss: 0.0269\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0307 - val_loss: 0.0261\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0297 - 120ms/epoch - 431us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0287 - 119ms/epoch - 427us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0296 - 118ms/epoch - 423us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0367 - 117ms/epoch - 423us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0272 - 118ms/epoch - 426us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0282 - 118ms/epoch - 425us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0461 - 117ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0368 - 119ms/epoch - 427us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0297 - 118ms/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0274 - 119ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0271 - 120ms/epoch - 431us/step\n",
      "70/70 - 0s - loss: 0.0268 - 79ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0268 - 107ms/epoch - 386us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1078 - val_loss: 0.1346\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0369 - val_loss: 0.0307\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0344 - val_loss: 0.0296\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0305 - val_loss: 0.0307\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0293 - val_loss: 0.0279\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0310 - val_loss: 0.0272\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0278 - val_loss: 0.0495\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0319 - val_loss: 0.0282\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0289 - val_loss: 0.0266\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 638us/step - loss: 0.0295 - val_loss: 0.0266\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0279 - val_loss: 0.0269\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0339 - 124ms/epoch - 445us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0363 - 123ms/epoch - 441us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0283 - 121ms/epoch - 434us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0275 - 118ms/epoch - 426us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0315 - 118ms/epoch - 424us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0282 - 117ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0330 - 117ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0380 - 116ms/epoch - 419us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0290 - 117ms/epoch - 422us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0290 - 122ms/epoch - 438us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0345 - 129ms/epoch - 466us/step\n",
      "70/70 - 0s - loss: 0.0308 - 76ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0307 - 93ms/epoch - 334us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3782 - val_loss: 0.3166\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.3327 - val_loss: 0.4504\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.3195 - val_loss: 0.3393\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 640us/step - loss: 0.3142 - val_loss: 0.2991\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 655us/step - loss: 0.3164 - val_loss: 0.2992\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 645us/step - loss: 0.3190 - val_loss: 0.3262\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.3080 - val_loss: 0.4860\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.3168 - val_loss: 0.2888\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.3281 - val_loss: 0.3004\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.3165 - val_loss: 0.2895\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 626us/step - loss: 0.3248 - val_loss: 0.3144\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3540 - 118ms/epoch - 423us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3588 - 112ms/epoch - 403us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3080 - 116ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.3944 - 124ms/epoch - 444us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3076 - 130ms/epoch - 469us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.4744 - 116ms/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4531 - 120ms/epoch - 432us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2998 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3002 - 119ms/epoch - 429us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3556 - 128ms/epoch - 461us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2999 - 118ms/epoch - 423us/step\n",
      "70/70 - 0s - loss: 0.2939 - 68ms/epoch - 967us/step\n",
      "278/278 - 0s - loss: 0.2939 - 92ms/epoch - 330us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3947 - val_loss: 0.3179\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 638us/step - loss: 0.3194 - val_loss: 0.2996\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.3185 - val_loss: 0.2978\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.3373 - val_loss: 0.3016\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 642us/step - loss: 0.3158 - val_loss: 0.2999\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.3178 - val_loss: 0.2952\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.3218 - val_loss: 0.2971\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.3168 - val_loss: 0.2865\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.3151 - val_loss: 0.3037\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3192 - val_loss: 0.2902\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.3218 - val_loss: 0.2987\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4631 - 137ms/epoch - 492us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3052 - 125ms/epoch - 451us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.2990 - 119ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.4327 - 157ms/epoch - 564us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3723 - 123ms/epoch - 441us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2992 - 120ms/epoch - 432us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4223 - 191ms/epoch - 688us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3772 - 131ms/epoch - 471us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2995 - 126ms/epoch - 453us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2970 - 127ms/epoch - 458us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.3899 - 127ms/epoch - 458us/step\n",
      "70/70 - 0s - loss: 0.2943 - 79ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2942 - 95ms/epoch - 341us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  21.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3624 - val_loss: 0.3177\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.3419 - val_loss: 0.3196\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.3412 - val_loss: 0.3667\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 667us/step - loss: 0.3314 - val_loss: 0.3096\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.3364 - val_loss: 0.3324\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.3355 - val_loss: 0.3025\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.3217 - val_loss: 0.3061\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.3149 - val_loss: 0.3035\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 631us/step - loss: 0.3260 - val_loss: 0.3380\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3121 - val_loss: 0.2881\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3141 - val_loss: 0.2969\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3835 - 122ms/epoch - 438us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.4016 - 119ms/epoch - 429us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3005 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.4045 - 115ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.4195 - 116ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.3345 - 116ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.3063 - 125ms/epoch - 449us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2954 - 127ms/epoch - 458us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3579 - 125ms/epoch - 449us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3304 - 120ms/epoch - 433us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.3007 - 121ms/epoch - 434us/step\n",
      "70/70 - 0s - loss: 0.2859 - 82ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2858 - 86ms/epoch - 310us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  21.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3558 - val_loss: 0.3438\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.3256 - val_loss: 0.5836\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.3140 - val_loss: 0.2953\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.3133 - val_loss: 0.2914\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 637us/step - loss: 0.3147 - val_loss: 1.5693\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3286 - val_loss: 0.3493\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.3031 - val_loss: 0.2912\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.3061 - val_loss: 0.2818\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 625us/step - loss: 0.2972 - val_loss: 0.4654\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.3142 - val_loss: 0.2764\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 632us/step - loss: 0.3095 - val_loss: 0.2926\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3065 - 130ms/epoch - 468us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3538 - 136ms/epoch - 489us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3296 - 126ms/epoch - 452us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.3106 - 124ms/epoch - 448us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.4847 - 126ms/epoch - 452us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2962 - 130ms/epoch - 466us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.2911 - 122ms/epoch - 437us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2918 - 123ms/epoch - 442us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3779 - 141ms/epoch - 507us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3753 - 136ms/epoch - 489us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2926 - 133ms/epoch - 478us/step\n",
      "70/70 - 0s - loss: 0.2845 - 74ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2847 - 89ms/epoch - 320us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3848 - val_loss: 0.3250\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.3340 - val_loss: 0.2941\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.3317 - val_loss: 0.3055\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.3139 - val_loss: 0.3066\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.3352 - val_loss: 0.3064\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3177 - val_loss: 0.2936\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.3240 - val_loss: 0.3031\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.3333 - val_loss: 0.2881\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.3098 - val_loss: 0.3041\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.3106 - val_loss: 0.2827\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.3217 - val_loss: 0.3031\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3835 - 134ms/epoch - 484us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3741 - 119ms/epoch - 430us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3056 - 118ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.4580 - 124ms/epoch - 447us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3001 - 134ms/epoch - 483us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2970 - 127ms/epoch - 455us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4821 - 126ms/epoch - 454us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3075 - 124ms/epoch - 446us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3250 - 119ms/epoch - 428us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3180 - 126ms/epoch - 453us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.4951 - 130ms/epoch - 466us/step\n",
      "70/70 - 0s - loss: 0.2916 - 74ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2917 - 90ms/epoch - 325us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.5s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 4\n",
      "n_resources: 33333\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3532 - val_loss: 0.0434\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0252 - val_loss: 0.0167\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0149 - val_loss: 0.0125\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0101 - val_loss: 0.0089\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0088 - val_loss: 0.0078\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0064 - val_loss: 0.0060\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0059 - val_loss: 0.0055\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0053 - 379ms/epoch - 455us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0053 - 376ms/epoch - 450us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0050 - 373ms/epoch - 448us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0053 - 356ms/epoch - 427us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0050 - 384ms/epoch - 461us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0048 - 392ms/epoch - 470us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0050 - 495ms/epoch - 593us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0048 - 398ms/epoch - 477us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0047 - 385ms/epoch - 462us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0053 - 387ms/epoch - 464us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0046 - 395ms/epoch - 473us/step\n",
      "209/209 - 0s - loss: 0.0045 - 107ms/epoch - 510us/step\n",
      "834/834 - 0s - loss: 0.0045 - 247ms/epoch - 297us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3719 - val_loss: 0.0465\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0306 - val_loss: 0.0232\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0185 - val_loss: 0.0155\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0133 - val_loss: 0.0114\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0106 - val_loss: 0.0090\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0085 - val_loss: 0.0080\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0081 - val_loss: 0.0074\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 684us/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 648us/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 647us/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0055 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0056 - 341ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0053 - 341ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0051 - 339ms/epoch - 406us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0054 - 342ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0049 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0052 - 361ms/epoch - 433us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0051 - 343ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0047 - 340ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0048 - 355ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0049 - 343ms/epoch - 411us/step\n",
      "209/209 - 0s - loss: 0.0047 - 105ms/epoch - 504us/step\n",
      "834/834 - 0s - loss: 0.0047 - 223ms/epoch - 267us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3726 - val_loss: 0.0481\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0336 - val_loss: 0.0253\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0191 - val_loss: 0.0159\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0147 - val_loss: 0.0128\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0121 - val_loss: 0.0108\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0104 - val_loss: 0.0095\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0091 - val_loss: 0.0084\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0083 - val_loss: 0.0077\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0068 - val_loss: 0.0063\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0064 - val_loss: 0.0059\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0066 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0061 - 359ms/epoch - 431us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0068 - 358ms/epoch - 429us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0057 - 362ms/epoch - 434us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0065 - 366ms/epoch - 439us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0056 - 353ms/epoch - 423us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0064 - 356ms/epoch - 427us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0060 - 355ms/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0054 - 356ms/epoch - 427us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0055 - 350ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0069 - 354ms/epoch - 425us/step\n",
      "209/209 - 0s - loss: 0.0053 - 103ms/epoch - 491us/step\n",
      "834/834 - 0s - loss: 0.0053 - 225ms/epoch - 270us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3506 - val_loss: 0.0491\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0345 - val_loss: 0.0253\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0210 - val_loss: 0.0172\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0156 - val_loss: 0.0134\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0124 - val_loss: 0.0113\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0104 - val_loss: 0.0094\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0071 - val_loss: 0.0066\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0067 - val_loss: 0.0062\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0067 - 350ms/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0062 - 349ms/epoch - 419us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0065 - 345ms/epoch - 413us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0060 - 345ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0070 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0057 - 347ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0061 - 351ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0056 - 346ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0060 - 345ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0057 - 346ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0054 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0056 - 107ms/epoch - 512us/step\n",
      "834/834 - 0s - loss: 0.0056 - 225ms/epoch - 269us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3701 - val_loss: 0.0489\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 662us/step - loss: 0.0344 - val_loss: 0.0242\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0207 - val_loss: 0.0168\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0155 - val_loss: 0.0134\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0093 - val_loss: 0.0087\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0081 - val_loss: 0.0071\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0068 - val_loss: 0.0062\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0068 - 349ms/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0061 - 345ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0072 - 344ms/epoch - 413us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0062 - 345ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0058 - 343ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0060 - 345ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0065 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0059 - 345ms/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0055 - 343ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0057 - 348ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0062 - 346ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0054 - 105ms/epoch - 502us/step\n",
      "834/834 - 0s - loss: 0.0054 - 228ms/epoch - 273us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  23.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0315 - val_loss: 0.0136\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0130 - val_loss: 0.0045\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0078 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0068 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0051 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0081 - val_loss: 0.0022\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0027 - val_loss: 0.0075\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0031 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0049 - val_loss: 0.0014\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0058 - 350ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0056 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0018 - 420ms/epoch - 504us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0128 - 346ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0023 - 347ms/epoch - 416us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0031 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0048 - 353ms/epoch - 423us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0086 - 343ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0021 - 345ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0053 - 348ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0032 - 345ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0018 - 101ms/epoch - 485us/step\n",
      "834/834 - 0s - loss: 0.0018 - 228ms/epoch - 273us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0320 - val_loss: 0.0056\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0094 - val_loss: 0.0031\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0085 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0049 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0044 - val_loss: 0.0063\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0054 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0029 - val_loss: 0.0343\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0048 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0036 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0077 - 346ms/epoch - 415us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0266 - 341ms/epoch - 408us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0036 - 343ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0028 - 345ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0046 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0040 - 351ms/epoch - 420us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0030 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0026 - 343ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0062 - 342ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0043 - 340ms/epoch - 408us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0024 - 344ms/epoch - 412us/step\n",
      "209/209 - 0s - loss: 0.0013 - 105ms/epoch - 502us/step\n",
      "834/834 - 0s - loss: 0.0013 - 227ms/epoch - 272us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0328 - val_loss: 0.0047\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0079 - val_loss: 0.0023\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0048 - val_loss: 0.0018\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0042 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0041 - val_loss: 0.0020\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0020 - val_loss: 0.0141\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0032 - val_loss: 0.0016\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0032 - val_loss: 0.0542\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0023 - val_loss: 0.0085\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0047 - 342ms/epoch - 410us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0055 - 342ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0044 - 342ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0031 - 344ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0062 - 341ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0017 - 344ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0048 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0034 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0034 - 343ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0037 - 344ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0042 - 349ms/epoch - 419us/step\n",
      "209/209 - 0s - loss: 0.0013 - 102ms/epoch - 490us/step\n",
      "834/834 - 0s - loss: 0.0013 - 229ms/epoch - 274us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0305 - val_loss: 0.0046\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0101 - val_loss: 0.0074\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0069 - val_loss: 0.0057\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0050 - val_loss: 0.0064\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0037 - val_loss: 0.0030\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0046 - val_loss: 0.0017\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0033 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0024 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0064 - val_loss: 0.0017\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0024 - 346ms/epoch - 415us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0126 - 342ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0038 - 339ms/epoch - 407us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0018 - 345ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0088 - 344ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0018 - 341ms/epoch - 408us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0051 - 339ms/epoch - 407us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0058 - 343ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0016 - 343ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0040 - 343ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0045 - 342ms/epoch - 410us/step\n",
      "209/209 - 0s - loss: 0.0017 - 103ms/epoch - 494us/step\n",
      "834/834 - 0s - loss: 0.0017 - 234ms/epoch - 281us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0318 - val_loss: 0.0064\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0089 - val_loss: 0.0142\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0073 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0059 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0068 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0042 - val_loss: 0.0029\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0031 - val_loss: 0.0019\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0035 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0061 - 350ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0032 - 350ms/epoch - 419us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0018 - 346ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0019 - 349ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0027 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0022 - 349ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0043 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0016 - 347ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0019 - 349ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0024 - 349ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0037 - 349ms/epoch - 419us/step\n",
      "209/209 - 0s - loss: 9.8222e-04 - 104ms/epoch - 499us/step\n",
      "834/834 - 0s - loss: 9.8258e-04 - 232ms/epoch - 279us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3783 - val_loss: 0.2375\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 0.1780 - val_loss: 0.1296\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0977 - val_loss: 0.0717\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0545 - val_loss: 0.0405\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0312 - val_loss: 0.0236\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 526us/step - loss: 0.0186 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0116 - val_loss: 0.0093\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 521us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0032 - 290ms/epoch - 348us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 272ms/epoch - 326us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0028 - 265ms/epoch - 317us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0027 - 268ms/epoch - 321us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 283ms/epoch - 339us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 267ms/epoch - 320us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 265ms/epoch - 318us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0023 - 291ms/epoch - 349us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 291ms/epoch - 349us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0021 - 355ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 262ms/epoch - 314us/step\n",
      "209/209 - 0s - loss: 0.0021 - 108ms/epoch - 519us/step\n",
      "834/834 - 0s - loss: 0.0021 - 232ms/epoch - 278us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3651 - val_loss: 0.2418\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.1812 - val_loss: 0.1319\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 526us/step - loss: 0.0994 - val_loss: 0.0729\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 523us/step - loss: 0.0555 - val_loss: 0.0412\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0318 - val_loss: 0.0240\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0189 - val_loss: 0.0147\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0118 - val_loss: 0.0095\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0079 - val_loss: 0.0069\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 525us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0032 - 287ms/epoch - 344us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 286ms/epoch - 343us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0028 - 276ms/epoch - 330us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0027 - 272ms/epoch - 326us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0025 - 269ms/epoch - 323us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0024 - 269ms/epoch - 323us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 270ms/epoch - 323us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 268ms/epoch - 322us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0022 - 310ms/epoch - 372us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0022 - 314ms/epoch - 377us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0021 - 279ms/epoch - 334us/step\n",
      "209/209 - 0s - loss: 0.0020 - 107ms/epoch - 513us/step\n",
      "834/834 - 0s - loss: 0.0020 - 235ms/epoch - 282us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3778 - val_loss: 0.2500\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.1872 - val_loss: 0.1362\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.1027 - val_loss: 0.0754\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0573 - val_loss: 0.0426\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0329 - val_loss: 0.0249\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0196 - val_loss: 0.0152\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0123 - val_loss: 0.0099\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0082 - val_loss: 0.0069\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 524us/step - loss: 0.0059 - val_loss: 0.0051\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 526us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0034 - 263ms/epoch - 316us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0032 - 260ms/epoch - 312us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0030 - 262ms/epoch - 314us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0028 - 261ms/epoch - 313us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0027 - 261ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0026 - 275ms/epoch - 330us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0025 - 260ms/epoch - 311us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 261ms/epoch - 312us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 273ms/epoch - 328us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0023 - 280ms/epoch - 335us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 273ms/epoch - 327us/step\n",
      "209/209 - 0s - loss: 0.0021 - 107ms/epoch - 513us/step\n",
      "834/834 - 0s - loss: 0.0021 - 233ms/epoch - 279us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3615 - val_loss: 0.2347\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.1759 - val_loss: 0.1282\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0967 - val_loss: 0.0710\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0540 - val_loss: 0.0402\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 537us/step - loss: 0.0310 - val_loss: 0.0235\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0185 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0117 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0079 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0033 - 273ms/epoch - 327us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0031 - 259ms/epoch - 311us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0029 - 264ms/epoch - 317us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0028 - 258ms/epoch - 309us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 267ms/epoch - 320us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 261ms/epoch - 313us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 263ms/epoch - 316us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 273ms/epoch - 328us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 258ms/epoch - 310us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0022 - 271ms/epoch - 325us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 261ms/epoch - 313us/step\n",
      "209/209 - 0s - loss: 0.0021 - 111ms/epoch - 531us/step\n",
      "834/834 - 0s - loss: 0.0021 - 233ms/epoch - 279us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3648 - val_loss: 0.2338\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.1753 - val_loss: 0.1277\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 524us/step - loss: 0.0963 - val_loss: 0.0707\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0539 - val_loss: 0.0401\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0310 - val_loss: 0.0235\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 521us/step - loss: 0.0185 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 525us/step - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 520us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0033 - 257ms/epoch - 308us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0031 - 256ms/epoch - 307us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0029 - 253ms/epoch - 304us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0027 - 260ms/epoch - 312us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 270ms/epoch - 324us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 256ms/epoch - 307us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 260ms/epoch - 312us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 254ms/epoch - 304us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 255ms/epoch - 306us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0022 - 254ms/epoch - 304us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 262ms/epoch - 315us/step\n",
      "209/209 - 0s - loss: 0.0020 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0020 - 224ms/epoch - 268us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  21.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1108 - val_loss: 0.0109\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0066 - val_loss: 0.0037\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 694us/step - loss: 0.0039 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 665us/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 655us/step - loss: 0.0036 - val_loss: 0.0019\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 672us/step - loss: 0.0028 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 656us/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 684us/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0020 - val_loss: 0.0036\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0046 - 345ms/epoch - 413us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0034 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0022 - 341ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0024 - 342ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0019 - 344ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0030 - 339ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0033 - 338ms/epoch - 406us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 339ms/epoch - 406us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0026 - 338ms/epoch - 405us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0039 - 336ms/epoch - 402us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0048 - 352ms/epoch - 421us/step\n",
      "209/209 - 0s - loss: 0.0014 - 102ms/epoch - 488us/step\n",
      "834/834 - 0s - loss: 0.0014 - 228ms/epoch - 273us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  25.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1175 - val_loss: 0.0102\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0074 - val_loss: 0.0050\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0039 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0034 - val_loss: 0.0023\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0031 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0026 - val_loss: 0.0019\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0022 - val_loss: 0.0074\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0055 - 357ms/epoch - 428us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0016 - 349ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0068 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0016 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0021 - 350ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0026 - 349ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0031 - 347ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0013 - 346ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0016 - 347ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0054 - 348ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0021 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0013 - 108ms/epoch - 517us/step\n",
      "834/834 - 0s - loss: 0.0013 - 226ms/epoch - 271us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1157 - val_loss: 0.0088\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0036 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0032 - val_loss: 0.0018\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0023 - val_loss: 0.0057\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0014 - val_loss: 9.3695e-04\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0045 - 345ms/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 341ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0017 - 342ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0048 - 340ms/epoch - 408us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0022 - 341ms/epoch - 409us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0029 - 340ms/epoch - 408us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0015 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0018 - 340ms/epoch - 408us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0019 - 339ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0046 - 346ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0015 - 342ms/epoch - 410us/step\n",
      "209/209 - 0s - loss: 0.0011 - 103ms/epoch - 495us/step\n",
      "834/834 - 0s - loss: 0.0011 - 223ms/epoch - 267us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1163 - val_loss: 0.0086\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0065 - val_loss: 0.0037\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0042 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0034 - val_loss: 0.0028\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0030 - val_loss: 0.0103\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0018 - val_loss: 0.0051\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0053 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0032 - 344ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0026 - 343ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0026 - 346ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 341ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0034 - 339ms/epoch - 407us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0032 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0021 - 340ms/epoch - 408us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0026 - 343ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0018 - 362ms/epoch - 434us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0052 - 340ms/epoch - 407us/step\n",
      "209/209 - 0s - loss: 0.0014 - 107ms/epoch - 510us/step\n",
      "834/834 - 0s - loss: 0.0014 - 225ms/epoch - 270us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1215 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0071 - val_loss: 0.0083\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0044 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0031 - val_loss: 0.0099\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0026 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 657us/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0042 - 356ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0035 - 339ms/epoch - 407us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0020 - 339ms/epoch - 406us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0021 - 342ms/epoch - 410us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0048 - 338ms/epoch - 405us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0013 - 341ms/epoch - 409us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0019 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0031 - 339ms/epoch - 407us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0018 - 339ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0030 - 344ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0019 - 342ms/epoch - 410us/step\n",
      "209/209 - 0s - loss: 0.0012 - 102ms/epoch - 487us/step\n",
      "834/834 - 0s - loss: 0.0012 - 231ms/epoch - 277us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  23.7s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 99999\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0301 - val_loss: 0.0054\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0105 - val_loss: 0.0036\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0059 - val_loss: 0.0041\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0049 - val_loss: 0.0029\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0035 - val_loss: 0.0015\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0022 - val_loss: 0.0010\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0028 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0019 - val_loss: 8.6016e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0042 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0040 - 1s/epoch - 427us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 445us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 417us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 416us/step\n",
      "625/625 - 0s - loss: 0.0020 - 206ms/epoch - 329us/step\n",
      "2500/2500 - 1s - loss: 0.0020 - 661ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  35.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0373 - val_loss: 0.0112\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0137 - val_loss: 0.0131\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0075 - val_loss: 0.0059\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0090 - val_loss: 0.0027\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0055 - val_loss: 0.0028\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0075 - val_loss: 0.0028\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0035 - val_loss: 0.0018\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0034 - val_loss: 0.0214\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0046 - val_loss: 0.0015\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0040 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0055 - 1s/epoch - 422us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 417us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 418us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 426us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0035 - 1s/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0011 - 1s/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0016 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 7.4554e-04 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 7.4529e-04 - 655ms/epoch - 262us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  34.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0356 - val_loss: 0.0044\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0116 - val_loss: 0.0040\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0074 - val_loss: 0.0034\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0083 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0059 - val_loss: 0.0027\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0064 - val_loss: 0.0019\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 729us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0026 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0042 - 1s/epoch - 409us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 417us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 9.0551e-04 - 213ms/epoch - 341us/step\n",
      "2500/2500 - 1s - loss: 9.0586e-04 - 674ms/epoch - 270us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  35.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.0311\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0086 - val_loss: 0.0199\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0100 - val_loss: 0.0038\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0063 - val_loss: 0.0026\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0040 - val_loss: 0.0019\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0042 - val_loss: 0.0015\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0021 - val_loss: 0.0045\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0031 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0026 - val_loss: 9.5754e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0048 - 1s/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 413us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0041 - 1s/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 426us/step\n",
      "625/625 - 0s - loss: 0.0011 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 0.0011 - 657ms/epoch - 263us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  34.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0274 - val_loss: 0.0052\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0089 - val_loss: 0.0039\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0081 - val_loss: 0.0041\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0054 - val_loss: 0.0026\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0038 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0042 - val_loss: 0.0053\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0041 - val_loss: 0.0022\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0033 - val_loss: 0.0102\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0030 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0025 - val_loss: 9.5333e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0057 - 1s/epoch - 416us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0068 - 1s/epoch - 422us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 423us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 421us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 426us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 433us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 413us/step\n",
      "625/625 - 0s - loss: 7.8962e-04 - 213ms/epoch - 341us/step\n",
      "2500/2500 - 1s - loss: 7.8973e-04 - 659ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  35.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1327 - val_loss: 0.0109\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0065 - val_loss: 0.0054\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0045 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 977ms/epoch - 391us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0016 - 1s/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 413us/step\n",
      "625/625 - 0s - loss: 0.0010 - 216ms/epoch - 346us/step\n",
      "2500/2500 - 1s - loss: 0.0010 - 655ms/epoch - 262us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1080 - val_loss: 0.0094\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0063 - val_loss: 0.0040\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0030 - val_loss: 0.0025\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0025 - val_loss: 0.0058\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0020 - val_loss: 9.9945e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 412us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 9.6913e-04 - 212ms/epoch - 339us/step\n",
      "2500/2500 - 1s - loss: 9.6887e-04 - 661ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1281 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0067 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0042 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0034 - val_loss: 0.0025\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0031 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0017 - val_loss: 0.0087\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0035 - 1s/epoch - 417us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 421us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 424us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 418us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 424us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 415us/step\n",
      "625/625 - 0s - loss: 0.0013 - 213ms/epoch - 341us/step\n",
      "2500/2500 - 1s - loss: 0.0013 - 656ms/epoch - 262us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  35.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1250 - val_loss: 0.0110\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0069 - val_loss: 0.0038\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0047 - val_loss: 0.0027\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0035 - val_loss: 0.0025\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0024 - val_loss: 0.0029\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 411us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0016 - 1s/epoch - 414us/step\n",
      "625/625 - 0s - loss: 9.1906e-04 - 217ms/epoch - 347us/step\n",
      "2500/2500 - 1s - loss: 9.1922e-04 - 660ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1172 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0062 - val_loss: 0.0039\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0038 - val_loss: 0.0030\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0032 - val_loss: 0.0046\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 411us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 416us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 411us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0039 - 1s/epoch - 449us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0014 - 1s/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 421us/step\n",
      "625/625 - 0s - loss: 0.0011 - 233ms/epoch - 373us/step\n",
      "2500/2500 - 1s - loss: 0.0011 - 702ms/epoch - 281us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1129 - val_loss: 0.0086\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0060 - val_loss: 0.0034\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0041 - val_loss: 0.0024\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0035 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0029 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0022 - val_loss: 0.0039\n",
      "Epoch 1/11\n",
      "3125/3125 - 1s - loss: 0.0028 - 1s/epoch - 447us/step\n",
      "Epoch 2/11\n",
      "3125/3125 - 1s - loss: 0.0024 - 1s/epoch - 452us/step\n",
      "Epoch 3/11\n",
      "3125/3125 - 1s - loss: 0.0020 - 1s/epoch - 444us/step\n",
      "Epoch 4/11\n",
      "3125/3125 - 1s - loss: 0.0027 - 1s/epoch - 450us/step\n",
      "Epoch 5/11\n",
      "3125/3125 - 1s - loss: 0.0022 - 1s/epoch - 447us/step\n",
      "Epoch 6/11\n",
      "3125/3125 - 1s - loss: 0.0018 - 1s/epoch - 468us/step\n",
      "Epoch 7/11\n",
      "3125/3125 - 1s - loss: 0.0019 - 1s/epoch - 451us/step\n",
      "Epoch 8/11\n",
      "3125/3125 - 1s - loss: 0.0026 - 1s/epoch - 448us/step\n",
      "Epoch 9/11\n",
      "3125/3125 - 1s - loss: 0.0018 - 1s/epoch - 446us/step\n",
      "Epoch 10/11\n",
      "3125/3125 - 1s - loss: 0.0017 - 1s/epoch - 451us/step\n",
      "Epoch 11/11\n",
      "3125/3125 - 1s - loss: 0.0018 - 1s/epoch - 446us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;,\n",
       "                    param_grid={&#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                                &#x27;optimizer&#x27;: [&#x27;sgd&#x27;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4070&gt;]},\n",
       "                    verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;,\n",
       "                    param_grid={&#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                                &#x27;optimizer&#x27;: [&#x27;sgd&#x27;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4070&gt;]},\n",
       "                    verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(error_score='raise',\n",
       "                    estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940>,\n",
       "                    param_grid={'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
       "                                'optimizer': ['sgd',\n",
       "                                              <keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>,\n",
       "                                              <keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>,\n",
       "                                              <keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>]},\n",
       "                    verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search selected hyperparameters\n",
    "\n",
    "# define base model function for grid search\n",
    "def create_model(activation='relu', optimizer='adam', kernel_regularizer='l2', batch_size=64):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'optimizer': ['sgd', Adam(learning_rate=0.001), Adam(learning_rate=0.01), Adam(learning_rate=0.1)],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform grid search hyperparameter tuning\n",
    "grid_search = HalvingGridSearchCV(estimator=keras_reg, param_grid=param_dist, cv=5, verbose=2, error_score='raise')\n",
    "grid_search.fit(Xtrain, ytrain, sample_weight=wtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>n_resources</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>16.997419</td>\n",
       "      <td>0.293825</td>\n",
       "      <td>0.079295</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>l1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': 'sgd'}</td>\n",
       "      <td>-0.014213</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012012</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.014210</td>\n",
       "      <td>-0.013634</td>\n",
       "      <td>-0.010464</td>\n",
       "      <td>-0.010404</td>\n",
       "      <td>-0.011347</td>\n",
       "      <td>-0.012012</td>\n",
       "      <td>0.001605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.082942</td>\n",
       "      <td>0.169986</td>\n",
       "      <td>0.081252</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.005693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005604</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.005692</td>\n",
       "      <td>-0.005540</td>\n",
       "      <td>-0.005397</td>\n",
       "      <td>-0.005991</td>\n",
       "      <td>-0.005397</td>\n",
       "      <td>-0.005604</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.774823</td>\n",
       "      <td>0.208586</td>\n",
       "      <td>0.081163</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.026682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027134</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.026685</td>\n",
       "      <td>-0.026955</td>\n",
       "      <td>-0.026410</td>\n",
       "      <td>-0.028737</td>\n",
       "      <td>-0.026886</td>\n",
       "      <td>-0.027135</td>\n",
       "      <td>0.000823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.576868</td>\n",
       "      <td>0.366474</td>\n",
       "      <td>0.079455</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.251134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249694</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.251033</td>\n",
       "      <td>-0.248961</td>\n",
       "      <td>-0.254116</td>\n",
       "      <td>-0.245601</td>\n",
       "      <td>-0.248622</td>\n",
       "      <td>-0.249666</td>\n",
       "      <td>0.002820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>17.627119</td>\n",
       "      <td>0.196109</td>\n",
       "      <td>0.079965</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': 'sgd'}</td>\n",
       "      <td>-0.002666</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002632</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.002665</td>\n",
       "      <td>-0.002537</td>\n",
       "      <td>-0.002637</td>\n",
       "      <td>-0.002655</td>\n",
       "      <td>-0.002661</td>\n",
       "      <td>-0.002631</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.685975</td>\n",
       "      <td>0.251114</td>\n",
       "      <td>0.078898</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001442</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.001248</td>\n",
       "      <td>-0.002222</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>-0.001109</td>\n",
       "      <td>-0.001138</td>\n",
       "      <td>-0.001421</td>\n",
       "      <td>0.000412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.925349</td>\n",
       "      <td>0.165671</td>\n",
       "      <td>0.079988</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003046</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>-0.003359</td>\n",
       "      <td>-0.001547</td>\n",
       "      <td>-0.007301</td>\n",
       "      <td>-0.001889</td>\n",
       "      <td>-0.003083</td>\n",
       "      <td>0.002226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.040764</td>\n",
       "      <td>0.241711</td>\n",
       "      <td>0.079797</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.005327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009157</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.005273</td>\n",
       "      <td>-0.005871</td>\n",
       "      <td>-0.005183</td>\n",
       "      <td>-0.012737</td>\n",
       "      <td>-0.015338</td>\n",
       "      <td>-0.008880</td>\n",
       "      <td>0.004297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>17.926032</td>\n",
       "      <td>0.387758</td>\n",
       "      <td>0.083417</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': '...</td>\n",
       "      <td>-0.011382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011686</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.011381</td>\n",
       "      <td>-0.012582</td>\n",
       "      <td>-0.011095</td>\n",
       "      <td>-0.011546</td>\n",
       "      <td>-0.011830</td>\n",
       "      <td>-0.011687</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.120462</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>0.083020</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': &lt;...</td>\n",
       "      <td>-0.007023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006413</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.007033</td>\n",
       "      <td>-0.006217</td>\n",
       "      <td>-0.005919</td>\n",
       "      <td>-0.006353</td>\n",
       "      <td>-0.006546</td>\n",
       "      <td>-0.006414</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.078545</td>\n",
       "      <td>0.151325</td>\n",
       "      <td>0.092128</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': &lt;...</td>\n",
       "      <td>-0.028657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028504</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.028676</td>\n",
       "      <td>-0.028378</td>\n",
       "      <td>-0.027875</td>\n",
       "      <td>-0.026800</td>\n",
       "      <td>-0.030723</td>\n",
       "      <td>-0.028491</td>\n",
       "      <td>0.001286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.761524</td>\n",
       "      <td>0.236672</td>\n",
       "      <td>0.089498</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': &lt;...</td>\n",
       "      <td>-0.293942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.290032</td>\n",
       "      <td>0.004074</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.293926</td>\n",
       "      <td>-0.294151</td>\n",
       "      <td>-0.285779</td>\n",
       "      <td>-0.284657</td>\n",
       "      <td>-0.291711</td>\n",
       "      <td>-0.290045</td>\n",
       "      <td>0.004048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>24.141023</td>\n",
       "      <td>0.367304</td>\n",
       "      <td>0.119679</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.004456</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005074</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.004456</td>\n",
       "      <td>-0.004653</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>-0.005607</td>\n",
       "      <td>-0.005352</td>\n",
       "      <td>-0.005073</td>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>23.382346</td>\n",
       "      <td>0.121729</td>\n",
       "      <td>0.117038</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.001846</td>\n",
       "      <td>-0.001345</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>-0.001696</td>\n",
       "      <td>-0.000983</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>21.985600</td>\n",
       "      <td>0.174433</td>\n",
       "      <td>0.121637</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': 'sgd'}</td>\n",
       "      <td>-0.002051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.002024</td>\n",
       "      <td>-0.002124</td>\n",
       "      <td>-0.002068</td>\n",
       "      <td>-0.002035</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>24.130400</td>\n",
       "      <td>0.423410</td>\n",
       "      <td>0.118057</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>-0.001089</td>\n",
       "      <td>-0.001441</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>99999</td>\n",
       "      <td>34.752902</td>\n",
       "      <td>0.121369</td>\n",
       "      <td>0.224412</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001110</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.001972</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>-0.000906</td>\n",
       "      <td>-0.001142</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>-0.001111</td>\n",
       "      <td>0.000452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>99999</td>\n",
       "      <td>34.617896</td>\n",
       "      <td>0.145296</td>\n",
       "      <td>0.233857</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001054</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>-0.000969</td>\n",
       "      <td>-0.001307</td>\n",
       "      <td>-0.000919</td>\n",
       "      <td>-0.001052</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
       "0      0        11111      16.997419      0.293825         0.079295   \n",
       "1      0        11111      19.082942      0.169986         0.081252   \n",
       "2      0        11111      19.774823      0.208586         0.081163   \n",
       "3      0        11111      19.576868      0.366474         0.079455   \n",
       "4      0        11111      17.627119      0.196109         0.079965   \n",
       "5      0        11111      19.685975      0.251114         0.078898   \n",
       "6      0        11111      19.925349      0.165671         0.079988   \n",
       "7      0        11111      20.040764      0.241711         0.079797   \n",
       "8      0        11111      17.926032      0.387758         0.083417   \n",
       "9      0        11111      20.120462      0.067826         0.083020   \n",
       "10     0        11111      20.078545      0.151325         0.092128   \n",
       "11     0        11111      20.761524      0.236672         0.089498   \n",
       "12     1        33333      24.141023      0.367304         0.119679   \n",
       "13     1        33333      23.382346      0.121729         0.117038   \n",
       "14     1        33333      21.985600      0.174433         0.121637   \n",
       "15     1        33333      24.130400      0.423410         0.118057   \n",
       "16     2        99999      34.752902      0.121369         0.224412   \n",
       "17     2        99999      34.617896      0.145296         0.233857   \n",
       "\n",
       "    std_score_time param_kernel_regularizer  \\\n",
       "0         0.001660                       l1   \n",
       "1         0.002900                       l1   \n",
       "2         0.002130                       l1   \n",
       "3         0.003233                       l1   \n",
       "4         0.000889                       l2   \n",
       "5         0.000797                       l2   \n",
       "6         0.003639                       l2   \n",
       "7         0.002502                       l2   \n",
       "8         0.002099                    l1_l2   \n",
       "9         0.001753                    l1_l2   \n",
       "10        0.008104                    l1_l2   \n",
       "11        0.005330                    l1_l2   \n",
       "12        0.002844                       l1   \n",
       "13        0.001417                       l2   \n",
       "14        0.002785                       l2   \n",
       "15        0.003221                       l2   \n",
       "16        0.002090                       l2   \n",
       "17        0.006886                       l2   \n",
       "\n",
       "                                      param_optimizer  \\\n",
       "0                                                 sgd   \n",
       "1   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "2   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "3   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "4                                                 sgd   \n",
       "5   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "6   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "7   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "8                                                 sgd   \n",
       "9   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "10  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "11  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "12  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "13  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "14                                                sgd   \n",
       "15  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "16  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "17  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "\n",
       "                                               params  split0_test_score  ...  \\\n",
       "0    {'kernel_regularizer': 'l1', 'optimizer': 'sgd'}          -0.014213  ...   \n",
       "1   {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.005693  ...   \n",
       "2   {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.026682  ...   \n",
       "3   {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.251134  ...   \n",
       "4    {'kernel_regularizer': 'l2', 'optimizer': 'sgd'}          -0.002666  ...   \n",
       "5   {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001247  ...   \n",
       "6   {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001318  ...   \n",
       "7   {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.005327  ...   \n",
       "8   {'kernel_regularizer': 'l1_l2', 'optimizer': '...          -0.011382  ...   \n",
       "9   {'kernel_regularizer': 'l1_l2', 'optimizer': <...          -0.007023  ...   \n",
       "10  {'kernel_regularizer': 'l1_l2', 'optimizer': <...          -0.028657  ...   \n",
       "11  {'kernel_regularizer': 'l1_l2', 'optimizer': <...          -0.293942  ...   \n",
       "12  {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.004456  ...   \n",
       "13  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001846  ...   \n",
       "14   {'kernel_regularizer': 'l2', 'optimizer': 'sgd'}          -0.002051  ...   \n",
       "15  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001426  ...   \n",
       "16  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001968  ...   \n",
       "17  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001019  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         -0.012012        0.001611               14           -0.014210   \n",
       "1         -0.005604        0.000223               10           -0.005692   \n",
       "2         -0.027134        0.000823               15           -0.026685   \n",
       "3         -0.249694        0.002871               17           -0.251033   \n",
       "4         -0.002632        0.000044                7           -0.002665   \n",
       "5         -0.001442        0.000451                5           -0.001248   \n",
       "6         -0.003046        0.002154                8           -0.001318   \n",
       "7         -0.009157        0.004497               12           -0.005273   \n",
       "8         -0.011686        0.000507               13           -0.011381   \n",
       "9         -0.006413        0.000367               11           -0.007033   \n",
       "10        -0.028504        0.001293               16           -0.028676   \n",
       "11        -0.290032        0.004074               18           -0.293926   \n",
       "12        -0.005074        0.000441                9           -0.004456   \n",
       "13        -0.001427        0.000309                4           -0.001846   \n",
       "14        -0.002061        0.000035                6           -0.002052   \n",
       "15        -0.001290        0.000134                3           -0.001427   \n",
       "16        -0.001110        0.000450                2           -0.001972   \n",
       "17        -0.001054        0.000135                1           -0.001019   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            -0.013634           -0.010464           -0.010404   \n",
       "1            -0.005540           -0.005397           -0.005991   \n",
       "2            -0.026955           -0.026410           -0.028737   \n",
       "3            -0.248961           -0.254116           -0.245601   \n",
       "4            -0.002537           -0.002637           -0.002655   \n",
       "5            -0.002222           -0.001386           -0.001109   \n",
       "6            -0.003359           -0.001547           -0.007301   \n",
       "7            -0.005871           -0.005183           -0.012737   \n",
       "8            -0.012582           -0.011095           -0.011546   \n",
       "9            -0.006217           -0.005919           -0.006353   \n",
       "10           -0.028378           -0.027875           -0.026800   \n",
       "11           -0.294151           -0.285779           -0.284657   \n",
       "12           -0.004653           -0.005298           -0.005607   \n",
       "13           -0.001345           -0.001265           -0.001696   \n",
       "14           -0.002024           -0.002124           -0.002068   \n",
       "15           -0.001292           -0.001089           -0.001441   \n",
       "16           -0.000745           -0.000906           -0.001142   \n",
       "17           -0.000969           -0.001307           -0.000919   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0            -0.011347         -0.012012         0.001605  \n",
       "1            -0.005397         -0.005604         0.000223  \n",
       "2            -0.026886         -0.027135         0.000823  \n",
       "3            -0.248622         -0.249666         0.002820  \n",
       "4            -0.002661         -0.002631         0.000048  \n",
       "5            -0.001138         -0.001421         0.000412  \n",
       "6            -0.001889         -0.003083         0.002226  \n",
       "7            -0.015338         -0.008880         0.004297  \n",
       "8            -0.011830         -0.011687         0.000507  \n",
       "9            -0.006546         -0.006414         0.000371  \n",
       "10           -0.030723         -0.028491         0.001286  \n",
       "11           -0.291711         -0.290045         0.004048  \n",
       "12           -0.005352         -0.005073         0.000441  \n",
       "13           -0.000983         -0.001427         0.000309  \n",
       "14           -0.002035         -0.002061         0.000035  \n",
       "15           -0.001202         -0.001290         0.000134  \n",
       "16           -0.000790         -0.001111         0.000452  \n",
       "17           -0.001052         -0.001053         0.000135  \n",
       "\n",
       "[18 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "display(grid_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'kernel_regularizer': 'l2', 'optimizer': <keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>}\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/377624069.py:23: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 33333\n",
      "max_resources_: 100000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 3\n",
      "n_resources: 33333\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1467 - val_loss: 0.0128\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0077 - val_loss: 0.0043\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0045 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0032 - val_loss: 0.0068\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0050 - 352ms/epoch - 423us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0026 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0021 - 349ms/epoch - 418us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0057 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0015 - 349ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0027 - 348ms/epoch - 417us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0041 - 365ms/epoch - 437us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0022 - 358ms/epoch - 429us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0024 - 362ms/epoch - 435us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0027 - 365ms/epoch - 438us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0027 - 385ms/epoch - 462us/step\n",
      "209/209 - 0s - loss: 0.0019 - 112ms/epoch - 536us/step\n",
      "834/834 - 0s - loss: 0.0019 - 239ms/epoch - 287us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1279 - val_loss: 0.0116\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0064 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0039 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0016 - val_loss: 0.0111\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0057 - 351ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0019 - 347ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0021 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0034 - 348ms/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0022 - 349ms/epoch - 418us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0035 - 347ms/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0016 - 364ms/epoch - 437us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0033 - 346ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0012 - 347ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0038 - 344ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0017 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0012 - 107ms/epoch - 514us/step\n",
      "834/834 - 0s - loss: 0.0012 - 227ms/epoch - 272us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1154 - val_loss: 0.0111\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0069 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0023 - val_loss: 0.0070\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0045 - 351ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0041 - 344ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0014 - 343ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0059 - 346ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0024 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0016 - 344ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0021 - 343ms/epoch - 412us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0048 - 345ms/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 344ms/epoch - 413us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0014 - 343ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0039 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0014 - 105ms/epoch - 501us/step\n",
      "834/834 - 0s - loss: 0.0014 - 229ms/epoch - 274us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1286 - val_loss: 0.0118\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0044 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0036 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0027 - val_loss: 0.0047\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0021 - val_loss: 0.0083\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0056 - 354ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0017 - 351ms/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0033 - 350ms/epoch - 420us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0025 - 350ms/epoch - 420us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0030 - 352ms/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 351ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0015 - 349ms/epoch - 419us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0022 - 351ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0031 - 354ms/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0024 - 366ms/epoch - 438us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0018 - 351ms/epoch - 421us/step\n",
      "209/209 - 0s - loss: 0.0016 - 105ms/epoch - 503us/step\n",
      "834/834 - 0s - loss: 0.0016 - 230ms/epoch - 275us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1365 - val_loss: 0.0104\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0073 - val_loss: 0.0057\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0047 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0035 - val_loss: 0.0024\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0028 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0025 - val_loss: 0.0052\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0058 - 350ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 359ms/epoch - 431us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0021 - 372ms/epoch - 446us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0021 - 357ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0025 - 380ms/epoch - 456us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0029 - 386ms/epoch - 463us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0036 - 350ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0019 - 351ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0022 - 344ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0053 - 344ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0021 - 346ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0015 - 104ms/epoch - 497us/step\n",
      "834/834 - 0s - loss: 0.0015 - 230ms/epoch - 276us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0291 - val_loss: 0.0171\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0088 - val_loss: 0.0028\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0060 - val_loss: 0.0453\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0071 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0034 - val_loss: 0.0015\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 536us/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0035 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0032 - val_loss: 0.0037\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0033 - val_loss: 0.0065\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0038 - 363ms/epoch - 436us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0048 - 344ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0039 - 342ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0033 - 357ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0052 - 356ms/epoch - 427us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0017 - 365ms/epoch - 438us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0039 - 345ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0042 - 350ms/epoch - 419us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0030 - 354ms/epoch - 425us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0075 - 377ms/epoch - 453us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0043 - 364ms/epoch - 437us/step\n",
      "209/209 - 0s - loss: 0.0014 - 104ms/epoch - 499us/step\n",
      "834/834 - 0s - loss: 0.0014 - 229ms/epoch - 274us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.0063\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0096 - val_loss: 0.0039\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 672us/step - loss: 0.0065 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0069 - val_loss: 0.0104\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0046 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0039 - val_loss: 0.0044\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 630us/step - loss: 0.0038 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0046 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0038 - 394ms/epoch - 473us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0036 - 363ms/epoch - 435us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0058 - 369ms/epoch - 443us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0045 - 373ms/epoch - 448us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0052 - 352ms/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0029 - 345ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 377ms/epoch - 452us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0132 - 371ms/epoch - 445us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0025 - 401ms/epoch - 481us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0039 - 377ms/epoch - 451us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0031 - 390ms/epoch - 468us/step\n",
      "209/209 - 0s - loss: 0.0019 - 114ms/epoch - 544us/step\n",
      "834/834 - 0s - loss: 0.0019 - 241ms/epoch - 289us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  24.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0366 - val_loss: 0.0508\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0094 - val_loss: 0.0137\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0075 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0063 - val_loss: 0.0029\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0047 - val_loss: 0.0027\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0050 - val_loss: 0.0020\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0035 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0020 - val_loss: 9.9140e-04\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0018 - val_loss: 9.0679e-04\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0032 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0037 - 346ms/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0030 - 344ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0021 - 343ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0093 - 343ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0012 - 343ms/epoch - 411us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0056 - 346ms/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0016 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0024 - 343ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0030 - 345ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 344ms/epoch - 413us/step\n",
      "209/209 - 0s - loss: 0.0011 - 105ms/epoch - 501us/step\n",
      "834/834 - 0s - loss: 0.0011 - 231ms/epoch - 277us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0290 - val_loss: 0.1035\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0100 - val_loss: 0.0038\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0069 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0053 - val_loss: 0.0099\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0053 - val_loss: 0.0302\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0044 - val_loss: 0.0149\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0047 - val_loss: 0.0197\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0047 - val_loss: 0.0016\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0045 - 351ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0015 - 348ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0045 - 344ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0046 - 346ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0019 - 347ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0061 - 346ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0022 - 374ms/epoch - 449us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0037 - 348ms/epoch - 417us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0063 - 346ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0034 - 346ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0015 - 345ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0017 - 103ms/epoch - 494us/step\n",
      "834/834 - 0s - loss: 0.0017 - 230ms/epoch - 275us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0285 - val_loss: 0.0054\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0108 - val_loss: 0.0376\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 671us/step - loss: 0.0060 - val_loss: 0.0108\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0046 - val_loss: 0.0156\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0066 - val_loss: 0.0025\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0031 - val_loss: 0.0055\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0054 - val_loss: 0.0026\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0020 - val_loss: 9.9628e-04\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0032 - val_loss: 9.7153e-04\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0111 - 350ms/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0017 - 344ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0028 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0058 - 345ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0039 - 358ms/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0041 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0028 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0029 - 346ms/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0029 - 346ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0026 - 347ms/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 345ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0010 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0010 - 228ms/epoch - 273us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0932 - val_loss: 0.0262\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0370 - val_loss: 0.0082\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0169 - val_loss: 0.0052\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0259 - val_loss: 0.0095\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0276 - val_loss: 0.0130\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0364 - val_loss: 0.0138\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0217 - val_loss: 0.0053\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0214 - val_loss: 0.0038\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0522 - val_loss: 0.0041\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0293 - val_loss: 0.0068\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0220 - val_loss: 0.0050\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0542 - 349ms/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.1068 - 345ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0710 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0542 - 344ms/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0290 - 344ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0725 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0229 - 346ms/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0408 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0608 - 347ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0762 - 346ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0452 - 343ms/epoch - 411us/step\n",
      "209/209 - 0s - loss: 0.0050 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0051 - 228ms/epoch - 273us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0937 - val_loss: 0.0294\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0345 - val_loss: 0.0095\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0197 - val_loss: 0.0057\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0283 - val_loss: 0.0257\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0376 - val_loss: 0.4935\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0198 - val_loss: 0.0038\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0351 - val_loss: 0.0040\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0269 - val_loss: 0.0036\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0196 - val_loss: 0.0424\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0198 - val_loss: 0.0033\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0276 - val_loss: 0.0040\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0755 - 354ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0967 - 351ms/epoch - 421us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0351 - 357ms/epoch - 428us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0412 - 348ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0716 - 350ms/epoch - 420us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0417 - 350ms/epoch - 420us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0276 - 349ms/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 1.5169 - 352ms/epoch - 422us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 1.5450 - 351ms/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 1.5481 - 351ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 1.5502 - 352ms/epoch - 422us/step\n",
      "209/209 - 0s - loss: 0.8013 - 106ms/epoch - 507us/step\n",
      "834/834 - 0s - loss: 0.8014 - 227ms/epoch - 272us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1096 - val_loss: 0.1054\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0358 - val_loss: 0.0166\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0253 - val_loss: 0.1548\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0269 - val_loss: 0.0124\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0430 - val_loss: 0.0049\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0157 - val_loss: 0.0038\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0221 - val_loss: 0.0041\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0215 - val_loss: 0.0080\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0313 - val_loss: 0.0058\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0159 - val_loss: 0.0161\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0169 - val_loss: 0.0043\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.1425 - 349ms/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0109 - 349ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0512 - 349ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0479 - 344ms/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.1069 - 342ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0236 - 345ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0566 - 343ms/epoch - 411us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0248 - 343ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0627 - 349ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0987 - 345ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0457 - 345ms/epoch - 413us/step\n",
      "209/209 - 0s - loss: 0.0037 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0038 - 230ms/epoch - 275us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0934 - val_loss: 0.1095\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0387 - val_loss: 0.0405\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0565 - val_loss: 0.0036\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0447 - val_loss: 0.0185\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0229 - val_loss: 0.0273\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0298 - val_loss: 0.0308\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0259 - val_loss: 0.0738\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0297 - val_loss: 0.0254\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0316 - val_loss: 0.0104\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0342 - val_loss: 0.0136\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0268 - val_loss: 0.0074\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.1429 - 345ms/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0496 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0444 - 346ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0441 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0174 - 345ms/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.1005 - 348ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0664 - 345ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0206 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0744 - 347ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0719 - 343ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0807 - 348ms/epoch - 417us/step\n",
      "209/209 - 0s - loss: 0.0127 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0127 - 228ms/epoch - 273us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0922 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0301 - val_loss: 0.0130\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0355 - val_loss: 0.0867\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0348 - val_loss: 0.0072\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0178 - val_loss: 0.3569\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0355 - val_loss: 0.0067\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0301 - val_loss: 0.0554\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0603 - val_loss: 0.0047\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0203 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0334 - val_loss: 0.0084\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0160 - val_loss: 0.0043\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0593 - 353ms/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0395 - 348ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0196 - 351ms/epoch - 421us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0470 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0435 - 360ms/epoch - 432us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0784 - 357ms/epoch - 428us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0150 - 347ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0642 - 348ms/epoch - 417us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0654 - 348ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0266 - 349ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.1292 - 348ms/epoch - 418us/step\n",
      "209/209 - 0s - loss: 0.0062 - 104ms/epoch - 499us/step\n",
      "834/834 - 0s - loss: 0.0062 - 231ms/epoch - 277us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.8s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1\n",
      "n_resources: 99999\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0356 - val_loss: 0.0091\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0111 - val_loss: 0.0086\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0097 - val_loss: 0.0058\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0040 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0042 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0060 - val_loss: 0.0036\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0029 - val_loss: 0.0058\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0036 - 1s/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0061 - 1s/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0044 - 1s/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0047 - 1s/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0062 - 1s/epoch - 416us/step\n",
      "625/625 - 0s - loss: 0.0013 - 213ms/epoch - 340us/step\n",
      "2500/2500 - 1s - loss: 0.0013 - 664ms/epoch - 265us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.0074\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0107 - val_loss: 0.0040\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0084 - val_loss: 0.0031\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0057 - val_loss: 0.0024\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0045 - val_loss: 0.0025\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0045 - val_loss: 0.0027\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0029 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0040 - val_loss: 0.0073\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0027 - val_loss: 0.0109\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0038 - 1s/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 417us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 421us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 422us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 0.0015 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 0.0015 - 663ms/epoch - 265us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0366 - val_loss: 0.0047\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0097 - val_loss: 0.0032\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0084 - val_loss: 0.0027\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0090 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0039 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0036 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0027 - val_loss: 0.0011\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0039 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0028 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0024 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0055 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 424us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0036 - 1s/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0043 - 1s/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 419us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0035 - 1s/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 0.0011 - 207ms/epoch - 331us/step\n",
      "2500/2500 - 1s - loss: 0.0011 - 655ms/epoch - 262us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0361 - val_loss: 0.0061\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0099 - val_loss: 0.0056\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0063 - val_loss: 0.0025\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0066 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0044 - val_loss: 0.0015\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0058 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0039 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0041 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0042 - val_loss: 0.0014\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0058 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 420us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0061 - 1s/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0075 - 1s/epoch - 422us/step\n",
      "625/625 - 0s - loss: 0.0017 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 0.0017 - 660ms/epoch - 264us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0358 - val_loss: 0.0076\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0101 - val_loss: 0.0045\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0153 - val_loss: 0.0080\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0067 - val_loss: 0.0284\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0046 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0063 - val_loss: 0.0019\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0037 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0049 - val_loss: 0.0017\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0020 - val_loss: 9.4648e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0060 - 1s/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0039 - 1s/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 415us/step\n",
      "625/625 - 0s - loss: 7.8604e-04 - 210ms/epoch - 336us/step\n",
      "2500/2500 - 1s - loss: 7.8599e-04 - 666ms/epoch - 266us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0344 - val_loss: 0.0090\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0090 - val_loss: 0.0036\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0079 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0074 - val_loss: 0.0085\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0047 - val_loss: 0.0074\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0041 - val_loss: 0.0013\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0024 - val_loss: 0.0499\n",
      "Epoch 1/11\n",
      "3125/3125 - 1s - loss: 0.0041 - 1s/epoch - 450us/step\n",
      "Epoch 2/11\n",
      "3125/3125 - 1s - loss: 0.0030 - 1s/epoch - 449us/step\n",
      "Epoch 3/11\n",
      "3125/3125 - 1s - loss: 0.0024 - 1s/epoch - 454us/step\n",
      "Epoch 4/11\n",
      "3125/3125 - 1s - loss: 0.0031 - 1s/epoch - 445us/step\n",
      "Epoch 5/11\n",
      "3125/3125 - 1s - loss: 0.0030 - 1s/epoch - 452us/step\n",
      "Epoch 6/11\n",
      "3125/3125 - 1s - loss: 0.0025 - 1s/epoch - 447us/step\n",
      "Epoch 7/11\n",
      "3125/3125 - 1s - loss: 0.0030 - 1s/epoch - 452us/step\n",
      "Epoch 8/11\n",
      "3125/3125 - 1s - loss: 0.0022 - 1s/epoch - 447us/step\n",
      "Epoch 9/11\n",
      "3125/3125 - 1s - loss: 0.0035 - 1s/epoch - 450us/step\n",
      "Epoch 10/11\n",
      "3125/3125 - 1s - loss: 0.0032 - 1s/epoch - 448us/step\n",
      "Epoch 11/11\n",
      "3125/3125 - 1s - loss: 0.0019 - 1s/epoch - 451us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;,\n",
       "                    param_grid={&#x27;learning_rate&#x27;: [0.001, 0.01, 0.1]},\n",
       "                    verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;,\n",
       "                    param_grid={&#x27;learning_rate&#x27;: [0.001, 0.01, 0.1]},\n",
       "                    verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(error_score='raise',\n",
       "                    estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20>,\n",
       "                    param_grid={'learning_rate': [0.001, 0.01, 0.1]},\n",
       "                    verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search learning rates\n",
    "\n",
    "# define base model function for grid search\n",
    "def create_model(activation='relu', learning_rate=0.001, kernel_regularizer='l2', batch_size=64):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform grid search hyperparameter tuning\n",
    "grid_search = HalvingGridSearchCV(estimator=keras_reg, param_grid=param_dist, cv=5, verbose=2, error_score='raise')\n",
    "grid_search.fit(Xtrain, ytrain, sample_weight=wtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0052 - val_loss: 0.0031\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 629us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 7.4837e-04 - val_loss: 2.9767e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 7.7995e-04 - val_loss: 4.0676e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 6.9497e-04 - val_loss: 0.0013\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 4.3383e-04 - val_loss: 2.5998e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 4.0091e-04 - val_loss: 2.9744e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 4.1961e-04 - val_loss: 2.0497e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 4.5444e-04 - val_loss: 6.9645e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 2.9007e-04 - val_loss: 2.3507e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 3.3412e-04 - val_loss: 4.4008e-04\n"
     ]
    }
   ],
   "source": [
    "# evaluate best model\n",
    "best_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "    Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "best_model.compile(optimizer=Adam(learning_rate=0.01), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = best_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=64, validation_data=(Xval, yval, wval))\n",
    "\n",
    "# save the best model\n",
    "best_model.save(\"models/best_dnn_model_raw_pd.h5\")\n",
    "\n",
    "# save model history in models\n",
    "with open('models/best_dnn_model_raw_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Huber Loss: 0.00033411901677027345\n",
      "Best Validation Huber Loss: 0.0004400824836920947\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_train_hl = best_model.history.history[\"loss\"][-1]\n",
    "best_val_hl = best_model.history.history[\"val_loss\"][-1]\n",
    "print(f\"Best Training Huber Loss: {best_train_hl}\")\n",
    "print(f\"Best Validation Huber Loss: {best_val_hl}\")\n",
    "\n",
    "plt.plot(best_model.history.history['loss'], label='Training Huber Loss')\n",
    "plt.plot(best_model.history.history['val_loss'], label='Validation Huber Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed PD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steering_angle</th>\n",
       "      <th>distance_to_road_center</th>\n",
       "      <th>angle_from_straight_in_rads</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.951861</td>\n",
       "      <td>1.203014</td>\n",
       "      <td>3.481764</td>\n",
       "      <td>-1.856692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.951861</td>\n",
       "      <td>1.203014</td>\n",
       "      <td>3.481764</td>\n",
       "      <td>-1.859236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.952644</td>\n",
       "      <td>1.210898</td>\n",
       "      <td>3.470854</td>\n",
       "      <td>-1.863646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.953835</td>\n",
       "      <td>1.225300</td>\n",
       "      <td>3.450489</td>\n",
       "      <td>-1.894529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.976408</td>\n",
       "      <td>1.245016</td>\n",
       "      <td>3.460665</td>\n",
       "      <td>-1.901220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113150</th>\n",
       "      <td>1.719868</td>\n",
       "      <td>0.140641</td>\n",
       "      <td>2.905026</td>\n",
       "      <td>-1.741388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113151</th>\n",
       "      <td>1.853628</td>\n",
       "      <td>0.165550</td>\n",
       "      <td>3.109106</td>\n",
       "      <td>-2.103937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113152</th>\n",
       "      <td>1.867676</td>\n",
       "      <td>0.208531</td>\n",
       "      <td>3.067399</td>\n",
       "      <td>-2.465488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113153</th>\n",
       "      <td>1.880882</td>\n",
       "      <td>0.251025</td>\n",
       "      <td>3.024923</td>\n",
       "      <td>-2.826038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113154</th>\n",
       "      <td>1.893263</td>\n",
       "      <td>0.293018</td>\n",
       "      <td>2.981733</td>\n",
       "      <td>-3.185560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113155 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        steering_angle  distance_to_road_center  angle_from_straight_in_rads  \\\n",
       "0             2.951861                 1.203014                     3.481764   \n",
       "1             2.951861                 1.203014                     3.481764   \n",
       "2             2.952644                 1.210898                     3.470854   \n",
       "3             2.953835                 1.225300                     3.450489   \n",
       "4             2.976408                 1.245016                     3.460665   \n",
       "...                ...                      ...                          ...   \n",
       "113150        1.719868                 0.140641                     2.905026   \n",
       "113151        1.853628                 0.165550                     3.109106   \n",
       "113152        1.867676                 0.208531                     3.067399   \n",
       "113153        1.880882                 0.251025                     3.024923   \n",
       "113154        1.893263                 0.293018                     2.981733   \n",
       "\n",
       "          reward  \n",
       "0      -1.856692  \n",
       "1      -1.859236  \n",
       "2      -1.863646  \n",
       "3      -1.894529  \n",
       "4      -1.901220  \n",
       "...          ...  \n",
       "113150 -1.741388  \n",
       "113151 -2.103937  \n",
       "113152 -2.465488  \n",
       "113153 -2.826038  \n",
       "113154 -3.185560  \n",
       "\n",
       "[113155 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_37274/1201342029.py:15: RuntimeWarning: divide by zero encountered in divide\n",
      "  weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n"
     ]
    }
   ],
   "source": [
    "# import data from csv to dataframe\n",
    "filename = \"processed_data_pd.csv\"\n",
    "df = pd.read_csv(f\"train_data/{filename}\")\n",
    "display(df)\n",
    "\n",
    "# split into input and target features\n",
    "X = df[['distance_to_road_center', 'angle_from_straight_in_rads']].values\n",
    "y = df['steering_angle'].values\n",
    "r = df['reward'].values\n",
    "\n",
    "# normalize rewards to [0, 1] range\n",
    "norm_r = (r - np.min(r)) / (np.max(r) - np.min(r))\n",
    "\n",
    "# calculate weights based on normalized rewards\n",
    "weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n",
    "\n",
    "# split into training and validation sets\n",
    "Xtrain, Xval, ytrain, yval, wtrain, wval = train_test_split(X, y, weights, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 14:01:11.281518: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2811/2811 [==============================] - 3s 1ms/step - loss: 0.0024 - val_loss: 8.6501e-06\n",
      "Epoch 2/100\n",
      "2811/2811 [==============================] - 1s 527us/step - loss: 5.6399e-05 - val_loss: 4.0272e-05\n",
      "Epoch 3/100\n",
      "2811/2811 [==============================] - 1s 531us/step - loss: 4.8377e-05 - val_loss: 2.3432e-05\n",
      "Epoch 4/100\n",
      "2811/2811 [==============================] - 1s 517us/step - loss: 3.4387e-05 - val_loss: 7.5596e-06\n",
      "Epoch 5/100\n",
      "2811/2811 [==============================] - 1s 519us/step - loss: 3.3832e-05 - val_loss: 3.0433e-06\n",
      "Epoch 6/100\n",
      "2811/2811 [==============================] - 1s 519us/step - loss: 3.2808e-05 - val_loss: 2.4607e-05\n",
      "Epoch 7/100\n",
      "2811/2811 [==============================] - 1s 529us/step - loss: 2.3007e-05 - val_loss: 1.5444e-05\n",
      "Epoch 8/100\n",
      "2811/2811 [==============================] - 1s 522us/step - loss: 1.6547e-05 - val_loss: 2.1885e-06\n",
      "Epoch 9/100\n",
      "2811/2811 [==============================] - 1s 531us/step - loss: 1.3518e-05 - val_loss: 1.2097e-05\n",
      "Epoch 10/100\n",
      "2811/2811 [==============================] - 1s 519us/step - loss: 1.4981e-05 - val_loss: 1.4493e-05\n",
      "Epoch 11/100\n",
      "2811/2811 [==============================] - 2s 536us/step - loss: 1.2117e-05 - val_loss: 1.8954e-06\n",
      "Epoch 12/100\n",
      "2811/2811 [==============================] - 1s 525us/step - loss: 1.2026e-05 - val_loss: 1.0914e-06\n",
      "Epoch 13/100\n",
      "2811/2811 [==============================] - 1s 529us/step - loss: 8.2301e-06 - val_loss: 8.5420e-07\n",
      "Epoch 14/100\n",
      "2811/2811 [==============================] - 1s 518us/step - loss: 1.1817e-05 - val_loss: 1.6044e-06\n",
      "Epoch 15/100\n",
      "2811/2811 [==============================] - 1s 525us/step - loss: 6.0166e-06 - val_loss: 3.8391e-07\n",
      "Epoch 16/100\n",
      "2811/2811 [==============================] - 1s 517us/step - loss: 6.5154e-06 - val_loss: 4.3212e-06\n",
      "Epoch 17/100\n",
      "2811/2811 [==============================] - 1s 524us/step - loss: 9.7324e-06 - val_loss: 1.2043e-05\n",
      "Epoch 18/100\n",
      "2811/2811 [==============================] - 2s 538us/step - loss: 7.6324e-06 - val_loss: 2.2973e-06\n",
      "Epoch 19/100\n",
      "2811/2811 [==============================] - 1s 526us/step - loss: 7.8802e-06 - val_loss: 4.7255e-07\n",
      "Epoch 20/100\n",
      "2811/2811 [==============================] - 1s 530us/step - loss: 8.0683e-06 - val_loss: 1.5001e-07\n",
      "Epoch 21/100\n",
      "2811/2811 [==============================] - 1s 528us/step - loss: 3.9282e-06 - val_loss: 8.7598e-07\n",
      "Epoch 22/100\n",
      "2811/2811 [==============================] - 1s 525us/step - loss: 7.6830e-06 - val_loss: 1.8842e-07\n",
      "Epoch 23/100\n",
      "2811/2811 [==============================] - 1s 528us/step - loss: 4.0459e-06 - val_loss: 1.1562e-06\n",
      "Epoch 24/100\n",
      "2811/2811 [==============================] - 1s 529us/step - loss: 7.4058e-06 - val_loss: 9.0845e-08\n",
      "Epoch 25/100\n",
      "2811/2811 [==============================] - 1s 531us/step - loss: 4.6351e-06 - val_loss: 1.1058e-05\n",
      "Epoch 26/100\n",
      "2811/2811 [==============================] - 1s 511us/step - loss: 7.7439e-06 - val_loss: 2.0704e-07\n",
      "Epoch 27/100\n",
      "2811/2811 [==============================] - 1s 518us/step - loss: 3.2321e-06 - val_loss: 1.6527e-05\n",
      "Epoch 28/100\n",
      "2811/2811 [==============================] - 1s 516us/step - loss: 7.4526e-06 - val_loss: 1.7813e-07\n",
      "Epoch 29/100\n",
      "2811/2811 [==============================] - 2s 534us/step - loss: 7.8490e-06 - val_loss: 4.2060e-07\n"
     ]
    }
   ],
   "source": [
    "# find optimal number of epochs using early stopping\n",
    "\n",
    "# define base model with default Huber loss delta = 1.0\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for steering angle prediction\n",
    "])\n",
    "\n",
    "model.compile(loss=Huber(delta=1.0), optimizer=Adam(learning_rate=0.001), weighted_metrics=[])\n",
    "\n",
    "# define early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=100, batch_size=32, validation_data = (Xval, yval, wval), callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "2811/2811 [==============================] - 3s 1ms/step - loss: 0.0024 - mae: 0.0114 - mse: 0.0042 - val_loss: 9.9738e-06 - val_mae: 0.0030 - val_mse: 1.5718e-05\n",
      "Epoch 2/28\n",
      "2811/2811 [==============================] - 2s 552us/step - loss: 4.5690e-05 - mae: 0.0037 - mse: 5.4071e-05 - val_loss: 8.8581e-05 - val_mae: 0.0073 - val_mse: 9.2983e-05\n",
      "Epoch 3/28\n",
      "2811/2811 [==============================] - 2s 561us/step - loss: 4.0564e-05 - mae: 0.0039 - mse: 4.8368e-05 - val_loss: 2.5715e-06 - val_mae: 0.0014 - val_mse: 3.6749e-06\n",
      "Epoch 4/28\n",
      "2811/2811 [==============================] - 2s 545us/step - loss: 3.4681e-05 - mae: 0.0036 - mse: 4.0779e-05 - val_loss: 4.7788e-05 - val_mae: 0.0056 - val_mse: 5.0012e-05\n",
      "Epoch 5/28\n",
      "2811/2811 [==============================] - 2s 550us/step - loss: 3.8174e-05 - mae: 0.0034 - mse: 4.4724e-05 - val_loss: 5.6011e-06 - val_mae: 0.0019 - val_mse: 7.6133e-06\n",
      "Epoch 6/28\n",
      "2811/2811 [==============================] - 2s 540us/step - loss: 1.4704e-05 - mae: 0.0023 - mse: 1.7774e-05 - val_loss: 5.2897e-06 - val_mae: 0.0016 - val_mse: 5.6848e-06\n",
      "Epoch 7/28\n",
      "2811/2811 [==============================] - 2s 553us/step - loss: 1.5469e-05 - mae: 0.0027 - mse: 1.8544e-05 - val_loss: 1.2242e-05 - val_mae: 0.0028 - val_mse: 1.6352e-05\n",
      "Epoch 8/28\n",
      "2811/2811 [==============================] - 2s 546us/step - loss: 1.2857e-05 - mae: 0.0022 - mse: 1.5681e-05 - val_loss: 6.3416e-06 - val_mae: 0.0020 - val_mse: 7.1550e-06\n",
      "Epoch 9/28\n",
      "2811/2811 [==============================] - 2s 557us/step - loss: 1.1887e-05 - mae: 0.0022 - mse: 1.4541e-05 - val_loss: 2.4750e-06 - val_mae: 0.0015 - val_mse: 3.5449e-06\n",
      "Epoch 10/28\n",
      "2811/2811 [==============================] - 2s 615us/step - loss: 9.5328e-06 - mae: 0.0018 - mse: 1.1374e-05 - val_loss: 1.0131e-04 - val_mae: 0.0089 - val_mse: 1.3789e-04\n",
      "Epoch 11/28\n",
      "2811/2811 [==============================] - 2s 665us/step - loss: 1.0087e-05 - mae: 0.0021 - mse: 1.2577e-05 - val_loss: 1.0252e-04 - val_mae: 0.0071 - val_mse: 1.5222e-04\n",
      "Epoch 12/28\n",
      "2811/2811 [==============================] - 2s 589us/step - loss: 1.0050e-05 - mae: 0.0018 - mse: 1.2456e-05 - val_loss: 2.9584e-05 - val_mae: 0.0034 - val_mse: 3.8101e-05\n",
      "Epoch 13/28\n",
      "2811/2811 [==============================] - 2s 573us/step - loss: 1.0718e-05 - mae: 0.0016 - mse: 1.4237e-05 - val_loss: 4.9759e-07 - val_mae: 6.9398e-04 - val_mse: 8.3589e-07\n",
      "Epoch 14/28\n",
      "2811/2811 [==============================] - 2s 594us/step - loss: 8.4295e-06 - mae: 0.0017 - mse: 1.0414e-05 - val_loss: 2.5534e-06 - val_mae: 0.0012 - val_mse: 2.8400e-06\n",
      "Epoch 15/28\n",
      "2811/2811 [==============================] - 2s 577us/step - loss: 4.9136e-06 - mae: 0.0014 - mse: 6.2356e-06 - val_loss: 2.4181e-07 - val_mae: 4.2480e-04 - val_mse: 3.7878e-07\n",
      "Epoch 16/28\n",
      "2811/2811 [==============================] - 2s 568us/step - loss: 1.0095e-05 - mae: 0.0016 - mse: 1.3089e-05 - val_loss: 9.8168e-07 - val_mae: 8.0142e-04 - val_mse: 1.4924e-06\n",
      "Epoch 17/28\n",
      "2811/2811 [==============================] - 2s 563us/step - loss: 8.3192e-06 - mae: 0.0014 - mse: 1.0887e-05 - val_loss: 1.2141e-06 - val_mae: 9.5558e-04 - val_mse: 1.6824e-06\n",
      "Epoch 18/28\n",
      "2811/2811 [==============================] - 2s 552us/step - loss: 4.2822e-06 - mae: 0.0012 - mse: 5.4352e-06 - val_loss: 1.0017e-06 - val_mae: 9.3505e-04 - val_mse: 1.3324e-06\n",
      "Epoch 19/28\n",
      "2811/2811 [==============================] - 2s 551us/step - loss: 9.8760e-06 - mae: 0.0014 - mse: 1.2411e-05 - val_loss: 2.8228e-07 - val_mae: 4.6501e-04 - val_mse: 3.5809e-07\n",
      "Epoch 20/28\n",
      "2811/2811 [==============================] - 2s 652us/step - loss: 1.0313e-05 - mae: 0.0012 - mse: 1.3334e-05 - val_loss: 1.0826e-06 - val_mae: 8.2176e-04 - val_mse: 1.4963e-06\n",
      "Epoch 21/28\n",
      "2811/2811 [==============================] - 2s 561us/step - loss: 4.9714e-06 - mae: 0.0011 - mse: 6.3380e-06 - val_loss: 3.7953e-06 - val_mae: 0.0016 - val_mse: 4.5534e-06\n",
      "Epoch 22/28\n",
      "2811/2811 [==============================] - 2s 554us/step - loss: 6.8623e-06 - mae: 0.0013 - mse: 9.1736e-06 - val_loss: 1.4726e-07 - val_mae: 3.3042e-04 - val_mse: 2.2175e-07\n",
      "Epoch 23/28\n",
      "2811/2811 [==============================] - 2s 557us/step - loss: 7.2492e-06 - mae: 9.7834e-04 - mse: 9.2295e-06 - val_loss: 4.0248e-07 - val_mae: 4.6509e-04 - val_mse: 5.3604e-07\n",
      "Epoch 24/28\n",
      "2811/2811 [==============================] - 2s 551us/step - loss: 1.0360e-05 - mae: 0.0013 - mse: 1.4162e-05 - val_loss: 1.5657e-07 - val_mae: 3.2205e-04 - val_mse: 1.9332e-07\n",
      "Epoch 25/28\n",
      "2811/2811 [==============================] - 2s 574us/step - loss: 3.2606e-06 - mae: 8.7346e-04 - mse: 3.8400e-06 - val_loss: 2.1782e-07 - val_mae: 4.7558e-04 - val_mse: 3.2337e-07\n",
      "Epoch 26/28\n",
      "2811/2811 [==============================] - 2s 558us/step - loss: 5.2275e-06 - mae: 0.0010 - mse: 6.3711e-06 - val_loss: 1.3417e-07 - val_mae: 3.1966e-04 - val_mse: 1.7417e-07\n",
      "Epoch 27/28\n",
      "2811/2811 [==============================] - 2s 591us/step - loss: 4.2992e-06 - mae: 0.0010 - mse: 6.0320e-06 - val_loss: 2.8820e-07 - val_mae: 4.6481e-04 - val_mse: 3.4643e-07\n",
      "Epoch 28/28\n",
      "2811/2811 [==============================] - 2s 569us/step - loss: 4.4294e-06 - mae: 0.0011 - mse: 5.3839e-06 - val_loss: 1.4439e-07 - val_mae: 3.9393e-04 - val_mse: 2.4598e-07\n",
      "Iteration 1: delta = 0.5, val_mae = 0.00039392506005242467, val_mse = 2.459757979522692e-07\n",
      "Epoch 1/28\n",
      "2811/2811 [==============================] - 3s 1ms/step - loss: 0.0037 - mae: 0.0121 - mse: 0.0070 - val_loss: 1.1830e-05 - val_mae: 0.0026 - val_mse: 1.3383e-05\n",
      "Epoch 2/28\n",
      "2811/2811 [==============================] - 2s 557us/step - loss: 2.7517e-05 - mae: 0.0034 - mse: 3.3312e-05 - val_loss: 1.3147e-05 - val_mae: 0.0027 - val_mse: 1.3982e-05\n",
      "Epoch 3/28\n",
      "2811/2811 [==============================] - 2s 559us/step - loss: 4.3459e-05 - mae: 0.0034 - mse: 5.2184e-05 - val_loss: 1.1680e-05 - val_mae: 0.0028 - val_mse: 1.3097e-05\n",
      "Epoch 4/28\n",
      "2811/2811 [==============================] - 2s 564us/step - loss: 2.9508e-05 - mae: 0.0031 - mse: 3.6039e-05 - val_loss: 3.3689e-05 - val_mae: 0.0044 - val_mse: 4.1861e-05\n",
      "Epoch 5/28\n",
      "2811/2811 [==============================] - 2s 558us/step - loss: 2.6675e-05 - mae: 0.0031 - mse: 3.2217e-05 - val_loss: 8.8504e-06 - val_mae: 0.0023 - val_mse: 9.0504e-06\n",
      "Epoch 6/28\n",
      "2811/2811 [==============================] - 2s 570us/step - loss: 2.3546e-05 - mae: 0.0029 - mse: 2.8439e-05 - val_loss: 2.1686e-06 - val_mae: 0.0011 - val_mse: 3.3094e-06\n",
      "Epoch 7/28\n",
      "2811/2811 [==============================] - 2s 656us/step - loss: 1.5101e-05 - mae: 0.0024 - mse: 1.9305e-05 - val_loss: 3.7265e-05 - val_mae: 0.0054 - val_mse: 4.8334e-05\n",
      "Epoch 8/28\n",
      "2811/2811 [==============================] - 2s 572us/step - loss: 1.2467e-05 - mae: 0.0022 - mse: 1.5214e-05 - val_loss: 1.9539e-05 - val_mae: 0.0036 - val_mse: 2.0862e-05\n",
      "Epoch 9/28\n",
      "2811/2811 [==============================] - 2s 575us/step - loss: 1.4141e-05 - mae: 0.0020 - mse: 1.8156e-05 - val_loss: 1.0628e-05 - val_mae: 0.0023 - val_mse: 1.2446e-05\n",
      "Epoch 10/28\n",
      "2811/2811 [==============================] - 2s 586us/step - loss: 9.7357e-06 - mae: 0.0019 - mse: 1.2281e-05 - val_loss: 5.9318e-06 - val_mae: 0.0017 - val_mse: 6.6852e-06\n",
      "Epoch 11/28\n",
      "2811/2811 [==============================] - 2s 677us/step - loss: 1.1469e-05 - mae: 0.0018 - mse: 1.4976e-05 - val_loss: 6.3535e-07 - val_mae: 7.0873e-04 - val_mse: 1.0628e-06\n",
      "Epoch 12/28\n",
      "2811/2811 [==============================] - 2s 603us/step - loss: 9.8457e-06 - mae: 0.0017 - mse: 1.3369e-05 - val_loss: 1.4872e-06 - val_mae: 9.3750e-04 - val_mse: 1.7246e-06\n",
      "Epoch 13/28\n",
      "2811/2811 [==============================] - 4s 1ms/step - loss: 7.1025e-06 - mae: 0.0013 - mse: 9.0083e-06 - val_loss: 1.1090e-06 - val_mae: 8.9645e-04 - val_mse: 1.2724e-06\n",
      "Epoch 14/28\n",
      "2811/2811 [==============================] - 2s 749us/step - loss: 5.2724e-06 - mae: 0.0013 - mse: 6.5422e-06 - val_loss: 2.9080e-05 - val_mae: 0.0052 - val_mse: 3.9826e-05\n",
      "Epoch 15/28\n",
      "2811/2811 [==============================] - 2s 621us/step - loss: 6.8658e-06 - mae: 0.0013 - mse: 8.2018e-06 - val_loss: 1.2076e-06 - val_mae: 8.2057e-04 - val_mse: 1.4568e-06\n",
      "Epoch 16/28\n",
      "2811/2811 [==============================] - 2s 616us/step - loss: 1.1686e-05 - mae: 0.0012 - mse: 1.5244e-05 - val_loss: 4.3239e-07 - val_mae: 5.2845e-04 - val_mse: 4.7564e-07\n",
      "Epoch 17/28\n",
      "2811/2811 [==============================] - 2s 627us/step - loss: 5.2804e-06 - mae: 0.0012 - mse: 6.0639e-06 - val_loss: 2.6418e-06 - val_mae: 0.0012 - val_mse: 2.4696e-06\n",
      "Epoch 18/28\n",
      "2811/2811 [==============================] - 2s 578us/step - loss: 8.8287e-06 - mae: 0.0012 - mse: 1.1594e-05 - val_loss: 3.2978e-06 - val_mae: 0.0011 - val_mse: 4.7544e-06\n",
      "Epoch 19/28\n",
      "2811/2811 [==============================] - 2s 554us/step - loss: 6.0168e-06 - mae: 0.0010 - mse: 7.8311e-06 - val_loss: 5.5547e-06 - val_mae: 0.0021 - val_mse: 5.8510e-06\n",
      "Epoch 20/28\n",
      "2811/2811 [==============================] - 2s 554us/step - loss: 6.4197e-06 - mae: 0.0012 - mse: 7.5791e-06 - val_loss: 1.6471e-07 - val_mae: 3.4094e-04 - val_mse: 2.0395e-07\n",
      "Epoch 21/28\n",
      "2811/2811 [==============================] - 2s 549us/step - loss: 6.5409e-06 - mae: 0.0011 - mse: 8.8419e-06 - val_loss: 3.2598e-06 - val_mae: 0.0012 - val_mse: 2.8683e-06\n",
      "Epoch 22/28\n",
      "2811/2811 [==============================] - 2s 550us/step - loss: 5.8531e-06 - mae: 0.0013 - mse: 7.2144e-06 - val_loss: 1.7369e-07 - val_mae: 3.9027e-04 - val_mse: 2.4468e-07\n",
      "Epoch 23/28\n",
      "2811/2811 [==============================] - 2s 554us/step - loss: 4.7997e-06 - mae: 0.0012 - mse: 6.3298e-06 - val_loss: 6.3818e-06 - val_mae: 0.0023 - val_mse: 7.3158e-06\n",
      "Epoch 24/28\n",
      "2811/2811 [==============================] - 2s 566us/step - loss: 7.4521e-06 - mae: 0.0011 - mse: 9.0525e-06 - val_loss: 1.3376e-04 - val_mae: 0.0117 - val_mse: 2.3573e-04\n",
      "Epoch 25/28\n",
      "2811/2811 [==============================] - 2s 534us/step - loss: 6.6568e-06 - mae: 0.0011 - mse: 8.9338e-06 - val_loss: 6.6892e-08 - val_mae: 2.1588e-04 - val_mse: 1.2027e-07\n",
      "Epoch 26/28\n",
      "2811/2811 [==============================] - 2s 540us/step - loss: 4.2855e-06 - mae: 9.8492e-04 - mse: 5.3918e-06 - val_loss: 1.0034e-05 - val_mae: 0.0025 - val_mse: 1.4021e-05\n",
      "Epoch 27/28\n",
      "2811/2811 [==============================] - 2s 550us/step - loss: 4.3796e-06 - mae: 9.3864e-04 - mse: 5.8161e-06 - val_loss: 2.2021e-05 - val_mae: 0.0037 - val_mse: 3.0437e-05\n",
      "Epoch 28/28\n",
      "2811/2811 [==============================] - 2s 556us/step - loss: 4.9353e-06 - mae: 0.0011 - mse: 6.5345e-06 - val_loss: 9.7414e-06 - val_mae: 0.0021 - val_mse: 8.6783e-06\n",
      "Iteration 2: delta = 0.6, val_mae = 0.002068557310849428, val_mse = 8.678250196680892e-06\n",
      "Epoch 1/28\n",
      "2811/2811 [==============================] - 3s 1ms/step - loss: 0.0034 - mae: 0.0109 - mse: 0.0059 - val_loss: 7.3521e-06 - val_mae: 0.0023 - val_mse: 8.5689e-06\n",
      "Epoch 2/28\n",
      "2811/2811 [==============================] - 2s 555us/step - loss: 4.4077e-05 - mae: 0.0035 - mse: 5.1118e-05 - val_loss: 5.0960e-06 - val_mae: 0.0021 - val_mse: 8.0534e-06\n",
      "Epoch 3/28\n",
      "2811/2811 [==============================] - 2s 557us/step - loss: 2.5820e-05 - mae: 0.0032 - mse: 3.0976e-05 - val_loss: 2.3948e-05 - val_mae: 0.0037 - val_mse: 2.5367e-05\n",
      "Epoch 4/28\n",
      "2811/2811 [==============================] - 2s 560us/step - loss: 2.6587e-05 - mae: 0.0031 - mse: 3.1587e-05 - val_loss: 7.7900e-06 - val_mae: 0.0020 - val_mse: 1.0040e-05\n",
      "Epoch 5/28\n",
      "2811/2811 [==============================] - 2s 546us/step - loss: 2.0196e-05 - mae: 0.0028 - mse: 2.2878e-05 - val_loss: 3.2336e-05 - val_mae: 0.0046 - val_mse: 3.9270e-05\n",
      "Epoch 6/28\n",
      "2811/2811 [==============================] - 2s 553us/step - loss: 1.8115e-05 - mae: 0.0027 - mse: 2.2436e-05 - val_loss: 2.0666e-06 - val_mae: 0.0010 - val_mse: 2.5814e-06\n",
      "Epoch 7/28\n",
      "2811/2811 [==============================] - 2s 561us/step - loss: 1.4055e-05 - mae: 0.0023 - mse: 1.6776e-05 - val_loss: 5.1413e-06 - val_mae: 0.0016 - val_mse: 5.5947e-06\n",
      "Epoch 8/28\n",
      "2811/2811 [==============================] - 2s 553us/step - loss: 1.4497e-05 - mae: 0.0023 - mse: 1.7527e-05 - val_loss: 3.1660e-05 - val_mae: 0.0044 - val_mse: 3.4380e-05\n",
      "Epoch 9/28\n",
      "2811/2811 [==============================] - 2s 567us/step - loss: 1.1388e-05 - mae: 0.0021 - mse: 1.3972e-05 - val_loss: 7.5932e-05 - val_mae: 0.0067 - val_mse: 1.1453e-04\n",
      "Epoch 10/28\n",
      "2811/2811 [==============================] - 2s 555us/step - loss: 8.8219e-06 - mae: 0.0018 - mse: 1.1168e-05 - val_loss: 6.0369e-07 - val_mae: 5.7411e-04 - val_mse: 8.2228e-07\n",
      "Epoch 11/28\n",
      "2811/2811 [==============================] - 2s 536us/step - loss: 1.0379e-05 - mae: 0.0018 - mse: 1.2992e-05 - val_loss: 1.6660e-05 - val_mae: 0.0025 - val_mse: 1.7243e-05\n",
      "Epoch 12/28\n",
      "2811/2811 [==============================] - 2s 552us/step - loss: 1.2003e-05 - mae: 0.0018 - mse: 1.4901e-05 - val_loss: 1.0197e-06 - val_mae: 7.7756e-04 - val_mse: 1.1422e-06\n",
      "Epoch 13/28\n",
      "2811/2811 [==============================] - 2s 555us/step - loss: 1.0432e-05 - mae: 0.0014 - mse: 1.3708e-05 - val_loss: 4.0198e-07 - val_mae: 6.1871e-04 - val_mse: 6.6026e-07\n",
      "Epoch 14/28\n",
      "2811/2811 [==============================] - 2s 555us/step - loss: 7.3310e-06 - mae: 0.0012 - mse: 8.2335e-06 - val_loss: 3.4840e-06 - val_mae: 0.0015 - val_mse: 4.8203e-06\n",
      "Epoch 15/28\n",
      "2811/2811 [==============================] - 2s 544us/step - loss: 1.3119e-05 - mae: 0.0016 - mse: 1.5333e-05 - val_loss: 2.0487e-07 - val_mae: 4.2945e-04 - val_mse: 3.0954e-07\n",
      "Epoch 16/28\n",
      "2811/2811 [==============================] - 2s 535us/step - loss: 5.1342e-06 - mae: 0.0012 - mse: 6.8186e-06 - val_loss: 1.1996e-06 - val_mae: 7.8154e-04 - val_mse: 1.1471e-06\n",
      "Epoch 17/28\n",
      "2811/2811 [==============================] - 2s 540us/step - loss: 8.4178e-06 - mae: 0.0014 - mse: 1.0233e-05 - val_loss: 2.9372e-07 - val_mae: 4.4116e-04 - val_mse: 3.8173e-07\n",
      "Epoch 18/28\n",
      "2811/2811 [==============================] - 2s 544us/step - loss: 1.6172e-05 - mae: 0.0016 - mse: 2.0432e-05 - val_loss: 3.2289e-06 - val_mae: 0.0017 - val_mse: 4.6294e-06\n",
      "Epoch 19/28\n",
      "2811/2811 [==============================] - 2s 554us/step - loss: 3.2847e-06 - mae: 9.1166e-04 - mse: 3.8253e-06 - val_loss: 9.8336e-07 - val_mae: 8.6710e-04 - val_mse: 1.3241e-06\n",
      "Epoch 20/28\n",
      "2811/2811 [==============================] - 2s 562us/step - loss: 5.5427e-06 - mae: 0.0011 - mse: 6.7180e-06 - val_loss: 9.9449e-06 - val_mae: 0.0027 - val_mse: 1.2735e-05\n",
      "Epoch 21/28\n",
      "2811/2811 [==============================] - 2s 567us/step - loss: 8.0413e-06 - mae: 0.0013 - mse: 9.9433e-06 - val_loss: 2.5333e-07 - val_mae: 4.3363e-04 - val_mse: 4.2862e-07\n",
      "Epoch 22/28\n",
      "2811/2811 [==============================] - 2s 653us/step - loss: 5.6133e-06 - mae: 0.0010 - mse: 6.8334e-06 - val_loss: 6.2228e-08 - val_mae: 1.8788e-04 - val_mse: 1.1022e-07\n",
      "Epoch 23/28\n",
      "2811/2811 [==============================] - 2s 583us/step - loss: 6.2404e-06 - mae: 0.0011 - mse: 7.9499e-06 - val_loss: 3.8336e-07 - val_mae: 5.3156e-04 - val_mse: 4.7671e-07\n",
      "Epoch 24/28\n",
      "2811/2811 [==============================] - 2s 581us/step - loss: 4.3077e-06 - mae: 0.0010 - mse: 5.4103e-06 - val_loss: 1.6432e-06 - val_mae: 0.0010 - val_mse: 1.7892e-06\n",
      "Epoch 25/28\n",
      "2811/2811 [==============================] - 2s 595us/step - loss: 5.4807e-06 - mae: 0.0011 - mse: 6.8700e-06 - val_loss: 2.2550e-07 - val_mae: 4.3597e-04 - val_mse: 2.9217e-07\n",
      "Epoch 26/28\n",
      "2811/2811 [==============================] - 2s 620us/step - loss: 5.4015e-06 - mae: 9.5333e-04 - mse: 6.6463e-06 - val_loss: 7.9767e-06 - val_mae: 0.0020 - val_mse: 7.5334e-06\n",
      "Epoch 27/28\n",
      "2811/2811 [==============================] - 2s 585us/step - loss: 4.6452e-06 - mae: 0.0010 - mse: 5.1717e-06 - val_loss: 1.5714e-06 - val_mae: 9.2274e-04 - val_mse: 2.3216e-06\n",
      "Epoch 28/28\n",
      "2811/2811 [==============================] - 2s 553us/step - loss: 4.4254e-06 - mae: 9.5061e-04 - mse: 5.8316e-06 - val_loss: 4.4676e-07 - val_mae: 4.2473e-04 - val_mse: 4.2655e-07\n",
      "Iteration 3: delta = 0.7, val_mae = 0.0004247338220011443, val_mse = 4.265510824552621e-07\n",
      "Epoch 1/28\n",
      "2811/2811 [==============================] - 3s 1ms/step - loss: 0.0047 - mae: 0.0139 - mse: 0.0075 - val_loss: 6.1867e-06 - val_mae: 0.0021 - val_mse: 9.4608e-06\n",
      "Epoch 2/28\n",
      "2811/2811 [==============================] - 2s 640us/step - loss: 6.3088e-05 - mae: 0.0050 - mse: 7.3586e-05 - val_loss: 1.2449e-04 - val_mae: 0.0107 - val_mse: 1.4421e-04\n",
      "Epoch 3/28\n",
      "2811/2811 [==============================] - 2s 607us/step - loss: 4.8731e-05 - mae: 0.0042 - mse: 5.6356e-05 - val_loss: 5.1336e-06 - val_mae: 0.0018 - val_mse: 7.0746e-06\n",
      "Epoch 4/28\n",
      "2811/2811 [==============================] - 2s 601us/step - loss: 5.0058e-05 - mae: 0.0043 - mse: 5.8468e-05 - val_loss: 7.6482e-06 - val_mae: 0.0021 - val_mse: 1.0151e-05\n",
      "Epoch 5/28\n",
      "2811/2811 [==============================] - 2s 665us/step - loss: 3.7728e-05 - mae: 0.0038 - mse: 4.3071e-05 - val_loss: 5.3815e-06 - val_mae: 0.0019 - val_mse: 6.5180e-06\n",
      "Epoch 6/28\n",
      "2811/2811 [==============================] - 2s 620us/step - loss: 2.7976e-05 - mae: 0.0033 - mse: 3.3413e-05 - val_loss: 1.8955e-05 - val_mae: 0.0036 - val_mse: 2.4611e-05\n",
      "Epoch 7/28\n",
      "2811/2811 [==============================] - 2s 562us/step - loss: 3.3059e-05 - mae: 0.0033 - mse: 4.2410e-05 - val_loss: 2.2701e-06 - val_mae: 0.0013 - val_mse: 2.9129e-06\n",
      "Epoch 8/28\n",
      "2811/2811 [==============================] - 2s 585us/step - loss: 1.6763e-05 - mae: 0.0025 - mse: 2.0820e-05 - val_loss: 3.8219e-06 - val_mae: 0.0016 - val_mse: 4.2481e-06\n",
      "Epoch 9/28\n",
      "2811/2811 [==============================] - 2s 596us/step - loss: 1.7755e-05 - mae: 0.0026 - mse: 2.1847e-05 - val_loss: 5.0056e-06 - val_mae: 0.0023 - val_mse: 8.2631e-06\n",
      "Epoch 10/28\n",
      "2811/2811 [==============================] - 2s 696us/step - loss: 2.4207e-05 - mae: 0.0025 - mse: 3.0835e-05 - val_loss: 4.1823e-06 - val_mae: 0.0017 - val_mse: 5.7992e-06\n",
      "Epoch 11/28\n",
      "2811/2811 [==============================] - 2s 670us/step - loss: 1.2476e-05 - mae: 0.0018 - mse: 1.6364e-05 - val_loss: 1.9496e-06 - val_mae: 0.0010 - val_mse: 2.5071e-06\n",
      "Epoch 12/28\n",
      "2811/2811 [==============================] - 2s 591us/step - loss: 1.1127e-05 - mae: 0.0018 - mse: 1.3452e-05 - val_loss: 1.5108e-06 - val_mae: 9.2220e-04 - val_mse: 2.0100e-06\n",
      "Epoch 13/28\n",
      "2811/2811 [==============================] - 2s 677us/step - loss: 1.7545e-05 - mae: 0.0022 - mse: 2.1463e-05 - val_loss: 2.6101e-06 - val_mae: 0.0014 - val_mse: 3.8429e-06\n",
      "Epoch 14/28\n",
      "2811/2811 [==============================] - 2s 609us/step - loss: 5.7063e-06 - mae: 0.0015 - mse: 7.3932e-06 - val_loss: 1.1395e-06 - val_mae: 9.0357e-04 - val_mse: 1.6416e-06\n",
      "Epoch 15/28\n",
      "2811/2811 [==============================] - 2s 586us/step - loss: 7.7835e-06 - mae: 0.0018 - mse: 9.5860e-06 - val_loss: 2.4202e-06 - val_mae: 0.0011 - val_mse: 3.2034e-06\n",
      "Epoch 16/28\n",
      "2811/2811 [==============================] - 2s 701us/step - loss: 6.6692e-06 - mae: 0.0016 - mse: 8.2461e-06 - val_loss: 7.6115e-07 - val_mae: 8.2794e-04 - val_mse: 1.2013e-06\n",
      "Epoch 17/28\n",
      "2811/2811 [==============================] - 2s 720us/step - loss: 6.6811e-06 - mae: 0.0015 - mse: 8.3921e-06 - val_loss: 2.6321e-06 - val_mae: 0.0011 - val_mse: 3.7235e-06\n",
      "Epoch 18/28\n",
      "2811/2811 [==============================] - 2s 636us/step - loss: 6.0904e-06 - mae: 0.0016 - mse: 7.6787e-06 - val_loss: 2.4693e-06 - val_mae: 0.0013 - val_mse: 3.1963e-06\n",
      "Epoch 19/28\n",
      "2811/2811 [==============================] - 2s 619us/step - loss: 7.3763e-06 - mae: 0.0016 - mse: 9.4802e-06 - val_loss: 9.5916e-07 - val_mae: 9.1480e-04 - val_mse: 1.4944e-06\n",
      "Epoch 20/28\n",
      "2811/2811 [==============================] - 2s 576us/step - loss: 6.0933e-06 - mae: 0.0012 - mse: 8.2267e-06 - val_loss: 1.4043e-06 - val_mae: 0.0011 - val_mse: 1.9100e-06\n",
      "Epoch 21/28\n",
      "2811/2811 [==============================] - 2s 599us/step - loss: 5.3532e-06 - mae: 0.0014 - mse: 6.6930e-06 - val_loss: 2.2493e-06 - val_mae: 0.0011 - val_mse: 2.5610e-06\n",
      "Epoch 22/28\n",
      "2811/2811 [==============================] - 2s 535us/step - loss: 6.7754e-06 - mae: 0.0013 - mse: 9.1861e-06 - val_loss: 2.8861e-07 - val_mae: 4.6842e-04 - val_mse: 3.7531e-07\n",
      "Epoch 23/28\n",
      "2811/2811 [==============================] - 2s 546us/step - loss: 6.7475e-06 - mae: 0.0011 - mse: 8.5730e-06 - val_loss: 6.1960e-05 - val_mae: 0.0054 - val_mse: 8.6757e-05\n",
      "Epoch 24/28\n",
      "2811/2811 [==============================] - 2s 554us/step - loss: 6.4718e-06 - mae: 0.0012 - mse: 8.4404e-06 - val_loss: 1.0012e-06 - val_mae: 7.7760e-04 - val_mse: 1.0086e-06\n",
      "Epoch 25/28\n",
      "2811/2811 [==============================] - 2s 535us/step - loss: 6.8943e-06 - mae: 0.0012 - mse: 8.9395e-06 - val_loss: 4.3933e-07 - val_mae: 5.4072e-04 - val_mse: 5.4248e-07\n",
      "Epoch 26/28\n",
      "2811/2811 [==============================] - 2s 549us/step - loss: 6.2771e-06 - mae: 0.0011 - mse: 8.2855e-06 - val_loss: 4.9787e-07 - val_mae: 5.3620e-04 - val_mse: 5.5708e-07\n",
      "Epoch 27/28\n",
      "2811/2811 [==============================] - 2s 621us/step - loss: 6.7940e-06 - mae: 0.0011 - mse: 8.7270e-06 - val_loss: 3.1093e-07 - val_mae: 5.2727e-04 - val_mse: 4.3273e-07\n",
      "Epoch 28/28\n",
      "2811/2811 [==============================] - 2s 660us/step - loss: 4.2140e-06 - mae: 8.9724e-04 - mse: 5.5333e-06 - val_loss: 3.8624e-06 - val_mae: 0.0017 - val_mse: 5.7593e-06\n",
      "Iteration 4: delta = 0.7999999999999999, val_mae = 0.0016946903197094798, val_mse = 5.759323812526418e-06\n",
      "Epoch 1/28\n",
      "2811/2811 [==============================] - 4s 1ms/step - loss: 0.0041 - mae: 0.0110 - mse: 0.0052 - val_loss: 8.1181e-06 - val_mae: 0.0024 - val_mse: 1.0625e-05\n",
      "Epoch 2/28\n",
      "2811/2811 [==============================] - 2s 630us/step - loss: 3.1032e-05 - mae: 0.0030 - mse: 3.7397e-05 - val_loss: 6.0008e-06 - val_mae: 0.0018 - val_mse: 7.7907e-06\n",
      "Epoch 3/28\n",
      "2811/2811 [==============================] - 2s 844us/step - loss: 4.8659e-05 - mae: 0.0041 - mse: 5.7096e-05 - val_loss: 3.7715e-05 - val_mae: 0.0045 - val_mse: 4.3836e-05\n",
      "Epoch 4/28\n",
      "2811/2811 [==============================] - 2s 789us/step - loss: 3.2883e-05 - mae: 0.0035 - mse: 3.8892e-05 - val_loss: 1.6052e-05 - val_mae: 0.0030 - val_mse: 2.0135e-05\n",
      "Epoch 5/28\n",
      "2811/2811 [==============================] - 2s 655us/step - loss: 3.0564e-05 - mae: 0.0036 - mse: 3.7381e-05 - val_loss: 1.0615e-05 - val_mae: 0.0030 - val_mse: 1.3516e-05\n",
      "Epoch 6/28\n",
      "2811/2811 [==============================] - 2s 643us/step - loss: 3.6642e-05 - mae: 0.0034 - mse: 4.4866e-05 - val_loss: 7.1012e-06 - val_mae: 0.0018 - val_mse: 1.0173e-05\n",
      "Epoch 7/28\n",
      "2811/2811 [==============================] - 2s 653us/step - loss: 2.1392e-05 - mae: 0.0021 - mse: 2.9047e-05 - val_loss: 1.9405e-06 - val_mae: 0.0012 - val_mse: 2.5065e-06\n",
      "Epoch 8/28\n",
      "2811/2811 [==============================] - 2s 618us/step - loss: 1.6788e-05 - mae: 0.0026 - mse: 2.0059e-05 - val_loss: 5.1956e-05 - val_mae: 0.0049 - val_mse: 5.2169e-05\n",
      "Epoch 9/28\n",
      "2811/2811 [==============================] - 2s 687us/step - loss: 1.6222e-05 - mae: 0.0021 - mse: 1.9512e-05 - val_loss: 6.9862e-07 - val_mae: 6.2841e-04 - val_mse: 1.0404e-06\n",
      "Epoch 10/28\n",
      "2811/2811 [==============================] - 2s 585us/step - loss: 1.6478e-05 - mae: 0.0024 - mse: 2.1879e-05 - val_loss: 7.5713e-07 - val_mae: 7.6218e-04 - val_mse: 1.1306e-06\n",
      "Epoch 11/28\n",
      "2811/2811 [==============================] - 2s 602us/step - loss: 1.1502e-05 - mae: 0.0019 - mse: 1.4277e-05 - val_loss: 6.5245e-07 - val_mae: 6.7338e-04 - val_mse: 9.7594e-07\n",
      "Epoch 12/28\n",
      "2811/2811 [==============================] - 2s 670us/step - loss: 1.0168e-05 - mae: 0.0018 - mse: 1.2345e-05 - val_loss: 8.8845e-06 - val_mae: 0.0024 - val_mse: 1.0928e-05\n",
      "Epoch 13/28\n",
      "2811/2811 [==============================] - 2s 556us/step - loss: 9.8164e-06 - mae: 0.0016 - mse: 1.2713e-05 - val_loss: 4.7143e-07 - val_mae: 6.0642e-04 - val_mse: 6.6553e-07\n",
      "Epoch 14/28\n",
      "2811/2811 [==============================] - 2s 551us/step - loss: 9.8180e-06 - mae: 0.0016 - mse: 1.1978e-05 - val_loss: 7.5923e-07 - val_mae: 7.2557e-04 - val_mse: 1.0623e-06\n",
      "Epoch 15/28\n",
      "2811/2811 [==============================] - 2s 546us/step - loss: 7.3906e-06 - mae: 0.0015 - mse: 9.3973e-06 - val_loss: 7.3185e-07 - val_mae: 8.0631e-04 - val_mse: 1.1289e-06\n",
      "Epoch 16/28\n",
      "2811/2811 [==============================] - 2s 580us/step - loss: 7.1834e-06 - mae: 0.0015 - mse: 8.7646e-06 - val_loss: 1.6238e-06 - val_mae: 7.0330e-04 - val_mse: 1.9421e-06\n",
      "Epoch 17/28\n",
      "2811/2811 [==============================] - 2s 650us/step - loss: 1.0290e-05 - mae: 0.0015 - mse: 1.3370e-05 - val_loss: 6.1733e-07 - val_mae: 5.3156e-04 - val_mse: 8.6605e-07\n",
      "Epoch 18/28\n",
      "2811/2811 [==============================] - 2s 569us/step - loss: 5.9208e-06 - mae: 0.0013 - mse: 7.6165e-06 - val_loss: 7.5803e-07 - val_mae: 6.9929e-04 - val_mse: 8.6945e-07\n",
      "Epoch 19/28\n",
      "2811/2811 [==============================] - 2s 614us/step - loss: 6.9578e-06 - mae: 0.0015 - mse: 9.3700e-06 - val_loss: 1.7983e-07 - val_mae: 3.4550e-04 - val_mse: 2.4516e-07\n",
      "Epoch 20/28\n",
      "2811/2811 [==============================] - 2s 622us/step - loss: 8.5198e-06 - mae: 0.0014 - mse: 1.0969e-05 - val_loss: 1.2081e-04 - val_mae: 0.0076 - val_mse: 1.1354e-04\n",
      "Epoch 21/28\n",
      "2811/2811 [==============================] - 2s 600us/step - loss: 5.1189e-06 - mae: 0.0010 - mse: 6.2025e-06 - val_loss: 3.2876e-07 - val_mae: 5.6379e-04 - val_mse: 5.0346e-07\n",
      "Epoch 22/28\n",
      "2811/2811 [==============================] - 2s 620us/step - loss: 9.8373e-06 - mae: 0.0012 - mse: 1.1787e-05 - val_loss: 1.3486e-07 - val_mae: 3.3893e-04 - val_mse: 2.4520e-07\n",
      "Epoch 23/28\n",
      "2811/2811 [==============================] - 2s 626us/step - loss: 5.0789e-06 - mae: 8.6014e-04 - mse: 6.7365e-06 - val_loss: 1.1728e-07 - val_mae: 2.8966e-04 - val_mse: 1.7546e-07\n",
      "Epoch 24/28\n",
      "2811/2811 [==============================] - 2s 601us/step - loss: 6.7396e-06 - mae: 0.0012 - mse: 8.7615e-06 - val_loss: 3.6025e-07 - val_mae: 6.2083e-04 - val_mse: 5.0019e-07\n",
      "Epoch 25/28\n",
      "2811/2811 [==============================] - 2s 627us/step - loss: 4.0349e-06 - mae: 0.0012 - mse: 4.9740e-06 - val_loss: 7.5537e-07 - val_mae: 7.8634e-04 - val_mse: 9.0847e-07\n",
      "Epoch 26/28\n",
      "2811/2811 [==============================] - 2s 600us/step - loss: 7.5105e-06 - mae: 0.0014 - mse: 9.9327e-06 - val_loss: 4.7391e-06 - val_mae: 0.0017 - val_mse: 5.2313e-06\n",
      "Epoch 27/28\n",
      "2811/2811 [==============================] - 2s 604us/step - loss: 3.9925e-06 - mae: 7.6831e-04 - mse: 5.0979e-06 - val_loss: 4.3429e-07 - val_mae: 4.5297e-04 - val_mse: 4.2696e-07\n",
      "Epoch 28/28\n",
      "2811/2811 [==============================] - 2s 591us/step - loss: 6.9148e-06 - mae: 0.0011 - mse: 8.9656e-06 - val_loss: 9.5854e-08 - val_mae: 3.0492e-04 - val_mse: 1.3841e-07\n",
      "Iteration 5: delta = 0.8999999999999999, val_mae = 0.0003049160004593432, val_mse = 1.384140944082901e-07\n",
      "Epoch 1/28\n",
      "2811/2811 [==============================] - 4s 1ms/step - loss: 0.0048 - mae: 0.0131 - mse: 0.0056 - val_loss: 1.0069e-05 - val_mae: 0.0028 - val_mse: 1.4811e-05\n",
      "Epoch 2/28\n",
      "2811/2811 [==============================] - 2s 559us/step - loss: 3.3321e-05 - mae: 0.0036 - mse: 4.1516e-05 - val_loss: 9.5220e-05 - val_mae: 0.0065 - val_mse: 1.1048e-04\n",
      "Epoch 3/28\n",
      "2811/2811 [==============================] - 2s 592us/step - loss: 4.2321e-05 - mae: 0.0038 - mse: 4.9317e-05 - val_loss: 1.2241e-05 - val_mae: 0.0033 - val_mse: 1.7378e-05\n",
      "Epoch 4/28\n",
      "2811/2811 [==============================] - 2s 639us/step - loss: 3.4058e-05 - mae: 0.0033 - mse: 4.2802e-05 - val_loss: 1.4110e-06 - val_mae: 0.0011 - val_mse: 2.0101e-06\n",
      "Epoch 5/28\n",
      "2811/2811 [==============================] - 2s 566us/step - loss: 2.4631e-05 - mae: 0.0030 - mse: 2.8594e-05 - val_loss: 3.3606e-06 - val_mae: 0.0015 - val_mse: 4.1535e-06\n",
      "Epoch 6/28\n",
      "2811/2811 [==============================] - 2s 555us/step - loss: 2.5543e-05 - mae: 0.0029 - mse: 3.0627e-05 - val_loss: 1.0429e-05 - val_mae: 0.0022 - val_mse: 1.2805e-05\n",
      "Epoch 7/28\n",
      "2811/2811 [==============================] - 2s 562us/step - loss: 1.3570e-05 - mae: 0.0024 - mse: 1.6178e-05 - val_loss: 2.3675e-05 - val_mae: 0.0052 - val_mse: 3.7051e-05\n",
      "Epoch 8/28\n",
      "2811/2811 [==============================] - 2s 567us/step - loss: 1.4309e-05 - mae: 0.0024 - mse: 1.7378e-05 - val_loss: 3.3146e-05 - val_mae: 0.0036 - val_mse: 4.1913e-05\n",
      "Epoch 9/28\n",
      "2811/2811 [==============================] - 1s 529us/step - loss: 1.4352e-05 - mae: 0.0022 - mse: 1.7571e-05 - val_loss: 2.3438e-06 - val_mae: 0.0013 - val_mse: 3.1322e-06\n",
      "Epoch 10/28\n",
      "2811/2811 [==============================] - 2s 558us/step - loss: 7.9952e-06 - mae: 0.0016 - mse: 9.2194e-06 - val_loss: 1.5215e-06 - val_mae: 0.0010 - val_mse: 1.6792e-06\n",
      "Epoch 11/28\n",
      "2811/2811 [==============================] - 2s 569us/step - loss: 1.0635e-05 - mae: 0.0019 - mse: 1.3627e-05 - val_loss: 1.7334e-06 - val_mae: 9.4737e-04 - val_mse: 2.1327e-06\n",
      "Epoch 12/28\n",
      "2811/2811 [==============================] - 2s 555us/step - loss: 9.2585e-06 - mae: 0.0017 - mse: 1.1282e-05 - val_loss: 1.9830e-05 - val_mae: 0.0028 - val_mse: 1.7744e-05\n",
      "Epoch 13/28\n",
      "2811/2811 [==============================] - 2s 555us/step - loss: 6.3817e-06 - mae: 0.0016 - mse: 7.5850e-06 - val_loss: 2.8009e-06 - val_mae: 0.0016 - val_mse: 4.8884e-06\n",
      "Epoch 14/28\n",
      "2811/2811 [==============================] - 2s 555us/step - loss: 7.2222e-06 - mae: 0.0015 - mse: 8.9478e-06 - val_loss: 4.8995e-07 - val_mae: 6.2042e-04 - val_mse: 6.2603e-07\n",
      "Epoch 15/28\n",
      "2811/2811 [==============================] - 2s 565us/step - loss: 8.9953e-06 - mae: 0.0013 - mse: 1.1688e-05 - val_loss: 2.2003e-07 - val_mae: 3.6654e-04 - val_mse: 3.1753e-07\n",
      "Epoch 16/28\n",
      "2811/2811 [==============================] - 2s 543us/step - loss: 1.6350e-05 - mae: 0.0016 - mse: 2.2551e-05 - val_loss: 3.4358e-07 - val_mae: 5.0621e-04 - val_mse: 5.4279e-07\n",
      "Epoch 17/28\n",
      "2811/2811 [==============================] - 2s 553us/step - loss: 3.1594e-06 - mae: 9.4433e-04 - mse: 3.6502e-06 - val_loss: 1.1267e-05 - val_mae: 0.0025 - val_mse: 1.3266e-05\n",
      "Epoch 18/28\n",
      "2811/2811 [==============================] - 2s 541us/step - loss: 5.8050e-06 - mae: 0.0012 - mse: 6.3879e-06 - val_loss: 7.7883e-07 - val_mae: 6.0196e-04 - val_mse: 1.1725e-06\n",
      "Epoch 19/28\n",
      "2811/2811 [==============================] - 2s 562us/step - loss: 5.9415e-06 - mae: 0.0011 - mse: 7.6305e-06 - val_loss: 8.4246e-08 - val_mae: 2.1246e-04 - val_mse: 1.2057e-07\n",
      "Epoch 20/28\n",
      "2811/2811 [==============================] - 2s 557us/step - loss: 9.3089e-06 - mae: 0.0011 - mse: 1.1828e-05 - val_loss: 3.0568e-07 - val_mae: 4.7095e-04 - val_mse: 3.6165e-07\n",
      "Epoch 21/28\n",
      "2811/2811 [==============================] - 2s 577us/step - loss: 3.6291e-06 - mae: 8.4203e-04 - mse: 4.2303e-06 - val_loss: 4.7974e-07 - val_mae: 4.9925e-04 - val_mse: 4.6887e-07\n",
      "Epoch 22/28\n",
      "2811/2811 [==============================] - 2s 607us/step - loss: 5.5843e-06 - mae: 0.0010 - mse: 7.0239e-06 - val_loss: 1.0645e-05 - val_mae: 0.0029 - val_mse: 1.6408e-05\n",
      "Epoch 23/28\n",
      "2811/2811 [==============================] - 2s 580us/step - loss: 4.2513e-06 - mae: 0.0011 - mse: 5.2985e-06 - val_loss: 1.1997e-05 - val_mae: 0.0031 - val_mse: 1.7102e-05\n",
      "Epoch 24/28\n",
      "2811/2811 [==============================] - 2s 573us/step - loss: 6.2606e-06 - mae: 0.0013 - mse: 7.6233e-06 - val_loss: 2.5200e-06 - val_mae: 0.0010 - val_mse: 3.6352e-06\n",
      "Epoch 25/28\n",
      "2811/2811 [==============================] - 2s 562us/step - loss: 4.5917e-06 - mae: 9.7418e-04 - mse: 5.9389e-06 - val_loss: 6.6725e-08 - val_mae: 2.0728e-04 - val_mse: 9.5042e-08\n",
      "Epoch 26/28\n",
      "2811/2811 [==============================] - 2s 546us/step - loss: 6.1489e-06 - mae: 9.7965e-04 - mse: 7.8396e-06 - val_loss: 6.2529e-08 - val_mae: 2.0682e-04 - val_mse: 8.6819e-08\n",
      "Epoch 27/28\n",
      "2811/2811 [==============================] - 1s 518us/step - loss: 3.8450e-06 - mae: 9.3678e-04 - mse: 4.6450e-06 - val_loss: 4.1802e-07 - val_mae: 4.8362e-04 - val_mse: 4.0476e-07\n",
      "Epoch 28/28\n",
      "2811/2811 [==============================] - 1s 530us/step - loss: 5.9307e-06 - mae: 0.0010 - mse: 7.9652e-06 - val_loss: 1.7625e-06 - val_mae: 0.0011 - val_mse: 2.5609e-06\n",
      "Iteration 6: delta = 0.9999999999999999, val_mae = 0.0011121290735900402, val_mse = 2.5609101612644736e-06\n",
      "Epoch 1/28\n",
      "2811/2811 [==============================] - 4s 1ms/step - loss: 0.0048 - mae: 0.0123 - mse: 0.0070 - val_loss: 9.6819e-06 - val_mae: 0.0027 - val_mse: 1.5186e-05\n",
      "Epoch 2/28\n",
      "2811/2811 [==============================] - 2s 534us/step - loss: 2.6736e-05 - mae: 0.0034 - mse: 3.2680e-05 - val_loss: 8.5951e-06 - val_mae: 0.0024 - val_mse: 1.2587e-05\n",
      "Epoch 3/28\n",
      "2811/2811 [==============================] - 2s 540us/step - loss: 3.1906e-05 - mae: 0.0038 - mse: 3.7856e-05 - val_loss: 3.3892e-05 - val_mae: 0.0040 - val_mse: 3.5575e-05\n",
      "Epoch 4/28\n",
      "2811/2811 [==============================] - 1s 531us/step - loss: 2.7869e-05 - mae: 0.0033 - mse: 3.3478e-05 - val_loss: 9.7474e-06 - val_mae: 0.0023 - val_mse: 1.2814e-05\n",
      "Epoch 5/28\n",
      "2811/2811 [==============================] - 2s 578us/step - loss: 2.1093e-05 - mae: 0.0028 - mse: 2.5727e-05 - val_loss: 5.0425e-06 - val_mae: 0.0016 - val_mse: 5.0181e-06\n",
      "Epoch 6/28\n",
      "2811/2811 [==============================] - 2s 554us/step - loss: 2.2077e-05 - mae: 0.0027 - mse: 2.7168e-05 - val_loss: 8.8004e-06 - val_mae: 0.0022 - val_mse: 1.2515e-05\n",
      "Epoch 7/28\n",
      "2811/2811 [==============================] - 2s 542us/step - loss: 1.9738e-05 - mae: 0.0024 - mse: 2.4485e-05 - val_loss: 6.2695e-05 - val_mae: 0.0061 - val_mse: 8.2781e-05\n",
      "Epoch 8/28\n",
      "2811/2811 [==============================] - 2s 569us/step - loss: 1.5767e-05 - mae: 0.0024 - mse: 1.9456e-05 - val_loss: 1.7793e-06 - val_mae: 9.9345e-04 - val_mse: 2.4570e-06\n",
      "Epoch 9/28\n",
      "2811/2811 [==============================] - 2s 552us/step - loss: 1.2230e-05 - mae: 0.0019 - mse: 1.5249e-05 - val_loss: 1.0654e-06 - val_mae: 7.3905e-04 - val_mse: 1.5066e-06\n",
      "Epoch 10/28\n",
      "2811/2811 [==============================] - 2s 608us/step - loss: 2.7233e-05 - mae: 0.0019 - mse: 3.7754e-05 - val_loss: 2.2672e-06 - val_mae: 0.0012 - val_mse: 2.3642e-06\n",
      "Epoch 11/28\n",
      "2811/2811 [==============================] - 2s 567us/step - loss: 6.5765e-06 - mae: 0.0016 - mse: 7.8564e-06 - val_loss: 2.0510e-07 - val_mae: 4.0865e-04 - val_mse: 4.0647e-07\n",
      "Epoch 12/28\n",
      "2811/2811 [==============================] - 1s 531us/step - loss: 1.1633e-05 - mae: 0.0017 - mse: 1.5547e-05 - val_loss: 8.6286e-06 - val_mae: 0.0026 - val_mse: 9.5111e-06\n",
      "Epoch 13/28\n",
      "2811/2811 [==============================] - 2s 759us/step - loss: 8.4284e-06 - mae: 0.0017 - mse: 1.1016e-05 - val_loss: 1.1508e-06 - val_mae: 8.6546e-04 - val_mse: 1.3337e-06\n",
      "Epoch 14/28\n",
      "2811/2811 [==============================] - 1s 515us/step - loss: 8.9553e-06 - mae: 0.0015 - mse: 1.1506e-05 - val_loss: 5.0074e-05 - val_mae: 0.0054 - val_mse: 6.9949e-05\n",
      "Epoch 15/28\n",
      "2811/2811 [==============================] - 2s 535us/step - loss: 6.2536e-06 - mae: 0.0014 - mse: 7.9668e-06 - val_loss: 5.7630e-06 - val_mae: 0.0019 - val_mse: 8.5085e-06\n",
      "Epoch 16/28\n",
      "2811/2811 [==============================] - 2s 588us/step - loss: 6.3763e-06 - mae: 0.0015 - mse: 7.9848e-06 - val_loss: 1.2194e-06 - val_mae: 8.8961e-04 - val_mse: 1.7050e-06\n",
      "Epoch 17/28\n",
      "2811/2811 [==============================] - 2s 595us/step - loss: 7.4155e-06 - mae: 0.0015 - mse: 9.4275e-06 - val_loss: 2.9104e-05 - val_mae: 0.0053 - val_mse: 3.7265e-05\n",
      "Epoch 18/28\n",
      "2811/2811 [==============================] - 1s 529us/step - loss: 6.8221e-06 - mae: 0.0014 - mse: 8.1142e-06 - val_loss: 1.1411e-07 - val_mae: 3.2286e-04 - val_mse: 1.7761e-07\n",
      "Epoch 19/28\n",
      "2811/2811 [==============================] - 2s 563us/step - loss: 6.5764e-06 - mae: 0.0011 - mse: 8.5775e-06 - val_loss: 6.5624e-07 - val_mae: 8.2848e-04 - val_mse: 1.0729e-06\n",
      "Epoch 20/28\n",
      "2811/2811 [==============================] - 2s 568us/step - loss: 6.2953e-06 - mae: 0.0013 - mse: 8.1165e-06 - val_loss: 2.9112e-05 - val_mae: 0.0040 - val_mse: 4.0663e-05\n",
      "Epoch 21/28\n",
      "2811/2811 [==============================] - 2s 565us/step - loss: 6.0475e-06 - mae: 0.0013 - mse: 7.5937e-06 - val_loss: 2.2486e-07 - val_mae: 3.9841e-04 - val_mse: 3.1954e-07\n",
      "Epoch 22/28\n",
      "2811/2811 [==============================] - 2s 559us/step - loss: 6.1098e-06 - mae: 0.0011 - mse: 8.0128e-06 - val_loss: 4.6814e-06 - val_mae: 0.0013 - val_mse: 4.0672e-06\n",
      "Epoch 23/28\n",
      "2811/2811 [==============================] - 2s 572us/step - loss: 5.2651e-06 - mae: 0.0011 - mse: 6.8613e-06 - val_loss: 1.4641e-06 - val_mae: 9.7995e-04 - val_mse: 2.1243e-06\n",
      "Epoch 24/28\n",
      "2811/2811 [==============================] - 2s 587us/step - loss: 6.0827e-06 - mae: 0.0011 - mse: 7.3200e-06 - val_loss: 4.0891e-07 - val_mae: 5.2446e-04 - val_mse: 6.3044e-07\n",
      "Epoch 25/28\n",
      "2811/2811 [==============================] - 2s 575us/step - loss: 7.0289e-06 - mae: 0.0012 - mse: 9.0458e-06 - val_loss: 3.5092e-07 - val_mae: 4.3432e-04 - val_mse: 3.8811e-07\n",
      "Epoch 26/28\n",
      "2811/2811 [==============================] - 2s 565us/step - loss: 6.0977e-06 - mae: 0.0010 - mse: 7.3150e-06 - val_loss: 1.6518e-06 - val_mae: 0.0011 - val_mse: 1.8511e-06\n",
      "Epoch 27/28\n",
      "2811/2811 [==============================] - 2s 586us/step - loss: 5.4438e-06 - mae: 9.3916e-04 - mse: 7.0562e-06 - val_loss: 2.7472e-07 - val_mae: 4.0278e-04 - val_mse: 3.0590e-07\n",
      "Epoch 28/28\n",
      "2811/2811 [==============================] - 2s 534us/step - loss: 5.5972e-06 - mae: 0.0011 - mse: 7.2847e-06 - val_loss: 2.1808e-07 - val_mae: 3.7472e-04 - val_mse: 2.7149e-07\n",
      "Iteration 7: delta = 1.0999999999999999, val_mae = 0.00037471650284714997, val_mse = 2.714876359277696e-07\n",
      "Epoch 1/28\n",
      "2811/2811 [==============================] - 3s 1ms/step - loss: 0.0066 - mae: 0.0153 - mse: 0.0096 - val_loss: 7.2060e-06 - val_mae: 0.0024 - val_mse: 1.2664e-05\n",
      "Epoch 2/28\n",
      "2811/2811 [==============================] - 2s 559us/step - loss: 5.7642e-05 - mae: 0.0043 - mse: 6.7681e-05 - val_loss: 1.4987e-04 - val_mae: 0.0086 - val_mse: 1.4764e-04\n",
      "Epoch 3/28\n",
      "2811/2811 [==============================] - 1s 517us/step - loss: 2.7788e-05 - mae: 0.0035 - mse: 3.3876e-05 - val_loss: 4.6297e-06 - val_mae: 0.0018 - val_mse: 6.4017e-06\n",
      "Epoch 4/28\n",
      "2811/2811 [==============================] - 1s 514us/step - loss: 3.9336e-05 - mae: 0.0034 - mse: 4.6771e-05 - val_loss: 2.1114e-06 - val_mae: 0.0013 - val_mse: 3.1043e-06\n",
      "Epoch 5/28\n",
      "2811/2811 [==============================] - 2s 587us/step - loss: 2.8000e-05 - mae: 0.0030 - mse: 3.4420e-05 - val_loss: 1.6819e-05 - val_mae: 0.0028 - val_mse: 1.9472e-05\n",
      "Epoch 6/28\n",
      "2811/2811 [==============================] - 2s 572us/step - loss: 2.2599e-05 - mae: 0.0030 - mse: 2.7942e-05 - val_loss: 5.9101e-05 - val_mae: 0.0047 - val_mse: 8.0337e-05\n",
      "Epoch 7/28\n",
      "2811/2811 [==============================] - 2s 584us/step - loss: 1.6387e-05 - mae: 0.0021 - mse: 2.1908e-05 - val_loss: 1.4088e-06 - val_mae: 0.0011 - val_mse: 2.0786e-06\n",
      "Epoch 8/28\n",
      "2811/2811 [==============================] - 2s 581us/step - loss: 1.9634e-05 - mae: 0.0025 - mse: 2.4298e-05 - val_loss: 2.8094e-06 - val_mae: 0.0014 - val_mse: 3.7414e-06\n",
      "Epoch 9/28\n",
      "2811/2811 [==============================] - 1s 533us/step - loss: 1.3394e-05 - mae: 0.0021 - mse: 1.6334e-05 - val_loss: 8.5748e-07 - val_mae: 7.9010e-04 - val_mse: 1.1415e-06\n",
      "Epoch 10/28\n",
      "2811/2811 [==============================] - 2s 555us/step - loss: 8.5472e-06 - mae: 0.0018 - mse: 1.1256e-05 - val_loss: 2.8414e-06 - val_mae: 0.0015 - val_mse: 3.8423e-06\n",
      "Epoch 11/28\n",
      "2811/2811 [==============================] - 2s 562us/step - loss: 8.3619e-06 - mae: 0.0018 - mse: 1.0892e-05 - val_loss: 9.2100e-06 - val_mae: 0.0023 - val_mse: 1.1517e-05\n",
      "Epoch 12/28\n",
      "2811/2811 [==============================] - 2s 589us/step - loss: 8.6782e-06 - mae: 0.0015 - mse: 1.0444e-05 - val_loss: 4.7477e-07 - val_mae: 5.7864e-04 - val_mse: 7.2114e-07\n",
      "Epoch 13/28\n",
      "2811/2811 [==============================] - 2s 564us/step - loss: 1.0623e-05 - mae: 0.0016 - mse: 1.4118e-05 - val_loss: 2.3819e-07 - val_mae: 4.4814e-04 - val_mse: 4.2418e-07\n",
      "Epoch 14/28\n",
      "2811/2811 [==============================] - 2s 569us/step - loss: 4.7851e-06 - mae: 0.0012 - mse: 6.0483e-06 - val_loss: 2.2884e-06 - val_mae: 0.0012 - val_mse: 2.7082e-06\n",
      "Epoch 15/28\n",
      "2811/2811 [==============================] - 1s 518us/step - loss: 7.0932e-06 - mae: 0.0014 - mse: 9.2920e-06 - val_loss: 1.2733e-07 - val_mae: 3.5159e-04 - val_mse: 2.5400e-07\n",
      "Epoch 16/28\n",
      "2811/2811 [==============================] - 1s 515us/step - loss: 4.2359e-06 - mae: 0.0012 - mse: 5.6183e-06 - val_loss: 4.5978e-07 - val_mae: 6.1132e-04 - val_mse: 6.9329e-07\n",
      "Epoch 17/28\n",
      "2811/2811 [==============================] - 1s 508us/step - loss: 6.4060e-06 - mae: 0.0013 - mse: 8.2393e-06 - val_loss: 1.3831e-06 - val_mae: 0.0011 - val_mse: 1.4747e-06\n",
      "Epoch 18/28\n",
      "2811/2811 [==============================] - 1s 517us/step - loss: 7.2774e-06 - mae: 0.0012 - mse: 9.6245e-06 - val_loss: 1.3580e-04 - val_mae: 0.0071 - val_mse: 1.8945e-04\n",
      "Epoch 19/28\n",
      "2811/2811 [==============================] - 1s 511us/step - loss: 6.9918e-06 - mae: 0.0011 - mse: 8.6033e-06 - val_loss: 2.2024e-05 - val_mae: 0.0039 - val_mse: 3.0817e-05\n",
      "Epoch 20/28\n",
      "2811/2811 [==============================] - 2s 622us/step - loss: 4.6407e-06 - mae: 0.0011 - mse: 6.1953e-06 - val_loss: 1.2190e-07 - val_mae: 3.0216e-04 - val_mse: 1.7994e-07\n",
      "Epoch 21/28\n",
      "2811/2811 [==============================] - 2s 636us/step - loss: 6.9267e-06 - mae: 0.0012 - mse: 9.5855e-06 - val_loss: 2.6963e-05 - val_mae: 0.0053 - val_mse: 3.9664e-05\n",
      "Epoch 22/28\n",
      "2811/2811 [==============================] - 2s 627us/step - loss: 6.3914e-06 - mae: 0.0012 - mse: 8.5065e-06 - val_loss: 9.8038e-08 - val_mae: 2.7747e-04 - val_mse: 1.6739e-07\n",
      "Epoch 23/28\n",
      "2811/2811 [==============================] - 2s 596us/step - loss: 5.5618e-06 - mae: 7.9354e-04 - mse: 6.8193e-06 - val_loss: 1.2432e-07 - val_mae: 3.3290e-04 - val_mse: 1.7271e-07\n",
      "Epoch 24/28\n",
      "2811/2811 [==============================] - 3s 1ms/step - loss: 3.8228e-06 - mae: 0.0010 - mse: 4.9590e-06 - val_loss: 6.6748e-07 - val_mae: 5.6194e-04 - val_mse: 7.1161e-07\n",
      "Epoch 25/28\n",
      "2811/2811 [==============================] - 2s 562us/step - loss: 4.4291e-06 - mae: 0.0011 - mse: 5.8309e-06 - val_loss: 1.2309e-05 - val_mae: 0.0029 - val_mse: 1.8156e-05\n",
      "Epoch 26/28\n",
      "2811/2811 [==============================] - 1s 521us/step - loss: 4.3539e-06 - mae: 9.9133e-04 - mse: 5.8570e-06 - val_loss: 7.3208e-07 - val_mae: 6.0039e-04 - val_mse: 6.8367e-07\n",
      "Epoch 27/28\n",
      "2811/2811 [==============================] - 1s 519us/step - loss: 5.0531e-06 - mae: 9.0382e-04 - mse: 6.4114e-06 - val_loss: 1.6733e-06 - val_mae: 0.0012 - val_mse: 2.4482e-06\n",
      "Epoch 28/28\n",
      "2811/2811 [==============================] - 1s 505us/step - loss: 7.6650e-06 - mae: 8.6708e-04 - mse: 1.0242e-05 - val_loss: 6.9144e-08 - val_mae: 2.4014e-04 - val_mse: 1.0520e-07\n",
      "Iteration 8: delta = 1.2, val_mae = 0.0002401369420113042, val_mse = 1.0520157900373306e-07\n",
      "Epoch 1/28\n",
      "2811/2811 [==============================] - 3s 1ms/step - loss: 0.0053 - mae: 0.0126 - mse: 0.0072 - val_loss: 7.6872e-06 - val_mae: 0.0023 - val_mse: 1.1004e-05\n",
      "Epoch 2/28\n",
      "2811/2811 [==============================] - 1s 517us/step - loss: 2.7360e-05 - mae: 0.0034 - mse: 3.1454e-05 - val_loss: 8.9937e-05 - val_mae: 0.0072 - val_mse: 1.0253e-04\n",
      "Epoch 3/28\n",
      "2811/2811 [==============================] - 1s 528us/step - loss: 5.4003e-05 - mae: 0.0042 - mse: 6.1400e-05 - val_loss: 3.7168e-05 - val_mae: 0.0044 - val_mse: 4.2945e-05\n",
      "Epoch 4/28\n",
      "2811/2811 [==============================] - 1s 527us/step - loss: 3.8484e-05 - mae: 0.0038 - mse: 4.3913e-05 - val_loss: 3.3696e-04 - val_mae: 0.0135 - val_mse: 3.9181e-04\n",
      "Epoch 5/28\n",
      "2811/2811 [==============================] - 1s 531us/step - loss: 2.5631e-05 - mae: 0.0031 - mse: 3.0679e-05 - val_loss: 3.1422e-06 - val_mae: 0.0014 - val_mse: 4.2059e-06\n",
      "Epoch 6/28\n",
      "2811/2811 [==============================] - 2s 535us/step - loss: 2.8609e-05 - mae: 0.0031 - mse: 3.3369e-05 - val_loss: 5.5422e-06 - val_mae: 0.0018 - val_mse: 6.9913e-06\n",
      "Epoch 7/28\n",
      "2811/2811 [==============================] - 2s 534us/step - loss: 1.9275e-05 - mae: 0.0027 - mse: 2.3826e-05 - val_loss: 6.7738e-06 - val_mae: 0.0020 - val_mse: 1.0040e-05\n",
      "Epoch 8/28\n",
      "2811/2811 [==============================] - 2s 536us/step - loss: 1.4496e-05 - mae: 0.0023 - mse: 1.7633e-05 - val_loss: 1.0308e-04 - val_mae: 0.0063 - val_mse: 8.9875e-05\n",
      "Epoch 9/28\n",
      "2811/2811 [==============================] - 2s 544us/step - loss: 1.4715e-05 - mae: 0.0024 - mse: 1.7891e-05 - val_loss: 1.9533e-05 - val_mae: 0.0041 - val_mse: 2.9196e-05\n",
      "Epoch 10/28\n",
      "2811/2811 [==============================] - 2s 649us/step - loss: 1.0318e-05 - mae: 0.0021 - mse: 1.2556e-05 - val_loss: 2.2311e-06 - val_mae: 0.0012 - val_mse: 2.9252e-06\n",
      "Epoch 11/28\n",
      "2811/2811 [==============================] - 2s 542us/step - loss: 1.0859e-05 - mae: 0.0021 - mse: 1.3202e-05 - val_loss: 1.5064e-05 - val_mae: 0.0036 - val_mse: 2.0608e-05\n",
      "Epoch 12/28\n",
      "2811/2811 [==============================] - 2s 586us/step - loss: 8.1665e-06 - mae: 0.0019 - mse: 1.0093e-05 - val_loss: 3.2314e-05 - val_mae: 0.0046 - val_mse: 3.3369e-05\n",
      "Epoch 13/28\n",
      "2811/2811 [==============================] - 2s 565us/step - loss: 8.3898e-06 - mae: 0.0018 - mse: 1.0327e-05 - val_loss: 1.2300e-05 - val_mae: 0.0026 - val_mse: 1.7429e-05\n",
      "Epoch 14/28\n",
      "2811/2811 [==============================] - 2s 555us/step - loss: 8.6821e-06 - mae: 0.0017 - mse: 1.1124e-05 - val_loss: 3.7199e-07 - val_mae: 5.4665e-04 - val_mse: 6.1212e-07\n",
      "Epoch 15/28\n",
      "2811/2811 [==============================] - 2s 554us/step - loss: 6.2439e-06 - mae: 0.0016 - mse: 7.7270e-06 - val_loss: 1.2387e-06 - val_mae: 0.0011 - val_mse: 1.7810e-06\n",
      "Epoch 16/28\n",
      "2811/2811 [==============================] - 2s 569us/step - loss: 7.7884e-06 - mae: 0.0015 - mse: 1.0283e-05 - val_loss: 1.8807e-05 - val_mae: 0.0039 - val_mse: 2.8510e-05\n",
      "Epoch 17/28\n",
      "2811/2811 [==============================] - 2s 615us/step - loss: 6.3960e-06 - mae: 0.0015 - mse: 7.7779e-06 - val_loss: 1.5143e-07 - val_mae: 3.7153e-04 - val_mse: 2.7238e-07\n",
      "Epoch 18/28\n",
      "2811/2811 [==============================] - 2s 604us/step - loss: 4.9979e-06 - mae: 0.0014 - mse: 6.1691e-06 - val_loss: 7.2771e-07 - val_mae: 9.4154e-04 - val_mse: 1.3665e-06\n",
      "Epoch 19/28\n",
      "2811/2811 [==============================] - 2s 600us/step - loss: 7.5035e-06 - mae: 0.0012 - mse: 8.9795e-06 - val_loss: 3.5584e-07 - val_mae: 5.1697e-04 - val_mse: 4.9622e-07\n",
      "Epoch 20/28\n",
      "2811/2811 [==============================] - 2s 613us/step - loss: 8.3110e-06 - mae: 0.0012 - mse: 1.1120e-05 - val_loss: 1.5813e-07 - val_mae: 3.4394e-04 - val_mse: 2.4477e-07\n",
      "Epoch 21/28\n",
      "2811/2811 [==============================] - 2s 583us/step - loss: 5.2415e-06 - mae: 0.0013 - mse: 6.7599e-06 - val_loss: 1.5249e-06 - val_mae: 0.0014 - val_mse: 2.5972e-06\n",
      "Epoch 22/28\n",
      "2811/2811 [==============================] - 2s 562us/step - loss: 9.4449e-06 - mae: 0.0011 - mse: 1.1197e-05 - val_loss: 1.1448e-07 - val_mae: 2.5619e-04 - val_mse: 1.7781e-07\n",
      "Epoch 23/28\n",
      "2811/2811 [==============================] - 2s 658us/step - loss: 4.3897e-06 - mae: 0.0012 - mse: 5.5513e-06 - val_loss: 2.5440e-06 - val_mae: 0.0012 - val_mse: 2.9386e-06\n",
      "Epoch 24/28\n",
      "2811/2811 [==============================] - 2s 649us/step - loss: 4.1327e-06 - mae: 0.0011 - mse: 5.0604e-06 - val_loss: 3.2173e-07 - val_mae: 4.3049e-04 - val_mse: 4.0572e-07\n",
      "Epoch 25/28\n",
      "2811/2811 [==============================] - 2s 539us/step - loss: 5.8950e-06 - mae: 0.0012 - mse: 7.0742e-06 - val_loss: 1.3937e-07 - val_mae: 3.5580e-04 - val_mse: 2.1195e-07\n",
      "Epoch 26/28\n",
      "2811/2811 [==============================] - 2s 535us/step - loss: 3.3224e-06 - mae: 8.4951e-04 - mse: 4.3127e-06 - val_loss: 1.4453e-07 - val_mae: 3.2504e-04 - val_mse: 1.7484e-07\n",
      "Epoch 27/28\n",
      "2811/2811 [==============================] - 2s 553us/step - loss: 6.9204e-06 - mae: 0.0011 - mse: 8.6845e-06 - val_loss: 1.3326e-07 - val_mae: 2.9907e-04 - val_mse: 1.9903e-07\n",
      "Epoch 28/28\n",
      "2811/2811 [==============================] - 1s 531us/step - loss: 3.3554e-06 - mae: 8.9990e-04 - mse: 4.3509e-06 - val_loss: 3.3043e-07 - val_mae: 4.4189e-04 - val_mse: 3.7669e-07\n",
      "Iteration 9: delta = 1.3, val_mae = 0.0004418881726451218, val_mse = 3.7668556274184084e-07\n",
      "Epoch 1/28\n",
      "2811/2811 [==============================] - 3s 1ms/step - loss: 0.0052 - mae: 0.0132 - mse: 0.0068 - val_loss: 4.9280e-06 - val_mae: 0.0022 - val_mse: 9.0628e-06\n",
      "Epoch 2/28\n",
      "2811/2811 [==============================] - 2s 568us/step - loss: 1.7748e-05 - mae: 0.0030 - mse: 2.3154e-05 - val_loss: 3.4803e-06 - val_mae: 0.0016 - val_mse: 5.0281e-06\n",
      "Epoch 3/28\n",
      "2811/2811 [==============================] - 2s 583us/step - loss: 3.5525e-05 - mae: 0.0036 - mse: 4.5235e-05 - val_loss: 5.3932e-05 - val_mae: 0.0057 - val_mse: 5.7158e-05\n",
      "Epoch 4/28\n",
      "2811/2811 [==============================] - 2s 566us/step - loss: 2.5748e-05 - mae: 0.0032 - mse: 3.0558e-05 - val_loss: 9.6831e-06 - val_mae: 0.0025 - val_mse: 1.1916e-05\n",
      "Epoch 5/28\n",
      "2811/2811 [==============================] - 2s 560us/step - loss: 1.7213e-05 - mae: 0.0025 - mse: 2.1440e-05 - val_loss: 6.9652e-06 - val_mae: 0.0020 - val_mse: 8.1249e-06\n",
      "Epoch 6/28\n",
      "2811/2811 [==============================] - 2s 561us/step - loss: 1.5440e-05 - mae: 0.0024 - mse: 1.9315e-05 - val_loss: 1.3465e-06 - val_mae: 0.0011 - val_mse: 1.9225e-06\n",
      "Epoch 7/28\n",
      "2811/2811 [==============================] - 2s 581us/step - loss: 1.0401e-05 - mae: 0.0021 - mse: 1.3235e-05 - val_loss: 1.6255e-06 - val_mae: 0.0011 - val_mse: 2.1133e-06\n",
      "Epoch 8/28\n",
      "2811/2811 [==============================] - 2s 549us/step - loss: 9.2681e-06 - mae: 0.0018 - mse: 1.2033e-05 - val_loss: 2.2846e-06 - val_mae: 0.0013 - val_mse: 3.0531e-06\n",
      "Epoch 9/28\n",
      "2811/2811 [==============================] - 1s 532us/step - loss: 1.0888e-05 - mae: 0.0019 - mse: 1.3818e-05 - val_loss: 3.1913e-06 - val_mae: 0.0013 - val_mse: 3.6572e-06\n",
      "Epoch 10/28\n",
      "2811/2811 [==============================] - 2s 561us/step - loss: 8.8388e-06 - mae: 0.0016 - mse: 1.1004e-05 - val_loss: 7.9809e-05 - val_mae: 0.0058 - val_mse: 1.0259e-04\n",
      "Epoch 11/28\n",
      "2811/2811 [==============================] - 2s 556us/step - loss: 6.8989e-06 - mae: 0.0014 - mse: 8.8065e-06 - val_loss: 4.8530e-06 - val_mae: 0.0014 - val_mse: 6.5267e-06\n",
      "Epoch 12/28\n",
      "2811/2811 [==============================] - 1s 532us/step - loss: 1.2372e-05 - mae: 0.0017 - mse: 1.6159e-05 - val_loss: 1.8618e-07 - val_mae: 4.1400e-04 - val_mse: 3.9119e-07\n",
      "Epoch 13/28\n",
      "2811/2811 [==============================] - 2s 579us/step - loss: 7.6045e-06 - mae: 0.0013 - mse: 9.6208e-06 - val_loss: 1.9943e-07 - val_mae: 3.6364e-04 - val_mse: 4.0930e-07\n",
      "Epoch 14/28\n",
      "2811/2811 [==============================] - 2s 566us/step - loss: 3.5492e-06 - mae: 9.2424e-04 - mse: 4.7172e-06 - val_loss: 9.3560e-07 - val_mae: 7.3672e-04 - val_mse: 1.0940e-06\n",
      "Epoch 15/28\n",
      "2811/2811 [==============================] - 1s 524us/step - loss: 7.3307e-06 - mae: 0.0012 - mse: 9.5774e-06 - val_loss: 4.2948e-06 - val_mae: 0.0016 - val_mse: 5.2763e-06\n",
      "Epoch 16/28\n",
      "2811/2811 [==============================] - 1s 525us/step - loss: 7.2466e-06 - mae: 0.0014 - mse: 8.2939e-06 - val_loss: 5.9588e-07 - val_mae: 6.7183e-04 - val_mse: 8.1032e-07\n",
      "Epoch 17/28\n",
      "2811/2811 [==============================] - 1s 528us/step - loss: 4.4411e-06 - mae: 9.7565e-04 - mse: 6.0095e-06 - val_loss: 9.3218e-06 - val_mae: 0.0021 - val_mse: 9.0044e-06\n",
      "Epoch 18/28\n",
      "2811/2811 [==============================] - 2s 592us/step - loss: 6.0855e-06 - mae: 0.0011 - mse: 8.0341e-06 - val_loss: 5.2957e-07 - val_mae: 5.7815e-04 - val_mse: 6.2664e-07\n",
      "Epoch 19/28\n",
      "2811/2811 [==============================] - 2s 601us/step - loss: 1.6505e-05 - mae: 0.0012 - mse: 2.3024e-05 - val_loss: 1.3512e-07 - val_mae: 3.2820e-04 - val_mse: 2.0351e-07\n",
      "Epoch 20/28\n",
      "2811/2811 [==============================] - 2s 565us/step - loss: 4.1764e-06 - mae: 0.0011 - mse: 5.3671e-06 - val_loss: 1.6874e-07 - val_mae: 3.3841e-04 - val_mse: 2.4389e-07\n",
      "Epoch 21/28\n",
      "2811/2811 [==============================] - 2s 544us/step - loss: 4.6574e-06 - mae: 8.1780e-04 - mse: 6.1165e-06 - val_loss: 2.4995e-07 - val_mae: 4.6858e-04 - val_mse: 3.6648e-07\n",
      "Epoch 22/28\n",
      "2811/2811 [==============================] - 1s 529us/step - loss: 8.5214e-06 - mae: 0.0013 - mse: 1.1620e-05 - val_loss: 1.6918e-07 - val_mae: 3.6110e-04 - val_mse: 2.4850e-07\n",
      "Epoch 23/28\n",
      "2811/2811 [==============================] - 1s 524us/step - loss: 3.9744e-06 - mae: 8.2416e-04 - mse: 4.9706e-06 - val_loss: 7.0073e-08 - val_mae: 2.3765e-04 - val_mse: 1.3617e-07\n",
      "Epoch 24/28\n",
      "2811/2811 [==============================] - 1s 527us/step - loss: 3.8817e-06 - mae: 9.9069e-04 - mse: 5.0992e-06 - val_loss: 5.5294e-08 - val_mae: 2.0279e-04 - val_mse: 9.7329e-08\n",
      "Epoch 25/28\n",
      "2811/2811 [==============================] - 1s 522us/step - loss: 3.9003e-06 - mae: 9.5013e-04 - mse: 4.7131e-06 - val_loss: 2.0020e-07 - val_mae: 4.6786e-04 - val_mse: 3.1771e-07\n",
      "Epoch 26/28\n",
      "2811/2811 [==============================] - 2s 538us/step - loss: 6.8770e-06 - mae: 0.0013 - mse: 8.8418e-06 - val_loss: 5.0066e-08 - val_mae: 1.7840e-04 - val_mse: 8.4499e-08\n",
      "Epoch 27/28\n",
      "2811/2811 [==============================] - 2s 539us/step - loss: 2.5417e-06 - mae: 6.9745e-04 - mse: 3.6289e-06 - val_loss: 2.0099e-06 - val_mae: 0.0011 - val_mse: 2.3293e-06\n",
      "Epoch 28/28\n",
      "2811/2811 [==============================] - 2s 540us/step - loss: 5.9903e-06 - mae: 0.0011 - mse: 7.8314e-06 - val_loss: 7.4715e-08 - val_mae: 2.2851e-04 - val_mse: 1.0913e-07\n",
      "Iteration 10: delta = 1.4000000000000001, val_mae = 0.00022851298854220659, val_mse = 1.0913132086898258e-07\n"
     ]
    }
   ],
   "source": [
    "# find best delta for loss function\n",
    "\n",
    "# function to train and evaluate model with a given delta\n",
    "def train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=delta), metrics=['mae', 'mse'], weighted_metrics=[])\n",
    "\n",
    "    history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    # get validation MAE, MSE and Huber loss\n",
    "    val_mae = history.history['val_mae'][-1]\n",
    "    val_mse = history.history['val_mse'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "    return val_mae, val_mse, val_loss\n",
    "\n",
    "# initialise delta value and step size\n",
    "delta = 0.5\n",
    "step_size = 0.1\n",
    "best_delta = delta\n",
    "best_val_mae = float('inf')\n",
    "best_val_mse = float('inf')\n",
    "\n",
    "# initialise arrays for plotting\n",
    "arr_val_mae = []\n",
    "arr_val_mse = []\n",
    "arr_val_loss = []\n",
    "\n",
    "# iterative search for optimal delta\n",
    "for i in range(10):\n",
    "    val_mae, val_mse, val_loss = train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval)\n",
    "\n",
    "    # print current results\n",
    "    print(f\"Iteration {i+1}: delta = {delta}, val_mae = {val_mae}, val_mse = {val_mse}\")\n",
    "\n",
    "    # update best delta if current results are better\n",
    "    if val_mae < best_val_mae and val_mse < best_val_mse: \n",
    "        best_delta = delta\n",
    "        best_val_mae = val_mae\n",
    "        best_val_mse = val_mse\n",
    "    \n",
    "    # store data\n",
    "    arr_val_mae.append(val_mae)\n",
    "    arr_val_mse.append(val_mse)\n",
    "    arr_val_loss.append(val_loss)\n",
    "    \n",
    "    # adjust delta for next iteration \n",
    "    delta += step_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best delta: 1.2\n",
      "Best validation MAE: 0.00029089985764585435\n",
      "Best validation MSE: 1.8254846168019867e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "deltas = np.arange(0.5, 1.5, 0.1)\n",
    "plt.plot(deltas, arr_val_mae, marker='o', linestyle='-', color='C0', label='MAE')\n",
    "plt.plot(deltas, arr_val_mse, marker='o', linestyle='-', color='C1', label='MSE')\n",
    "plt.plot(deltas, arr_val_loss, marker='o', linestyle='-', color='C2', label='Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Best delta: {best_delta}\")\n",
    "print(f\"Best validation MAE: {best_val_mae}\")\n",
    "print(f\"Best validation MSE: {best_val_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 3s 1ms/step - loss: 0.0052 - val_loss: 6.1077e-06\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 564us/step - loss: 1.2979e-05 - val_loss: 2.8237e-05\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 1s 561us/step - loss: 3.9175e-05 - val_loss: 9.9009e-06\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 570us/step - loss: 2.8132e-05 - val_loss: 2.3209e-05\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 563us/step - loss: 2.0126e-05 - val_loss: 5.8459e-06\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 568us/step - loss: 1.8011e-05 - val_loss: 1.3920e-05\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 569us/step - loss: 2.2421e-05 - val_loss: 1.0035e-05\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 563us/step - loss: 9.1726e-06 - val_loss: 5.9296e-05\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 563us/step - loss: 1.0892e-05 - val_loss: 8.4594e-06\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 570us/step - loss: 1.1122e-05 - val_loss: 2.2340e-06\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 565us/step - loss: 8.5739e-06 - val_loss: 7.0867e-07\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 1s 561us/step - loss: 9.6188e-06 - val_loss: 2.8681e-04\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 571us/step - loss: 8.9971e-06 - val_loss: 8.1033e-07\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 565us/step - loss: 6.8583e-06 - val_loss: 3.9558e-07\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 575us/step - loss: 8.0247e-06 - val_loss: 4.4088e-06\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 564us/step - loss: 3.9364e-06 - val_loss: 3.4393e-06\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 565us/step - loss: 4.6630e-06 - val_loss: 5.9572e-07\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 571us/step - loss: 4.3532e-06 - val_loss: 7.3358e-08\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 564us/step - loss: 4.3381e-06 - val_loss: 5.9679e-06\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 1s 560us/step - loss: 4.3988e-06 - val_loss: 3.9053e-06\n",
      "Training Huber Loss: 4.398849796416471e-06\n",
      "Validation Huber Loss: 3.90532704841462e-06\n"
     ]
    }
   ],
   "source": [
    "# evaluate base model \n",
    "base_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "base_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = base_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])\n",
    "\n",
    "# save model\n",
    "base_model.save(\"models/base_dnn_model_processed_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/base_dnn_model_processed_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "base_train_hl = history.history[\"loss\"][-1]\n",
    "base_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {base_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {base_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0476 - val_loss: 0.0066\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 583us/step - loss: 0.0053 - val_loss: 0.0045\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 582us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 589us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 583us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 582us/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 590us/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 579us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 583us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 590us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 580us/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 583us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 590us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 583us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 590us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 580us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 584us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 593us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 581us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 581us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Training Huber Loss: 0.0019612140022218227\n",
      "Validation Huber Loss: 0.0018785912543535233\n"
     ]
    }
   ],
   "source": [
    "# evaluate regularised model\n",
    "reg_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],), kernel_regularizer='l2'), \n",
    "        Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "reg_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "history = reg_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])\n",
    "\n",
    "# save model\n",
    "base_model.save(\"models/reg_dnn_model_processed_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/reg_dnn_model_processed_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "reg_train_hl = history.history[\"loss\"][-1]\n",
    "reg_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {reg_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {reg_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_8512/1497506516.py:28: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2728 - val_loss: 0.0211\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.0131 - val_loss: 0.0092\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 634us/step - loss: 0.0079 - val_loss: 0.0068\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 627us/step - loss: 0.0062 - val_loss: 0.0056\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 634us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 632us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 629us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 639us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 626us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0031 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 623ms/epoch - 584us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 676ms/epoch - 634us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 706ms/epoch - 662us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0031 - val_loss: 0.0029 - 675ms/epoch - 632us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 705ms/epoch - 661us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 741ms/epoch - 695us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 691ms/epoch - 647us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 749ms/epoch - 702us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 692ms/epoch - 648us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 660ms/epoch - 619us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 675ms/epoch - 633us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 640ms/epoch - 600us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 681ms/epoch - 638us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 670ms/epoch - 628us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 654ms/epoch - 613us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 644ms/epoch - 603us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 629ms/epoch - 589us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 624ms/epoch - 585us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 621ms/epoch - 582us/step\n",
      "267/267 - 0s - loss: 0.0028 - 147ms/epoch - 551us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  46.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2730 - val_loss: 0.0199\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 0.0134 - val_loss: 0.0099\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0084 - val_loss: 0.0072\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0045 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 656us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 663us/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 639us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 749us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 716us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 649us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 1ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 702us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0029 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 629ms/epoch - 589us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 1s/epoch - 1ms/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 1s/epoch - 1ms/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 1s/epoch - 1ms/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 2s - loss: 0.0029 - val_loss: 0.0028 - 2s/epoch - 2ms/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 2s - loss: 0.0029 - val_loss: 0.0029 - 2s/epoch - 2ms/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 2s - loss: 0.0029 - val_loss: 0.0029 - 2s/epoch - 2ms/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 1s/epoch - 1ms/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 729ms/epoch - 684us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0029 - 741ms/epoch - 694us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 661ms/epoch - 620us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 687ms/epoch - 644us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0027 - 713ms/epoch - 668us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0027 - 635ms/epoch - 595us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 663ms/epoch - 621us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0027 - 674ms/epoch - 631us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0027 - val_loss: 0.0027 - 662ms/epoch - 621us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0027 - 701ms/epoch - 657us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0027 - val_loss: 0.0027 - 696ms/epoch - 653us/step\n",
      "267/267 - 0s - loss: 0.0027 - 200ms/epoch - 751us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  54.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 7s 5ms/step - loss: 0.2745 - val_loss: 0.0219\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0150 - val_loss: 0.0109\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0089 - val_loss: 0.0076\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0066 - val_loss: 0.0059\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 3s 3ms/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 891us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 6s - loss: 0.0030 - val_loss: 0.0031 - 6s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0030 - 3s/epoch - 3ms/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0030 - 3s/epoch - 3ms/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0030 - 3s/epoch - 3ms/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 3s - loss: 0.0031 - val_loss: 0.0030 - 3s/epoch - 3ms/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 4s - loss: 0.0030 - val_loss: 0.0029 - 4s/epoch - 3ms/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0033 - 3s/epoch - 3ms/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0031 - 3s/epoch - 3ms/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0045 - 3s/epoch - 3ms/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0028 - 3s/epoch - 3ms/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0028 - 3s/epoch - 3ms/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0029 - 3s/epoch - 3ms/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 3s - loss: 0.0029 - val_loss: 0.0028 - 3s/epoch - 3ms/step\n",
      "267/267 - 0s - loss: 0.0028 - 484ms/epoch - 2ms/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 2.7min\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 8s 6ms/step - loss: 0.2708 - val_loss: 0.0167\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0115 - val_loss: 0.0086\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0073 - val_loss: 0.0064\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.0058 - val_loss: 0.0052\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 840us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 855us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 689us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 852us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 1/20\n",
      "1067/1067 - 3s - loss: 0.0030 - val_loss: 0.0029 - 3s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 664ms/epoch - 622us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 641ms/epoch - 601us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 648ms/epoch - 608us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 645ms/epoch - 605us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 662ms/epoch - 620us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 632ms/epoch - 592us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 638ms/epoch - 598us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 639ms/epoch - 599us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 694ms/epoch - 651us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0058 - 659ms/epoch - 618us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 659ms/epoch - 617us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 642ms/epoch - 601us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0029 - 648ms/epoch - 608us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 631ms/epoch - 591us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 647ms/epoch - 606us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 637ms/epoch - 597us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 722ms/epoch - 677us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 641ms/epoch - 601us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 641ms/epoch - 601us/step\n",
      "267/267 - 0s - loss: 0.0028 - 160ms/epoch - 598us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2827 - val_loss: 0.0250\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.0165 - val_loss: 0.0115\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0093 - val_loss: 0.0076\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 659us/step - loss: 0.0067 - val_loss: 0.0061\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 763us/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 652us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 662us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 722us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 715us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 738us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 676us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 740us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 718us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 673ms/epoch - 630us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 629ms/epoch - 589us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 663ms/epoch - 622us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 640ms/epoch - 600us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 663ms/epoch - 621us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 630ms/epoch - 591us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 639ms/epoch - 599us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 643ms/epoch - 603us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 675ms/epoch - 633us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 641ms/epoch - 601us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 653ms/epoch - 612us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 665ms/epoch - 623us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 600ms/epoch - 562us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 675ms/epoch - 633us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 649ms/epoch - 609us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 612ms/epoch - 574us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 699ms/epoch - 655us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0029 - 660ms/epoch - 618us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 660ms/epoch - 618us/step\n",
      "267/267 - 0s - loss: 0.0028 - 151ms/epoch - 566us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  47.3s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3135 - val_loss: 0.4296\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.2534 - val_loss: 0.2805\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.3880 - val_loss: 0.2659\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.3401 - val_loss: 0.2589\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.3011 - val_loss: 0.3367\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.3070 - val_loss: 0.3784\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 624us/step - loss: 0.2695 - val_loss: 0.2847\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.2791 - val_loss: 0.2403\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.3477 - val_loss: 0.3069\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.2882 - val_loss: 0.2847\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.3797 - val_loss: 0.3057\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.2864 - val_loss: 0.2826\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 643us/step - loss: 0.2931 - val_loss: 0.2475\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 686us/step - loss: 0.3001 - val_loss: 0.3221\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 659us/step - loss: 0.2720 - val_loss: 0.2426\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.3779 - val_loss: 0.2971\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 691us/step - loss: 0.3253 - val_loss: 0.2714\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 677us/step - loss: 0.2916 - val_loss: 0.3312\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.4243 - val_loss: 0.2914\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.3076 - val_loss: 0.4380\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.3215 - val_loss: 0.5445 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.3328 - val_loss: 0.2973 - 666ms/epoch - 624us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.3046 - val_loss: 0.3039 - 628ms/epoch - 588us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.3009 - val_loss: 0.3118 - 617ms/epoch - 578us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.3138 - val_loss: 0.2618 - 652ms/epoch - 611us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.3007 - val_loss: 0.2954 - 621ms/epoch - 582us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.3017 - val_loss: 0.2300 - 636ms/epoch - 596us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.2852 - val_loss: 0.2934 - 653ms/epoch - 612us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.2868 - val_loss: 0.2568 - 681ms/epoch - 638us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.3943 - val_loss: 0.2629 - 725ms/epoch - 679us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.2802 - val_loss: 0.2515 - 622ms/epoch - 583us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.2964 - val_loss: 0.3109 - 627ms/epoch - 588us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.2910 - val_loss: 0.2606 - 647ms/epoch - 606us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.3059 - val_loss: 0.2646 - 623ms/epoch - 584us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.2909 - val_loss: 0.3793 - 645ms/epoch - 605us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.3147 - val_loss: 0.3514 - 634ms/epoch - 594us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.3482 - val_loss: 0.2715 - 635ms/epoch - 595us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.3059 - val_loss: 0.2596 - 667ms/epoch - 625us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.2764 - val_loss: 0.2456 - 648ms/epoch - 607us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.2824 - val_loss: 0.2847 - 658ms/epoch - 616us/step\n",
      "267/267 - 0s - loss: 0.2695 - 148ms/epoch - 556us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  45.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3029 - val_loss: 0.2329\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.3788 - val_loss: 0.2390\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 654us/step - loss: 0.2493 - val_loss: 0.2260\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 627us/step - loss: 0.3752 - val_loss: 0.2788\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.2595 - val_loss: 0.2620\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 639us/step - loss: 0.2721 - val_loss: 0.2346\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 0.3416 - val_loss: 0.2456\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 703us/step - loss: 0.3345 - val_loss: 0.2601\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.2823 - val_loss: 0.2727\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 629us/step - loss: 0.3345 - val_loss: 0.3070\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.2752 - val_loss: 0.2916\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 652us/step - loss: 0.2945 - val_loss: 0.2398\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.3551 - val_loss: 0.3155\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 632us/step - loss: 0.2769 - val_loss: 0.2566\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.3419 - val_loss: 0.4186\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 0.3041 - val_loss: 0.2384\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.2783 - val_loss: 0.3369\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 635us/step - loss: 0.2928 - val_loss: 0.3313\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 691us/step - loss: 0.3426 - val_loss: 0.2465\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 668us/step - loss: 0.2714 - val_loss: 0.3878\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.3873 - val_loss: 0.2391 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.2895 - val_loss: 0.2710 - 723ms/epoch - 677us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.2751 - val_loss: 0.3137 - 671ms/epoch - 629us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.2985 - val_loss: 0.3685 - 655ms/epoch - 614us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.3536 - val_loss: 0.2695 - 715ms/epoch - 670us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.3226 - val_loss: 0.2878 - 660ms/epoch - 619us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.2835 - val_loss: 0.2990 - 719ms/epoch - 674us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.3049 - val_loss: 0.5973 - 638ms/epoch - 598us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.3114 - val_loss: 0.9463 - 667ms/epoch - 626us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.3641 - val_loss: 0.3021 - 627ms/epoch - 588us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.3185 - val_loss: 0.2846 - 631ms/epoch - 591us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 2s - loss: 0.3153 - val_loss: 0.3338 - 2s/epoch - 2ms/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 3s - loss: 0.2864 - val_loss: 0.3752 - 3s/epoch - 2ms/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 2s - loss: 0.3279 - val_loss: 0.2687 - 2s/epoch - 2ms/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 2s - loss: 0.2906 - val_loss: 0.5176 - 2s/epoch - 2ms/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.3027 - val_loss: 0.3625 - 857ms/epoch - 804us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.3261 - val_loss: 0.2941 - 714ms/epoch - 669us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.3044 - val_loss: 0.2767 - 641ms/epoch - 601us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.2820 - val_loss: 0.2543 - 600ms/epoch - 563us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.3097 - val_loss: 0.3055 - 629ms/epoch - 590us/step\n",
      "267/267 - 0s - loss: 0.2890 - 143ms/epoch - 534us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  53.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3090 - val_loss: 0.2247\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3616 - val_loss: 0.6529\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.2800 - val_loss: 0.2593\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.4295 - val_loss: 0.3692\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.3751 - val_loss: 0.3111\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.3494 - val_loss: 0.2763\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.3076 - val_loss: 0.2852\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.3045 - val_loss: 0.3573\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.3395 - val_loss: 0.2611\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.3174 - val_loss: 0.3679\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.3100 - val_loss: 0.2813\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 831us/step - loss: 0.3256 - val_loss: 0.2737\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 790us/step - loss: 0.2932 - val_loss: 0.3170\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.3019 - val_loss: 0.2415\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 720us/step - loss: 0.3095 - val_loss: 0.2913\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 666us/step - loss: 0.2969 - val_loss: 0.3109\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 692us/step - loss: 0.3082 - val_loss: 0.2681\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 716us/step - loss: 0.3143 - val_loss: 0.2935\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.2960 - val_loss: 0.2517\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 686us/step - loss: 0.3654 - val_loss: 0.2891\n",
      "Epoch 1/20\n",
      "1067/1067 - 3s - loss: 0.2809 - val_loss: 0.2561 - 3s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.2839 - val_loss: 0.3417 - 1s/epoch - 1ms/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.3287 - val_loss: 0.2695 - 823ms/epoch - 771us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.2609 - val_loss: 0.2678 - 827ms/epoch - 775us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.3278 - val_loss: 0.2620 - 650ms/epoch - 609us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.2892 - val_loss: 0.2795 - 682ms/epoch - 639us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.2982 - val_loss: 0.3243 - 726ms/epoch - 680us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.3473 - val_loss: 0.3266 - 696ms/epoch - 652us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.2955 - val_loss: 0.2636 - 721ms/epoch - 676us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.2941 - val_loss: 0.3036 - 729ms/epoch - 683us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.3266 - val_loss: 0.4020 - 821ms/epoch - 769us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.3109 - val_loss: 0.2504 - 746ms/epoch - 699us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.2918 - val_loss: 0.2331 - 771ms/epoch - 723us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.2978 - val_loss: 0.3381 - 626ms/epoch - 586us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.3068 - val_loss: 0.2352 - 669ms/epoch - 627us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.3043 - val_loss: 0.2504 - 618ms/epoch - 580us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.3250 - val_loss: 0.3464 - 716ms/epoch - 671us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.2778 - val_loss: 0.2547 - 678ms/epoch - 635us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.3033 - val_loss: 0.3014 - 655ms/epoch - 614us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.2915 - val_loss: 0.2723 - 636ms/epoch - 596us/step\n",
      "267/267 - 0s - loss: 0.2677 - 151ms/epoch - 564us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3990 - val_loss: 1.6006\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 663us/step - loss: 0.3569 - val_loss: 0.2820\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 739us/step - loss: 0.2747 - val_loss: 0.3783\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.2749 - val_loss: 0.2357\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.4716 - val_loss: 0.3599\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.3216 - val_loss: 0.3135\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 761us/step - loss: 0.2824 - val_loss: 0.2750\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 737us/step - loss: 0.3579 - val_loss: 0.3345\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 795us/step - loss: 0.2834 - val_loss: 0.2711\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.3115 - val_loss: 0.2842\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 697us/step - loss: 0.3700 - val_loss: 0.4374\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 712us/step - loss: 0.3147 - val_loss: 0.3368\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 695us/step - loss: 0.3047 - val_loss: 0.3555\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.3270 - val_loss: 0.3021\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 699us/step - loss: 0.3101 - val_loss: 0.2790\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.3034 - val_loss: 0.2614\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 655us/step - loss: 0.3030 - val_loss: 0.2911\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 759us/step - loss: 0.2963 - val_loss: 0.2338\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 707us/step - loss: 0.2849 - val_loss: 0.2399\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 713us/step - loss: 0.3107 - val_loss: 0.2388\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.3229 - val_loss: 0.2639 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.2729 - val_loss: 0.2488 - 714ms/epoch - 670us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.3113 - val_loss: 0.2578 - 721ms/epoch - 676us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.2648 - val_loss: 0.3335 - 700ms/epoch - 656us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.3285 - val_loss: 0.2478 - 708ms/epoch - 664us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.2915 - val_loss: 0.2527 - 711ms/epoch - 667us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.2681 - val_loss: 0.2679 - 777ms/epoch - 728us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.3387 - val_loss: 0.2580 - 689ms/epoch - 646us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.2981 - val_loss: 0.2811 - 736ms/epoch - 689us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.2882 - val_loss: 0.3163 - 687ms/epoch - 644us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.3005 - val_loss: 0.3147 - 751ms/epoch - 704us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.2934 - val_loss: 0.2562 - 772ms/epoch - 724us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.3241 - val_loss: 0.7396 - 702ms/epoch - 658us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.2866 - val_loss: 0.2330 - 692ms/epoch - 649us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.2803 - val_loss: 0.3246 - 712ms/epoch - 667us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.3408 - val_loss: 0.3307 - 691ms/epoch - 648us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.2732 - val_loss: 0.2282 - 705ms/epoch - 661us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.2991 - val_loss: 0.2369 - 679ms/epoch - 636us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.3238 - val_loss: 0.2933 - 727ms/epoch - 682us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.3322 - val_loss: 0.2438 - 715ms/epoch - 670us/step\n",
      "267/267 - 0s - loss: 0.2423 - 157ms/epoch - 589us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  49.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3151 - val_loss: 0.2214\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 732us/step - loss: 0.3896 - val_loss: 0.6390\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.2960 - val_loss: 0.5644\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.2842 - val_loss: 0.2352\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 696us/step - loss: 0.3219 - val_loss: 0.2606\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 667us/step - loss: 0.4132 - val_loss: 0.4111\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.3104 - val_loss: 0.2668\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 651us/step - loss: 0.2844 - val_loss: 0.2518\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 647us/step - loss: 0.2858 - val_loss: 0.2686\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 676us/step - loss: 0.2981 - val_loss: 0.2435\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 654us/step - loss: 0.3335 - val_loss: 0.2532\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 659us/step - loss: 0.2653 - val_loss: 0.2826\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.4155 - val_loss: 0.3017\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 0.3104 - val_loss: 0.2827\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.3314 - val_loss: 0.3438\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.2938 - val_loss: 0.2466\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.2935 - val_loss: 0.2992\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 706us/step - loss: 0.3001 - val_loss: 0.2596\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 709us/step - loss: 0.2786 - val_loss: 0.2512\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.3312 - val_loss: 0.2412\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.2793 - val_loss: 0.2403 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.3240 - val_loss: 0.2448 - 703ms/epoch - 659us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.2746 - val_loss: 0.2734 - 677ms/epoch - 635us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.4815 - val_loss: 0.3269 - 698ms/epoch - 654us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.3418 - val_loss: 0.3362 - 670ms/epoch - 628us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.3018 - val_loss: 0.3060 - 672ms/epoch - 630us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.3564 - val_loss: 0.2568 - 686ms/epoch - 643us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.3420 - val_loss: 0.3076 - 704ms/epoch - 660us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.3001 - val_loss: 0.3670 - 689ms/epoch - 645us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.3069 - val_loss: 0.3139 - 648ms/epoch - 607us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.2929 - val_loss: 0.2806 - 649ms/epoch - 608us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.3031 - val_loss: 0.3887 - 758ms/epoch - 710us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.3043 - val_loss: 0.3212 - 679ms/epoch - 637us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.3523 - val_loss: 0.2424 - 711ms/epoch - 666us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.2818 - val_loss: 0.3041 - 701ms/epoch - 657us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.3341 - val_loss: 0.3729 - 670ms/epoch - 628us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.3182 - val_loss: 0.2864 - 653ms/epoch - 612us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.2878 - val_loss: 0.3641 - 668ms/epoch - 626us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.3513 - val_loss: 0.3125 - 681ms/epoch - 638us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.2987 - val_loss: 0.2601 - 696ms/epoch - 652us/step\n",
      "267/267 - 0s - loss: 0.2581 - 165ms/epoch - 617us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  48.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.5217 - val_loss: 0.6210\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.2357 - val_loss: 0.0662\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 877us/step - loss: 0.0507 - val_loss: 0.0412\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.0355 - val_loss: 0.0308\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0273 - val_loss: 0.0245\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0221 - val_loss: 0.0202\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0187 - val_loss: 0.0174\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0165 - val_loss: 0.0156\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0149 - val_loss: 0.0144\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0139 - val_loss: 0.0134\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0129 - val_loss: 0.0125\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0086 - val_loss: 0.0084 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0084 - val_loss: 0.0083 - 560ms/epoch - 524us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0082 - val_loss: 0.0081 - 636ms/epoch - 596us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0080 - val_loss: 0.0080 - 563ms/epoch - 528us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0079 - val_loss: 0.0078 - 613ms/epoch - 575us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0078 - val_loss: 0.0077 - 660ms/epoch - 618us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0076 - val_loss: 0.0075 - 574ms/epoch - 538us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0075 - val_loss: 0.0074 - 588ms/epoch - 551us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0074 - val_loss: 0.0073 - 538ms/epoch - 504us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0073 - val_loss: 0.0072 - 522ms/epoch - 490us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0071 - 549ms/epoch - 514us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0070 - 527ms/epoch - 494us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0069 - 660ms/epoch - 619us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0068 - 573ms/epoch - 537us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0067 - 546ms/epoch - 512us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0066 - 576ms/epoch - 540us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0065 - 600ms/epoch - 562us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0064 - 551ms/epoch - 516us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0063 - 538ms/epoch - 504us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0062 - 529ms/epoch - 495us/step\n",
      "267/267 - 0s - loss: 0.0062 - 147ms/epoch - 551us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  43.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.4762 - val_loss: 0.6051\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.2353 - val_loss: 0.0729\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0544 - val_loss: 0.0431\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0362 - val_loss: 0.0307\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0266 - val_loss: 0.0233\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0210 - val_loss: 0.0193\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0180 - val_loss: 0.0169\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0160 - val_loss: 0.0151\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 524us/step - loss: 0.0143 - val_loss: 0.0137\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 518us/step - loss: 0.0132 - val_loss: 0.0128\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 0.0124 - val_loss: 0.0121\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0118 - val_loss: 0.0115\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0084 - val_loss: 0.0083 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0083 - val_loss: 0.0082 - 613ms/epoch - 574us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0081 - val_loss: 0.0080 - 619ms/epoch - 580us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0080 - val_loss: 0.0079 - 548ms/epoch - 513us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0078 - val_loss: 0.0078 - 521ms/epoch - 488us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0077 - val_loss: 0.0077 - 527ms/epoch - 494us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0076 - val_loss: 0.0075 - 521ms/epoch - 488us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0075 - val_loss: 0.0074 - 523ms/epoch - 490us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0074 - val_loss: 0.0073 - 524ms/epoch - 491us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0072 - val_loss: 0.0072 - 554ms/epoch - 519us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0071 - 519ms/epoch - 486us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0070 - 522ms/epoch - 489us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0069 - 523ms/epoch - 490us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0068 - 524ms/epoch - 491us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0067 - 525ms/epoch - 492us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0067 - 521ms/epoch - 489us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0066 - 522ms/epoch - 489us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 523ms/epoch - 490us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0065 - 527ms/epoch - 493us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0064 - 522ms/epoch - 490us/step\n",
      "267/267 - 0s - loss: 0.0064 - 150ms/epoch - 560us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.4786 - val_loss: 0.5916\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.2214 - val_loss: 0.0631\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0486 - val_loss: 0.0383\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 495us/step - loss: 0.0318 - val_loss: 0.0268\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 514us/step - loss: 0.0236 - val_loss: 0.0208\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 503us/step - loss: 0.0186 - val_loss: 0.0167\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 492us/step - loss: 0.0153 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 492us/step - loss: 0.0134 - val_loss: 0.0127\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 513us/step - loss: 0.0123 - val_loss: 0.0118\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 516us/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0077 - val_loss: 0.0077 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0076 - val_loss: 0.0075 - 539ms/epoch - 505us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0074 - val_loss: 0.0074 - 576ms/epoch - 540us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0073 - val_loss: 0.0073 - 564ms/epoch - 529us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0072 - val_loss: 0.0072 - 544ms/epoch - 510us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0070 - 534ms/epoch - 500us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0069 - 545ms/epoch - 511us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0068 - 552ms/epoch - 518us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0067 - 554ms/epoch - 519us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 561ms/epoch - 526us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0066 - 535ms/epoch - 501us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0065 - 554ms/epoch - 519us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0064 - 535ms/epoch - 501us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0063 - 527ms/epoch - 494us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0062 - 539ms/epoch - 505us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0062 - 547ms/epoch - 513us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0061 - 525ms/epoch - 492us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0060 - 516ms/epoch - 484us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0060 - 531ms/epoch - 498us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0059 - 547ms/epoch - 512us/step\n",
      "267/267 - 0s - loss: 0.0059 - 151ms/epoch - 566us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.5666 - val_loss: 0.6449\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 0.2408 - val_loss: 0.0591\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 500us/step - loss: 0.0446 - val_loss: 0.0354\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0308 - val_loss: 0.0272\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0245 - val_loss: 0.0221\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 504us/step - loss: 0.0204 - val_loss: 0.0189\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 501us/step - loss: 0.0179 - val_loss: 0.0170\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 522us/step - loss: 0.0162 - val_loss: 0.0154\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0148 - val_loss: 0.0143\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0138 - val_loss: 0.0133\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0129 - val_loss: 0.0126\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0123 - val_loss: 0.0120\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0086 - val_loss: 0.0085 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0084 - val_loss: 0.0083 - 530ms/epoch - 497us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0082 - val_loss: 0.0082 - 549ms/epoch - 515us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0081 - val_loss: 0.0080 - 525ms/epoch - 492us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0079 - val_loss: 0.0078 - 525ms/epoch - 492us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0078 - val_loss: 0.0077 - 561ms/epoch - 526us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0076 - val_loss: 0.0076 - 558ms/epoch - 523us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0075 - val_loss: 0.0074 - 587ms/epoch - 550us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0074 - val_loss: 0.0073 - 533ms/epoch - 500us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0073 - val_loss: 0.0072 - 529ms/epoch - 496us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0071 - 513ms/epoch - 481us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0069 - 548ms/epoch - 514us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0068 - 531ms/epoch - 498us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0067 - 531ms/epoch - 498us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 538ms/epoch - 505us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 542ms/epoch - 508us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0064 - 522ms/epoch - 489us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0063 - 571ms/epoch - 535us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0063 - 590ms/epoch - 553us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0062 - 554ms/epoch - 519us/step\n",
      "267/267 - 0s - loss: 0.0062 - 151ms/epoch - 565us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.5085 - val_loss: 0.5994\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.2268 - val_loss: 0.0643\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0485 - val_loss: 0.0388\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0333 - val_loss: 0.0290\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0259 - val_loss: 0.0233\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0215 - val_loss: 0.0199\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0185 - val_loss: 0.0173\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0162 - val_loss: 0.0152\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0143 - val_loss: 0.0136\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0130 - val_loss: 0.0124\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0120 - val_loss: 0.0116\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0090 - val_loss: 0.0088\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0083 - val_loss: 0.0082 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0081 - val_loss: 0.0080 - 539ms/epoch - 505us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0079 - val_loss: 0.0079 - 548ms/epoch - 513us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0078 - val_loss: 0.0077 - 533ms/epoch - 500us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0076 - val_loss: 0.0076 - 558ms/epoch - 523us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0075 - val_loss: 0.0075 - 582ms/epoch - 546us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0074 - val_loss: 0.0073 - 536ms/epoch - 502us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0072 - val_loss: 0.0072 - 578ms/epoch - 541us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0071 - 531ms/epoch - 498us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0069 - 601ms/epoch - 563us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0068 - 547ms/epoch - 513us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0067 - 543ms/epoch - 509us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 575ms/epoch - 539us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 561ms/epoch - 526us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0064 - 529ms/epoch - 495us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0064 - 532ms/epoch - 498us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0063 - 535ms/epoch - 501us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0062 - 536ms/epoch - 502us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0061 - 560ms/epoch - 525us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0061 - 542ms/epoch - 508us/step\n",
      "267/267 - 0s - loss: 0.0061 - 148ms/epoch - 554us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  42.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "5335/5335 [==============================] - 5s 986us/step - loss: 0.4005 - val_loss: 0.4877\n",
      "Epoch 2/20\n",
      "5335/5335 [==============================] - 4s 692us/step - loss: 0.3907 - val_loss: 0.2727\n",
      "Epoch 3/20\n",
      "5335/5335 [==============================] - 3s 640us/step - loss: 0.4056 - val_loss: 0.1936\n",
      "Epoch 4/20\n",
      "5335/5335 [==============================] - 4s 707us/step - loss: 0.3558 - val_loss: 0.3037\n",
      "Epoch 5/20\n",
      "5335/5335 [==============================] - 3s 637us/step - loss: 0.3592 - val_loss: 0.2566\n",
      "Epoch 6/20\n",
      "5335/5335 [==============================] - 4s 663us/step - loss: 0.3625 - val_loss: 0.1606\n",
      "Epoch 7/20\n",
      "5335/5335 [==============================] - 4s 668us/step - loss: 0.3153 - val_loss: 0.2606\n",
      "Epoch 8/20\n",
      "5335/5335 [==============================] - 4s 677us/step - loss: 0.3039 - val_loss: 0.1889\n",
      "Epoch 9/20\n",
      "5335/5335 [==============================] - 3s 615us/step - loss: 0.3267 - val_loss: 0.3785\n",
      "Epoch 10/20\n",
      "5335/5335 [==============================] - 3s 623us/step - loss: 0.3225 - val_loss: 0.3563\n",
      "Epoch 11/20\n",
      "5335/5335 [==============================] - 3s 627us/step - loss: 0.3148 - val_loss: 0.8553\n",
      "Epoch 12/20\n",
      "5335/5335 [==============================] - 3s 652us/step - loss: 0.3146 - val_loss: 0.1783\n",
      "Epoch 13/20\n",
      "5335/5335 [==============================] - 3s 600us/step - loss: 0.2923 - val_loss: 0.4070\n",
      "Epoch 14/20\n",
      "5335/5335 [==============================] - 3s 589us/step - loss: 0.3099 - val_loss: 0.1591\n",
      "Epoch 15/20\n",
      "5335/5335 [==============================] - 3s 598us/step - loss: 0.2717 - val_loss: 0.2572\n",
      "Epoch 16/20\n",
      "5335/5335 [==============================] - 3s 589us/step - loss: 0.3134 - val_loss: 0.8471\n",
      "Epoch 17/20\n",
      "5335/5335 [==============================] - 3s 600us/step - loss: 0.3103 - val_loss: 0.3732\n",
      "Epoch 18/20\n",
      "5335/5335 [==============================] - 3s 593us/step - loss: 0.2833 - val_loss: 0.3130\n",
      "Epoch 19/20\n",
      "5335/5335 [==============================] - 3s 591us/step - loss: 0.2846 - val_loss: 0.1620\n",
      "Epoch 20/20\n",
      "5335/5335 [==============================] - 3s 586us/step - loss: 0.2719 - val_loss: 0.4311\n",
      "Epoch 1/20\n",
      "4268/4268 - 4s - loss: 0.2923 - val_loss: 0.1082 - 4s/epoch - 930us/step\n",
      "Epoch 2/20\n",
      "4268/4268 - 2s - loss: 0.3030 - val_loss: 0.2379 - 2s/epoch - 546us/step\n",
      "Epoch 3/20\n",
      "4268/4268 - 2s - loss: 0.3199 - val_loss: 0.2700 - 2s/epoch - 538us/step\n",
      "Epoch 4/20\n",
      "4268/4268 - 2s - loss: 0.2692 - val_loss: 0.1076 - 2s/epoch - 547us/step\n",
      "Epoch 5/20\n",
      "4268/4268 - 2s - loss: 0.2713 - val_loss: 0.2326 - 2s/epoch - 542us/step\n",
      "Epoch 6/20\n",
      "4268/4268 - 2s - loss: 0.2786 - val_loss: 0.1608 - 2s/epoch - 547us/step\n",
      "Epoch 7/20\n",
      "4268/4268 - 2s - loss: 0.2818 - val_loss: 0.2229 - 2s/epoch - 541us/step\n",
      "Epoch 8/20\n",
      "4268/4268 - 2s - loss: 0.2794 - val_loss: 0.2436 - 2s/epoch - 548us/step\n",
      "Epoch 9/20\n",
      "4268/4268 - 2s - loss: 0.2709 - val_loss: 0.3953 - 2s/epoch - 543us/step\n",
      "Epoch 10/20\n",
      "4268/4268 - 2s - loss: 0.2976 - val_loss: 0.1445 - 2s/epoch - 546us/step\n",
      "Epoch 11/20\n",
      "4268/4268 - 2s - loss: 0.2956 - val_loss: 0.2585 - 2s/epoch - 542us/step\n",
      "Epoch 12/20\n",
      "4268/4268 - 2s - loss: 0.2782 - val_loss: 0.3614 - 2s/epoch - 548us/step\n",
      "Epoch 13/20\n",
      "4268/4268 - 2s - loss: 0.2812 - val_loss: 0.2465 - 2s/epoch - 541us/step\n",
      "Epoch 14/20\n",
      "4268/4268 - 2s - loss: 0.2851 - val_loss: 0.3156 - 2s/epoch - 548us/step\n",
      "Epoch 15/20\n",
      "4268/4268 - 2s - loss: 0.2603 - val_loss: 0.1982 - 2s/epoch - 541us/step\n",
      "Epoch 16/20\n",
      "4268/4268 - 2s - loss: 0.2829 - val_loss: 0.1604 - 2s/epoch - 550us/step\n",
      "Epoch 17/20\n",
      "4268/4268 - 3s - loss: 0.2984 - val_loss: 0.2250 - 3s/epoch - 601us/step\n",
      "Epoch 18/20\n",
      "4268/4268 - 2s - loss: 0.2943 - val_loss: 0.1350 - 2s/epoch - 558us/step\n",
      "Epoch 19/20\n",
      "4268/4268 - 2s - loss: 0.2852 - val_loss: 0.2603 - 2s/epoch - 548us/step\n",
      "Epoch 20/20\n",
      "4268/4268 - 2s - loss: 0.2901 - val_loss: 0.1844 - 2s/epoch - 558us/step\n",
      "1067/1067 - 0s - loss: 0.1511 - 370ms/epoch - 347us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.2min\n",
      "Epoch 1/20\n",
      "5335/5335 [==============================] - 5s 920us/step - loss: 0.3435 - val_loss: 0.2719\n",
      "Epoch 2/20\n",
      "5335/5335 [==============================] - 3s 594us/step - loss: 0.3996 - val_loss: 0.6741\n",
      "Epoch 3/20\n",
      "5335/5335 [==============================] - 3s 612us/step - loss: 0.3244 - val_loss: 0.2444\n",
      "Epoch 4/20\n",
      "5335/5335 [==============================] - 3s 656us/step - loss: 0.3090 - val_loss: 0.2177\n",
      "Epoch 5/20\n",
      "5335/5335 [==============================] - 4s 666us/step - loss: 0.3321 - val_loss: 0.7220\n",
      "Epoch 6/20\n",
      "5335/5335 [==============================] - 4s 672us/step - loss: 0.2835 - val_loss: 0.2862\n",
      "Epoch 7/20\n",
      "5335/5335 [==============================] - 4s 680us/step - loss: 0.3294 - val_loss: 0.2477\n",
      "Epoch 8/20\n",
      "5335/5335 [==============================] - 3s 640us/step - loss: 0.3083 - val_loss: 0.2419\n",
      "Epoch 9/20\n",
      "5335/5335 [==============================] - 3s 620us/step - loss: 0.2785 - val_loss: 0.1681\n",
      "Epoch 10/20\n",
      "5335/5335 [==============================] - 4s 680us/step - loss: 0.2948 - val_loss: 0.1577\n",
      "Epoch 11/20\n",
      "5335/5335 [==============================] - 3s 631us/step - loss: 0.2737 - val_loss: 0.2978\n",
      "Epoch 12/20\n",
      "5335/5335 [==============================] - 3s 617us/step - loss: 0.2894 - val_loss: 0.3878\n",
      "Epoch 13/20\n",
      "5335/5335 [==============================] - 3s 639us/step - loss: 0.2932 - val_loss: 0.1863\n",
      "Epoch 14/20\n",
      "5335/5335 [==============================] - 3s 620us/step - loss: 0.3262 - val_loss: 0.1987\n",
      "Epoch 15/20\n",
      "5335/5335 [==============================] - 3s 648us/step - loss: 0.3216 - val_loss: 0.2618\n",
      "Epoch 16/20\n",
      "5335/5335 [==============================] - 3s 609us/step - loss: 0.3092 - val_loss: 0.1387\n",
      "Epoch 17/20\n",
      "5335/5335 [==============================] - 4s 660us/step - loss: 0.3006 - val_loss: 0.1668\n",
      "Epoch 18/20\n",
      "5335/5335 [==============================] - 4s 657us/step - loss: 0.3723 - val_loss: 0.2153\n",
      "Epoch 19/20\n",
      "5335/5335 [==============================] - 3s 608us/step - loss: 0.3583 - val_loss: 0.2665\n",
      "Epoch 20/20\n",
      "5335/5335 [==============================] - 3s 614us/step - loss: 0.3411 - val_loss: 0.1850\n",
      "Epoch 1/20\n",
      "4268/4268 - 4s - loss: 0.3337 - val_loss: 0.4036 - 4s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "4268/4268 - 2s - loss: 0.3155 - val_loss: 0.3267 - 2s/epoch - 581us/step\n",
      "Epoch 3/20\n",
      "4268/4268 - 3s - loss: 0.2727 - val_loss: 0.2967 - 3s/epoch - 619us/step\n",
      "Epoch 4/20\n",
      "4268/4268 - 3s - loss: 0.2922 - val_loss: 0.2640 - 3s/epoch - 602us/step\n",
      "Epoch 5/20\n",
      "4268/4268 - 3s - loss: 0.3152 - val_loss: 0.3445 - 3s/epoch - 604us/step\n",
      "Epoch 6/20\n",
      "4268/4268 - 3s - loss: 0.2671 - val_loss: 0.2319 - 3s/epoch - 601us/step\n",
      "Epoch 7/20\n",
      "4268/4268 - 3s - loss: 0.2940 - val_loss: 0.1428 - 3s/epoch - 609us/step\n",
      "Epoch 8/20\n",
      "4268/4268 - 3s - loss: 0.3067 - val_loss: 0.4697 - 3s/epoch - 587us/step\n",
      "Epoch 9/20\n",
      "4268/4268 - 2s - loss: 0.3084 - val_loss: 0.1915 - 2s/epoch - 557us/step\n",
      "Epoch 10/20\n",
      "4268/4268 - 2s - loss: 0.3258 - val_loss: 0.5209 - 2s/epoch - 556us/step\n",
      "Epoch 11/20\n",
      "4268/4268 - 3s - loss: 0.3025 - val_loss: 0.5993 - 3s/epoch - 586us/step\n",
      "Epoch 12/20\n",
      "4268/4268 - 2s - loss: 0.2753 - val_loss: 0.6049 - 2s/epoch - 555us/step\n",
      "Epoch 13/20\n",
      "4268/4268 - 2s - loss: 0.2883 - val_loss: 0.1786 - 2s/epoch - 582us/step\n",
      "Epoch 14/20\n",
      "4268/4268 - 3s - loss: 0.3224 - val_loss: 0.2990 - 3s/epoch - 590us/step\n",
      "Epoch 15/20\n",
      "4268/4268 - 3s - loss: 0.2801 - val_loss: 0.1543 - 3s/epoch - 616us/step\n",
      "Epoch 16/20\n",
      "4268/4268 - 3s - loss: 0.2870 - val_loss: 0.0802 - 3s/epoch - 620us/step\n",
      "Epoch 17/20\n",
      "4268/4268 - 2s - loss: 0.2768 - val_loss: 0.4077 - 2s/epoch - 562us/step\n",
      "Epoch 18/20\n",
      "4268/4268 - 2s - loss: 0.3245 - val_loss: 0.1934 - 2s/epoch - 568us/step\n",
      "Epoch 19/20\n",
      "4268/4268 - 3s - loss: 0.2898 - val_loss: 0.5856 - 3s/epoch - 598us/step\n",
      "Epoch 20/20\n",
      "4268/4268 - 2s - loss: 0.3192 - val_loss: 0.1794 - 2s/epoch - 566us/step\n",
      "1067/1067 - 0s - loss: 0.1439 - 348ms/epoch - 326us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.2min\n",
      "Epoch 1/20\n",
      "5335/5335 [==============================] - 5s 992us/step - loss: 0.3624 - val_loss: 0.3494\n",
      "Epoch 2/20\n",
      "5335/5335 [==============================] - 4s 659us/step - loss: 0.3385 - val_loss: 1.1616\n",
      "Epoch 3/20\n",
      "5335/5335 [==============================] - 3s 649us/step - loss: 0.3422 - val_loss: 0.1369\n",
      "Epoch 4/20\n",
      "5335/5335 [==============================] - 3s 625us/step - loss: 0.3148 - val_loss: 0.4281\n",
      "Epoch 5/20\n",
      "5335/5335 [==============================] - 3s 653us/step - loss: 0.3153 - val_loss: 0.2246\n",
      "Epoch 6/20\n",
      "5335/5335 [==============================] - 3s 656us/step - loss: 0.3060 - val_loss: 0.1709\n",
      "Epoch 7/20\n",
      "5335/5335 [==============================] - 3s 650us/step - loss: 0.2922 - val_loss: 0.3635\n",
      "Epoch 8/20\n",
      "5335/5335 [==============================] - 3s 653us/step - loss: 0.2930 - val_loss: 0.1638\n",
      "Epoch 9/20\n",
      "5335/5335 [==============================] - 4s 707us/step - loss: 0.3476 - val_loss: 0.2073\n",
      "Epoch 10/20\n",
      "5335/5335 [==============================] - 3s 643us/step - loss: 0.3005 - val_loss: 0.8481\n",
      "Epoch 11/20\n",
      "5335/5335 [==============================] - 3s 641us/step - loss: 0.3203 - val_loss: 0.2576\n",
      "Epoch 12/20\n",
      "5335/5335 [==============================] - 3s 654us/step - loss: 0.3329 - val_loss: 0.3266\n",
      "Epoch 13/20\n",
      "5335/5335 [==============================] - 3s 642us/step - loss: 0.2988 - val_loss: 0.3788\n",
      "Epoch 14/20\n",
      "5335/5335 [==============================] - 3s 635us/step - loss: 0.2933 - val_loss: 0.1421\n",
      "Epoch 15/20\n",
      "5335/5335 [==============================] - 3s 647us/step - loss: 0.2769 - val_loss: 0.2699\n",
      "Epoch 16/20\n",
      "5335/5335 [==============================] - 3s 655us/step - loss: 0.2919 - val_loss: 0.3496\n",
      "Epoch 17/20\n",
      "5335/5335 [==============================] - 4s 675us/step - loss: 0.2943 - val_loss: 0.1545\n",
      "Epoch 18/20\n",
      "5335/5335 [==============================] - 4s 668us/step - loss: 0.2986 - val_loss: 0.5638\n",
      "Epoch 19/20\n",
      "5335/5335 [==============================] - 3s 656us/step - loss: 0.3022 - val_loss: 0.2403\n",
      "Epoch 20/20\n",
      "5335/5335 [==============================] - 4s 659us/step - loss: 0.3223 - val_loss: 0.2180\n",
      "Epoch 1/20\n",
      "4268/4268 - 4s - loss: 0.3247 - val_loss: 0.5128 - 4s/epoch - 976us/step\n",
      "Epoch 2/20\n",
      "4268/4268 - 2s - loss: 0.3291 - val_loss: 0.3462 - 2s/epoch - 561us/step\n",
      "Epoch 3/20\n",
      "4268/4268 - 2s - loss: 0.3360 - val_loss: 0.5357 - 2s/epoch - 564us/step\n",
      "Epoch 4/20\n",
      "4268/4268 - 3s - loss: 0.3107 - val_loss: 0.1196 - 3s/epoch - 601us/step\n",
      "Epoch 5/20\n",
      "4268/4268 - 3s - loss: 0.3106 - val_loss: 0.1413 - 3s/epoch - 598us/step\n",
      "Epoch 6/20\n",
      "4268/4268 - 3s - loss: 0.2635 - val_loss: 0.1533 - 3s/epoch - 602us/step\n",
      "Epoch 7/20\n",
      "4268/4268 - 3s - loss: 0.2664 - val_loss: 0.3573 - 3s/epoch - 608us/step\n",
      "Epoch 8/20\n",
      "4268/4268 - 3s - loss: 0.2640 - val_loss: 0.2442 - 3s/epoch - 611us/step\n",
      "Epoch 9/20\n",
      "4268/4268 - 3s - loss: 0.3126 - val_loss: 0.4511 - 3s/epoch - 613us/step\n",
      "Epoch 10/20\n",
      "4268/4268 - 3s - loss: 0.3535 - val_loss: 0.2909 - 3s/epoch - 588us/step\n",
      "Epoch 11/20\n",
      "4268/4268 - 3s - loss: 0.3088 - val_loss: 0.3252 - 3s/epoch - 604us/step\n",
      "Epoch 12/20\n",
      "4268/4268 - 3s - loss: 0.3381 - val_loss: 0.2192 - 3s/epoch - 609us/step\n",
      "Epoch 13/20\n",
      "4268/4268 - 3s - loss: 0.2832 - val_loss: 0.1632 - 3s/epoch - 590us/step\n",
      "Epoch 14/20\n",
      "4268/4268 - 3s - loss: 0.3159 - val_loss: 0.3555 - 3s/epoch - 599us/step\n",
      "Epoch 15/20\n",
      "4268/4268 - 3s - loss: 0.3068 - val_loss: 0.1764 - 3s/epoch - 622us/step\n",
      "Epoch 16/20\n",
      "4268/4268 - 3s - loss: 0.2731 - val_loss: 0.3905 - 3s/epoch - 593us/step\n",
      "Epoch 17/20\n",
      "4268/4268 - 3s - loss: 0.3229 - val_loss: 0.1829 - 3s/epoch - 600us/step\n",
      "Epoch 18/20\n",
      "4268/4268 - 3s - loss: 0.2846 - val_loss: 0.4096 - 3s/epoch - 588us/step\n",
      "Epoch 19/20\n",
      "4268/4268 - 3s - loss: 0.2863 - val_loss: 0.1137 - 3s/epoch - 608us/step\n",
      "Epoch 20/20\n",
      "4268/4268 - 3s - loss: 0.3283 - val_loss: 0.2824 - 3s/epoch - 617us/step\n",
      "1067/1067 - 0s - loss: 0.2749 - 400ms/epoch - 375us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Epoch 1/20\n",
      "5335/5335 [==============================] - 5s 943us/step - loss: 0.3085 - val_loss: 0.7397\n",
      "Epoch 2/20\n",
      "5335/5335 [==============================] - 3s 630us/step - loss: 0.3866 - val_loss: 0.3753\n",
      "Epoch 3/20\n",
      "5335/5335 [==============================] - 3s 640us/step - loss: 0.3286 - val_loss: 0.1761\n",
      "Epoch 4/20\n",
      "5335/5335 [==============================] - 3s 630us/step - loss: 0.3131 - val_loss: 0.2437\n",
      "Epoch 5/20\n",
      "5335/5335 [==============================] - 3s 639us/step - loss: 0.2850 - val_loss: 0.3594\n",
      "Epoch 6/20\n",
      "5335/5335 [==============================] - 3s 638us/step - loss: 0.2968 - val_loss: 0.1682\n",
      "Epoch 7/20\n",
      "5335/5335 [==============================] - 3s 600us/step - loss: 0.2956 - val_loss: 0.7119\n",
      "Epoch 8/20\n",
      "5335/5335 [==============================] - 3s 605us/step - loss: 0.3179 - val_loss: 0.1702\n",
      "Epoch 9/20\n",
      "5335/5335 [==============================] - 3s 601us/step - loss: 0.3067 - val_loss: 0.1775\n",
      "Epoch 10/20\n",
      "5335/5335 [==============================] - 3s 607us/step - loss: 0.3178 - val_loss: 0.1899\n",
      "Epoch 11/20\n",
      "5335/5335 [==============================] - 3s 628us/step - loss: 0.2859 - val_loss: 0.2505\n",
      "Epoch 12/20\n",
      "5335/5335 [==============================] - 3s 645us/step - loss: 0.3046 - val_loss: 0.1672\n",
      "Epoch 13/20\n",
      "5335/5335 [==============================] - 4s 680us/step - loss: 0.2887 - val_loss: 0.1371\n",
      "Epoch 14/20\n",
      "5335/5335 [==============================] - 4s 658us/step - loss: 0.3302 - val_loss: 0.1845\n",
      "Epoch 15/20\n",
      "5335/5335 [==============================] - 3s 651us/step - loss: 0.2662 - val_loss: 0.4325\n",
      "Epoch 16/20\n",
      "5335/5335 [==============================] - 3s 587us/step - loss: 0.3177 - val_loss: 0.3856\n",
      "Epoch 17/20\n",
      "5335/5335 [==============================] - 4s 666us/step - loss: 0.2955 - val_loss: 0.4129\n",
      "Epoch 18/20\n",
      "5335/5335 [==============================] - 4s 660us/step - loss: 0.2948 - val_loss: 0.2380\n",
      "Epoch 19/20\n",
      "5335/5335 [==============================] - 4s 671us/step - loss: 0.3123 - val_loss: 0.4232\n",
      "Epoch 20/20\n",
      "5335/5335 [==============================] - 3s 637us/step - loss: 0.2639 - val_loss: 0.6904\n",
      "Epoch 1/20\n",
      "4268/4268 - 4s - loss: 0.3238 - val_loss: 0.6521 - 4s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "4268/4268 - 2s - loss: 0.3103 - val_loss: 0.3719 - 2s/epoch - 551us/step\n",
      "Epoch 3/20\n",
      "4268/4268 - 2s - loss: 0.3129 - val_loss: 0.2393 - 2s/epoch - 546us/step\n",
      "Epoch 4/20\n",
      "4268/4268 - 2s - loss: 0.3055 - val_loss: 0.2098 - 2s/epoch - 567us/step\n",
      "Epoch 5/20\n",
      "4268/4268 - 2s - loss: 0.2832 - val_loss: 0.1885 - 2s/epoch - 572us/step\n",
      "Epoch 6/20\n",
      "4268/4268 - 2s - loss: 0.3052 - val_loss: 0.1868 - 2s/epoch - 584us/step\n",
      "Epoch 7/20\n",
      "4268/4268 - 2s - loss: 0.2868 - val_loss: 0.3806 - 2s/epoch - 579us/step\n",
      "Epoch 8/20\n",
      "4268/4268 - 2s - loss: 0.3326 - val_loss: 0.1579 - 2s/epoch - 570us/step\n",
      "Epoch 9/20\n",
      "4268/4268 - 2s - loss: 0.3030 - val_loss: 1.1720 - 2s/epoch - 566us/step\n",
      "Epoch 10/20\n",
      "4268/4268 - 3s - loss: 0.2958 - val_loss: 0.3906 - 3s/epoch - 589us/step\n",
      "Epoch 11/20\n",
      "4268/4268 - 3s - loss: 0.3159 - val_loss: 0.3337 - 3s/epoch - 597us/step\n",
      "Epoch 12/20\n",
      "4268/4268 - 3s - loss: 0.3193 - val_loss: 0.4202 - 3s/epoch - 588us/step\n",
      "Epoch 13/20\n",
      "4268/4268 - 3s - loss: 0.3079 - val_loss: 0.0989 - 3s/epoch - 588us/step\n",
      "Epoch 14/20\n",
      "4268/4268 - 3s - loss: 0.2727 - val_loss: 0.1878 - 3s/epoch - 647us/step\n",
      "Epoch 15/20\n",
      "4268/4268 - 3s - loss: 0.3323 - val_loss: 0.2155 - 3s/epoch - 639us/step\n",
      "Epoch 16/20\n",
      "4268/4268 - 3s - loss: 0.2928 - val_loss: 0.1893 - 3s/epoch - 593us/step\n",
      "Epoch 17/20\n",
      "4268/4268 - 2s - loss: 0.3050 - val_loss: 0.4396 - 2s/epoch - 544us/step\n",
      "Epoch 18/20\n",
      "4268/4268 - 2s - loss: 0.2687 - val_loss: 0.3149 - 2s/epoch - 579us/step\n",
      "Epoch 19/20\n",
      "4268/4268 - 3s - loss: 0.2813 - val_loss: 0.4077 - 3s/epoch - 600us/step\n",
      "Epoch 20/20\n",
      "4268/4268 - 2s - loss: 0.3293 - val_loss: 0.3834 - 2s/epoch - 555us/step\n",
      "1067/1067 - 0s - loss: 0.3108 - 374ms/epoch - 351us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.2min\n",
      "Epoch 1/20\n",
      "5335/5335 [==============================] - 6s 1ms/step - loss: 0.3665 - val_loss: 0.6210\n",
      "Epoch 2/20\n",
      "5335/5335 [==============================] - 4s 670us/step - loss: 0.3598 - val_loss: 0.2912\n",
      "Epoch 3/20\n",
      "5335/5335 [==============================] - 3s 636us/step - loss: 0.3641 - val_loss: 0.1955\n",
      "Epoch 4/20\n",
      "5335/5335 [==============================] - 3s 647us/step - loss: 0.3285 - val_loss: 0.2718\n",
      "Epoch 5/20\n",
      "5335/5335 [==============================] - 3s 646us/step - loss: 0.3267 - val_loss: 0.2945\n",
      "Epoch 6/20\n",
      "5335/5335 [==============================] - 3s 636us/step - loss: 0.3402 - val_loss: 0.2478\n",
      "Epoch 7/20\n",
      "5335/5335 [==============================] - 3s 638us/step - loss: 0.3152 - val_loss: 0.2214\n",
      "Epoch 8/20\n",
      "5335/5335 [==============================] - 3s 585us/step - loss: 0.3056 - val_loss: 0.3033\n",
      "Epoch 9/20\n",
      "5335/5335 [==============================] - 3s 618us/step - loss: 0.3024 - val_loss: 0.1813\n",
      "Epoch 10/20\n",
      "5335/5335 [==============================] - 3s 641us/step - loss: 0.3423 - val_loss: 0.3030\n",
      "Epoch 11/20\n",
      "5335/5335 [==============================] - 4s 737us/step - loss: 0.2775 - val_loss: 0.2643\n",
      "Epoch 12/20\n",
      "5335/5335 [==============================] - 3s 624us/step - loss: 0.3240 - val_loss: 0.3432\n",
      "Epoch 13/20\n",
      "5335/5335 [==============================] - 3s 602us/step - loss: 0.2870 - val_loss: 0.3586\n",
      "Epoch 14/20\n",
      "5335/5335 [==============================] - 3s 594us/step - loss: 0.2956 - val_loss: 0.1611\n",
      "Epoch 15/20\n",
      "5335/5335 [==============================] - 3s 588us/step - loss: 0.2874 - val_loss: 0.1961\n",
      "Epoch 16/20\n",
      "5335/5335 [==============================] - 3s 592us/step - loss: 0.3379 - val_loss: 0.4310\n",
      "Epoch 17/20\n",
      "5335/5335 [==============================] - 3s 587us/step - loss: 0.3471 - val_loss: 0.3853\n",
      "Epoch 18/20\n",
      "5335/5335 [==============================] - 3s 589us/step - loss: 0.3006 - val_loss: 0.2111\n",
      "Epoch 19/20\n",
      "5335/5335 [==============================] - 3s 588us/step - loss: 0.3085 - val_loss: 0.2547\n",
      "Epoch 20/20\n",
      "5335/5335 [==============================] - 3s 597us/step - loss: 0.2877 - val_loss: 0.2782\n",
      "Epoch 1/20\n",
      "4268/4268 - 4s - loss: 0.2883 - val_loss: 1.3555 - 4s/epoch - 931us/step\n",
      "Epoch 2/20\n",
      "4268/4268 - 2s - loss: 0.2925 - val_loss: 0.1833 - 2s/epoch - 548us/step\n",
      "Epoch 3/20\n",
      "4268/4268 - 2s - loss: 0.2965 - val_loss: 0.5194 - 2s/epoch - 541us/step\n",
      "Epoch 4/20\n",
      "4268/4268 - 2s - loss: 0.2942 - val_loss: 0.3170 - 2s/epoch - 547us/step\n",
      "Epoch 5/20\n",
      "4268/4268 - 2s - loss: 0.2838 - val_loss: 0.1201 - 2s/epoch - 538us/step\n",
      "Epoch 6/20\n",
      "4268/4268 - 2s - loss: 0.2870 - val_loss: 0.1612 - 2s/epoch - 547us/step\n",
      "Epoch 7/20\n",
      "4268/4268 - 2s - loss: 0.3019 - val_loss: 0.1755 - 2s/epoch - 542us/step\n",
      "Epoch 8/20\n",
      "4268/4268 - 2s - loss: 0.3118 - val_loss: 0.2166 - 2s/epoch - 548us/step\n",
      "Epoch 9/20\n",
      "4268/4268 - 2s - loss: 0.3265 - val_loss: 0.2009 - 2s/epoch - 551us/step\n",
      "Epoch 10/20\n",
      "4268/4268 - 3s - loss: 0.3122 - val_loss: 0.2392 - 3s/epoch - 632us/step\n",
      "Epoch 11/20\n",
      "4268/4268 - 2s - loss: 0.2976 - val_loss: 0.1720 - 2s/epoch - 542us/step\n",
      "Epoch 12/20\n",
      "4268/4268 - 2s - loss: 0.2987 - val_loss: 0.1505 - 2s/epoch - 550us/step\n",
      "Epoch 13/20\n",
      "4268/4268 - 2s - loss: 0.3179 - val_loss: 0.3772 - 2s/epoch - 564us/step\n",
      "Epoch 14/20\n",
      "4268/4268 - 2s - loss: 0.2815 - val_loss: 0.1520 - 2s/epoch - 546us/step\n",
      "Epoch 15/20\n",
      "4268/4268 - 2s - loss: 0.3236 - val_loss: 0.3671 - 2s/epoch - 541us/step\n",
      "Epoch 16/20\n",
      "4268/4268 - 2s - loss: 0.2958 - val_loss: 0.2501 - 2s/epoch - 552us/step\n",
      "Epoch 17/20\n",
      "4268/4268 - 2s - loss: 0.2922 - val_loss: 0.2501 - 2s/epoch - 572us/step\n",
      "Epoch 18/20\n",
      "4268/4268 - 2s - loss: 0.2897 - val_loss: 0.3036 - 2s/epoch - 547us/step\n",
      "Epoch 19/20\n",
      "4268/4268 - 2s - loss: 0.2948 - val_loss: 0.3241 - 2s/epoch - 543us/step\n",
      "Epoch 20/20\n",
      "4268/4268 - 2s - loss: 0.3115 - val_loss: 0.1729 - 2s/epoch - 549us/step\n",
      "1067/1067 - 0s - loss: 0.1395 - 372ms/epoch - 348us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.2min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.1187 - val_loss: 0.1549\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 597us/step - loss: 0.1864 - val_loss: 0.5310\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 610us/step - loss: 0.2239 - val_loss: 0.0905\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 607us/step - loss: 0.1840 - val_loss: 0.1115\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 604us/step - loss: 0.1892 - val_loss: 0.1608\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 607us/step - loss: 0.2635 - val_loss: 0.1257\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 609us/step - loss: 0.1989 - val_loss: 0.0881\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 594us/step - loss: 0.2338 - val_loss: 0.2525\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 608us/step - loss: 0.2116 - val_loss: 0.0985\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 608us/step - loss: 0.2115 - val_loss: 0.2998\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 606us/step - loss: 0.2367 - val_loss: 0.1263\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 610us/step - loss: 0.1835 - val_loss: 0.1685\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 607us/step - loss: 0.1808 - val_loss: 0.1290\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 604us/step - loss: 0.2134 - val_loss: 0.3156\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 605us/step - loss: 0.2176 - val_loss: 0.1826\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 607us/step - loss: 0.1870 - val_loss: 0.3084\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 609us/step - loss: 0.2025 - val_loss: 0.2501\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 617us/step - loss: 0.1855 - val_loss: 0.1580\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 653us/step - loss: 0.1899 - val_loss: 0.1380\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 690us/step - loss: 0.1839 - val_loss: 0.2392\n",
      "Epoch 1/20\n",
      "2134/2134 - 3s - loss: 0.2202 - val_loss: 0.1128 - 3s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 0.1854 - val_loss: 0.1403 - 1s/epoch - 571us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 0.1849 - val_loss: 0.1525 - 1s/epoch - 583us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 0.1909 - val_loss: 0.4531 - 1s/epoch - 653us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.1988 - val_loss: 0.1372 - 1s/epoch - 660us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 0.1944 - val_loss: 0.1283 - 1s/epoch - 624us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 0.1768 - val_loss: 0.2256 - 1s/epoch - 598us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 0.2111 - val_loss: 0.1859 - 1s/epoch - 566us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 0.1918 - val_loss: 0.0903 - 1s/epoch - 579us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.1899 - val_loss: 0.1244 - 1s/epoch - 595us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 0.1904 - val_loss: 0.2594 - 1s/epoch - 572us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 0.1866 - val_loss: 0.0768 - 1s/epoch - 559us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 0.1833 - val_loss: 0.1053 - 1s/epoch - 566us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 0.1803 - val_loss: 0.3934 - 1s/epoch - 590us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 0.1886 - val_loss: 0.1814 - 1s/epoch - 592us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 0.2126 - val_loss: 0.5430 - 1s/epoch - 648us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 0.1807 - val_loss: 0.0721 - 1s/epoch - 610us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 0.1750 - val_loss: 0.2105 - 1s/epoch - 611us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.2244 - val_loss: 0.1456 - 1s/epoch - 613us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 0.1880 - val_loss: 0.0887 - 1s/epoch - 637us/step\n",
      "534/534 - 0s - loss: 0.0769 - 240ms/epoch - 450us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.2min\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.1568 - val_loss: 0.1649\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 664us/step - loss: 0.1696 - val_loss: 0.2586\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 691us/step - loss: 0.1822 - val_loss: 0.1732\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 667us/step - loss: 0.1973 - val_loss: 0.4681\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 665us/step - loss: 0.2675 - val_loss: 0.2064\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 695us/step - loss: 0.1695 - val_loss: 0.1206\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 740us/step - loss: 0.2643 - val_loss: 0.1386\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 681us/step - loss: 0.2138 - val_loss: 0.2943\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 638us/step - loss: 0.1918 - val_loss: 0.4226\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 671us/step - loss: 0.2111 - val_loss: 0.1579\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 691us/step - loss: 0.2136 - val_loss: 0.1740\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 695us/step - loss: 0.1766 - val_loss: 0.2957\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 682us/step - loss: 0.1753 - val_loss: 0.0755\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 676us/step - loss: 0.1583 - val_loss: 0.0799\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 679us/step - loss: 0.1995 - val_loss: 0.1795\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 702us/step - loss: 0.2018 - val_loss: 0.1393\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 663us/step - loss: 0.2149 - val_loss: 0.1026\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 689us/step - loss: 0.1584 - val_loss: 0.4371\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 688us/step - loss: 0.1984 - val_loss: 0.2166\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 651us/step - loss: 0.1826 - val_loss: 0.0926\n",
      "Epoch 1/20\n",
      "2134/2134 - 3s - loss: 0.2017 - val_loss: 0.3733 - 3s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 0.2320 - val_loss: 0.1700 - 1s/epoch - 623us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 0.1996 - val_loss: 0.5448 - 1s/epoch - 624us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 0.2079 - val_loss: 0.1320 - 1s/epoch - 634us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.2009 - val_loss: 0.1824 - 1s/epoch - 594us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 0.1836 - val_loss: 0.1227 - 1s/epoch - 594us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 0.1717 - val_loss: 0.1587 - 1s/epoch - 603us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 0.1740 - val_loss: 0.0981 - 1s/epoch - 637us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 0.1837 - val_loss: 0.5076 - 1s/epoch - 635us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.2193 - val_loss: 0.1747 - 1s/epoch - 604us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 0.1914 - val_loss: 0.1114 - 1s/epoch - 621us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 0.1818 - val_loss: 0.2113 - 1s/epoch - 616us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 0.1972 - val_loss: 0.2049 - 1s/epoch - 618us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 0.1956 - val_loss: 0.1317 - 1s/epoch - 605us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 0.2015 - val_loss: 0.1544 - 1s/epoch - 629us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 0.1658 - val_loss: 0.1705 - 1s/epoch - 668us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 0.1995 - val_loss: 0.0882 - 1s/epoch - 585us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 0.2071 - val_loss: 0.2316 - 1s/epoch - 600us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.1750 - val_loss: 0.0744 - 1s/epoch - 630us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 0.1776 - val_loss: 0.2050 - 1s/epoch - 608us/step\n",
      "534/534 - 0s - loss: 0.1656 - 245ms/epoch - 459us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.0983 - val_loss: 0.0705\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 660us/step - loss: 0.2648 - val_loss: 0.1412\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 678us/step - loss: 0.1912 - val_loss: 0.1512\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 829us/step - loss: 0.2046 - val_loss: 0.4180\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 801us/step - loss: 0.2307 - val_loss: 0.0683\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 626us/step - loss: 0.1917 - val_loss: 0.4732\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 667us/step - loss: 0.2071 - val_loss: 0.3920\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 667us/step - loss: 0.2177 - val_loss: 0.2684\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 652us/step - loss: 0.1914 - val_loss: 0.1955\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 738us/step - loss: 0.1789 - val_loss: 0.1275\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 640us/step - loss: 0.1766 - val_loss: 0.2051\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 670us/step - loss: 0.2040 - val_loss: 0.1557\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 681us/step - loss: 0.1711 - val_loss: 0.1302\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 646us/step - loss: 0.1831 - val_loss: 0.1462\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 662us/step - loss: 0.2067 - val_loss: 0.1685\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 636us/step - loss: 0.2219 - val_loss: 0.2883\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 669us/step - loss: 0.1848 - val_loss: 0.3990\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 676us/step - loss: 0.2180 - val_loss: 0.3372\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 647us/step - loss: 0.1967 - val_loss: 0.0835\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 681us/step - loss: 0.2043 - val_loss: 0.3934\n",
      "Epoch 1/20\n",
      "2134/2134 - 3s - loss: 0.1981 - val_loss: 0.1894 - 3s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 0.2164 - val_loss: 0.1288 - 1s/epoch - 591us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 0.2141 - val_loss: 0.2802 - 1s/epoch - 587us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 0.1773 - val_loss: 0.2478 - 1s/epoch - 623us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.1567 - val_loss: 0.1050 - 1s/epoch - 610us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 2s - loss: 0.1742 - val_loss: 0.1400 - 2s/epoch - 746us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 0.1640 - val_loss: 0.0758 - 1s/epoch - 666us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 0.2090 - val_loss: 0.1443 - 1s/epoch - 605us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 0.1760 - val_loss: 0.0917 - 1s/epoch - 621us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.1998 - val_loss: 0.1474 - 1s/epoch - 612us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 0.2188 - val_loss: 0.1501 - 1s/epoch - 610us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 0.1774 - val_loss: 0.2087 - 1s/epoch - 633us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 0.1999 - val_loss: 0.2847 - 1s/epoch - 559us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 0.2139 - val_loss: 0.1297 - 1s/epoch - 570us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 0.1858 - val_loss: 0.1145 - 1s/epoch - 574us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 0.1849 - val_loss: 0.1640 - 1s/epoch - 607us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 0.2065 - val_loss: 0.2837 - 1s/epoch - 603us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 0.2226 - val_loss: 0.7963 - 1s/epoch - 648us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.1991 - val_loss: 0.1350 - 1s/epoch - 602us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 0.2153 - val_loss: 0.1926 - 1s/epoch - 634us/step\n",
      "534/534 - 0s - loss: 0.1483 - 244ms/epoch - 457us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.1696 - val_loss: 0.0989\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 660us/step - loss: 0.1282 - val_loss: 0.1342\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 673us/step - loss: 0.1248 - val_loss: 0.0730\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 667us/step - loss: 0.1343 - val_loss: 0.1175\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 644us/step - loss: 0.2091 - val_loss: 0.0853\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 709us/step - loss: 0.2004 - val_loss: 0.2125\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 633us/step - loss: 0.1979 - val_loss: 0.1619\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 635us/step - loss: 0.1948 - val_loss: 0.0863\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 640us/step - loss: 0.1821 - val_loss: 0.2360\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 657us/step - loss: 0.2122 - val_loss: 0.1069\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 624us/step - loss: 0.2348 - val_loss: 0.5104\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 669us/step - loss: 0.1802 - val_loss: 0.1617\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 641us/step - loss: 0.2085 - val_loss: 0.3182\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 655us/step - loss: 0.1768 - val_loss: 0.1583\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 626us/step - loss: 0.1780 - val_loss: 0.0544\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 626us/step - loss: 0.1746 - val_loss: 0.3028\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 626us/step - loss: 0.2009 - val_loss: 0.4489\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 615us/step - loss: 0.1922 - val_loss: 0.4516\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 627us/step - loss: 0.2224 - val_loss: 0.2564\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 737us/step - loss: 0.1847 - val_loss: 0.2477\n",
      "Epoch 1/20\n",
      "2134/2134 - 3s - loss: 0.1810 - val_loss: 0.4370 - 3s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 0.1919 - val_loss: 0.0866 - 1s/epoch - 631us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 0.1872 - val_loss: 0.3292 - 1s/epoch - 625us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 0.1723 - val_loss: 0.1138 - 1s/epoch - 607us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.1914 - val_loss: 0.0874 - 1s/epoch - 590us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 0.1965 - val_loss: 0.2296 - 1s/epoch - 587us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 0.1830 - val_loss: 0.2578 - 1s/epoch - 583us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 0.1866 - val_loss: 0.1352 - 1s/epoch - 612us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 0.1619 - val_loss: 0.0588 - 1s/epoch - 621us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.1772 - val_loss: 0.2026 - 1s/epoch - 617us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 0.1501 - val_loss: 0.2307 - 1s/epoch - 567us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 0.1830 - val_loss: 0.1900 - 1s/epoch - 669us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 0.2037 - val_loss: 0.1342 - 1s/epoch - 629us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 0.1589 - val_loss: 0.0976 - 1s/epoch - 568us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 0.1594 - val_loss: 0.3521 - 1s/epoch - 637us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 0.2216 - val_loss: 0.2522 - 1s/epoch - 596us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 0.2458 - val_loss: 0.0909 - 1s/epoch - 667us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 0.1915 - val_loss: 0.1029 - 1s/epoch - 612us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.1745 - val_loss: 0.2851 - 1s/epoch - 672us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 0.1815 - val_loss: 0.1313 - 1s/epoch - 615us/step\n",
      "534/534 - 0s - loss: 0.1107 - 232ms/epoch - 434us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/20\n",
      "2668/2668 [==============================] - 4s 1ms/step - loss: 0.2034 - val_loss: 0.0897\n",
      "Epoch 2/20\n",
      "2668/2668 [==============================] - 2s 590us/step - loss: 0.1639 - val_loss: 0.1845\n",
      "Epoch 3/20\n",
      "2668/2668 [==============================] - 2s 608us/step - loss: 0.2035 - val_loss: 0.1037\n",
      "Epoch 4/20\n",
      "2668/2668 [==============================] - 2s 604us/step - loss: 0.1683 - val_loss: 0.1934\n",
      "Epoch 5/20\n",
      "2668/2668 [==============================] - 2s 603us/step - loss: 0.1939 - val_loss: 0.5960\n",
      "Epoch 6/20\n",
      "2668/2668 [==============================] - 2s 623us/step - loss: 0.2076 - val_loss: 0.1151\n",
      "Epoch 7/20\n",
      "2668/2668 [==============================] - 2s 623us/step - loss: 0.2199 - val_loss: 0.4115\n",
      "Epoch 8/20\n",
      "2668/2668 [==============================] - 2s 631us/step - loss: 0.2019 - val_loss: 0.1032\n",
      "Epoch 9/20\n",
      "2668/2668 [==============================] - 2s 640us/step - loss: 0.2254 - val_loss: 0.0682\n",
      "Epoch 10/20\n",
      "2668/2668 [==============================] - 2s 676us/step - loss: 0.1982 - val_loss: 0.1299\n",
      "Epoch 11/20\n",
      "2668/2668 [==============================] - 2s 601us/step - loss: 0.2013 - val_loss: 0.1496\n",
      "Epoch 12/20\n",
      "2668/2668 [==============================] - 2s 591us/step - loss: 0.1828 - val_loss: 0.1841\n",
      "Epoch 13/20\n",
      "2668/2668 [==============================] - 2s 606us/step - loss: 0.1797 - val_loss: 0.0878\n",
      "Epoch 14/20\n",
      "2668/2668 [==============================] - 2s 602us/step - loss: 0.1973 - val_loss: 0.1595\n",
      "Epoch 15/20\n",
      "2668/2668 [==============================] - 2s 605us/step - loss: 0.1943 - val_loss: 0.1942\n",
      "Epoch 16/20\n",
      "2668/2668 [==============================] - 2s 608us/step - loss: 0.1833 - val_loss: 0.1193\n",
      "Epoch 17/20\n",
      "2668/2668 [==============================] - 2s 621us/step - loss: 0.1871 - val_loss: 0.1768\n",
      "Epoch 18/20\n",
      "2668/2668 [==============================] - 2s 632us/step - loss: 0.2149 - val_loss: 0.1286\n",
      "Epoch 19/20\n",
      "2668/2668 [==============================] - 2s 697us/step - loss: 0.1777 - val_loss: 0.2351\n",
      "Epoch 20/20\n",
      "2668/2668 [==============================] - 2s 670us/step - loss: 0.2218 - val_loss: 0.2585\n",
      "Epoch 1/20\n",
      "2134/2134 - 3s - loss: 0.1914 - val_loss: 0.0762 - 3s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 2s - loss: 0.1644 - val_loss: 0.1929 - 2s/epoch - 749us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 0.1935 - val_loss: 0.0926 - 1s/epoch - 613us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 0.1847 - val_loss: 0.1915 - 1s/epoch - 639us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.2100 - val_loss: 0.2492 - 1s/epoch - 639us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 0.1848 - val_loss: 0.1479 - 1s/epoch - 618us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 0.1981 - val_loss: 0.1058 - 1s/epoch - 600us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 0.1878 - val_loss: 0.1143 - 1s/epoch - 588us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 0.1825 - val_loss: 0.1987 - 1s/epoch - 629us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.1990 - val_loss: 0.1092 - 1s/epoch - 572us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 0.1834 - val_loss: 0.4309 - 1s/epoch - 558us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 0.1790 - val_loss: 0.1919 - 1s/epoch - 623us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 0.1715 - val_loss: 0.1578 - 1s/epoch - 653us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 0.2089 - val_loss: 0.0936 - 1s/epoch - 606us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 0.1746 - val_loss: 0.4068 - 1s/epoch - 621us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 0.1793 - val_loss: 0.0753 - 1s/epoch - 617us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 0.1774 - val_loss: 0.3430 - 1s/epoch - 610us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 0.1796 - val_loss: 0.2426 - 1s/epoch - 604us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.1937 - val_loss: 0.0634 - 1s/epoch - 583us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 0.1679 - val_loss: 0.4141 - 1s/epoch - 570us/step\n",
      "534/534 - 0s - loss: 0.4000 - 223ms/epoch - 417us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2737 - val_loss: 0.0451\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 696us/step - loss: 0.0372 - val_loss: 0.0364\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 677us/step - loss: 0.0301 - val_loss: 0.0274\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 667us/step - loss: 0.0253 - val_loss: 0.0226\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 692us/step - loss: 0.0204 - val_loss: 0.0179\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 693us/step - loss: 0.0167 - val_loss: 0.0164\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 669us/step - loss: 0.0149 - val_loss: 0.0142\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0136 - val_loss: 0.0132\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 644us/step - loss: 0.0127 - val_loss: 0.0121\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 678us/step - loss: 0.0118 - val_loss: 0.0114\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 662us/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 702us/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 664us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 677us/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 735us/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 701us/step - loss: 0.0080 - val_loss: 0.0078\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 690us/step - loss: 0.0078 - val_loss: 0.0079\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 717us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0074 - val_loss: 0.0073 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0072 - val_loss: 0.0072 - 646ms/epoch - 606us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0071 - val_loss: 0.0072 - 624ms/epoch - 585us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0069 - 658ms/epoch - 617us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0068 - 625ms/epoch - 586us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 623ms/epoch - 584us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0066 - 630ms/epoch - 590us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0064 - 644ms/epoch - 603us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0064 - 654ms/epoch - 613us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0062 - 667ms/epoch - 625us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0061 - 667ms/epoch - 625us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0060 - 631ms/epoch - 591us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0060 - 635ms/epoch - 595us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0058 - 634ms/epoch - 594us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0058 - 692ms/epoch - 649us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0056 - 648ms/epoch - 607us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0056 - 624ms/epoch - 585us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0056 - 632ms/epoch - 592us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0054 - 647ms/epoch - 606us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0055 - 647ms/epoch - 606us/step\n",
      "267/267 - 0s - loss: 0.0054 - 145ms/epoch - 543us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  46.9s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2851 - val_loss: 0.0447\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0380 - val_loss: 0.0336\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0303 - val_loss: 0.0268\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0257 - val_loss: 0.0232\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 659us/step - loss: 0.0212 - val_loss: 0.0181\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 630us/step - loss: 0.0168 - val_loss: 0.0151\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.0146 - val_loss: 0.0137\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0133 - val_loss: 0.0128\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 0.0123 - val_loss: 0.0118\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 668us/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 655us/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 670us/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 640us/step - loss: 0.0094 - val_loss: 0.0091\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0088 - val_loss: 0.0085\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 676us/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 673us/step - loss: 0.0080 - val_loss: 0.0077\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 880us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 723us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 638ms/epoch - 598us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0065 - 671ms/epoch - 629us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0063 - 682ms/epoch - 640us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0063 - 662ms/epoch - 620us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0062 - 683ms/epoch - 640us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0060 - 666ms/epoch - 624us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0059 - 660ms/epoch - 618us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0058 - 676ms/epoch - 634us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0059 - 638ms/epoch - 598us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0057 - 624ms/epoch - 585us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0056 - 630ms/epoch - 591us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0056 - 662ms/epoch - 620us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0055 - 670ms/epoch - 628us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0054 - 738ms/epoch - 692us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0056 - 642ms/epoch - 602us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0054 - 632ms/epoch - 592us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0053 - val_loss: 0.0053 - 651ms/epoch - 610us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0053 - val_loss: 0.0053 - 697ms/epoch - 653us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0052 - val_loss: 0.0052 - 679ms/epoch - 637us/step\n",
      "267/267 - 0s - loss: 0.0052 - 178ms/epoch - 666us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  47.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2611 - val_loss: 0.0445\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 714us/step - loss: 0.0379 - val_loss: 0.0320\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.0297 - val_loss: 0.0270\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 710us/step - loss: 0.0254 - val_loss: 0.0237\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 702us/step - loss: 0.0217 - val_loss: 0.0194\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 714us/step - loss: 0.0178 - val_loss: 0.0156\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 742us/step - loss: 0.0149 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 709us/step - loss: 0.0136 - val_loss: 0.0132\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 684us/step - loss: 0.0127 - val_loss: 0.0121\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0119 - val_loss: 0.0116\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 704us/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 719us/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 673us/step - loss: 0.0099 - val_loss: 0.0095\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 728us/step - loss: 0.0092 - val_loss: 0.0089\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 722us/step - loss: 0.0086 - val_loss: 0.0083\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 684us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 764us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 731us/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 704us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 722us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0070 - val_loss: 0.0068 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0068 - val_loss: 0.0068 - 727ms/epoch - 681us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0068 - 711ms/epoch - 666us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 666ms/epoch - 624us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0068 - 713ms/epoch - 668us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0063 - 638ms/epoch - 598us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0063 - 663ms/epoch - 621us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0060 - 632ms/epoch - 592us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0060 - 656ms/epoch - 615us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0060 - 639ms/epoch - 599us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0058 - 685ms/epoch - 642us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0059 - 738ms/epoch - 692us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0057 - 669ms/epoch - 627us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0056 - 701ms/epoch - 657us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0056 - 681ms/epoch - 638us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0055 - 714ms/epoch - 669us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0056 - 708ms/epoch - 664us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0054 - 691ms/epoch - 647us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0054 - 713ms/epoch - 668us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0053 - val_loss: 0.0054 - 707ms/epoch - 663us/step\n",
      "267/267 - 0s - loss: 0.0054 - 145ms/epoch - 542us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  48.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2565 - val_loss: 0.0448\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0380 - val_loss: 0.0319\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 706us/step - loss: 0.0297 - val_loss: 0.0271\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 707us/step - loss: 0.0248 - val_loss: 0.0224\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 738us/step - loss: 0.0196 - val_loss: 0.0172\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 764us/step - loss: 0.0163 - val_loss: 0.0151\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.0147 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 693us/step - loss: 0.0134 - val_loss: 0.0128\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 731us/step - loss: 0.0125 - val_loss: 0.0119\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 711us/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 669us/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 720us/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 779us/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 707us/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 687us/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 694us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 737us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 726us/step - loss: 0.0076 - val_loss: 0.0073\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 676us/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 752us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0069 - val_loss: 0.0067 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 664ms/epoch - 622us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0066 - 716ms/epoch - 671us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0063 - 685ms/epoch - 642us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0062 - 691ms/epoch - 648us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0062 - 624ms/epoch - 585us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0060 - 629ms/epoch - 590us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0059 - 650ms/epoch - 610us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0059 - 745ms/epoch - 698us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0059 - 690ms/epoch - 647us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0056 - 664ms/epoch - 622us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0057 - 630ms/epoch - 591us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0055 - 625ms/epoch - 585us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0055 - 649ms/epoch - 608us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0054 - 658ms/epoch - 617us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0054 - 713ms/epoch - 668us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0053 - 636ms/epoch - 596us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0053 - val_loss: 0.0053 - 636ms/epoch - 596us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0053 - val_loss: 0.0052 - 682ms/epoch - 639us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0052 - val_loss: 0.0051 - 656ms/epoch - 615us/step\n",
      "267/267 - 0s - loss: 0.0051 - 142ms/epoch - 533us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  48.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2539 - val_loss: 0.0439\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 749us/step - loss: 0.0370 - val_loss: 0.0319\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 717us/step - loss: 0.0299 - val_loss: 0.0271\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 724us/step - loss: 0.0255 - val_loss: 0.0250\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 735us/step - loss: 0.0228 - val_loss: 0.0233\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 719us/step - loss: 0.0204 - val_loss: 0.0192\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 754us/step - loss: 0.0182 - val_loss: 0.0182\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 739us/step - loss: 0.0151 - val_loss: 0.0137\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 777us/step - loss: 0.0131 - val_loss: 0.0125\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 660us/step - loss: 0.0121 - val_loss: 0.0121\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 669us/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 670us/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 630us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0093 - val_loss: 0.0093\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 666us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 664us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 647us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 656us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0072 - val_loss: 0.0070 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0070 - val_loss: 0.0070 - 729ms/epoch - 683us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0069 - val_loss: 0.0067 - 650ms/epoch - 609us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0067 - val_loss: 0.0066 - 643ms/epoch - 602us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0066 - val_loss: 0.0065 - 635ms/epoch - 595us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0065 - val_loss: 0.0064 - 635ms/epoch - 595us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0064 - val_loss: 0.0064 - 635ms/epoch - 595us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0063 - val_loss: 0.0063 - 673ms/epoch - 631us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0062 - val_loss: 0.0060 - 641ms/epoch - 600us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0061 - 641ms/epoch - 601us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0060 - val_loss: 0.0058 - 634ms/epoch - 594us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0060 - 677ms/epoch - 634us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0056 - 634ms/epoch - 594us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0057 - 670ms/epoch - 628us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0056 - 658ms/epoch - 616us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0057 - 631ms/epoch - 591us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0055 - 629ms/epoch - 590us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0054 - 626ms/epoch - 587us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0053 - 661ms/epoch - 619us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0053 - 650ms/epoch - 609us/step\n",
      "267/267 - 0s - loss: 0.0053 - 143ms/epoch - 536us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  48.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0525 - val_loss: 0.0071\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 632us/step - loss: 0.0979 - val_loss: 0.3381\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0989 - val_loss: 0.0204\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0374 - val_loss: 0.0337\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 691us/step - loss: 0.0407 - val_loss: 0.1199\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 704us/step - loss: 0.0887 - val_loss: 0.0528\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 748us/step - loss: 0.0566 - val_loss: 0.0431\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 720us/step - loss: 0.0360 - val_loss: 0.0746\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 704us/step - loss: 0.0530 - val_loss: 0.0148\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 708us/step - loss: 0.0547 - val_loss: 0.0450\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 722us/step - loss: 0.0523 - val_loss: 0.0255\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 632us/step - loss: 0.1999 - val_loss: 0.0340\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.0889 - val_loss: 0.0803\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 695us/step - loss: 0.0779 - val_loss: 0.1395\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 642us/step - loss: 0.0905 - val_loss: 0.0490\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 705us/step - loss: 0.0784 - val_loss: 0.0262\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 834us/step - loss: 0.0745 - val_loss: 0.0443\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 746us/step - loss: 0.0768 - val_loss: 0.1534\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 712us/step - loss: 0.0775 - val_loss: 0.1036\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 675us/step - loss: 0.0743 - val_loss: 0.0651\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0923 - val_loss: 0.0680 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.1021 - val_loss: 0.2053 - 629ms/epoch - 589us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.1432 - val_loss: 0.2783 - 619ms/epoch - 580us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.1383 - val_loss: 0.1565 - 668ms/epoch - 626us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0892 - val_loss: 0.0527 - 625ms/epoch - 585us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.1271 - val_loss: 0.0717 - 711ms/epoch - 666us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.1022 - val_loss: 0.0874 - 641ms/epoch - 600us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0910 - val_loss: 0.0714 - 674ms/epoch - 632us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0991 - val_loss: 0.0648 - 667ms/epoch - 625us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.1163 - val_loss: 0.1371 - 638ms/epoch - 598us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.1005 - val_loss: 0.1743 - 625ms/epoch - 586us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0981 - val_loss: 0.1846 - 615ms/epoch - 577us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0962 - val_loss: 0.0531 - 619ms/epoch - 580us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0906 - val_loss: 0.0444 - 621ms/epoch - 582us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.1253 - val_loss: 0.0852 - 637ms/epoch - 597us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.1151 - val_loss: 0.0835 - 648ms/epoch - 607us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.1227 - val_loss: 0.1914 - 625ms/epoch - 586us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.1007 - val_loss: 0.0767 - 645ms/epoch - 604us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.1022 - val_loss: 0.0357 - 714ms/epoch - 669us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.1661 - val_loss: 0.0782 - 754ms/epoch - 707us/step\n",
      "267/267 - 0s - loss: 0.0779 - 158ms/epoch - 591us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  47.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0539 - val_loss: 0.0087\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.1133 - val_loss: 0.7082\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 656us/step - loss: 0.1332 - val_loss: 0.0465\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 0.0588 - val_loss: 0.1442\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0413 - val_loss: 0.0180\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 654us/step - loss: 0.0872 - val_loss: 0.0829\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.0479 - val_loss: 0.0153\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 670us/step - loss: 0.0508 - val_loss: 0.0792\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.1598 - val_loss: 0.1008\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 756us/step - loss: 0.1310 - val_loss: 0.1718\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 718us/step - loss: 0.0891 - val_loss: 0.0642\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 701us/step - loss: 0.0910 - val_loss: 0.2253\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.1443 - val_loss: 0.3984\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 761us/step - loss: 0.1258 - val_loss: 0.1642\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 727us/step - loss: 0.0979 - val_loss: 0.0692\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 688us/step - loss: 0.0934 - val_loss: 0.0894\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 687us/step - loss: 0.1109 - val_loss: 0.0489\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 756us/step - loss: 0.1109 - val_loss: 0.2393\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 692us/step - loss: 0.1376 - val_loss: 0.0530\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 698us/step - loss: 0.1023 - val_loss: 0.1261\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.1064 - val_loss: 0.0721 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0998 - val_loss: 0.1669 - 617ms/epoch - 579us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.1231 - val_loss: 0.1724 - 617ms/epoch - 579us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.1068 - val_loss: 0.0752 - 621ms/epoch - 582us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.1074 - val_loss: 0.1442 - 619ms/epoch - 580us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0892 - val_loss: 0.0719 - 617ms/epoch - 578us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.1172 - val_loss: 0.1647 - 637ms/epoch - 597us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0977 - val_loss: 0.0621 - 622ms/epoch - 583us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0948 - val_loss: 0.2172 - 617ms/epoch - 578us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.1009 - val_loss: 0.2698 - 609ms/epoch - 571us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.1070 - val_loss: 0.0938 - 615ms/epoch - 576us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.1424 - val_loss: 0.0790 - 617ms/epoch - 578us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0977 - val_loss: 0.0373 - 617ms/epoch - 579us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.1232 - val_loss: 0.0653 - 626ms/epoch - 587us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.1124 - val_loss: 0.1517 - 620ms/epoch - 581us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0994 - val_loss: 0.0733 - 619ms/epoch - 580us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.1369 - val_loss: 0.3275 - 617ms/epoch - 578us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.1097 - val_loss: 0.0783 - 617ms/epoch - 578us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.1114 - val_loss: 0.0380 - 611ms/epoch - 573us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0911 - val_loss: 0.4377 - 616ms/epoch - 577us/step\n",
      "267/267 - 0s - loss: 0.3877 - 143ms/epoch - 535us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  47.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0717 - val_loss: 0.0268\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 743us/step - loss: 0.0764 - val_loss: 0.0182\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 677us/step - loss: 0.0448 - val_loss: 0.0210\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 694us/step - loss: 0.1467 - val_loss: 0.0651\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 660us/step - loss: 0.0394 - val_loss: 0.0336\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0419 - val_loss: 0.0254\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 655us/step - loss: 0.0779 - val_loss: 0.0158\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 634us/step - loss: 0.0918 - val_loss: 0.0733\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 666us/step - loss: 0.0403 - val_loss: 0.0179\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.1638 - val_loss: 0.1965\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.0759 - val_loss: 0.0159\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.1207 - val_loss: 0.0566\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 623us/step - loss: 0.0863 - val_loss: 0.0703\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 661us/step - loss: 0.0680 - val_loss: 0.1770\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0821 - val_loss: 0.0690\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.1010 - val_loss: 0.0176\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 0.0769 - val_loss: 0.0305\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 639us/step - loss: 0.0933 - val_loss: 0.1426\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 655us/step - loss: 0.0921 - val_loss: 0.0339\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 663us/step - loss: 0.0806 - val_loss: 0.0880\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0634 - val_loss: 0.0502 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.1325 - val_loss: 0.0266 - 621ms/epoch - 582us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0928 - val_loss: 0.0486 - 616ms/epoch - 578us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0930 - val_loss: 0.0595 - 616ms/epoch - 577us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0901 - val_loss: 0.0216 - 621ms/epoch - 582us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0797 - val_loss: 0.0548 - 626ms/epoch - 587us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.1033 - val_loss: 0.2459 - 622ms/epoch - 583us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0984 - val_loss: 0.0540 - 620ms/epoch - 581us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0879 - val_loss: 0.0663 - 617ms/epoch - 578us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.1030 - val_loss: 0.0964 - 617ms/epoch - 578us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.1017 - val_loss: 0.1256 - 622ms/epoch - 583us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0943 - val_loss: 0.0501 - 617ms/epoch - 579us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0929 - val_loss: 0.1294 - 619ms/epoch - 581us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.1071 - val_loss: 0.0461 - 622ms/epoch - 583us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0882 - val_loss: 0.0361 - 617ms/epoch - 578us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.1129 - val_loss: 0.1324 - 618ms/epoch - 579us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0889 - val_loss: 0.0895 - 623ms/epoch - 584us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.1033 - val_loss: 0.1037 - 643ms/epoch - 603us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.1049 - val_loss: 0.0652 - 622ms/epoch - 583us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0953 - val_loss: 0.0908 - 620ms/epoch - 581us/step\n",
      "267/267 - 0s - loss: 0.0836 - 142ms/epoch - 531us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  46.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0663 - val_loss: 0.0740\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 649us/step - loss: 0.0451 - val_loss: 0.0090\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 671us/step - loss: 0.2853 - val_loss: 0.1119\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 660us/step - loss: 0.0924 - val_loss: 0.0751\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0938 - val_loss: 0.0308\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.0649 - val_loss: 0.0250\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0905 - val_loss: 0.2086\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.0914 - val_loss: 0.1623\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0683 - val_loss: 0.1073\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 667us/step - loss: 0.0898 - val_loss: 0.0358\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0792 - val_loss: 0.0416\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 711us/step - loss: 0.0660 - val_loss: 0.0816\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 718us/step - loss: 0.0796 - val_loss: 0.1712\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 651us/step - loss: 0.0732 - val_loss: 0.0620\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 713us/step - loss: 0.0701 - val_loss: 0.0777\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 702us/step - loss: 0.0800 - val_loss: 0.0265\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 664us/step - loss: 0.1291 - val_loss: 0.0313\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 681us/step - loss: 0.0875 - val_loss: 0.0321\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 737us/step - loss: 0.0702 - val_loss: 0.0221\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.0913 - val_loss: 0.0523\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0918 - val_loss: 0.1409 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.1210 - val_loss: 0.0810 - 706ms/epoch - 662us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0793 - val_loss: 0.0589 - 654ms/epoch - 613us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0703 - val_loss: 0.0908 - 624ms/epoch - 585us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0745 - val_loss: 0.0348 - 621ms/epoch - 582us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0938 - val_loss: 0.0965 - 623ms/epoch - 584us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0988 - val_loss: 0.0413 - 622ms/epoch - 583us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0687 - val_loss: 0.0640 - 626ms/epoch - 587us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.1273 - val_loss: 0.0563 - 621ms/epoch - 582us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.1303 - val_loss: 0.0367 - 634ms/epoch - 594us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.1146 - val_loss: 0.0864 - 648ms/epoch - 607us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0990 - val_loss: 0.0977 - 625ms/epoch - 586us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.1000 - val_loss: 0.0664 - 627ms/epoch - 587us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0965 - val_loss: 0.1638 - 624ms/epoch - 585us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.1056 - val_loss: 0.0685 - 631ms/epoch - 592us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0931 - val_loss: 0.1248 - 624ms/epoch - 585us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0986 - val_loss: 0.0520 - 623ms/epoch - 584us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.1111 - val_loss: 0.0361 - 630ms/epoch - 590us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.1086 - val_loss: 0.0854 - 621ms/epoch - 582us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.1064 - val_loss: 0.0596 - 625ms/epoch - 586us/step\n",
      "267/267 - 0s - loss: 0.0503 - 142ms/epoch - 531us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  46.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0591 - val_loss: 0.0199\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 652us/step - loss: 0.0878 - val_loss: 0.0236\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.0371 - val_loss: 0.0733\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.1702 - val_loss: 0.1235\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 660us/step - loss: 0.0422 - val_loss: 0.0419\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 729us/step - loss: 0.0636 - val_loss: 0.0188\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 713us/step - loss: 0.0462 - val_loss: 0.1180\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 670us/step - loss: 0.0736 - val_loss: 0.0208\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 688us/step - loss: 0.0355 - val_loss: 0.0591\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0723 - val_loss: 0.0145\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 640us/step - loss: 0.0544 - val_loss: 0.1058\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 665us/step - loss: 0.0407 - val_loss: 0.0185\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.1266 - val_loss: 0.0612\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 721us/step - loss: 0.0467 - val_loss: 0.0175\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 760us/step - loss: 0.0438 - val_loss: 0.2312\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 731us/step - loss: 0.1344 - val_loss: 0.0200\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 663us/step - loss: 0.0644 - val_loss: 0.0600\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 708us/step - loss: 0.0601 - val_loss: 0.0248\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 762us/step - loss: 0.1036 - val_loss: 0.0609\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 708us/step - loss: 0.0795 - val_loss: 0.0385\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0786 - val_loss: 0.0423 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.1018 - val_loss: 0.1499 - 681ms/epoch - 638us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0681 - val_loss: 0.1194 - 736ms/epoch - 690us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.1107 - val_loss: 0.0909 - 649ms/epoch - 608us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.1221 - val_loss: 0.1806 - 670ms/epoch - 628us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0950 - val_loss: 0.0429 - 689ms/epoch - 645us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0724 - val_loss: 0.0631 - 711ms/epoch - 666us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0935 - val_loss: 0.0488 - 697ms/epoch - 654us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0679 - val_loss: 0.0901 - 693ms/epoch - 649us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0765 - val_loss: 0.0711 - 717ms/epoch - 672us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0788 - val_loss: 0.1228 - 698ms/epoch - 654us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0817 - val_loss: 0.0988 - 681ms/epoch - 638us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0890 - val_loss: 0.0973 - 671ms/epoch - 629us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0718 - val_loss: 0.0709 - 720ms/epoch - 675us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0954 - val_loss: 0.0663 - 700ms/epoch - 656us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0690 - val_loss: 0.0265 - 701ms/epoch - 657us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.1552 - val_loss: 0.0559 - 710ms/epoch - 666us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0956 - val_loss: 0.0605 - 679ms/epoch - 636us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.1204 - val_loss: 0.2282 - 702ms/epoch - 658us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.1452 - val_loss: 0.1035 - 681ms/epoch - 638us/step\n",
      "267/267 - 0s - loss: 0.0845 - 180ms/epoch - 673us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  48.4s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3463 - val_loss: 0.2532\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.1966 - val_loss: 0.1496\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.1167 - val_loss: 0.0894\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0701 - val_loss: 0.0541\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0428 - val_loss: 0.0334\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0268 - val_loss: 0.0213\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0173 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0083 - val_loss: 0.0072\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0024 - val_loss: 0.0024 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 512ms/epoch - 479us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 558ms/epoch - 523us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 570ms/epoch - 534us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 591ms/epoch - 554us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 503ms/epoch - 471us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 507ms/epoch - 475us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 513ms/epoch - 480us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0020 - 547ms/epoch - 513us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 531ms/epoch - 497us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 514ms/epoch - 482us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0019 - 573ms/epoch - 537us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 544ms/epoch - 509us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 510ms/epoch - 478us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 557ms/epoch - 522us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 518ms/epoch - 486us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 566ms/epoch - 530us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 516ms/epoch - 484us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 512ms/epoch - 480us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0017 - val_loss: 0.0017 - 552ms/epoch - 518us/step\n",
      "267/267 - 0s - loss: 0.0016 - 154ms/epoch - 577us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  42.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3351 - val_loss: 0.2507\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.1947 - val_loss: 0.1481\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.1156 - val_loss: 0.0884\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0694 - val_loss: 0.0535\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0423 - val_loss: 0.0330\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0264 - val_loss: 0.0210\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0170 - val_loss: 0.0138\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0115 - val_loss: 0.0096\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0082 - val_loss: 0.0071\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0062 - val_loss: 0.0055\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 624us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0026 - val_loss: 0.0026 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0025 - 514ms/epoch - 482us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0025 - 501ms/epoch - 470us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0025 - 520ms/epoch - 487us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 531ms/epoch - 498us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0024 - 504ms/epoch - 472us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 501ms/epoch - 470us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 543ms/epoch - 509us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 516ms/epoch - 484us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 531ms/epoch - 498us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 524ms/epoch - 491us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 562ms/epoch - 526us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0022 - 534ms/epoch - 500us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 555ms/epoch - 520us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 560ms/epoch - 524us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 541ms/epoch - 507us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0021 - 519ms/epoch - 486us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0021 - 532ms/epoch - 499us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 542ms/epoch - 508us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 528ms/epoch - 495us/step\n",
      "267/267 - 0s - loss: 0.0019 - 149ms/epoch - 559us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  42.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3421 - val_loss: 0.2557\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 621us/step - loss: 0.1987 - val_loss: 0.1513\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.1180 - val_loss: 0.0904\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0709 - val_loss: 0.0547\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.0433 - val_loss: 0.0338\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0270 - val_loss: 0.0215\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0174 - val_loss: 0.0142\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0084 - val_loss: 0.0072\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 653us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 621us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0024 - val_loss: 0.0024 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 588ms/epoch - 551us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 533ms/epoch - 500us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 531ms/epoch - 498us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 515ms/epoch - 483us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 506ms/epoch - 474us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 507ms/epoch - 475us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 505ms/epoch - 473us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 505ms/epoch - 473us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 604ms/epoch - 566us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 548ms/epoch - 514us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 511ms/epoch - 479us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 546ms/epoch - 512us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0019 - 569ms/epoch - 533us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 546ms/epoch - 511us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 546ms/epoch - 512us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 520ms/epoch - 488us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0017 - val_loss: 0.0017 - 613ms/epoch - 574us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0017 - val_loss: 0.0017 - 549ms/epoch - 515us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0017 - val_loss: 0.0017 - 539ms/epoch - 505us/step\n",
      "267/267 - 0s - loss: 0.0016 - 144ms/epoch - 539us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  43.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3336 - val_loss: 0.2482\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.1927 - val_loss: 0.1465\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.1143 - val_loss: 0.0874\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0685 - val_loss: 0.0528\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0417 - val_loss: 0.0326\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0260 - val_loss: 0.0206\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0167 - val_loss: 0.0136\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0112 - val_loss: 0.0094\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0080 - val_loss: 0.0069\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0060 - val_loss: 0.0054\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0025 - val_loss: 0.0025 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0025 - 553ms/epoch - 518us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0024 - 533ms/epoch - 500us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 616ms/epoch - 577us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 580ms/epoch - 544us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 549ms/epoch - 514us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 542ms/epoch - 508us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 517ms/epoch - 485us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 613ms/epoch - 574us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 579ms/epoch - 543us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 545ms/epoch - 511us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 553ms/epoch - 518us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 545ms/epoch - 511us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 542ms/epoch - 508us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 538ms/epoch - 504us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 538ms/epoch - 504us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 544ms/epoch - 510us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 616ms/epoch - 578us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 577ms/epoch - 540us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 569ms/epoch - 533us/step\n",
      "267/267 - 0s - loss: 0.0018 - 154ms/epoch - 576us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  42.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3422 - val_loss: 0.2553\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.1984 - val_loss: 0.1510\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.1178 - val_loss: 0.0902\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0707 - val_loss: 0.0546\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0431 - val_loss: 0.0337\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0269 - val_loss: 0.0214\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0174 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0083 - val_loss: 0.0072\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 635us/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0026 - val_loss: 0.0026 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0025 - 558ms/epoch - 523us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0025 - val_loss: 0.0025 - 510ms/epoch - 478us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 572ms/epoch - 536us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0024 - val_loss: 0.0024 - 509ms/epoch - 477us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0024 - 509ms/epoch - 477us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 507ms/epoch - 476us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0023 - 519ms/epoch - 486us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 505ms/epoch - 473us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0022 - val_loss: 0.0022 - 540ms/epoch - 506us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0022 - 579ms/epoch - 542us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 596ms/epoch - 559us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0021 - val_loss: 0.0021 - 563ms/epoch - 528us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0021 - 527ms/epoch - 494us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 516ms/epoch - 484us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 514ms/epoch - 481us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0020 - val_loss: 0.0020 - 511ms/epoch - 479us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0020 - 513ms/epoch - 481us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 516ms/epoch - 484us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0019 - 510ms/epoch - 478us/step\n",
      "267/267 - 0s - loss: 0.0018 - 143ms/epoch - 536us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  42.6s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 1.4293 - val_loss: 0.3482\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 0.1321 - val_loss: 0.0740\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.0642 - val_loss: 0.0583\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 640us/step - loss: 0.0536 - val_loss: 0.0507\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 621us/step - loss: 0.0479 - val_loss: 0.0461\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0441 - val_loss: 0.0430\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.0413 - val_loss: 0.0409\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0390 - val_loss: 0.0391\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0372 - val_loss: 0.0367\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 0.0356 - val_loss: 0.0358\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.0343 - val_loss: 0.0346\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0332 - val_loss: 0.0330\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 0.0321 - val_loss: 0.0319\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0311 - val_loss: 0.0313\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0303 - val_loss: 0.0312\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0296 - val_loss: 0.0293\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.0289 - val_loss: 0.0294\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 0.0283 - val_loss: 0.0280\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0278 - val_loss: 0.0279\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 0.0274 - val_loss: 0.0280\n",
      "Epoch 1/20\n",
      "1067/1067 - 3s - loss: 0.0269 - val_loss: 0.0271 - 3s/epoch - 3ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0266 - val_loss: 0.0261 - 726ms/epoch - 680us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0263 - val_loss: 0.0258 - 585ms/epoch - 548us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0261 - val_loss: 0.0261 - 547ms/epoch - 513us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0258 - val_loss: 0.0259 - 555ms/epoch - 520us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0255 - val_loss: 0.0253 - 566ms/epoch - 530us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0251 - val_loss: 0.0246 - 565ms/epoch - 529us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0249 - val_loss: 0.0267 - 546ms/epoch - 512us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0248 - val_loss: 0.0241 - 547ms/epoch - 513us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0245 - val_loss: 0.0239 - 566ms/epoch - 530us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0243 - val_loss: 0.0237 - 599ms/epoch - 561us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0240 - val_loss: 0.0235 - 548ms/epoch - 513us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0239 - val_loss: 0.0233 - 541ms/epoch - 507us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 0s - loss: 0.0237 - val_loss: 0.0234 - 471ms/epoch - 442us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 0s - loss: 0.0234 - val_loss: 0.0228 - 488ms/epoch - 457us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 0s - loss: 0.0233 - val_loss: 0.0228 - 490ms/epoch - 459us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0231 - val_loss: 0.0226 - 506ms/epoch - 475us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0229 - val_loss: 0.0228 - 576ms/epoch - 540us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0227 - val_loss: 0.0226 - 580ms/epoch - 544us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0225 - val_loss: 0.0219 - 565ms/epoch - 530us/step\n",
      "267/267 - 0s - loss: 0.0207 - 138ms/epoch - 517us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  44.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 1.4036 - val_loss: 0.3449\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.1290 - val_loss: 0.0691\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 500us/step - loss: 0.0594 - val_loss: 0.0534\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0492 - val_loss: 0.0463\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 485us/step - loss: 0.0438 - val_loss: 0.0422\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0405 - val_loss: 0.0394\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 501us/step - loss: 0.0380 - val_loss: 0.0378\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 496us/step - loss: 0.0361 - val_loss: 0.0355\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0345 - val_loss: 0.0339\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 490us/step - loss: 0.0332 - val_loss: 0.0326\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 489us/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 446us/step - loss: 0.0310 - val_loss: 0.0307\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0302 - val_loss: 0.0300\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 474us/step - loss: 0.0294 - val_loss: 0.0289\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0287 - val_loss: 0.0282\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 488us/step - loss: 0.0281 - val_loss: 0.0299\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 482us/step - loss: 0.0274 - val_loss: 0.0272\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 484us/step - loss: 0.0269 - val_loss: 0.0265\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 494us/step - loss: 0.0264 - val_loss: 0.0261\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 516us/step - loss: 0.0259 - val_loss: 0.0258\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0255 - val_loss: 0.0260 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 0s - loss: 0.0251 - val_loss: 0.0257 - 462ms/epoch - 433us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 0s - loss: 0.0249 - val_loss: 0.0265 - 419ms/epoch - 393us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 0s - loss: 0.0246 - val_loss: 0.0243 - 453ms/epoch - 425us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 0s - loss: 0.0243 - val_loss: 0.0241 - 478ms/epoch - 448us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 0s - loss: 0.0240 - val_loss: 0.0258 - 455ms/epoch - 426us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 0s - loss: 0.0238 - val_loss: 0.0239 - 491ms/epoch - 460us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 0s - loss: 0.0236 - val_loss: 0.0234 - 466ms/epoch - 436us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 0s - loss: 0.0234 - val_loss: 0.0231 - 438ms/epoch - 411us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 0s - loss: 0.0232 - val_loss: 0.0229 - 458ms/epoch - 430us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0229 - val_loss: 0.0225 - 525ms/epoch - 492us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 0s - loss: 0.0228 - val_loss: 0.0244 - 474ms/epoch - 445us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0225 - val_loss: 0.0222 - 513ms/epoch - 481us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 0s - loss: 0.0223 - val_loss: 0.0222 - 473ms/epoch - 443us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 0s - loss: 0.0222 - val_loss: 0.0229 - 457ms/epoch - 428us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 0s - loss: 0.0221 - val_loss: 0.0217 - 447ms/epoch - 419us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 0s - loss: 0.0219 - val_loss: 0.0218 - 488ms/epoch - 458us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 0s - loss: 0.0218 - val_loss: 0.0223 - 444ms/epoch - 416us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 0s - loss: 0.0216 - val_loss: 0.0217 - 457ms/epoch - 428us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 0s - loss: 0.0214 - val_loss: 0.0212 - 448ms/epoch - 420us/step\n",
      "267/267 - 0s - loss: 0.0205 - 124ms/epoch - 465us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  35.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 1.4384 - val_loss: 0.3577\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 499us/step - loss: 0.1302 - val_loss: 0.0706\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0610 - val_loss: 0.0554\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 509us/step - loss: 0.0507 - val_loss: 0.0487\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 515us/step - loss: 0.0452 - val_loss: 0.0438\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 512us/step - loss: 0.0415 - val_loss: 0.0402\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 439us/step - loss: 0.0388 - val_loss: 0.0376\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 473us/step - loss: 0.0366 - val_loss: 0.0366\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 460us/step - loss: 0.0349 - val_loss: 0.0344\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 478us/step - loss: 0.0334 - val_loss: 0.0334\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 437us/step - loss: 0.0321 - val_loss: 0.0320\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 471us/step - loss: 0.0310 - val_loss: 0.0304\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 442us/step - loss: 0.0300 - val_loss: 0.0312\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 512us/step - loss: 0.0292 - val_loss: 0.0287\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 471us/step - loss: 0.0284 - val_loss: 0.0288\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 501us/step - loss: 0.0277 - val_loss: 0.0272\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0272 - val_loss: 0.0271\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 486us/step - loss: 0.0267 - val_loss: 0.0264\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 518us/step - loss: 0.0262 - val_loss: 0.0258\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 474us/step - loss: 0.0257 - val_loss: 0.0253\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0253 - val_loss: 0.0249 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 0s - loss: 0.0250 - val_loss: 0.0248 - 494ms/epoch - 463us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 0s - loss: 0.0247 - val_loss: 0.0244 - 424ms/epoch - 397us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 0s - loss: 0.0246 - val_loss: 0.0241 - 430ms/epoch - 403us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 0s - loss: 0.0242 - val_loss: 0.0246 - 427ms/epoch - 400us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 0s - loss: 0.0240 - val_loss: 0.0235 - 450ms/epoch - 422us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 0s - loss: 0.0237 - val_loss: 0.0232 - 427ms/epoch - 400us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 0s - loss: 0.0236 - val_loss: 0.0239 - 428ms/epoch - 401us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 0s - loss: 0.0234 - val_loss: 0.0235 - 427ms/epoch - 400us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 0s - loss: 0.0232 - val_loss: 0.0228 - 424ms/epoch - 398us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 0s - loss: 0.0230 - val_loss: 0.0245 - 429ms/epoch - 402us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 0s - loss: 0.0228 - val_loss: 0.0226 - 434ms/epoch - 407us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 0s - loss: 0.0226 - val_loss: 0.0225 - 427ms/epoch - 400us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 0s - loss: 0.0225 - val_loss: 0.0224 - 427ms/epoch - 400us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 0s - loss: 0.0223 - val_loss: 0.0225 - 428ms/epoch - 402us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 0s - loss: 0.0222 - val_loss: 0.0222 - 426ms/epoch - 399us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 0s - loss: 0.0221 - val_loss: 0.0218 - 428ms/epoch - 401us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 0s - loss: 0.0219 - val_loss: 0.0217 - 425ms/epoch - 398us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 0s - loss: 0.0217 - val_loss: 0.0225 - 424ms/epoch - 398us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 0s - loss: 0.0217 - val_loss: 0.0216 - 425ms/epoch - 399us/step\n",
      "267/267 - 0s - loss: 0.0204 - 125ms/epoch - 470us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  34.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 1.4155 - val_loss: 0.3452\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 452us/step - loss: 0.1276 - val_loss: 0.0705\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 477us/step - loss: 0.0621 - val_loss: 0.0569\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 509us/step - loss: 0.0531 - val_loss: 0.0505\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 503us/step - loss: 0.0482 - val_loss: 0.0477\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0447 - val_loss: 0.0439\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 503us/step - loss: 0.0420 - val_loss: 0.0409\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 458us/step - loss: 0.0399 - val_loss: 0.0404\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 501us/step - loss: 0.0381 - val_loss: 0.0377\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0367 - val_loss: 0.0373\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 0.0355 - val_loss: 0.0348\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 521us/step - loss: 0.0343 - val_loss: 0.0338\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0334 - val_loss: 0.0329\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 473us/step - loss: 0.0325 - val_loss: 0.0322\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0316 - val_loss: 0.0318\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0309 - val_loss: 0.0307\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 460us/step - loss: 0.0302 - val_loss: 0.0298\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 479us/step - loss: 0.0296 - val_loss: 0.0291\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 510us/step - loss: 0.0291 - val_loss: 0.0285\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 453us/step - loss: 0.0285 - val_loss: 0.0287\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0282 - val_loss: 0.0277 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 0s - loss: 0.0279 - val_loss: 0.0273 - 447ms/epoch - 419us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 0s - loss: 0.0275 - val_loss: 0.0271 - 440ms/epoch - 413us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 0s - loss: 0.0272 - val_loss: 0.0266 - 446ms/epoch - 418us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 0s - loss: 0.0268 - val_loss: 0.0265 - 446ms/epoch - 418us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 0s - loss: 0.0266 - val_loss: 0.0261 - 432ms/epoch - 405us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 0s - loss: 0.0262 - val_loss: 0.0256 - 442ms/epoch - 414us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 0s - loss: 0.0259 - val_loss: 0.0252 - 447ms/epoch - 419us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 0s - loss: 0.0256 - val_loss: 0.0250 - 451ms/epoch - 423us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 0s - loss: 0.0253 - val_loss: 0.0248 - 450ms/epoch - 421us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 0s - loss: 0.0251 - val_loss: 0.0243 - 484ms/epoch - 453us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 0s - loss: 0.0248 - val_loss: 0.0243 - 468ms/epoch - 439us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 0s - loss: 0.0245 - val_loss: 0.0239 - 451ms/epoch - 423us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 0s - loss: 0.0243 - val_loss: 0.0240 - 432ms/epoch - 405us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 0s - loss: 0.0240 - val_loss: 0.0242 - 428ms/epoch - 401us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 0s - loss: 0.0239 - val_loss: 0.0250 - 425ms/epoch - 399us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0236 - val_loss: 0.0228 - 502ms/epoch - 471us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 0s - loss: 0.0235 - val_loss: 0.0231 - 499ms/epoch - 468us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 0s - loss: 0.0232 - val_loss: 0.0241 - 487ms/epoch - 457us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 0s - loss: 0.0231 - val_loss: 0.0225 - 424ms/epoch - 397us/step\n",
      "267/267 - 0s - loss: 0.0211 - 124ms/epoch - 465us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  34.9s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 1.4027 - val_loss: 0.3300\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.1282 - val_loss: 0.0752\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 500us/step - loss: 0.0654 - val_loss: 0.0593\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 506us/step - loss: 0.0552 - val_loss: 0.0525\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 0.0497 - val_loss: 0.0479\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 450us/step - loss: 0.0458 - val_loss: 0.0448\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0428 - val_loss: 0.0417\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 513us/step - loss: 0.0405 - val_loss: 0.0397\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 473us/step - loss: 0.0385 - val_loss: 0.0376\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0369 - val_loss: 0.0366\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 510us/step - loss: 0.0355 - val_loss: 0.0375\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 514us/step - loss: 0.0344 - val_loss: 0.0360\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0333 - val_loss: 0.0386\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0324 - val_loss: 0.0318\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 477us/step - loss: 0.0316 - val_loss: 0.0313\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 485us/step - loss: 0.0307 - val_loss: 0.0312\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 484us/step - loss: 0.0301 - val_loss: 0.0297\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 497us/step - loss: 0.0294 - val_loss: 0.0292\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 517us/step - loss: 0.0288 - val_loss: 0.0286\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 516us/step - loss: 0.0283 - val_loss: 0.0278\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0277 - val_loss: 0.0275 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 0s - loss: 0.0274 - val_loss: 0.0276 - 434ms/epoch - 407us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 0s - loss: 0.0271 - val_loss: 0.0277 - 492ms/epoch - 461us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0266 - val_loss: 0.0268 - 540ms/epoch - 506us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 0s - loss: 0.0264 - val_loss: 0.0260 - 450ms/epoch - 421us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 0s - loss: 0.0261 - val_loss: 0.0256 - 489ms/epoch - 458us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 0s - loss: 0.0257 - val_loss: 0.0260 - 444ms/epoch - 416us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 0s - loss: 0.0254 - val_loss: 0.0253 - 422ms/epoch - 395us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 0s - loss: 0.0252 - val_loss: 0.0247 - 420ms/epoch - 394us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 0s - loss: 0.0248 - val_loss: 0.0248 - 424ms/epoch - 397us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 0s - loss: 0.0247 - val_loss: 0.0240 - 424ms/epoch - 398us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 0s - loss: 0.0244 - val_loss: 0.0246 - 453ms/epoch - 425us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 0s - loss: 0.0242 - val_loss: 0.0237 - 443ms/epoch - 415us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 0s - loss: 0.0239 - val_loss: 0.0234 - 423ms/epoch - 396us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 0s - loss: 0.0238 - val_loss: 0.0249 - 421ms/epoch - 394us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 0s - loss: 0.0235 - val_loss: 0.0231 - 422ms/epoch - 395us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 0s - loss: 0.0234 - val_loss: 0.0227 - 418ms/epoch - 392us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 0s - loss: 0.0233 - val_loss: 0.0244 - 427ms/epoch - 400us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 0s - loss: 0.0231 - val_loss: 0.0227 - 431ms/epoch - 404us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0229 - val_loss: 0.0222 - 510ms/epoch - 478us/step\n",
      "267/267 - 0s - loss: 0.0214 - 135ms/epoch - 506us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  35.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2741 - val_loss: 0.0180\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0126 - val_loss: 0.0095\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 0.0081 - val_loss: 0.0070\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0063 - val_loss: 0.0057\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 625us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 630us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 592ms/epoch - 555us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 624ms/epoch - 584us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 620ms/epoch - 582us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 571ms/epoch - 535us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 583ms/epoch - 546us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0031 - 579ms/epoch - 543us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 571ms/epoch - 535us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 658ms/epoch - 617us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 557ms/epoch - 522us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 619ms/epoch - 580us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0031 - 590ms/epoch - 553us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 539ms/epoch - 505us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 539ms/epoch - 505us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 550ms/epoch - 515us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 544ms/epoch - 510us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 541ms/epoch - 507us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 542ms/epoch - 508us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 538ms/epoch - 504us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0039 - 546ms/epoch - 512us/step\n",
      "267/267 - 0s - loss: 0.0036 - 121ms/epoch - 453us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  40.3s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2841 - val_loss: 0.0193\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0136 - val_loss: 0.0103\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0087 - val_loss: 0.0074\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 712us/step - loss: 0.0065 - val_loss: 0.0057\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 635us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 649us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 627us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0031 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0031 - 544ms/epoch - 509us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 550ms/epoch - 516us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 547ms/epoch - 513us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 569ms/epoch - 533us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 549ms/epoch - 514us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 552ms/epoch - 518us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 545ms/epoch - 511us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 554ms/epoch - 520us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 547ms/epoch - 513us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 545ms/epoch - 511us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 547ms/epoch - 512us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 544ms/epoch - 510us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 544ms/epoch - 510us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 540ms/epoch - 506us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 543ms/epoch - 509us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 542ms/epoch - 508us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 546ms/epoch - 511us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 549ms/epoch - 515us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0038 - 625ms/epoch - 586us/step\n",
      "267/267 - 0s - loss: 0.0034 - 127ms/epoch - 474us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  39.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2827 - val_loss: 0.0190\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0129 - val_loss: 0.0094\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0079 - val_loss: 0.0069\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0062 - val_loss: 0.0057\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 626us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 635us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 823us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 689us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0031 - val_loss: 0.0033\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 544ms/epoch - 510us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 575ms/epoch - 539us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 635ms/epoch - 595us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 584ms/epoch - 548us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 543ms/epoch - 509us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 552ms/epoch - 517us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 540ms/epoch - 506us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 543ms/epoch - 509us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 543ms/epoch - 509us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 573ms/epoch - 537us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0028 - 576ms/epoch - 539us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 573ms/epoch - 537us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 552ms/epoch - 517us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 541ms/epoch - 507us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 543ms/epoch - 509us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 541ms/epoch - 507us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 540ms/epoch - 506us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0029 - 553ms/epoch - 519us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 540ms/epoch - 506us/step\n",
      "267/267 - 0s - loss: 0.0028 - 121ms/epoch - 452us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  39.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2871 - val_loss: 0.0221\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0152 - val_loss: 0.0106\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0087 - val_loss: 0.0072\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0064 - val_loss: 0.0058\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 643us/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 642us/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 627us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 672us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 695us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 701us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 723us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 675us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 554ms/epoch - 519us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 545ms/epoch - 511us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0035 - 544ms/epoch - 510us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0031 - val_loss: 0.0029 - 562ms/epoch - 527us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 555ms/epoch - 520us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 544ms/epoch - 510us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 566ms/epoch - 530us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 545ms/epoch - 511us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 542ms/epoch - 508us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 558ms/epoch - 523us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 547ms/epoch - 512us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 610ms/epoch - 572us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 553ms/epoch - 518us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 552ms/epoch - 517us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 544ms/epoch - 510us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 550ms/epoch - 515us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 555ms/epoch - 521us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 552ms/epoch - 517us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 555ms/epoch - 520us/step\n",
      "267/267 - 0s - loss: 0.0028 - 122ms/epoch - 456us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  40.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2831 - val_loss: 0.0192\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.0137 - val_loss: 0.0101\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 654us/step - loss: 0.0084 - val_loss: 0.0070\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 711us/step - loss: 0.0062 - val_loss: 0.0056\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 670us/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 728us/step - loss: 0.0046 - val_loss: 0.0043\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 680us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 657us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 667us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 623us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 625us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 662us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 623us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "1067/1067 - 2s - loss: 0.0030 - val_loss: 0.0030 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 534ms/epoch - 500us/step\n",
      "Epoch 3/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 582ms/epoch - 545us/step\n",
      "Epoch 4/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0030 - 560ms/epoch - 525us/step\n",
      "Epoch 5/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 571ms/epoch - 535us/step\n",
      "Epoch 6/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 573ms/epoch - 537us/step\n",
      "Epoch 7/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 562ms/epoch - 526us/step\n",
      "Epoch 8/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 569ms/epoch - 533us/step\n",
      "Epoch 9/20\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 556ms/epoch - 521us/step\n",
      "Epoch 10/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 561ms/epoch - 526us/step\n",
      "Epoch 11/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 556ms/epoch - 521us/step\n",
      "Epoch 12/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 553ms/epoch - 518us/step\n",
      "Epoch 13/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 549ms/epoch - 515us/step\n",
      "Epoch 14/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 555ms/epoch - 521us/step\n",
      "Epoch 15/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0029 - 551ms/epoch - 517us/step\n",
      "Epoch 16/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 552ms/epoch - 518us/step\n",
      "Epoch 17/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0029 - 562ms/epoch - 527us/step\n",
      "Epoch 18/20\n",
      "1067/1067 - 1s - loss: 0.0029 - val_loss: 0.0028 - 560ms/epoch - 525us/step\n",
      "Epoch 19/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0039 - 555ms/epoch - 520us/step\n",
      "Epoch 20/20\n",
      "1067/1067 - 1s - loss: 0.0028 - val_loss: 0.0028 - 566ms/epoch - 530us/step\n",
      "267/267 - 0s - loss: 0.0028 - 126ms/epoch - 472us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  41.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3459 - val_loss: 0.2569\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 482us/step - loss: 0.1996 - val_loss: 0.1519\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 503us/step - loss: 0.1185 - val_loss: 0.0908\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 504us/step - loss: 0.0712 - val_loss: 0.0550\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 514us/step - loss: 0.0435 - val_loss: 0.0340\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 473us/step - loss: 0.0272 - val_loss: 0.0217\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 487us/step - loss: 0.0176 - val_loss: 0.0144\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 494us/step - loss: 0.0119 - val_loss: 0.0100\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 0.0085 - val_loss: 0.0074\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 464us/step - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 474us/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 482us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 445us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 447us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 478us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 481us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 458us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 469us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 1/20\n",
      "1334/1334 - 2s - loss: 0.0027 - val_loss: 0.0027 - 2s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "1334/1334 - 1s - loss: 0.0026 - val_loss: 0.0026 - 600ms/epoch - 449us/step\n",
      "Epoch 3/20\n",
      "1334/1334 - 1s - loss: 0.0026 - val_loss: 0.0026 - 580ms/epoch - 434us/step\n",
      "Epoch 4/20\n",
      "1334/1334 - 1s - loss: 0.0025 - val_loss: 0.0025 - 571ms/epoch - 428us/step\n",
      "Epoch 5/20\n",
      "1334/1334 - 1s - loss: 0.0025 - val_loss: 0.0025 - 530ms/epoch - 398us/step\n",
      "Epoch 6/20\n",
      "1334/1334 - 1s - loss: 0.0024 - val_loss: 0.0024 - 522ms/epoch - 392us/step\n",
      "Epoch 7/20\n",
      "1334/1334 - 1s - loss: 0.0024 - val_loss: 0.0024 - 525ms/epoch - 394us/step\n",
      "Epoch 8/20\n",
      "1334/1334 - 1s - loss: 0.0024 - val_loss: 0.0024 - 543ms/epoch - 407us/step\n",
      "Epoch 9/20\n",
      "1334/1334 - 1s - loss: 0.0023 - val_loss: 0.0023 - 567ms/epoch - 425us/step\n",
      "Epoch 10/20\n",
      "1334/1334 - 1s - loss: 0.0023 - val_loss: 0.0023 - 547ms/epoch - 410us/step\n",
      "Epoch 11/20\n",
      "1334/1334 - 1s - loss: 0.0022 - val_loss: 0.0022 - 553ms/epoch - 414us/step\n",
      "Epoch 12/20\n",
      "1334/1334 - 1s - loss: 0.0022 - val_loss: 0.0022 - 578ms/epoch - 433us/step\n",
      "Epoch 13/20\n",
      "1334/1334 - 1s - loss: 0.0022 - val_loss: 0.0022 - 612ms/epoch - 459us/step\n",
      "Epoch 14/20\n",
      "1334/1334 - 1s - loss: 0.0021 - val_loss: 0.0022 - 562ms/epoch - 421us/step\n",
      "Epoch 15/20\n",
      "1334/1334 - 1s - loss: 0.0021 - val_loss: 0.0021 - 599ms/epoch - 449us/step\n",
      "Epoch 16/20\n",
      "1334/1334 - 1s - loss: 0.0021 - val_loss: 0.0021 - 602ms/epoch - 451us/step\n",
      "Epoch 17/20\n",
      "1334/1334 - 1s - loss: 0.0021 - val_loss: 0.0021 - 528ms/epoch - 396us/step\n",
      "Epoch 18/20\n",
      "1334/1334 - 1s - loss: 0.0020 - val_loss: 0.0021 - 525ms/epoch - 393us/step\n",
      "Epoch 19/20\n",
      "1334/1334 - 1s - loss: 0.0020 - val_loss: 0.0020 - 519ms/epoch - 389us/step\n",
      "Epoch 20/20\n",
      "1334/1334 - 1s - loss: 0.0020 - val_loss: 0.0020 - 546ms/epoch - 409us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x317289cd0&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x317289cd0&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x317289cd0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x317289cd0&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x317289cd0>,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={'activation': ['relu', 'tanh'],\n",
       "                             'batch_size': [16, 32, 64],\n",
       "                             'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
       "                             'learning_rate': [0.001, 0.01, 0.1],\n",
       "                             'optimizer': ['sgd', 'adam']},\n",
       "              verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define base model function for bayes search\n",
    "def create_model(activation='relu', optimizer='adam', learning_rate=0.001, kernel_regularizer=None, batch_size=32):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'optimizer': ['sgd', 'adam'],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform random search hyperparameter tuning\n",
    "bayes_search = BayesSearchCV(estimator=keras_reg, search_spaces=param_dist, n_iter=10, cv=5, verbose=2, random_state=42, error_score='raise')\n",
    "bayes_search.fit(Xtrain, ytrain, sample_weight=wtrain, validation_data=(Xval, yval, wval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.505213</td>\n",
       "      <td>45.772704</td>\n",
       "      <td>0.254974</td>\n",
       "      <td>0.142270</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.002796</td>\n",
       "      <td>-0.002730</td>\n",
       "      <td>-0.002799</td>\n",
       "      <td>-0.002793</td>\n",
       "      <td>-0.002795</td>\n",
       "      <td>-0.002783</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.452278</td>\n",
       "      <td>11.530733</td>\n",
       "      <td>0.171502</td>\n",
       "      <td>0.008555</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.269539</td>\n",
       "      <td>-0.289035</td>\n",
       "      <td>-0.267694</td>\n",
       "      <td>-0.242272</td>\n",
       "      <td>-0.258110</td>\n",
       "      <td>-0.265330</td>\n",
       "      <td>0.015291</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.991496</td>\n",
       "      <td>0.848519</td>\n",
       "      <td>0.167091</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.006232</td>\n",
       "      <td>-0.006389</td>\n",
       "      <td>-0.005917</td>\n",
       "      <td>-0.006209</td>\n",
       "      <td>-0.006068</td>\n",
       "      <td>-0.006163</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132.749105</td>\n",
       "      <td>2.708723</td>\n",
       "      <td>0.390171</td>\n",
       "      <td>0.018474</td>\n",
       "      <td>tanh</td>\n",
       "      <td>16</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 16, 'kern...</td>\n",
       "      <td>-0.151107</td>\n",
       "      <td>-0.143950</td>\n",
       "      <td>-0.274928</td>\n",
       "      <td>-0.310833</td>\n",
       "      <td>-0.139528</td>\n",
       "      <td>-0.204069</td>\n",
       "      <td>0.073491</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.800706</td>\n",
       "      <td>1.794140</td>\n",
       "      <td>0.255131</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 32, 'kern...</td>\n",
       "      <td>-0.076903</td>\n",
       "      <td>-0.165621</td>\n",
       "      <td>-0.148347</td>\n",
       "      <td>-0.110676</td>\n",
       "      <td>-0.400030</td>\n",
       "      <td>-0.180315</td>\n",
       "      <td>0.114067</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>47.685086</td>\n",
       "      <td>0.784581</td>\n",
       "      <td>0.168622</td>\n",
       "      <td>0.013626</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.005406</td>\n",
       "      <td>-0.005187</td>\n",
       "      <td>-0.005390</td>\n",
       "      <td>-0.005070</td>\n",
       "      <td>-0.005279</td>\n",
       "      <td>-0.005267</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>47.084572</td>\n",
       "      <td>0.752071</td>\n",
       "      <td>0.170252</td>\n",
       "      <td>0.014930</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.077937</td>\n",
       "      <td>-0.387723</td>\n",
       "      <td>-0.083635</td>\n",
       "      <td>-0.050334</td>\n",
       "      <td>-0.084537</td>\n",
       "      <td>-0.136833</td>\n",
       "      <td>0.126065</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42.273023</td>\n",
       "      <td>0.395488</td>\n",
       "      <td>0.167234</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.001617</td>\n",
       "      <td>-0.001884</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>-0.001792</td>\n",
       "      <td>-0.001818</td>\n",
       "      <td>-0.001741</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36.761418</td>\n",
       "      <td>3.757783</td>\n",
       "      <td>0.144581</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.020660</td>\n",
       "      <td>-0.020468</td>\n",
       "      <td>-0.020449</td>\n",
       "      <td>-0.021147</td>\n",
       "      <td>-0.021387</td>\n",
       "      <td>-0.020822</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40.125172</td>\n",
       "      <td>0.551569</td>\n",
       "      <td>0.137422</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.003582</td>\n",
       "      <td>-0.003393</td>\n",
       "      <td>-0.002792</td>\n",
       "      <td>-0.002813</td>\n",
       "      <td>-0.002806</td>\n",
       "      <td>-0.003077</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      83.505213     45.772704         0.254974        0.142270   \n",
       "1      54.452278     11.530733         0.171502        0.008555   \n",
       "2      41.991496      0.848519         0.167091        0.001235   \n",
       "3     132.749105      2.708723         0.390171        0.018474   \n",
       "4      76.800706      1.794140         0.255131        0.009214   \n",
       "5      47.685086      0.784581         0.168622        0.013626   \n",
       "6      47.084572      0.752071         0.170252        0.014930   \n",
       "7      42.273023      0.395488         0.167234        0.005081   \n",
       "8      36.761418      3.757783         0.144581        0.006500   \n",
       "9      40.125172      0.551569         0.137422        0.003076   \n",
       "\n",
       "  param_activation param_batch_size param_kernel_regularizer  \\\n",
       "0             relu               64                    l1_l2   \n",
       "1             tanh               64                       l1   \n",
       "2             relu               64                       l1   \n",
       "3             tanh               16                       l2   \n",
       "4             tanh               32                       l2   \n",
       "5             tanh               64                       l1   \n",
       "6             tanh               64                       l2   \n",
       "7             tanh               64                       l2   \n",
       "8             tanh               64                    l1_l2   \n",
       "9             relu               64                    l1_l2   \n",
       "\n",
       "  param_learning_rate param_optimizer  \\\n",
       "0               0.001            adam   \n",
       "1                 0.1            adam   \n",
       "2                0.01             sgd   \n",
       "3                 0.1            adam   \n",
       "4                 0.1            adam   \n",
       "5               0.001            adam   \n",
       "6                 0.1            adam   \n",
       "7                 0.1             sgd   \n",
       "8                0.01             sgd   \n",
       "9               0.001            adam   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.002796   \n",
       "1  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.269539   \n",
       "2  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.006232   \n",
       "3  {'activation': 'tanh', 'batch_size': 16, 'kern...          -0.151107   \n",
       "4  {'activation': 'tanh', 'batch_size': 32, 'kern...          -0.076903   \n",
       "5  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.005406   \n",
       "6  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.077937   \n",
       "7  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.001617   \n",
       "8  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.020660   \n",
       "9  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.003582   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0          -0.002730          -0.002799          -0.002793          -0.002795   \n",
       "1          -0.289035          -0.267694          -0.242272          -0.258110   \n",
       "2          -0.006389          -0.005917          -0.006209          -0.006068   \n",
       "3          -0.143950          -0.274928          -0.310833          -0.139528   \n",
       "4          -0.165621          -0.148347          -0.110676          -0.400030   \n",
       "5          -0.005187          -0.005390          -0.005070          -0.005279   \n",
       "6          -0.387723          -0.083635          -0.050334          -0.084537   \n",
       "7          -0.001884          -0.001595          -0.001792          -0.001818   \n",
       "8          -0.020468          -0.020449          -0.021147          -0.021387   \n",
       "9          -0.003393          -0.002792          -0.002813          -0.002806   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0        -0.002783        0.000026                2  \n",
       "1        -0.265330        0.015291               10  \n",
       "2        -0.006163        0.000160                5  \n",
       "3        -0.204069        0.073491                9  \n",
       "4        -0.180315        0.114067                8  \n",
       "5        -0.005267        0.000126                4  \n",
       "6        -0.136833        0.126065                7  \n",
       "7        -0.001741        0.000114                1  \n",
       "8        -0.020822        0.000378                6  \n",
       "9        -0.003077        0.000340                3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bayes_search_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "\n",
    "display(bayes_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_8512/3282579545.py:23: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 28452\n",
      "max_resources_: 85357\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 6\n",
      "n_resources: 28452\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2702 - val_loss: 0.0429\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0366 - val_loss: 0.0324\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0294 - val_loss: 0.0283\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0253 - val_loss: 0.0239\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0223 - val_loss: 0.0206\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0194 - val_loss: 0.0174\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0157 - val_loss: 0.0144\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.0136 - val_loss: 0.0131\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0126 - val_loss: 0.0119\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0118 - val_loss: 0.0113\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0091 - val_loss: 0.0088\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 0.0078 - val_loss: 0.0076\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 621us/step - loss: 0.0073 - val_loss: 0.0075\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0071 - 299ms/epoch - 420us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0070 - 309ms/epoch - 434us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0069 - 301ms/epoch - 422us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0068 - 288ms/epoch - 404us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0067 - 297ms/epoch - 418us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0067 - 322ms/epoch - 453us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0066 - 292ms/epoch - 410us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0065 - 303ms/epoch - 426us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0065 - 300ms/epoch - 422us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0064 - 334ms/epoch - 470us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0064 - 304ms/epoch - 427us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0063 - 292ms/epoch - 410us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0062 - 291ms/epoch - 409us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0062 - 295ms/epoch - 415us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0062 - 316ms/epoch - 443us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0061 - 298ms/epoch - 419us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0060 - 295ms/epoch - 415us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0060 - 292ms/epoch - 410us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0060 - 285ms/epoch - 400us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0059 - 300ms/epoch - 422us/step\n",
      "178/178 - 0s - loss: 0.0061 - 100ms/epoch - 563us/step\n",
      "712/712 - 0s - loss: 0.0061 - 219ms/epoch - 307us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  30.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2987 - val_loss: 0.0456\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0392 - val_loss: 0.0390\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 0.0310 - val_loss: 0.0281\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0259 - val_loss: 0.0249\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0218 - val_loss: 0.0189\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0171 - val_loss: 0.0161\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0150 - val_loss: 0.0142\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0137 - val_loss: 0.0130\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0127 - val_loss: 0.0124\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0117 - val_loss: 0.0113\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 520us/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0090 - val_loss: 0.0086\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 738us/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0071 - 345ms/epoch - 485us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0069 - 331ms/epoch - 465us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0069 - 332ms/epoch - 466us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0068 - 290ms/epoch - 407us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0068 - 288ms/epoch - 405us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0066 - 321ms/epoch - 451us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0066 - 295ms/epoch - 415us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0065 - 313ms/epoch - 440us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0065 - 298ms/epoch - 419us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0064 - 296ms/epoch - 416us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0064 - 293ms/epoch - 412us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0063 - 296ms/epoch - 416us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0063 - 300ms/epoch - 421us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0062 - 303ms/epoch - 425us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0062 - 289ms/epoch - 406us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0061 - 288ms/epoch - 405us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0061 - 296ms/epoch - 416us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0060 - 300ms/epoch - 421us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0060 - 305ms/epoch - 428us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0060 - 307ms/epoch - 431us/step\n",
      "178/178 - 0s - loss: 0.0058 - 107ms/epoch - 601us/step\n",
      "712/712 - 0s - loss: 0.0057 - 236ms/epoch - 331us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  29.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2455 - val_loss: 0.0414\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0356 - val_loss: 0.0310\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 0.0290 - val_loss: 0.0265\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0248 - val_loss: 0.0223\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0202 - val_loss: 0.0178\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0165 - val_loss: 0.0156\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 0.0148 - val_loss: 0.0140\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0137 - val_loss: 0.0129\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0127 - val_loss: 0.0123\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 0.0098 - val_loss: 0.0093\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0088 - val_loss: 0.0085\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0072 - 288ms/epoch - 404us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0070 - 286ms/epoch - 402us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0069 - 285ms/epoch - 401us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0068 - 288ms/epoch - 405us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0067 - 284ms/epoch - 399us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0066 - 287ms/epoch - 403us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0066 - 283ms/epoch - 397us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0066 - 288ms/epoch - 404us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0065 - 286ms/epoch - 401us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0064 - 285ms/epoch - 400us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0063 - 285ms/epoch - 401us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0063 - 287ms/epoch - 404us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0063 - 308ms/epoch - 432us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0062 - 286ms/epoch - 402us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0061 - 284ms/epoch - 399us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0061 - 285ms/epoch - 400us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0061 - 283ms/epoch - 397us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0060 - 285ms/epoch - 400us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0060 - 289ms/epoch - 405us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0059 - 288ms/epoch - 404us/step\n",
      "178/178 - 0s - loss: 0.0059 - 93ms/epoch - 525us/step\n",
      "712/712 - 0s - loss: 0.0058 - 188ms/epoch - 263us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  29.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2322 - val_loss: 0.0410\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0364 - val_loss: 0.0318\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0297 - val_loss: 0.0271\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0257 - val_loss: 0.0254\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0208 - val_loss: 0.0183\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 0.0169 - val_loss: 0.0154\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0151 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0138 - val_loss: 0.0137\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 522us/step - loss: 0.0127 - val_loss: 0.0124\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0118 - val_loss: 0.0114\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 521us/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0090 - val_loss: 0.0087\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 524us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0072 - 286ms/epoch - 402us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0071 - 284ms/epoch - 398us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0070 - 283ms/epoch - 397us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0069 - 286ms/epoch - 401us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0068 - 283ms/epoch - 398us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0067 - 284ms/epoch - 399us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0067 - 284ms/epoch - 399us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0067 - 283ms/epoch - 398us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0065 - 284ms/epoch - 399us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0065 - 285ms/epoch - 401us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0064 - 326ms/epoch - 458us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0064 - 289ms/epoch - 406us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0063 - 284ms/epoch - 399us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0063 - 284ms/epoch - 399us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0063 - 283ms/epoch - 398us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0062 - 285ms/epoch - 400us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0062 - 283ms/epoch - 398us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0061 - 310ms/epoch - 435us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0061 - 314ms/epoch - 442us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0060 - 283ms/epoch - 397us/step\n",
      "178/178 - 0s - loss: 0.0059 - 95ms/epoch - 536us/step\n",
      "712/712 - 0s - loss: 0.0060 - 188ms/epoch - 264us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  29.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3049 - val_loss: 0.0483\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0398 - val_loss: 0.0339\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0306 - val_loss: 0.0282\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0259 - val_loss: 0.0236\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 0.0227 - val_loss: 0.0209\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0197 - val_loss: 0.0175\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0161 - val_loss: 0.0144\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0137 - val_loss: 0.0133\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0126 - val_loss: 0.0119\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0117 - val_loss: 0.0119\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0089 - val_loss: 0.0085\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0085 - val_loss: 0.0082\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0072 - 295ms/epoch - 414us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0071 - 481ms/epoch - 676us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0071 - 324ms/epoch - 455us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0070 - 322ms/epoch - 452us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0069 - 287ms/epoch - 402us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0069 - 289ms/epoch - 406us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0068 - 293ms/epoch - 411us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0067 - 319ms/epoch - 448us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0066 - 302ms/epoch - 425us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0066 - 343ms/epoch - 482us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0066 - 311ms/epoch - 437us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0065 - 289ms/epoch - 406us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0064 - 287ms/epoch - 404us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0064 - 315ms/epoch - 442us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0063 - 325ms/epoch - 456us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0063 - 324ms/epoch - 455us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0062 - 344ms/epoch - 483us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0062 - 306ms/epoch - 429us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0062 - 325ms/epoch - 457us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0062 - 317ms/epoch - 445us/step\n",
      "178/178 - 0s - loss: 0.0060 - 96ms/epoch - 541us/step\n",
      "712/712 - 0s - loss: 0.0060 - 187ms/epoch - 263us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  29.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0636 - val_loss: 0.0041\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0012 - val_loss: 9.1677e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 9.9316e-04 - val_loss: 7.8574e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 9.6023e-04 - val_loss: 0.0010\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 9.1996e-04 - val_loss: 6.9983e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 8.7277e-04 - val_loss: 0.0020\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 9.2581e-04 - val_loss: 7.4659e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 8.5349e-04 - val_loss: 7.3798e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 8.1265e-04 - val_loss: 6.2289e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 8.8748e-04 - val_loss: 6.0924e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 7.7065e-04 - val_loss: 6.8096e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 8.5081e-04 - val_loss: 6.2276e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 7.8043e-04 - val_loss: 5.5898e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 9.1696e-04 - 301ms/epoch - 423us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0011 - 307ms/epoch - 431us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0011 - 289ms/epoch - 405us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 7.8322e-04 - 294ms/epoch - 413us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0010 - 300ms/epoch - 422us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0011 - 301ms/epoch - 422us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 9.6353e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0012 - 290ms/epoch - 407us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 8.7459e-04 - 310ms/epoch - 435us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 9.7411e-04 - 290ms/epoch - 407us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 9.4979e-04 - 287ms/epoch - 403us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 8.7803e-04 - 302ms/epoch - 423us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 8.7515e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 9.6249e-04 - 300ms/epoch - 421us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 8.3450e-04 - 303ms/epoch - 425us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0011 - 308ms/epoch - 433us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0010 - 317ms/epoch - 445us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 8.0046e-04 - 293ms/epoch - 412us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 9.5286e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 9.6723e-04 - 292ms/epoch - 411us/step\n",
      "178/178 - 0s - loss: 5.3872e-04 - 100ms/epoch - 564us/step\n",
      "712/712 - 0s - loss: 5.4062e-04 - 203ms/epoch - 285us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0736 - val_loss: 0.0040\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0029 - val_loss: 0.0021\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0011 - val_loss: 9.0283e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0010 - val_loss: 7.8236e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 8.9758e-04 - val_loss: 7.2605e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 8.9630e-04 - val_loss: 6.4442e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 8.1002e-04 - val_loss: 6.7754e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 8.1330e-04 - val_loss: 6.1533e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 7.5225e-04 - val_loss: 6.3136e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 7.0115e-04 - val_loss: 9.3229e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 524us/step - loss: 7.2214e-04 - val_loss: 6.4005e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 6.8400e-04 - val_loss: 0.0011\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 6.8444e-04 - val_loss: 6.1563e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 6.2951e-04 - val_loss: 0.0012\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 521us/step - loss: 6.8112e-04 - val_loss: 7.8356e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 6.1777e-04 - val_loss: 0.0010\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 6.1372e-04 - val_loss: 8.0141e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 8.8636e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 8.1884e-04 - 283ms/epoch - 398us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 9.0465e-04 - 283ms/epoch - 397us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 8.3738e-04 - 294ms/epoch - 413us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 7.0057e-04 - 375ms/epoch - 527us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 7.7151e-04 - 349ms/epoch - 490us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 8.8219e-04 - 315ms/epoch - 442us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 7.3520e-04 - 302ms/epoch - 424us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 7.4424e-04 - 342ms/epoch - 480us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 8.0089e-04 - 335ms/epoch - 470us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 7.4510e-04 - 309ms/epoch - 434us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 8.4922e-04 - 289ms/epoch - 407us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 8.3310e-04 - 328ms/epoch - 461us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 7.7766e-04 - 336ms/epoch - 473us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 6.8905e-04 - 370ms/epoch - 520us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0010 - 331ms/epoch - 464us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 5.8204e-04 - 312ms/epoch - 438us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 8.1894e-04 - 321ms/epoch - 451us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 8.0951e-04 - 323ms/epoch - 454us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 7.8586e-04 - 310ms/epoch - 435us/step\n",
      "178/178 - 0s - loss: 6.0381e-04 - 95ms/epoch - 536us/step\n",
      "712/712 - 0s - loss: 5.9075e-04 - 188ms/epoch - 263us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0676 - val_loss: 0.0033\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0026 - val_loss: 0.0019\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0011 - val_loss: 9.2976e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0010 - val_loss: 9.3339e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 9.3098e-04 - val_loss: 7.3364e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 8.6409e-04 - val_loss: 0.0012\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 790us/step - loss: 8.2639e-04 - val_loss: 9.6681e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 672us/step - loss: 7.8378e-04 - val_loss: 6.3112e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 7.9126e-04 - val_loss: 5.4812e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 7.2436e-04 - val_loss: 5.6053e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 7.2356e-04 - val_loss: 5.5284e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 6.9959e-04 - val_loss: 5.0838e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 6.1950e-04 - val_loss: 4.8499e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 6.5612e-04 - val_loss: 9.3581e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 6.6063e-04 - val_loss: 5.5049e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 6.2308e-04 - val_loss: 5.3285e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 6.4126e-04 - val_loss: 4.3894e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 5.6007e-04 - val_loss: 4.7128e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0011 - 290ms/epoch - 407us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 7.3618e-04 - 285ms/epoch - 401us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 6.4423e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 7.1120e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 7.8853e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 6.2724e-04 - 271ms/epoch - 380us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 8.3135e-04 - 252ms/epoch - 354us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 6.0406e-04 - 288ms/epoch - 405us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 8.4770e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 7.3026e-04 - 323ms/epoch - 454us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 7.9899e-04 - 291ms/epoch - 409us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 7.2161e-04 - 291ms/epoch - 409us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 8.8931e-04 - 286ms/epoch - 402us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 6.3998e-04 - 282ms/epoch - 397us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 9.7998e-04 - 283ms/epoch - 398us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 6.2698e-04 - 287ms/epoch - 403us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 9.4142e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 6.4851e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 7.8842e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 7.8052e-04 - 281ms/epoch - 395us/step\n",
      "178/178 - 0s - loss: 5.7786e-04 - 94ms/epoch - 528us/step\n",
      "712/712 - 0s - loss: 5.7385e-04 - 186ms/epoch - 261us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0736 - val_loss: 0.0038\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0012 - val_loss: 9.3500e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 0.0011 - val_loss: 9.1200e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0010 - val_loss: 0.0024\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 9.3994e-04 - val_loss: 9.3041e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 9.5869e-04 - val_loss: 7.1454e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 8.7084e-04 - val_loss: 0.0012\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 9.1470e-04 - val_loss: 9.0216e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 8.7564e-04 - val_loss: 6.9273e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 8.7835e-04 - val_loss: 6.4247e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 8.0217e-04 - val_loss: 0.0013\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 8.6199e-04 - val_loss: 5.8411e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 8.0979e-04 - val_loss: 8.5778e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 8.0882e-04 - val_loss: 5.9530e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0014 - 287ms/epoch - 403us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 9.0090e-04 - 285ms/epoch - 401us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 9.3735e-04 - 292ms/epoch - 411us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 9.1295e-04 - 290ms/epoch - 407us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 8.9804e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 8.8573e-04 - 305ms/epoch - 428us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0011 - 309ms/epoch - 434us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0011 - 331ms/epoch - 465us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 9.4730e-04 - 321ms/epoch - 451us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 8.5011e-04 - 317ms/epoch - 445us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 9.9331e-04 - 322ms/epoch - 452us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 8.9836e-04 - 342ms/epoch - 481us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 9.5030e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0012 - 301ms/epoch - 423us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 7.6186e-04 - 304ms/epoch - 427us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0010 - 306ms/epoch - 430us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 8.9986e-04 - 306ms/epoch - 430us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0011 - 301ms/epoch - 423us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 9.0481e-04 - 301ms/epoch - 422us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 8.3757e-04 - 330ms/epoch - 463us/step\n",
      "178/178 - 0s - loss: 5.0953e-04 - 158ms/epoch - 887us/step\n",
      "712/712 - 0s - loss: 5.1653e-04 - 231ms/epoch - 324us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.3s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0616 - val_loss: 0.0035\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0013 - val_loss: 9.9422e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 9.3339e-04 - val_loss: 7.9821e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 9.5204e-04 - val_loss: 7.2052e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 9.2679e-04 - val_loss: 6.8772e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 8.7810e-04 - val_loss: 0.0024\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 8.7902e-04 - val_loss: 7.1993e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 7.9799e-04 - val_loss: 6.2939e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 7.9586e-04 - val_loss: 5.8022e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 8.3272e-04 - val_loss: 0.0012\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 7.5730e-04 - val_loss: 0.0013\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 7.5863e-04 - val_loss: 5.5145e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 7.6443e-04 - val_loss: 7.1899e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0011 - 298ms/epoch - 418us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 9.8922e-04 - 395ms/epoch - 555us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 9.6770e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 8.9430e-04 - 326ms/epoch - 458us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 9.5282e-04 - 316ms/epoch - 443us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 8.7747e-04 - 302ms/epoch - 424us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 8.5198e-04 - 290ms/epoch - 407us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 8.8333e-04 - 377ms/epoch - 530us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0010 - 292ms/epoch - 410us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 7.8991e-04 - 297ms/epoch - 417us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 9.9667e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 8.6676e-04 - 300ms/epoch - 421us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 8.8890e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 8.6809e-04 - 332ms/epoch - 466us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0010 - 291ms/epoch - 409us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 8.7239e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 9.9363e-04 - 382ms/epoch - 537us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 8.5742e-04 - 305ms/epoch - 429us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 8.4572e-04 - 367ms/epoch - 515us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 8.0631e-04 - 312ms/epoch - 438us/step\n",
      "178/178 - 0s - loss: 4.7523e-04 - 103ms/epoch - 581us/step\n",
      "712/712 - 0s - loss: 4.7434e-04 - 200ms/epoch - 281us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.9s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2983 - val_loss: 0.0457\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0385 - val_loss: 0.0349\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0301 - val_loss: 0.0304\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0257 - val_loss: 0.0242\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0225 - val_loss: 0.0218\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0191 - val_loss: 0.0169\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0157 - val_loss: 0.0150\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0140 - val_loss: 0.0135\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0128 - val_loss: 0.0129\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0120 - val_loss: 0.0113\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0098 - val_loss: 0.0095\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 625us/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0074 - 305ms/epoch - 429us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0073 - 320ms/epoch - 449us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0071 - 299ms/epoch - 419us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0071 - 300ms/epoch - 421us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0070 - 299ms/epoch - 420us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0070 - 312ms/epoch - 438us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0069 - 315ms/epoch - 442us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0069 - 320ms/epoch - 449us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0068 - 306ms/epoch - 430us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0067 - 309ms/epoch - 433us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0066 - 305ms/epoch - 428us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0066 - 294ms/epoch - 413us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0065 - 303ms/epoch - 425us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0065 - 301ms/epoch - 423us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0065 - 301ms/epoch - 422us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0064 - 313ms/epoch - 440us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0064 - 304ms/epoch - 426us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0063 - 324ms/epoch - 455us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0063 - 296ms/epoch - 416us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0063 - 321ms/epoch - 451us/step\n",
      "178/178 - 0s - loss: 0.0068 - 140ms/epoch - 787us/step\n",
      "712/712 - 0s - loss: 0.0068 - 200ms/epoch - 281us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  30.3s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2975 - val_loss: 0.0474\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0388 - val_loss: 0.0331\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 687us/step - loss: 0.0305 - val_loss: 0.0271\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 625us/step - loss: 0.0261 - val_loss: 0.0235\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 642us/step - loss: 0.0225 - val_loss: 0.0202\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0189 - val_loss: 0.0167\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0159 - val_loss: 0.0161\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 674us/step - loss: 0.0144 - val_loss: 0.0134\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 642us/step - loss: 0.0132 - val_loss: 0.0134\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 0.0122 - val_loss: 0.0115\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0094 - val_loss: 0.0094\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0090 - val_loss: 0.0087\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.0086 - val_loss: 0.0084\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 667us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0078 - val_loss: 0.0075\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0076 - 343ms/epoch - 482us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0074 - 315ms/epoch - 442us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0073 - 317ms/epoch - 445us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0073 - 315ms/epoch - 443us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0071 - 316ms/epoch - 444us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0071 - 329ms/epoch - 462us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0071 - 337ms/epoch - 473us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0069 - 291ms/epoch - 408us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0070 - 334ms/epoch - 469us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0069 - 367ms/epoch - 515us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0068 - 293ms/epoch - 411us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0068 - 315ms/epoch - 442us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0067 - 310ms/epoch - 436us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0068 - 303ms/epoch - 425us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0067 - 310ms/epoch - 436us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0066 - 332ms/epoch - 466us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0066 - 326ms/epoch - 459us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0066 - 308ms/epoch - 432us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0065 - 298ms/epoch - 419us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0064 - 309ms/epoch - 434us/step\n",
      "178/178 - 0s - loss: 0.0063 - 110ms/epoch - 616us/step\n",
      "712/712 - 0s - loss: 0.0062 - 203ms/epoch - 285us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  31.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3023 - val_loss: 0.0452\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0376 - val_loss: 0.0317\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0295 - val_loss: 0.0291\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.0250 - val_loss: 0.0233\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 640us/step - loss: 0.0214 - val_loss: 0.0194\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0174 - val_loss: 0.0159\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0150 - val_loss: 0.0138\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0136 - val_loss: 0.0128\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0126 - val_loss: 0.0122\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0096 - val_loss: 0.0092\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0091 - val_loss: 0.0087\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0087 - val_loss: 0.0084\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0075 - 309ms/epoch - 433us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0074 - 293ms/epoch - 411us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0072 - 289ms/epoch - 405us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0072 - 288ms/epoch - 405us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0071 - 291ms/epoch - 409us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0071 - 289ms/epoch - 406us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0070 - 289ms/epoch - 405us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0069 - 289ms/epoch - 406us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0069 - 290ms/epoch - 407us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0069 - 289ms/epoch - 406us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0068 - 288ms/epoch - 404us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0068 - 290ms/epoch - 408us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0067 - 290ms/epoch - 407us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0066 - 289ms/epoch - 406us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0066 - 291ms/epoch - 409us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0065 - 288ms/epoch - 405us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0065 - 291ms/epoch - 408us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0064 - 289ms/epoch - 405us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0064 - 297ms/epoch - 417us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0064 - 289ms/epoch - 407us/step\n",
      "178/178 - 0s - loss: 0.0061 - 97ms/epoch - 546us/step\n",
      "712/712 - 0s - loss: 0.0060 - 190ms/epoch - 267us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  29.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3010 - val_loss: 0.0466\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0388 - val_loss: 0.0339\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0305 - val_loss: 0.0274\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0263 - val_loss: 0.0250\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0228 - val_loss: 0.0213\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0189 - val_loss: 0.0168\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0160 - val_loss: 0.0176\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0132 - val_loss: 0.0143\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0123 - val_loss: 0.0125\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0096 - val_loss: 0.0091\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0091 - val_loss: 0.0088\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0087 - val_loss: 0.0091\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0078 - 292ms/epoch - 410us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0076 - 289ms/epoch - 406us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0075 - 290ms/epoch - 407us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0074 - 292ms/epoch - 410us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0074 - 291ms/epoch - 408us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0072 - 291ms/epoch - 408us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0072 - 293ms/epoch - 411us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0071 - 293ms/epoch - 411us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0071 - 288ms/epoch - 404us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0069 - 295ms/epoch - 414us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0070 - 288ms/epoch - 404us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0068 - 299ms/epoch - 420us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0069 - 290ms/epoch - 407us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0068 - 285ms/epoch - 400us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0068 - 305ms/epoch - 429us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0067 - 285ms/epoch - 401us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0067 - 284ms/epoch - 398us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0066 - 289ms/epoch - 405us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0066 - 284ms/epoch - 400us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0065 - 303ms/epoch - 425us/step\n",
      "178/178 - 0s - loss: 0.0063 - 101ms/epoch - 565us/step\n",
      "712/712 - 0s - loss: 0.0064 - 189ms/epoch - 266us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  29.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2946 - val_loss: 0.0457\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 623us/step - loss: 0.0370 - val_loss: 0.0325\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0293 - val_loss: 0.0262\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.0250 - val_loss: 0.0274\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0219 - val_loss: 0.0203\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0180 - val_loss: 0.0160\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0155 - val_loss: 0.0147\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0140 - val_loss: 0.0135\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0130 - val_loss: 0.0127\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 698us/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 669us/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 624us/step - loss: 0.0085 - val_loss: 0.0087\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 0.0079 - val_loss: 0.0087\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0076 - val_loss: 0.0084\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0074 - 317ms/epoch - 445us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0073 - 316ms/epoch - 443us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0072 - 310ms/epoch - 436us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0071 - 310ms/epoch - 435us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0071 - 318ms/epoch - 447us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0070 - 308ms/epoch - 432us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0070 - 316ms/epoch - 443us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0069 - 320ms/epoch - 450us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0069 - 312ms/epoch - 438us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0068 - 330ms/epoch - 464us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0067 - 303ms/epoch - 425us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0067 - 318ms/epoch - 447us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0067 - 319ms/epoch - 448us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0066 - 315ms/epoch - 443us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0065 - 314ms/epoch - 442us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0066 - 374ms/epoch - 525us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0064 - 314ms/epoch - 441us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0064 - 328ms/epoch - 460us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0064 - 307ms/epoch - 431us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0063 - 306ms/epoch - 429us/step\n",
      "178/178 - 0s - loss: 0.0061 - 107ms/epoch - 600us/step\n",
      "712/712 - 0s - loss: 0.0061 - 199ms/epoch - 279us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  31.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2476 - val_loss: 0.0219\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 0.0136 - val_loss: 0.0092\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0077 - val_loss: 0.0064\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 622us/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 699us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 629us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0029 - val_loss: 0.0043\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0031 - 291ms/epoch - 409us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0029 - 291ms/epoch - 409us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0029 - 321ms/epoch - 451us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 302ms/epoch - 424us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0028 - 318ms/epoch - 446us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0029 - 284ms/epoch - 399us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 283ms/epoch - 398us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0028 - 289ms/epoch - 406us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 312ms/epoch - 438us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0028 - 303ms/epoch - 425us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 307ms/epoch - 431us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0032 - 303ms/epoch - 425us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0028 - 304ms/epoch - 427us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0028 - 327ms/epoch - 459us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 302ms/epoch - 424us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0028 - 304ms/epoch - 427us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0030 - 305ms/epoch - 428us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0029 - 304ms/epoch - 427us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0028 - 310ms/epoch - 435us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0028 - 298ms/epoch - 418us/step\n",
      "178/178 - 0s - loss: 0.0028 - 102ms/epoch - 573us/step\n",
      "712/712 - 0s - loss: 0.0028 - 206ms/epoch - 289us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  30.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2686 - val_loss: 0.0217\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0143 - val_loss: 0.0102\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0085 - val_loss: 0.0072\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0064 - val_loss: 0.0058\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0047 - val_loss: 0.0044\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 676us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 654us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0033 - 288ms/epoch - 405us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0029 - 284ms/epoch - 398us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 284ms/epoch - 399us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 281ms/epoch - 394us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0030 - 283ms/epoch - 397us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0029 - 301ms/epoch - 422us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 296ms/epoch - 416us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0029 - 303ms/epoch - 426us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 296ms/epoch - 416us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0030 - 289ms/epoch - 406us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 301ms/epoch - 422us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0030 - 282ms/epoch - 397us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0028 - 281ms/epoch - 394us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0029 - 294ms/epoch - 413us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0030 - 290ms/epoch - 408us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0028 - 281ms/epoch - 395us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0028 - 282ms/epoch - 396us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0029 - 286ms/epoch - 402us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 297ms/epoch - 417us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0028 - 292ms/epoch - 410us/step\n",
      "178/178 - 0s - loss: 0.0028 - 101ms/epoch - 567us/step\n",
      "712/712 - 0s - loss: 0.0028 - 195ms/epoch - 273us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  31.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2477 - val_loss: 0.0224\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0142 - val_loss: 0.0098\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0083 - val_loss: 0.0072\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 0.0064 - val_loss: 0.0059\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0044 - val_loss: 0.0048\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0032 - 305ms/epoch - 428us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 297ms/epoch - 417us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 321ms/epoch - 451us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 309ms/epoch - 434us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0030 - 298ms/epoch - 418us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 289ms/epoch - 406us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0031 - 301ms/epoch - 422us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0029 - 323ms/epoch - 454us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 307ms/epoch - 431us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0031 - 318ms/epoch - 446us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 308ms/epoch - 432us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0029 - 305ms/epoch - 428us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0030 - 292ms/epoch - 410us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0029 - 285ms/epoch - 400us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 305ms/epoch - 428us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0029 - 319ms/epoch - 448us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0033 - 296ms/epoch - 416us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0028 - 294ms/epoch - 413us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 310ms/epoch - 435us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0030 - 303ms/epoch - 425us/step\n",
      "178/178 - 0s - loss: 0.0029 - 96ms/epoch - 542us/step\n",
      "712/712 - 0s - loss: 0.0029 - 227ms/epoch - 318us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  30.3s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2470 - val_loss: 0.0166\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 639us/step - loss: 0.0116 - val_loss: 0.0084\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0073 - val_loss: 0.0063\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 524us/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0049 - val_loss: 0.0045\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 522us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0031 - val_loss: 0.0033\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 521us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0033 - 284ms/epoch - 399us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0029 - 302ms/epoch - 425us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 298ms/epoch - 419us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0029 - 318ms/epoch - 447us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0030 - 317ms/epoch - 445us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 407us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 287ms/epoch - 403us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0029 - 289ms/epoch - 406us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 283ms/epoch - 398us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0029 - 291ms/epoch - 409us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0030 - 285ms/epoch - 400us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0029 - 283ms/epoch - 397us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0029 - 283ms/epoch - 397us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0030 - 283ms/epoch - 398us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 281ms/epoch - 395us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0031 - 292ms/epoch - 410us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 407us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0030 - 287ms/epoch - 403us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0028 - 288ms/epoch - 404us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0029 - 284ms/epoch - 399us/step\n",
      "178/178 - 0s - loss: 0.0028 - 94ms/epoch - 527us/step\n",
      "712/712 - 0s - loss: 0.0028 - 232ms/epoch - 325us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  29.0s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2571 - val_loss: 0.0229\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 524us/step - loss: 0.0153 - val_loss: 0.0110\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0088 - val_loss: 0.0074\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0066 - val_loss: 0.0059\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 518us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 520us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0030 - val_loss: 0.0056\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0031 - 322ms/epoch - 453us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 294ms/epoch - 413us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 308ms/epoch - 433us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0031 - 307ms/epoch - 431us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0030 - 309ms/epoch - 434us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 366ms/epoch - 513us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 286ms/epoch - 401us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 407us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 316ms/epoch - 443us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0030 - 329ms/epoch - 463us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0030 - 310ms/epoch - 435us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0029 - 384ms/epoch - 539us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0029 - 309ms/epoch - 433us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0030 - 303ms/epoch - 426us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0031 - 367ms/epoch - 515us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0029 - 293ms/epoch - 411us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 407us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0030 - 288ms/epoch - 404us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 286ms/epoch - 402us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0030 - 289ms/epoch - 405us/step\n",
      "178/178 - 0s - loss: 0.0028 - 98ms/epoch - 552us/step\n",
      "712/712 - 0s - loss: 0.0028 - 191ms/epoch - 268us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  29.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0635 - val_loss: 0.0023\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0015 - val_loss: 9.9710e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 8.8471e-04 - val_loss: 6.2602e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 6.7155e-04 - val_loss: 4.8345e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 5.3223e-04 - val_loss: 3.7894e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 4.7853e-04 - val_loss: 3.0975e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 4.3801e-04 - val_loss: 2.7641e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 4.1851e-04 - val_loss: 3.3645e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 3.2140e-04 - val_loss: 2.8607e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 4.1444e-04 - val_loss: 2.2163e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 3.4879e-04 - val_loss: 2.0446e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 3.5056e-04 - val_loss: 2.0562e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 3.2330e-04 - val_loss: 1.8803e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 3.4069e-04 - val_loss: 2.0894e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 526us/step - loss: 3.4158e-04 - val_loss: 1.8026e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 2.3990e-04 - val_loss: 1.7315e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 2.9932e-04 - val_loss: 1.6471e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 2.8010e-04 - val_loss: 2.0883e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 3.2401e-04 - val_loss: 3.0947e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 2.8358e-04 - val_loss: 1.9942e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 7.3190e-04 - 310ms/epoch - 436us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 4.6710e-04 - 306ms/epoch - 429us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 6.8766e-04 - 311ms/epoch - 437us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 2.8799e-04 - 287ms/epoch - 403us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 3.4799e-04 - 315ms/epoch - 443us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 3.8437e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 5.2544e-04 - 364ms/epoch - 511us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 1.9738e-04 - 336ms/epoch - 472us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 3.6380e-04 - 311ms/epoch - 437us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 8.0577e-04 - 335ms/epoch - 470us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 2.2026e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 3.1208e-04 - 350ms/epoch - 491us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 2.6126e-04 - 304ms/epoch - 427us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 6.0984e-04 - 339ms/epoch - 476us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 2.0672e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 3.4767e-04 - 301ms/epoch - 423us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 3.6408e-04 - 294ms/epoch - 412us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 2.1989e-04 - 297ms/epoch - 418us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 5.4372e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 3.6143e-04 - 349ms/epoch - 490us/step\n",
      "178/178 - 0s - loss: 2.0673e-04 - 110ms/epoch - 620us/step\n",
      "712/712 - 0s - loss: 2.0755e-04 - 216ms/epoch - 303us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  30.1s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0590 - val_loss: 0.0023\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 0.0015 - val_loss: 9.2614e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 518us/step - loss: 8.5254e-04 - val_loss: 5.9684e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 6.3707e-04 - val_loss: 9.7767e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 5.1165e-04 - val_loss: 4.5212e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 4.8219e-04 - val_loss: 3.1162e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 4.3738e-04 - val_loss: 2.8554e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 6.8020e-04 - val_loss: 2.7594e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 3.7879e-04 - val_loss: 2.4805e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 627us/step - loss: 3.4863e-04 - val_loss: 2.6135e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 3.8723e-04 - val_loss: 0.0131\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 514us/step - loss: 5.3301e-04 - val_loss: 2.2023e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 2.7518e-04 - val_loss: 2.1818e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 3.6225e-04 - val_loss: 1.9772e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 2.8339e-04 - val_loss: 6.0861e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 3.6697e-04 - val_loss: 1.8612e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 2.1495e-04 - val_loss: 1.6482e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 3.2455e-04 - val_loss: 1.6830e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 2.8390e-04 - val_loss: 1.9821e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 2.5910e-04 - val_loss: 1.5881e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 8.2717e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 1.7680e-04 - 282ms/epoch - 397us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 5.6338e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 2.2230e-04 - 295ms/epoch - 415us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 4.8100e-04 - 282ms/epoch - 397us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 1.8284e-04 - 290ms/epoch - 407us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 4.7554e-04 - 311ms/epoch - 436us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 3.6429e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 5.5401e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 4.2139e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 1.8775e-04 - 317ms/epoch - 445us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 3.2982e-04 - 318ms/epoch - 446us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 2.5101e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 3.6718e-04 - 285ms/epoch - 400us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 6.1465e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 2.7808e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 5.8696e-04 - 316ms/epoch - 444us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 1.8378e-04 - 286ms/epoch - 401us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 2.8730e-04 - 283ms/epoch - 397us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 7.2139e-04 - 280ms/epoch - 393us/step\n",
      "178/178 - 0s - loss: 1.8080e-04 - 95ms/epoch - 531us/step\n",
      "712/712 - 0s - loss: 1.8028e-04 - 188ms/epoch - 264us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  29.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0706 - val_loss: 0.0027\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 9.4495e-04 - val_loss: 6.5930e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 6.7335e-04 - val_loss: 7.5913e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 525us/step - loss: 6.2611e-04 - val_loss: 4.1450e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 520us/step - loss: 5.0243e-04 - val_loss: 4.3316e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 4.4557e-04 - val_loss: 4.2530e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 4.0781e-04 - val_loss: 2.6219e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 3.5883e-04 - val_loss: 2.3518e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 522us/step - loss: 4.2707e-04 - val_loss: 3.2680e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 519us/step - loss: 2.4570e-04 - val_loss: 3.5712e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 3.1709e-04 - val_loss: 3.0610e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 522us/step - loss: 3.3305e-04 - val_loss: 1.9147e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 3.1136e-04 - val_loss: 3.0777e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 2.9362e-04 - val_loss: 1.7172e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 520us/step - loss: 2.6760e-04 - val_loss: 2.2408e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 2.6068e-04 - val_loss: 1.7545e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 2.6300e-04 - val_loss: 1.8720e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 3.9587e-04 - val_loss: 1.6867e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 2.0672e-04 - val_loss: 1.6554e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 6.5819e-04 - 294ms/epoch - 412us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 3.4444e-04 - 290ms/epoch - 407us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 3.6064e-04 - 285ms/epoch - 400us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 3.6529e-04 - 285ms/epoch - 401us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 2.9284e-04 - 286ms/epoch - 401us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 3.9554e-04 - 308ms/epoch - 432us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 6.2729e-04 - 329ms/epoch - 462us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 1.8025e-04 - 315ms/epoch - 442us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 2.2062e-04 - 286ms/epoch - 402us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 4.3638e-04 - 288ms/epoch - 404us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 2.9314e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 4.2294e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 3.3557e-04 - 288ms/epoch - 405us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 2.1056e-04 - 288ms/epoch - 405us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 2.7976e-04 - 292ms/epoch - 410us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 9.3347e-04 - 287ms/epoch - 402us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 2.0192e-04 - 291ms/epoch - 408us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 1.9357e-04 - 297ms/epoch - 418us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 2.7621e-04 - 314ms/epoch - 441us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 2.7081e-04 - 305ms/epoch - 428us/step\n",
      "178/178 - 0s - loss: 0.0012 - 100ms/epoch - 561us/step\n",
      "712/712 - 0s - loss: 0.0011 - 210ms/epoch - 295us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  28.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0581 - val_loss: 0.0026\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0016 - val_loss: 9.7937e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 9.5362e-04 - val_loss: 6.5791e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 6.5523e-04 - val_loss: 6.8319e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 5.9265e-04 - val_loss: 4.4199e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 6.8885e-04 - val_loss: 3.7758e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 4.3177e-04 - val_loss: 3.1596e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 6.9498e-04 - val_loss: 3.7719e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 4.7381e-04 - val_loss: 3.5421e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 5.3667e-04 - val_loss: 0.0018\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 3.5391e-04 - val_loss: 0.0010\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 4.6642e-04 - val_loss: 2.8395e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 3.6474e-04 - val_loss: 2.4805e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 4.0575e-04 - val_loss: 2.1450e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 3.5202e-04 - val_loss: 2.0237e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 5.0414e-04 - val_loss: 2.6931e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 2.9822e-04 - val_loss: 0.0015\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 3.7718e-04 - val_loss: 2.1998e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 3.1639e-04 - val_loss: 1.9238e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 4.5717e-04 - val_loss: 1.9682e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 6.4195e-04 - 310ms/epoch - 435us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 4.8144e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 5.0602e-04 - 319ms/epoch - 448us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 5.4384e-04 - 326ms/epoch - 458us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 8.2059e-04 - 286ms/epoch - 402us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 2.3999e-04 - 286ms/epoch - 402us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 3.5050e-04 - 291ms/epoch - 409us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 5.8879e-04 - 287ms/epoch - 403us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 3.6408e-04 - 295ms/epoch - 415us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 7.5512e-04 - 299ms/epoch - 420us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 2.6233e-04 - 295ms/epoch - 415us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 3.4655e-04 - 305ms/epoch - 429us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 4.2696e-04 - 307ms/epoch - 431us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 4.4573e-04 - 297ms/epoch - 417us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 3.9361e-04 - 294ms/epoch - 413us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 5.2029e-04 - 312ms/epoch - 438us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 3.7009e-04 - 297ms/epoch - 418us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 3.2891e-04 - 284ms/epoch - 399us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 4.4368e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 3.9567e-04 - 320ms/epoch - 450us/step\n",
      "178/178 - 0s - loss: 4.9541e-04 - 98ms/epoch - 550us/step\n",
      "712/712 - 0s - loss: 5.0457e-04 - 199ms/epoch - 279us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  30.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0673 - val_loss: 0.0027\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 8.7562e-04 - val_loss: 7.0433e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 8.1820e-04 - val_loss: 4.9956e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 5.4431e-04 - val_loss: 4.0213e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 5.3957e-04 - val_loss: 3.5315e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 3.9873e-04 - val_loss: 2.8655e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 3.9886e-04 - val_loss: 3.9135e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 4.0027e-04 - val_loss: 2.7350e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 642us/step - loss: 3.4521e-04 - val_loss: 2.1812e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 644us/step - loss: 4.2866e-04 - val_loss: 2.2321e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 8.7457e-04 - val_loss: 2.7490e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 2.8182e-04 - val_loss: 2.1896e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 623us/step - loss: 2.9153e-04 - val_loss: 2.3536e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 2.9859e-04 - val_loss: 1.9990e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 3.6462e-04 - val_loss: 6.4342e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 2.9530e-04 - val_loss: 1.7956e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 2.6828e-04 - val_loss: 3.1756e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 4.5817e-04 - val_loss: 1.7168e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 3.6588e-04 - val_loss: 1.7197e-04\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 4.3494e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 5.7133e-04 - 342ms/epoch - 480us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 5.3472e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 2.3165e-04 - 301ms/epoch - 423us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 4.0002e-04 - 311ms/epoch - 436us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 3.8710e-04 - 357ms/epoch - 501us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 5.1700e-04 - 310ms/epoch - 435us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 2.5371e-04 - 283ms/epoch - 398us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 7.1625e-04 - 285ms/epoch - 400us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 2.2031e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 2.6844e-04 - 324ms/epoch - 455us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 4.8718e-04 - 299ms/epoch - 419us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 5.1118e-04 - 303ms/epoch - 426us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 2.4593e-04 - 299ms/epoch - 419us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 4.3338e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 3.2745e-04 - 285ms/epoch - 400us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 3.4585e-04 - 285ms/epoch - 401us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 2.9476e-04 - 294ms/epoch - 413us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 5.7308e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 2.2239e-04 - 286ms/epoch - 402us/step\n",
      "178/178 - 0s - loss: 1.8123e-04 - 100ms/epoch - 562us/step\n",
      "712/712 - 0s - loss: 1.8167e-04 - 207ms/epoch - 291us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  30.2s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2876 - val_loss: 0.0201\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0139 - val_loss: 0.0102\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0085 - val_loss: 0.0073\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 605us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0032 - 305ms/epoch - 428us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 296ms/epoch - 416us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0031 - 334ms/epoch - 470us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 299ms/epoch - 420us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0031 - 295ms/epoch - 415us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 284ms/epoch - 398us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0031 - 309ms/epoch - 434us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0030 - 300ms/epoch - 422us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0032 - 297ms/epoch - 417us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0030 - 299ms/epoch - 421us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 310ms/epoch - 435us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0029 - 305ms/epoch - 429us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0032 - 285ms/epoch - 400us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0029 - 286ms/epoch - 402us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0034 - 288ms/epoch - 405us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0029 - 297ms/epoch - 418us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 285ms/epoch - 400us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0029 - 284ms/epoch - 398us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0030 - 290ms/epoch - 408us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0029 - 301ms/epoch - 422us/step\n",
      "178/178 - 0s - loss: 0.0029 - 98ms/epoch - 550us/step\n",
      "712/712 - 0s - loss: 0.0029 - 186ms/epoch - 261us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  30.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2865 - val_loss: 0.0183\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 520us/step - loss: 0.0124 - val_loss: 0.0090\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 0.0076 - val_loss: 0.0066\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0060 - val_loss: 0.0055\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0046 - val_loss: 0.0043\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 517us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0031 - val_loss: 0.0033\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0036 - 289ms/epoch - 406us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 283ms/epoch - 398us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 296ms/epoch - 415us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0032 - 297ms/epoch - 417us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0031 - 301ms/epoch - 423us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 297ms/epoch - 417us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0032 - 298ms/epoch - 419us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0030 - 312ms/epoch - 439us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 330ms/epoch - 463us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0030 - 326ms/epoch - 458us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0032 - 300ms/epoch - 421us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0030 - 314ms/epoch - 442us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0030 - 360ms/epoch - 506us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0030 - 380ms/epoch - 533us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0030 - 333ms/epoch - 468us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0031 - 343ms/epoch - 481us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 304ms/epoch - 427us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0031 - 317ms/epoch - 445us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 303ms/epoch - 426us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0032 - 296ms/epoch - 415us/step\n",
      "178/178 - 0s - loss: 0.0029 - 103ms/epoch - 577us/step\n",
      "712/712 - 0s - loss: 0.0029 - 196ms/epoch - 275us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  29.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2776 - val_loss: 0.0184\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0130 - val_loss: 0.0099\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0083 - val_loss: 0.0071\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 754us/step - loss: 0.0047 - val_loss: 0.0044\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 529us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0032 - 301ms/epoch - 423us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 314ms/epoch - 441us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 314ms/epoch - 442us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 311ms/epoch - 437us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0031 - 301ms/epoch - 423us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 297ms/epoch - 417us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 304ms/epoch - 427us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0030 - 302ms/epoch - 424us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0031 - 308ms/epoch - 433us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0031 - 303ms/epoch - 425us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 303ms/epoch - 426us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0030 - 307ms/epoch - 432us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0034 - 305ms/epoch - 428us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0029 - 300ms/epoch - 421us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 323ms/epoch - 453us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0031 - 301ms/epoch - 423us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 302ms/epoch - 424us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0030 - 295ms/epoch - 415us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 408us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0029 - 287ms/epoch - 403us/step\n",
      "178/178 - 0s - loss: 0.0029 - 97ms/epoch - 547us/step\n",
      "712/712 - 0s - loss: 0.0029 - 191ms/epoch - 269us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  29.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2681 - val_loss: 0.0175\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0124 - val_loss: 0.0093\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0079 - val_loss: 0.0067\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0062 - val_loss: 0.0056\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0031 - 308ms/epoch - 432us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0034 - 365ms/epoch - 512us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 316ms/epoch - 443us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 313ms/epoch - 440us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0031 - 301ms/epoch - 422us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0032 - 298ms/epoch - 418us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 290ms/epoch - 407us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0030 - 296ms/epoch - 416us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0031 - 300ms/epoch - 421us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0029 - 309ms/epoch - 433us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0030 - 288ms/epoch - 404us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0032 - 291ms/epoch - 409us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0029 - 295ms/epoch - 415us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0036 - 294ms/epoch - 414us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 321ms/epoch - 451us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0029 - 295ms/epoch - 414us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 297ms/epoch - 417us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0029 - 295ms/epoch - 415us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0030 - 294ms/epoch - 414us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0029 - 290ms/epoch - 407us/step\n",
      "178/178 - 0s - loss: 0.0052 - 102ms/epoch - 573us/step\n",
      "712/712 - 0s - loss: 0.0052 - 193ms/epoch - 271us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  29.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2930 - val_loss: 0.0192\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0132 - val_loss: 0.0097\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0080 - val_loss: 0.0068\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 521us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0046 - val_loss: 0.0043\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0032 - val_loss: 0.0036\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 1/20\n",
      "712/712 - 0s - loss: 0.0036 - 302ms/epoch - 424us/step\n",
      "Epoch 2/20\n",
      "712/712 - 0s - loss: 0.0030 - 297ms/epoch - 417us/step\n",
      "Epoch 3/20\n",
      "712/712 - 0s - loss: 0.0030 - 298ms/epoch - 419us/step\n",
      "Epoch 4/20\n",
      "712/712 - 0s - loss: 0.0030 - 295ms/epoch - 414us/step\n",
      "Epoch 5/20\n",
      "712/712 - 0s - loss: 0.0030 - 304ms/epoch - 427us/step\n",
      "Epoch 6/20\n",
      "712/712 - 0s - loss: 0.0030 - 310ms/epoch - 435us/step\n",
      "Epoch 7/20\n",
      "712/712 - 0s - loss: 0.0030 - 316ms/epoch - 444us/step\n",
      "Epoch 8/20\n",
      "712/712 - 0s - loss: 0.0030 - 284ms/epoch - 399us/step\n",
      "Epoch 9/20\n",
      "712/712 - 0s - loss: 0.0030 - 288ms/epoch - 404us/step\n",
      "Epoch 10/20\n",
      "712/712 - 0s - loss: 0.0032 - 287ms/epoch - 403us/step\n",
      "Epoch 11/20\n",
      "712/712 - 0s - loss: 0.0029 - 302ms/epoch - 424us/step\n",
      "Epoch 12/20\n",
      "712/712 - 0s - loss: 0.0030 - 304ms/epoch - 428us/step\n",
      "Epoch 13/20\n",
      "712/712 - 0s - loss: 0.0030 - 299ms/epoch - 420us/step\n",
      "Epoch 14/20\n",
      "712/712 - 0s - loss: 0.0032 - 311ms/epoch - 437us/step\n",
      "Epoch 15/20\n",
      "712/712 - 0s - loss: 0.0029 - 350ms/epoch - 492us/step\n",
      "Epoch 16/20\n",
      "712/712 - 0s - loss: 0.0032 - 302ms/epoch - 425us/step\n",
      "Epoch 17/20\n",
      "712/712 - 0s - loss: 0.0029 - 307ms/epoch - 432us/step\n",
      "Epoch 18/20\n",
      "712/712 - 0s - loss: 0.0030 - 306ms/epoch - 429us/step\n",
      "Epoch 19/20\n",
      "712/712 - 0s - loss: 0.0029 - 312ms/epoch - 438us/step\n",
      "Epoch 20/20\n",
      "712/712 - 0s - loss: 0.0034 - 293ms/epoch - 412us/step\n",
      "178/178 - 0s - loss: 0.0029 - 115ms/epoch - 648us/step\n",
      "712/712 - 0s - loss: 0.0029 - 227ms/epoch - 318us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  29.5s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 2\n",
      "n_resources: 85356\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0726 - val_loss: 0.0045\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 0.0028 - val_loss: 0.0020\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0010 - val_loss: 9.9659e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 9.9130e-04 - val_loss: 7.7674e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 9.0221e-04 - val_loss: 6.9492e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 8.3015e-04 - val_loss: 6.7069e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 8.2299e-04 - val_loss: 8.1617e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 7.6831e-04 - val_loss: 0.0013\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 7.7200e-04 - val_loss: 7.9166e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 7.1736e-04 - val_loss: 6.2282e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 6.9906e-04 - val_loss: 0.0016\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 6.9297e-04 - val_loss: 8.1574e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 6.7799e-04 - val_loss: 4.9092e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 634us/step - loss: 7.4912e-04 - val_loss: 5.0908e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 7.1508e-04 - val_loss: 4.5536e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 7.3288e-04 - val_loss: 5.5948e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 6.3516e-04 - val_loss: 4.7729e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 8.2130e-04 - 852ms/epoch - 399us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 7.4987e-04 - 999ms/epoch - 468us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 8.5827e-04 - 927ms/epoch - 435us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 7.8940e-04 - 897ms/epoch - 420us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 9.3069e-04 - 858ms/epoch - 402us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 9.1992e-04 - 875ms/epoch - 410us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 7.9214e-04 - 892ms/epoch - 418us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 9.6470e-04 - 948ms/epoch - 444us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 9.1302e-04 - 930ms/epoch - 436us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 8.3057e-04 - 899ms/epoch - 421us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 8.0294e-04 - 843ms/epoch - 395us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 9.3294e-04 - 877ms/epoch - 411us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 9.5415e-04 - 914ms/epoch - 428us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 7.4319e-04 - 862ms/epoch - 404us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 7.6113e-04 - 886ms/epoch - 415us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 8.6584e-04 - 871ms/epoch - 408us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 8.7599e-04 - 874ms/epoch - 409us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 8.1325e-04 - 886ms/epoch - 415us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 7.7191e-04 - 887ms/epoch - 415us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 7.6710e-04 - 855ms/epoch - 401us/step\n",
      "534/534 - 0s - loss: 4.2933e-04 - 181ms/epoch - 338us/step\n",
      "2134/2134 - 1s - loss: 4.3201e-04 - 551ms/epoch - 258us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  44.5s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0805 - val_loss: 0.0042\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0031 - val_loss: 0.0021\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 527us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0011 - val_loss: 9.5888e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 649us/step - loss: 9.6502e-04 - val_loss: 0.0012\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 9.7051e-04 - val_loss: 9.8390e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 8.2513e-04 - val_loss: 7.4489e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 8.4500e-04 - val_loss: 7.0151e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 8.1574e-04 - val_loss: 0.0038\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 7.7701e-04 - val_loss: 0.0037\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 7.8479e-04 - val_loss: 7.0383e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 821us/step - loss: 7.2531e-04 - val_loss: 5.7180e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 6.7415e-04 - val_loss: 6.6713e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 6.8927e-04 - val_loss: 4.7412e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 6.1046e-04 - val_loss: 4.4133e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 6.7307e-04 - val_loss: 4.8580e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 7.5012e-04 - val_loss: 5.5223e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 5.8843e-04 - val_loss: 4.7156e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 7.7921e-04 - 900ms/epoch - 422us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 8.4564e-04 - 907ms/epoch - 425us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 9.4033e-04 - 892ms/epoch - 418us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 8.0078e-04 - 931ms/epoch - 436us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 8.2422e-04 - 949ms/epoch - 445us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 7.7536e-04 - 873ms/epoch - 409us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 8.9120e-04 - 870ms/epoch - 408us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 7.2565e-04 - 880ms/epoch - 413us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 7.7819e-04 - 890ms/epoch - 417us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 8.1418e-04 - 837ms/epoch - 392us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 8.6149e-04 - 844ms/epoch - 396us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 8.2850e-04 - 874ms/epoch - 410us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 8.2078e-04 - 843ms/epoch - 395us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 7.4607e-04 - 917ms/epoch - 430us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 8.3624e-04 - 895ms/epoch - 420us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 7.5024e-04 - 915ms/epoch - 429us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 7.3144e-04 - 899ms/epoch - 421us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 8.6473e-04 - 899ms/epoch - 421us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 8.3448e-04 - 953ms/epoch - 447us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 8.0525e-04 - 850ms/epoch - 398us/step\n",
      "534/534 - 0s - loss: 4.6565e-04 - 187ms/epoch - 351us/step\n",
      "2134/2134 - 1s - loss: 4.6570e-04 - 574ms/epoch - 269us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  44.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0682 - val_loss: 0.0039\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 0.0031 - val_loss: 0.0025\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 9.7256e-04 - val_loss: 7.7798e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 9.2413e-04 - val_loss: 6.9307e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 8.0228e-04 - val_loss: 6.1307e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 7.9912e-04 - val_loss: 6.7406e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 7.9032e-04 - val_loss: 6.2856e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 7.4254e-04 - val_loss: 5.7353e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 7.2606e-04 - val_loss: 5.9115e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 6.5763e-04 - val_loss: 4.8385e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 6.8254e-04 - val_loss: 5.4037e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 6.9964e-04 - val_loss: 5.0401e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 6.3048e-04 - val_loss: 6.0125e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 6.3742e-04 - val_loss: 6.2055e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 6.9332e-04 - val_loss: 6.5845e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 5.8723e-04 - val_loss: 0.0018\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 8.6347e-04 - 956ms/epoch - 448us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 8.3795e-04 - 932ms/epoch - 437us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 8.9964e-04 - 929ms/epoch - 436us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 7.7017e-04 - 892ms/epoch - 418us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 0.0010 - 913ms/epoch - 428us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 8.4334e-04 - 867ms/epoch - 406us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 7.4514e-04 - 897ms/epoch - 420us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 7.9949e-04 - 871ms/epoch - 408us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 8.0692e-04 - 850ms/epoch - 398us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 7.9209e-04 - 850ms/epoch - 398us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 8.1380e-04 - 859ms/epoch - 402us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 8.8211e-04 - 870ms/epoch - 408us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 8.7669e-04 - 855ms/epoch - 401us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 8.0693e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 7.7326e-04 - 868ms/epoch - 407us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 7.4097e-04 - 881ms/epoch - 413us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 8.1149e-04 - 911ms/epoch - 427us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 9.0293e-04 - 882ms/epoch - 413us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 6.8666e-04 - 895ms/epoch - 419us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 8.1083e-04 - 869ms/epoch - 407us/step\n",
      "534/534 - 0s - loss: 6.8295e-04 - 180ms/epoch - 338us/step\n",
      "2134/2134 - 1s - loss: 6.8793e-04 - 557ms/epoch - 261us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  44.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0647 - val_loss: 0.0039\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0012 - val_loss: 9.4086e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0010 - val_loss: 9.2040e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0010 - val_loss: 9.5112e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 8.8323e-04 - val_loss: 8.3514e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 8.9851e-04 - val_loss: 6.4008e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 7.7979e-04 - val_loss: 5.7666e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 7.6414e-04 - val_loss: 6.0143e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 7.7412e-04 - val_loss: 8.2900e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 6.9606e-04 - val_loss: 6.2399e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 6.8390e-04 - val_loss: 4.8479e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 888us/step - loss: 6.7648e-04 - val_loss: 5.8168e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 6.8771e-04 - val_loss: 4.6704e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 710us/step - loss: 6.9455e-04 - val_loss: 5.1577e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 6.3855e-04 - val_loss: 5.0395e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 6.0650e-04 - val_loss: 5.8178e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 6.1360e-04 - val_loss: 4.5393e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 8.7805e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 7.9218e-04 - 908ms/epoch - 426us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 8.5060e-04 - 903ms/epoch - 423us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 7.4913e-04 - 882ms/epoch - 413us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 8.1204e-04 - 923ms/epoch - 432us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 7.9553e-04 - 941ms/epoch - 441us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 7.5461e-04 - 902ms/epoch - 423us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 8.6089e-04 - 946ms/epoch - 443us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 7.5737e-04 - 912ms/epoch - 427us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 8.6013e-04 - 915ms/epoch - 429us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 7.4190e-04 - 908ms/epoch - 426us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 8.3689e-04 - 938ms/epoch - 439us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 8.8207e-04 - 975ms/epoch - 457us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 7.4972e-04 - 898ms/epoch - 421us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 7.5690e-04 - 864ms/epoch - 405us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 7.4434e-04 - 921ms/epoch - 432us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 8.5740e-04 - 864ms/epoch - 405us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 6.7297e-04 - 940ms/epoch - 441us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 6.8552e-04 - 902ms/epoch - 422us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 7.3472e-04 - 907ms/epoch - 425us/step\n",
      "534/534 - 0s - loss: 5.5818e-04 - 183ms/epoch - 343us/step\n",
      "2134/2134 - 1s - loss: 5.6126e-04 - 567ms/epoch - 266us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  45.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0911 - val_loss: 0.0043\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0032 - val_loss: 0.0025\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0010 - val_loss: 8.7964e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 9.9939e-04 - val_loss: 8.9065e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 9.5875e-04 - val_loss: 7.5794e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 9.4149e-04 - val_loss: 7.5286e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 8.5425e-04 - val_loss: 6.8206e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 9.0083e-04 - val_loss: 6.3825e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 8.4490e-04 - val_loss: 6.0751e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 7.9612e-04 - val_loss: 0.0019\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 8.5403e-04 - val_loss: 6.9381e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 7.9171e-04 - val_loss: 5.5681e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 8.2550e-04 - val_loss: 7.2325e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 9.5743e-04 - 953ms/epoch - 446us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 9.1389e-04 - 954ms/epoch - 447us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 9.7658e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 9.5798e-04 - 878ms/epoch - 411us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 9.4893e-04 - 928ms/epoch - 435us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 9.1920e-04 - 914ms/epoch - 428us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 9.5255e-04 - 919ms/epoch - 431us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 9.1757e-04 - 1s/epoch - 534us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 9.2613e-04 - 1s/epoch - 545us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 0.0010 - 1s/epoch - 484us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 9.5337e-04 - 1s/epoch - 480us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 8.8785e-04 - 947ms/epoch - 444us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 9.7836e-04 - 889ms/epoch - 417us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 9.0225e-04 - 865ms/epoch - 406us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 8.9508e-04 - 960ms/epoch - 450us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 8.1324e-04 - 894ms/epoch - 419us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 8.3027e-04 - 889ms/epoch - 416us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 8.3851e-04 - 884ms/epoch - 414us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 0.0011 - 954ms/epoch - 447us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 9.5435e-04 - 864ms/epoch - 405us/step\n",
      "534/534 - 0s - loss: 0.0012 - 180ms/epoch - 337us/step\n",
      "2134/2134 - 1s - loss: 0.0012 - 534ms/epoch - 250us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  45.7s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0657 - val_loss: 0.0025\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0010 - val_loss: 6.6875e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 6.6056e-04 - val_loss: 9.0635e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 530us/step - loss: 5.4027e-04 - val_loss: 4.8309e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 6.4373e-04 - val_loss: 3.5592e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 4.2147e-04 - val_loss: 3.2303e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 4.0736e-04 - val_loss: 2.9534e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 5.4373e-04 - val_loss: 5.7285e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 3.1473e-04 - val_loss: 2.4461e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 3.6637e-04 - val_loss: 6.3995e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 3.3695e-04 - val_loss: 2.3380e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 3.9622e-04 - val_loss: 2.0874e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 4.2813e-04 - val_loss: 1.9620e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 3.0878e-04 - val_loss: 1.8422e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 4.3434e-04 - val_loss: 0.0010\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 3.8225e-04 - val_loss: 1.9662e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 2.4957e-04 - val_loss: 1.7538e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 3.2421e-04 - val_loss: 7.7619e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 2.6105e-04 - val_loss: 2.4642e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 4.4148e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 3.9824e-04 - 884ms/epoch - 414us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 5.1510e-04 - 937ms/epoch - 439us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 2.9700e-04 - 925ms/epoch - 433us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 5.5779e-04 - 997ms/epoch - 467us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 2.6719e-04 - 963ms/epoch - 451us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 3.8647e-04 - 946ms/epoch - 443us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 3.3207e-04 - 914ms/epoch - 428us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 3.5364e-04 - 906ms/epoch - 424us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 3.6014e-04 - 907ms/epoch - 425us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 3.3666e-04 - 918ms/epoch - 430us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 4.0838e-04 - 924ms/epoch - 433us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 3.9715e-04 - 981ms/epoch - 460us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 3.2157e-04 - 933ms/epoch - 437us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 2.9408e-04 - 927ms/epoch - 434us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 4.3336e-04 - 980ms/epoch - 459us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 2.5929e-04 - 893ms/epoch - 419us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 2.9271e-04 - 903ms/epoch - 423us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 2.4967e-04 - 915ms/epoch - 429us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 3.0396e-04 - 882ms/epoch - 413us/step\n",
      "534/534 - 0s - loss: 1.5108e-04 - 200ms/epoch - 374us/step\n",
      "2134/2134 - 1s - loss: 1.5100e-04 - 614ms/epoch - 288us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  44.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0565 - val_loss: 0.0023\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0015 - val_loss: 9.2305e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 8.5337e-04 - val_loss: 6.1522e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 6.6450e-04 - val_loss: 5.2322e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 5.7473e-04 - val_loss: 8.7556e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 4.5861e-04 - val_loss: 3.4993e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 4.5401e-04 - val_loss: 2.8823e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 4.0671e-04 - val_loss: 2.5680e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 3.7431e-04 - val_loss: 2.5606e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 3.8734e-04 - val_loss: 2.1793e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 3.1395e-04 - val_loss: 2.4284e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 4.5022e-04 - val_loss: 2.0708e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 3.4519e-04 - val_loss: 1.9626e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 2.9763e-04 - val_loss: 1.8630e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 3.0716e-04 - val_loss: 2.1653e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 2.7893e-04 - val_loss: 2.8508e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 3.6557e-04 - val_loss: 1.6785e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 534us/step - loss: 3.6336e-04 - val_loss: 1.8453e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 2.8016e-04 - val_loss: 1.7408e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 2.6071e-04 - val_loss: 1.6643e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 4.6098e-04 - 892ms/epoch - 418us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 4.2152e-04 - 902ms/epoch - 423us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 5.1158e-04 - 876ms/epoch - 410us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 3.5872e-04 - 1s/epoch - 469us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 3.5640e-04 - 1s/epoch - 471us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 4.2389e-04 - 881ms/epoch - 413us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 2.9846e-04 - 912ms/epoch - 427us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 5.9419e-04 - 922ms/epoch - 432us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 2.3042e-04 - 872ms/epoch - 409us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 4.2004e-04 - 880ms/epoch - 412us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 4.0161e-04 - 891ms/epoch - 418us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 2.7257e-04 - 933ms/epoch - 437us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 3.0420e-04 - 874ms/epoch - 409us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 2.7568e-04 - 857ms/epoch - 402us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 3.9932e-04 - 861ms/epoch - 404us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 3.0599e-04 - 848ms/epoch - 397us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 2.1519e-04 - 865ms/epoch - 405us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 3.1010e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 2.1139e-04 - 999ms/epoch - 468us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 2.6313e-04 - 933ms/epoch - 437us/step\n",
      "534/534 - 1s - loss: 1.7689e-04 - 551ms/epoch - 1ms/step\n",
      "2134/2134 - 1s - loss: 1.7611e-04 - 615ms/epoch - 288us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  44.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0606 - val_loss: 0.0021\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0014 - val_loss: 9.0106e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 8.7634e-04 - val_loss: 6.2237e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 6.6373e-04 - val_loss: 4.4551e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 5.6227e-04 - val_loss: 0.0026\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 6.4631e-04 - val_loss: 0.0014\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 3.5705e-04 - val_loss: 3.0402e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 3.8205e-04 - val_loss: 3.2737e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 4.2719e-04 - val_loss: 2.7583e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 3.9103e-04 - val_loss: 2.2817e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 4.2862e-04 - val_loss: 2.2057e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 635us/step - loss: 3.2203e-04 - val_loss: 2.0165e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 3.8837e-04 - val_loss: 2.0201e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 3.4428e-04 - val_loss: 1.9705e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 4.4056e-04 - val_loss: 2.0266e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 523us/step - loss: 3.3849e-04 - val_loss: 0.0022\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 2.6626e-04 - val_loss: 6.7088e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 3.2450e-04 - val_loss: 1.8845e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 3.1605e-04 - val_loss: 1.8030e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 3.1496e-04 - val_loss: 2.2016e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 5.5278e-04 - 946ms/epoch - 444us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 5.5057e-04 - 940ms/epoch - 440us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 3.5301e-04 - 975ms/epoch - 457us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 3.5305e-04 - 868ms/epoch - 407us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 3.1857e-04 - 953ms/epoch - 447us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 3.3645e-04 - 883ms/epoch - 414us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 3.5475e-04 - 861ms/epoch - 403us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 2.8204e-04 - 931ms/epoch - 436us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 4.0868e-04 - 906ms/epoch - 425us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 3.9037e-04 - 861ms/epoch - 403us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 2.6342e-04 - 858ms/epoch - 402us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 2.9688e-04 - 880ms/epoch - 412us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 2.6822e-04 - 979ms/epoch - 459us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 3.1335e-04 - 994ms/epoch - 466us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 3.0586e-04 - 994ms/epoch - 466us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 3.2647e-04 - 904ms/epoch - 424us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 2.5333e-04 - 957ms/epoch - 448us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 2.9268e-04 - 918ms/epoch - 430us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 3.1050e-04 - 917ms/epoch - 430us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 2.9782e-04 - 927ms/epoch - 434us/step\n",
      "534/534 - 0s - loss: 1.4696e-04 - 202ms/epoch - 379us/step\n",
      "2134/2134 - 1s - loss: 1.4712e-04 - 562ms/epoch - 263us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  45.4s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0537 - val_loss: 0.0022\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.0014 - val_loss: 9.1673e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 8.5795e-04 - val_loss: 5.9369e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 5.9864e-04 - val_loss: 4.9743e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 5.6942e-04 - val_loss: 4.1564e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 4.3646e-04 - val_loss: 5.0207e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 4.7344e-04 - val_loss: 2.9106e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 3.5044e-04 - val_loss: 2.3986e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 3.6499e-04 - val_loss: 2.4320e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 3.3331e-04 - val_loss: 0.0071\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 3.0877e-04 - val_loss: 1.9523e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 3.4918e-04 - val_loss: 3.5109e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 643us/step - loss: 3.3684e-04 - val_loss: 1.7999e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 644us/step - loss: 3.0581e-04 - val_loss: 1.8314e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 607us/step - loss: 2.9481e-04 - val_loss: 1.8751e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 2.6883e-04 - val_loss: 1.6025e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 3.6881e-04 - val_loss: 1.7075e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 2.2197e-04 - val_loss: 1.5977e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 4.7297e-04 - val_loss: 3.6249e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 2.3738e-04 - val_loss: 1.5934e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 7.4324e-04 - 880ms/epoch - 412us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 2.5256e-04 - 887ms/epoch - 416us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 3.4687e-04 - 903ms/epoch - 423us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 4.7802e-04 - 882ms/epoch - 413us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 3.3626e-04 - 981ms/epoch - 460us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 2.9293e-04 - 983ms/epoch - 461us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 2.6853e-04 - 913ms/epoch - 428us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 3.1688e-04 - 951ms/epoch - 446us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 2.6996e-04 - 965ms/epoch - 452us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 3.8983e-04 - 932ms/epoch - 437us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 2.3255e-04 - 931ms/epoch - 436us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 3.1397e-04 - 868ms/epoch - 407us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 3.4293e-04 - 978ms/epoch - 459us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 2.6541e-04 - 890ms/epoch - 417us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 2.5122e-04 - 863ms/epoch - 404us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 2.5879e-04 - 865ms/epoch - 405us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 3.1325e-04 - 881ms/epoch - 413us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 2.2145e-04 - 915ms/epoch - 429us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 2.4795e-04 - 906ms/epoch - 425us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 2.8972e-04 - 901ms/epoch - 422us/step\n",
      "534/534 - 0s - loss: 1.2991e-04 - 182ms/epoch - 341us/step\n",
      "2134/2134 - 1s - loss: 1.3021e-04 - 588ms/epoch - 275us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  45.8s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0585 - val_loss: 0.0029\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 9.5884e-04 - val_loss: 6.8322e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 6.7225e-04 - val_loss: 4.6639e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 528us/step - loss: 5.5250e-04 - val_loss: 3.6630e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 5.2967e-04 - val_loss: 3.2313e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 3.9880e-04 - val_loss: 2.7478e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 4.3887e-04 - val_loss: 4.5525e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 4.1200e-04 - val_loss: 3.0510e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 4.7191e-04 - val_loss: 2.2873e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 2.6633e-04 - val_loss: 2.8261e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 3.5447e-04 - val_loss: 1.9909e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 4.7807e-04 - val_loss: 2.1517e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 2.8834e-04 - val_loss: 1.8699e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 3.9360e-04 - val_loss: 1.8869e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 3.3915e-04 - val_loss: 1.8013e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 3.3765e-04 - val_loss: 4.9443e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 3.1487e-04 - val_loss: 1.7688e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 4.6200e-04 - val_loss: 1.9159e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 3.8841e-04 - val_loss: 1.7862e-04\n",
      "Epoch 1/20\n",
      "2134/2134 - 1s - loss: 5.3457e-04 - 930ms/epoch - 436us/step\n",
      "Epoch 2/20\n",
      "2134/2134 - 1s - loss: 6.7494e-04 - 866ms/epoch - 406us/step\n",
      "Epoch 3/20\n",
      "2134/2134 - 1s - loss: 2.4911e-04 - 872ms/epoch - 409us/step\n",
      "Epoch 4/20\n",
      "2134/2134 - 1s - loss: 4.6864e-04 - 917ms/epoch - 430us/step\n",
      "Epoch 5/20\n",
      "2134/2134 - 1s - loss: 5.1216e-04 - 945ms/epoch - 443us/step\n",
      "Epoch 6/20\n",
      "2134/2134 - 1s - loss: 4.3064e-04 - 871ms/epoch - 408us/step\n",
      "Epoch 7/20\n",
      "2134/2134 - 1s - loss: 3.0724e-04 - 909ms/epoch - 426us/step\n",
      "Epoch 8/20\n",
      "2134/2134 - 1s - loss: 4.6509e-04 - 912ms/epoch - 427us/step\n",
      "Epoch 9/20\n",
      "2134/2134 - 1s - loss: 3.1482e-04 - 924ms/epoch - 433us/step\n",
      "Epoch 10/20\n",
      "2134/2134 - 1s - loss: 3.5992e-04 - 977ms/epoch - 458us/step\n",
      "Epoch 11/20\n",
      "2134/2134 - 1s - loss: 3.5467e-04 - 874ms/epoch - 410us/step\n",
      "Epoch 12/20\n",
      "2134/2134 - 1s - loss: 3.8087e-04 - 901ms/epoch - 422us/step\n",
      "Epoch 13/20\n",
      "2134/2134 - 1s - loss: 2.4953e-04 - 949ms/epoch - 445us/step\n",
      "Epoch 14/20\n",
      "2134/2134 - 1s - loss: 4.4264e-04 - 916ms/epoch - 429us/step\n",
      "Epoch 15/20\n",
      "2134/2134 - 1s - loss: 2.8235e-04 - 963ms/epoch - 451us/step\n",
      "Epoch 16/20\n",
      "2134/2134 - 1s - loss: 3.2328e-04 - 932ms/epoch - 437us/step\n",
      "Epoch 17/20\n",
      "2134/2134 - 1s - loss: 2.5865e-04 - 948ms/epoch - 444us/step\n",
      "Epoch 18/20\n",
      "2134/2134 - 1s - loss: 4.3405e-04 - 943ms/epoch - 442us/step\n",
      "Epoch 19/20\n",
      "2134/2134 - 1s - loss: 2.6000e-04 - 934ms/epoch - 438us/step\n",
      "Epoch 20/20\n",
      "2134/2134 - 1s - loss: 2.7976e-04 - 878ms/epoch - 412us/step\n",
      "534/534 - 0s - loss: 1.6443e-04 - 192ms/epoch - 360us/step\n",
      "2134/2134 - 1s - loss: 1.6489e-04 - 622ms/epoch - 292us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  44.6s\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0561 - val_loss: 0.0021\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 0.0014 - val_loss: 9.2383e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 9.7716e-04 - val_loss: 6.6481e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 675us/step - loss: 6.7259e-04 - val_loss: 5.3679e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 649us/step - loss: 5.8071e-04 - val_loss: 4.2788e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 837us/step - loss: 4.6934e-04 - val_loss: 3.3424e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 682us/step - loss: 5.4553e-04 - val_loss: 3.1576e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 4.5306e-04 - val_loss: 2.8795e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 3.6620e-04 - val_loss: 4.4593e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 4.0128e-04 - val_loss: 2.6293e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 3.1721e-04 - val_loss: 2.1372e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 4.9296e-04 - val_loss: 4.0549e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 3.1576e-04 - val_loss: 2.1868e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 2.8523e-04 - val_loss: 3.3181e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 3.4791e-04 - val_loss: 5.9747e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 648us/step - loss: 3.3262e-04 - val_loss: 1.8322e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 629us/step - loss: 3.4467e-04 - val_loss: 1.7439e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 4.3113e-04 - val_loss: 1.8619e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 625us/step - loss: 2.6136e-04 - val_loss: 1.7173e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 2.9119e-04 - val_loss: 1.6241e-04\n",
      "Epoch 1/20\n",
      "2668/2668 - 1s - loss: 5.8920e-04 - 1s/epoch - 436us/step\n",
      "Epoch 2/20\n",
      "2668/2668 - 1s - loss: 4.1278e-04 - 1s/epoch - 410us/step\n",
      "Epoch 3/20\n",
      "2668/2668 - 1s - loss: 4.7269e-04 - 1s/epoch - 411us/step\n",
      "Epoch 4/20\n",
      "2668/2668 - 1s - loss: 4.9087e-04 - 1s/epoch - 429us/step\n",
      "Epoch 5/20\n",
      "2668/2668 - 1s - loss: 3.7035e-04 - 1s/epoch - 421us/step\n",
      "Epoch 6/20\n",
      "2668/2668 - 1s - loss: 4.4278e-04 - 1s/epoch - 420us/step\n",
      "Epoch 7/20\n",
      "2668/2668 - 1s - loss: 3.0055e-04 - 1s/epoch - 407us/step\n",
      "Epoch 8/20\n",
      "2668/2668 - 1s - loss: 3.9533e-04 - 1s/epoch - 420us/step\n",
      "Epoch 9/20\n",
      "2668/2668 - 1s - loss: 3.1738e-04 - 1s/epoch - 419us/step\n",
      "Epoch 10/20\n",
      "2668/2668 - 1s - loss: 3.2711e-04 - 1s/epoch - 432us/step\n",
      "Epoch 11/20\n",
      "2668/2668 - 1s - loss: 3.2944e-04 - 1s/epoch - 471us/step\n",
      "Epoch 12/20\n",
      "2668/2668 - 1s - loss: 2.7291e-04 - 1s/epoch - 450us/step\n",
      "Epoch 13/20\n",
      "2668/2668 - 1s - loss: 3.1980e-04 - 1s/epoch - 446us/step\n",
      "Epoch 14/20\n",
      "2668/2668 - 1s - loss: 2.5195e-04 - 1s/epoch - 445us/step\n",
      "Epoch 15/20\n",
      "2668/2668 - 1s - loss: 3.2511e-04 - 1s/epoch - 431us/step\n",
      "Epoch 16/20\n",
      "2668/2668 - 1s - loss: 2.2790e-04 - 1s/epoch - 460us/step\n",
      "Epoch 17/20\n",
      "2668/2668 - 1s - loss: 2.8532e-04 - 1s/epoch - 425us/step\n",
      "Epoch 18/20\n",
      "2668/2668 - 1s - loss: 4.2994e-04 - 1s/epoch - 410us/step\n",
      "Epoch 19/20\n",
      "2668/2668 - 1s - loss: 2.8164e-04 - 1s/epoch - 420us/step\n",
      "Epoch 20/20\n",
      "2668/2668 - 1s - loss: 2.6543e-04 - 1s/epoch - 409us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31451edc0&gt;,\n",
       "                    param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;]},\n",
       "                    verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31451edc0&gt;,\n",
       "                    param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;]},\n",
       "                    verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31451edc0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31451edc0&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(error_score='raise',\n",
       "                    estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x31451edc0>,\n",
       "                    param_grid={'activation': ['tanh', 'relu'],\n",
       "                                'kernel_regularizer': ['l1', 'l2', 'l1_l2']},\n",
       "                    verbose=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search selected hyperparameters\n",
    "\n",
    "# define base model function for grid search\n",
    "def create_model(activation='relu', kernel_regularizer='l2'):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=64, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2']\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform grid search hyperparameter tuning\n",
    "grid_search = HalvingGridSearchCV(estimator=keras_reg, param_grid=param_dist, cv=5, verbose=2, error_score='raise')\n",
    "grid_search.fit(Xtrain, ytrain, sample_weight=wtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>n_resources</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>29.277782</td>\n",
       "      <td>0.438441</td>\n",
       "      <td>0.112907</td>\n",
       "      <td>0.005524</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.006056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005936</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.006106</td>\n",
       "      <td>-0.005748</td>\n",
       "      <td>-0.005817</td>\n",
       "      <td>-0.006003</td>\n",
       "      <td>-0.005998</td>\n",
       "      <td>-0.005934</td>\n",
       "      <td>0.000132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>29.347707</td>\n",
       "      <td>0.258205</td>\n",
       "      <td>0.127457</td>\n",
       "      <td>0.028411</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>-0.000591</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.000517</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>30.181596</td>\n",
       "      <td>0.923040</td>\n",
       "      <td>0.129607</td>\n",
       "      <td>0.016089</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.006757</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006316</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.006789</td>\n",
       "      <td>-0.006201</td>\n",
       "      <td>-0.006036</td>\n",
       "      <td>-0.006361</td>\n",
       "      <td>-0.006121</td>\n",
       "      <td>-0.006302</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>29.927985</td>\n",
       "      <td>0.787953</td>\n",
       "      <td>0.179043</td>\n",
       "      <td>0.132378</td>\n",
       "      <td>relu</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.002785</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002810</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.002786</td>\n",
       "      <td>-0.002781</td>\n",
       "      <td>-0.002864</td>\n",
       "      <td>-0.002800</td>\n",
       "      <td>-0.002822</td>\n",
       "      <td>-0.002811</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>29.578543</td>\n",
       "      <td>0.730748</td>\n",
       "      <td>0.116243</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000452</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.001140</td>\n",
       "      <td>-0.000505</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>-0.000443</td>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>29.643308</td>\n",
       "      <td>0.384341</td>\n",
       "      <td>0.117571</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>relu</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.002896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003368</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.002898</td>\n",
       "      <td>-0.002908</td>\n",
       "      <td>-0.002881</td>\n",
       "      <td>-0.005189</td>\n",
       "      <td>-0.002923</td>\n",
       "      <td>-0.003360</td>\n",
       "      <td>0.000915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>85356</td>\n",
       "      <td>44.846986</td>\n",
       "      <td>0.544574</td>\n",
       "      <td>0.197627</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000674</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.000432</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.000688</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>-0.001232</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>85356</td>\n",
       "      <td>44.786772</td>\n",
       "      <td>0.520556</td>\n",
       "      <td>0.281276</td>\n",
       "      <td>0.143043</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
       "0     0        28452      29.277782      0.438441         0.112907   \n",
       "1     0        28452      29.347707      0.258205         0.127457   \n",
       "2     0        28452      30.181596      0.923040         0.129607   \n",
       "3     0        28452      29.927985      0.787953         0.179043   \n",
       "4     0        28452      29.578543      0.730748         0.116243   \n",
       "5     0        28452      29.643308      0.384341         0.117571   \n",
       "6     1        85356      44.846986      0.544574         0.197627   \n",
       "7     1        85356      44.786772      0.520556         0.281276   \n",
       "\n",
       "   std_score_time param_activation param_kernel_regularizer  \\\n",
       "0        0.005524             tanh                       l1   \n",
       "1        0.028411             tanh                       l2   \n",
       "2        0.016089             tanh                    l1_l2   \n",
       "3        0.132378             relu                       l1   \n",
       "4        0.006994             relu                       l2   \n",
       "5        0.007166             relu                    l1_l2   \n",
       "6        0.002991             tanh                       l2   \n",
       "7        0.143043             relu                       l2   \n",
       "\n",
       "                                              params  split0_test_score  ...  \\\n",
       "0  {'activation': 'tanh', 'kernel_regularizer': '...          -0.006056  ...   \n",
       "1  {'activation': 'tanh', 'kernel_regularizer': '...          -0.000539  ...   \n",
       "2  {'activation': 'tanh', 'kernel_regularizer': '...          -0.006757  ...   \n",
       "3  {'activation': 'relu', 'kernel_regularizer': '...          -0.002785  ...   \n",
       "4  {'activation': 'relu', 'kernel_regularizer': '...          -0.000207  ...   \n",
       "5  {'activation': 'relu', 'kernel_regularizer': '...          -0.002896  ...   \n",
       "6  {'activation': 'tanh', 'kernel_regularizer': '...          -0.000429  ...   \n",
       "7  {'activation': 'relu', 'kernel_regularizer': '...          -0.000151  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0        -0.005936        0.000083                7           -0.006106   \n",
       "1        -0.000541        0.000046                3           -0.000541   \n",
       "2        -0.006316        0.000230                8           -0.006789   \n",
       "3        -0.002810        0.000031                5           -0.002786   \n",
       "4        -0.000452        0.000390                2           -0.000208   \n",
       "5        -0.003368        0.000933                6           -0.002898   \n",
       "6        -0.000674        0.000293                4           -0.000432   \n",
       "7        -0.000154        0.000016                1           -0.000151   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0           -0.005748           -0.005817           -0.006003   \n",
       "1           -0.000591           -0.000574           -0.000517   \n",
       "2           -0.006201           -0.006036           -0.006361   \n",
       "3           -0.002781           -0.002864           -0.002800   \n",
       "4           -0.000180           -0.001140           -0.000505   \n",
       "5           -0.002908           -0.002881           -0.005189   \n",
       "6           -0.000466           -0.000688           -0.000561   \n",
       "7           -0.000176           -0.000147           -0.000130   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0           -0.005998         -0.005934         0.000132  \n",
       "1           -0.000474         -0.000539         0.000041  \n",
       "2           -0.006121         -0.006302         0.000266  \n",
       "3           -0.002822         -0.002811         0.000030  \n",
       "4           -0.000182         -0.000443         0.000369  \n",
       "5           -0.002923         -0.003360         0.000915  \n",
       "6           -0.001232         -0.000676         0.000292  \n",
       "7           -0.000165         -0.000154         0.000016  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "display(grid_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'activation': 'relu', 'kernel_regularizer': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0519 - val_loss: 0.0023\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0015 - val_loss: 9.1489e-04\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 8.3574e-04 - val_loss: 6.1465e-04\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 6.6634e-04 - val_loss: 4.6885e-04\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 5.7144e-04 - val_loss: 3.6956e-04\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 4.8092e-04 - val_loss: 3.2419e-04\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 4.7032e-04 - val_loss: 2.7739e-04\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 4.3496e-04 - val_loss: 2.5733e-04\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 3.3716e-04 - val_loss: 2.9419e-04\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 4.1987e-04 - val_loss: 8.3258e-04\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 3.7042e-04 - val_loss: 2.1454e-04\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 3.4379e-04 - val_loss: 2.2366e-04\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 4.1834e-04 - val_loss: 2.4765e-04\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 2.7459e-04 - val_loss: 1.9947e-04\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 1s 619us/step - loss: 4.2787e-04 - val_loss: 2.0451e-04\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 2.5853e-04 - val_loss: 2.8759e-04\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 3.3572e-04 - val_loss: 1.7864e-04\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 4.7710e-04 - val_loss: 1.8278e-04\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 3.3296e-04 - val_loss: 1.9861e-04\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 1s 537us/step - loss: 4.8454e-04 - val_loss: 1.9839e-04\n"
     ]
    }
   ],
   "source": [
    "# evaluate best model\n",
    "best_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "    Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "best_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = best_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=64, validation_data=(Xval, yval, wval))\n",
    "\n",
    "# save the best model\n",
    "best_model.save(\"models/best_dnn_model_processed_pd.h5\")\n",
    "\n",
    "# save model history in models\n",
    "with open('models/best_dnn_model_processed_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Huber Loss: 0.0004845440562348813\n",
      "Best Validation Huber Loss: 0.0001983915426535532\n"
     ]
    }
   ],
   "source": [
    "best_train_hl = best_model.history.history[\"loss\"][-1]\n",
    "best_val_hl = best_model.history.history[\"val_loss\"][-1]\n",
    "print(f\"Best Training Huber Loss: {best_train_hl}\")\n",
    "print(f\"Best Validation Huber Loss: {best_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.0034 - val_loss: 9.3905e-06\n",
      "Epoch 2/28\n",
      "2829/2829 [==============================] - 2s 736us/step - loss: 9.8081e-05 - val_loss: 8.7413e-06\n",
      "Epoch 3/28\n",
      "2829/2829 [==============================] - 2s 750us/step - loss: 1.1788e-04 - val_loss: 1.1943e-05\n",
      "Epoch 4/28\n",
      "2829/2829 [==============================] - 2s 831us/step - loss: 7.3283e-05 - val_loss: 2.4040e-05\n",
      "Epoch 5/28\n",
      "2829/2829 [==============================] - 2s 782us/step - loss: 4.9077e-05 - val_loss: 1.0799e-05\n",
      "Epoch 6/28\n",
      "2829/2829 [==============================] - 2s 780us/step - loss: 6.5821e-05 - val_loss: 3.8145e-06\n",
      "Epoch 7/28\n",
      "2829/2829 [==============================] - 2s 739us/step - loss: 5.0129e-05 - val_loss: 3.9991e-05\n",
      "Epoch 8/28\n",
      "2829/2829 [==============================] - 3s 894us/step - loss: 4.0896e-05 - val_loss: 1.8399e-06\n",
      "Epoch 9/28\n",
      "2829/2829 [==============================] - 2s 798us/step - loss: 3.3362e-05 - val_loss: 1.7501e-06\n",
      "Epoch 10/28\n",
      "2829/2829 [==============================] - 2s 790us/step - loss: 4.3279e-05 - val_loss: 7.5431e-06\n",
      "Epoch 11/28\n",
      "2829/2829 [==============================] - 2s 827us/step - loss: 4.1557e-05 - val_loss: 7.4014e-07\n",
      "Epoch 12/28\n",
      "2829/2829 [==============================] - 3s 944us/step - loss: 7.5943e-05 - val_loss: 6.5780e-07\n",
      "Epoch 13/28\n",
      "2829/2829 [==============================] - 2s 771us/step - loss: 1.2705e-05 - val_loss: 2.7435e-05\n",
      "Epoch 14/28\n",
      "2829/2829 [==============================] - 2s 768us/step - loss: 2.9126e-05 - val_loss: 4.5832e-07\n",
      "Epoch 15/28\n",
      "2829/2829 [==============================] - 2s 825us/step - loss: 1.5522e-05 - val_loss: 2.0626e-07\n",
      "Epoch 16/28\n",
      "2829/2829 [==============================] - 2s 766us/step - loss: 1.7650e-05 - val_loss: 3.7001e-06\n",
      "Epoch 17/28\n",
      "2829/2829 [==============================] - 2s 786us/step - loss: 3.1372e-05 - val_loss: 4.2205e-07\n",
      "Epoch 18/28\n",
      "2829/2829 [==============================] - 2s 758us/step - loss: 1.7281e-05 - val_loss: 1.9007e-07\n",
      "Epoch 19/28\n",
      "2829/2829 [==============================] - 2s 771us/step - loss: 3.3024e-05 - val_loss: 3.9632e-07\n",
      "Epoch 20/28\n",
      "2829/2829 [==============================] - 2s 846us/step - loss: 1.7264e-05 - val_loss: 5.8502e-07\n",
      "Epoch 21/28\n",
      "2829/2829 [==============================] - 2s 757us/step - loss: 2.6755e-05 - val_loss: 7.7477e-06\n",
      "Epoch 22/28\n",
      "2829/2829 [==============================] - 2s 768us/step - loss: 2.2098e-05 - val_loss: 1.6067e-07\n",
      "Epoch 23/28\n",
      "2829/2829 [==============================] - 2s 789us/step - loss: 1.2255e-05 - val_loss: 7.7692e-05\n",
      "Epoch 24/28\n",
      "2829/2829 [==============================] - 2s 763us/step - loss: 1.9902e-05 - val_loss: 2.1851e-07\n",
      "Epoch 25/28\n",
      "2829/2829 [==============================] - 2s 807us/step - loss: 1.9602e-05 - val_loss: 2.2939e-05\n",
      "Epoch 26/28\n",
      "2829/2829 [==============================] - 2s 749us/step - loss: 9.2702e-06 - val_loss: 1.3775e-05\n",
      "Epoch 27/28\n",
      "2829/2829 [==============================] - 2s 777us/step - loss: 1.8473e-05 - val_loss: 5.2099e-07\n",
      "Epoch 28/28\n",
      "2829/2829 [==============================] - 2s 788us/step - loss: 1.8116e-05 - val_loss: 9.0140e-08\n",
      "Training Huber Loss: 1.811589754652232e-05\n",
      "Validation Huber Loss: 9.014046753463845e-08\n"
     ]
    }
   ],
   "source": [
    "epochs = 28 # computed from previous session's early stopping\n",
    "best_delta = 1.2 # computed from previous session's iterative search\n",
    "\n",
    "# evaluate base model \n",
    "base_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer with 1 neuron for regression\n",
    "])\n",
    "\n",
    "base_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = base_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=epochs, batch_size=32, validation_data=(Xval, yval, wval))\n",
    "\n",
    "# save model\n",
    "base_model.save(\"models/extended_dnn_model_processed_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/extended_dnn_model_processed_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "base_train_hl = history.history[\"loss\"][-1]\n",
    "base_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {base_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {base_val_hl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_37274/1588764074.py:30: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=epochs, verbose=2)\n",
      "2024-06-10 16:10:18.994953: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 16:10:19.109461: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 16:10:19.111610: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 16:10:19.135863: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 16:10:19.188568: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 1.0691 - val_loss: 0.0938\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 1.0657 - val_loss: 0.0976\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 1.0605 - val_loss: 0.0837\n",
      "  44/1415 [..............................] - ETA: 1s - loss: 0.0970Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 1.0554 - val_loss: 0.0917\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 1.0703 - val_loss: 0.1086\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0766 - val_loss: 0.0630\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0815 - val_loss: 0.0691\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0690 - val_loss: 0.0611\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0760 - val_loss: 0.0621\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0889 - val_loss: 0.0748\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0585 - val_loss: 0.0543\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0604 - val_loss: 0.0553\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0576 - val_loss: 0.0542\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0624 - val_loss: 0.0529\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0577 - val_loss: 0.0528\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0505 - val_loss: 0.0471\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0522 - val_loss: 0.0496\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0518 - val_loss: 0.0492\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0492 - val_loss: 0.0469\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0506 - val_loss: 0.0483\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0456 - val_loss: 0.0440\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0475 - val_loss: 0.0457\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0478 - val_loss: 0.0460\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0468 - val_loss: 0.0452\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0457 - val_loss: 0.0443\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0429 - val_loss: 0.0416\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0443 - val_loss: 0.0429\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0451 - val_loss: 0.0438\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0442 - val_loss: 0.0428\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0432 - val_loss: 0.0421\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0420 - val_loss: 0.0409\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0410 - val_loss: 0.0400\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0429 - val_loss: 0.0418\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0421 - val_loss: 0.0412\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0412 - val_loss: 0.0403\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0402 - val_loss: 0.0395\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0394 - val_loss: 0.0388\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0412 - val_loss: 0.0402\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0405 - val_loss: 0.0396\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0397 - val_loss: 0.0388\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0387 - val_loss: 0.0381\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0398 - val_loss: 0.0391\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0382 - val_loss: 0.0375\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0392 - val_loss: 0.0385\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0385 - val_loss: 0.0379\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0375 - val_loss: 0.0363\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0386 - val_loss: 0.0382\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0371 - val_loss: 0.0366\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0381 - val_loss: 0.0374\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0374 - val_loss: 0.0369\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0355 - val_loss: 0.0351\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0377 - val_loss: 0.0372\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0362 - val_loss: 0.0358\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0371 - val_loss: 0.0366\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0366 - val_loss: 0.0363\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0347 - val_loss: 0.0344\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0369 - val_loss: 0.0364\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0355 - val_loss: 0.0353\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0363 - val_loss: 0.0358\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0358 - val_loss: 0.0354\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0341 - val_loss: 0.0336\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0362 - val_loss: 0.0361\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0348 - val_loss: 0.0344\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0356 - val_loss: 0.0350\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0352 - val_loss: 0.0347\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0335 - val_loss: 0.0332\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0356 - val_loss: 0.0352\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0343 - val_loss: 0.0339\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0350 - val_loss: 0.0346\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0346 - val_loss: 0.0343\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0331 - val_loss: 0.0329\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0351 - val_loss: 0.0348\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0338 - val_loss: 0.0337\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0344 - val_loss: 0.0340\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0341 - val_loss: 0.0338\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0326 - val_loss: 0.0324\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0346 - val_loss: 0.0343\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0333 - val_loss: 0.0331\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0339 - val_loss: 0.0336\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0337 - val_loss: 0.0334\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0323 - val_loss: 0.0322\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0342 - val_loss: 0.0338\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0329 - val_loss: 0.0326\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0335 - val_loss: 0.0333\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0332 - val_loss: 0.0330\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0320 - val_loss: 0.0318\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0338 - val_loss: 0.0337\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0325 - val_loss: 0.0324\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0331 - val_loss: 0.0328\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0329 - val_loss: 0.0327\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0317 - val_loss: 0.0315\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0334 - val_loss: 0.0333\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0322 - val_loss: 0.0320\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0328 - val_loss: 0.0326\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0325 - val_loss: 0.0323\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0314 - val_loss: 0.0313\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0331 - val_loss: 0.0329\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0319 - val_loss: 0.0318\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0324 - val_loss: 0.0322\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0322 - val_loss: 0.0319\n",
      " 171/1415 [==>...........................] - ETA: 1s - loss: 0.0317Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0311 - val_loss: 0.0309\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0328 - val_loss: 0.0327\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0316 - val_loss: 0.0315\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0321 - val_loss: 0.0319\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0319 - val_loss: 0.0318\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0325 - val_loss: 0.0325\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0309 - val_loss: 0.0309\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0314 - val_loss: 0.0312\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0317 - val_loss: 0.0315\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0323 - val_loss: 0.0321\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0307 - val_loss: 0.0307\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0312 - val_loss: 0.0311\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0316 - val_loss: 0.0315\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0314 - val_loss: 0.0313\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0321 - val_loss: 0.0320\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0305 - val_loss: 0.0305\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0309 - val_loss: 0.0308\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0314 - val_loss: 0.0315\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0312 - val_loss: 0.0310\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0318 - val_loss: 0.0316\n",
      "1199/1415 [========================>.....] - ETA: 0s - loss: 0.0307Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0303 - val_loss: 0.0301\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0307 - val_loss: 0.0306\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0312 - val_loss: 0.0311\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0310 - val_loss: 0.0308\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0316 - val_loss: 0.0316\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0302 - val_loss: 0.0300\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0305 - val_loss: 0.0304\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0310 - val_loss: 0.0307\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0308 - val_loss: 0.0306\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0314 - val_loss: 0.0315\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0300 - val_loss: 0.0300\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0308 - val_loss: 0.0309\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0303 - val_loss: 0.0302\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0306 - val_loss: 0.0306\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0313 - val_loss: 0.0311\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0299 - val_loss: 0.0298\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0306 - val_loss: 0.0303\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0302 - val_loss: 0.0302\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0304 - val_loss: 0.0303\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1132/1132 - 3s - loss: 0.0311 - val_loss: 0.0310 - 3s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.0297 - val_loss: 0.0296 - 3s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.0300 - val_loss: 0.0300 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.0305 - val_loss: 0.0304 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.0303 - val_loss: 0.0301 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 0.0310 - val_loss: 0.0310 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0296 - val_loss: 0.0296 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0299 - val_loss: 0.0299 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0303 - val_loss: 0.0302 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0302 - val_loss: 0.0302 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0309 - val_loss: 0.0307 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0295 - val_loss: 0.0295 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0298 - val_loss: 0.0297 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0302 - val_loss: 0.0301 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0300 - val_loss: 0.0299 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0294 - val_loss: 0.0294 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0307 - val_loss: 0.0307 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0297 - val_loss: 0.0296 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0301 - val_loss: 0.0300 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0299 - val_loss: 0.0298 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0306 - val_loss: 0.0306 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0293 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0296 - val_loss: 0.0295 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0300 - val_loss: 0.0299 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0298 - val_loss: 0.0296 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0305 - val_loss: 0.0303 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0293 - val_loss: 0.0292 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0295 - val_loss: 0.0294 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0299 - val_loss: 0.0298 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0297 - val_loss: 0.0296 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0304 - val_loss: 0.0303 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0292 - val_loss: 0.0292 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0294 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0297 - val_loss: 0.0297 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0296 - val_loss: 0.0296 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0303 - val_loss: 0.0302 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0293 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0297 - val_loss: 0.0295 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0295 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0290 - val_loss: 0.0289 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0302 - val_loss: 0.0301 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0292 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0296 - val_loss: 0.0295 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0294 - val_loss: 0.0294 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0301 - val_loss: 0.0302 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0289 - val_loss: 0.0289 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0295 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0293 - val_loss: 0.0292 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 2s - loss: 0.0300 - val_loss: 0.0299 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 2s - loss: 0.0289 - val_loss: 0.0289 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 2s - loss: 0.0294 - val_loss: 0.0293 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 2s - loss: 0.0290 - val_loss: 0.0291 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 2s - loss: 0.0292 - val_loss: 0.0290 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0299 - val_loss: 0.0299 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0288 - val_loss: 0.0287 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0293 - val_loss: 0.0292 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0290 - val_loss: 0.0289 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0299 - val_loss: 0.0297 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0287 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0292 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0289 - val_loss: 0.0288 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0290 - val_loss: 0.0289 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0298 - val_loss: 0.0296 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0288 - val_loss: 0.0288 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0290 - val_loss: 0.0289 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0297 - val_loss: 0.0296 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0286 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0290 - val_loss: 0.0289 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0287 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0289 - val_loss: 0.0288 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0296 - val_loss: 0.0294 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0285 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0290 - val_loss: 0.0290 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0288 - val_loss: 0.0289 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0295 - val_loss: 0.0295 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0285 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0289 - val_loss: 0.0288 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0286 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0287 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0294 - val_loss: 0.0292 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0284 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0288 - val_loss: 0.0287 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0294 - val_loss: 0.0294 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0284 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0287 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0285 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0286 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0293 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0283 - val_loss: 0.0283 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0285 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0284 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0283 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0283 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0293 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0286 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0283 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0284 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0282 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0292 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0285 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0283 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0283 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0282 - val_loss: 0.0281 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0284 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0283 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0281 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0290 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0284 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0282 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0283 - val_loss: 0.0283 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0281 - val_loss: 0.0280 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0290 - val_loss: 0.0288 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0283 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0282 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0281 - val_loss: 0.0281 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 2s - loss: 0.0280 - val_loss: 0.0280 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 2s - loss: 0.0289 - val_loss: 0.0289 - 2s/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 2s - loss: 0.0283 - val_loss: 0.0284 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 2s - loss: 0.0282 - val_loss: 0.0281 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 2s - loss: 0.0281 - val_loss: 0.0281 - 2s/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0289 - val_loss: 0.0288 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0280 - val_loss: 0.0280 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0283 - val_loss: 0.0281 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0281 - val_loss: 0.0281 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0280 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0288 - val_loss: 0.0288 - 1s/epoch - 1ms/step\n",
      "1132/1132 - 1s - loss: 0.0280 - val_loss: 0.0279 - 1s/epoch - 1ms/step\n",
      "283/283 - 0s - loss: 0.0288 - 218ms/epoch - 769us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "283/283 - 0s - loss: 0.0279 - 215ms/epoch - 758us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "1132/1132 - 1s - loss: 0.0282 - val_loss: 0.0281 - 1s/epoch - 1ms/step\n",
      "283/283 - 0s - loss: 0.0281 - 233ms/epoch - 824us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "1132/1132 - 1s - loss: 0.0281 - val_loss: 0.0280 - 1s/epoch - 1ms/step\n",
      "1132/1132 - 1s - loss: 0.0280 - val_loss: 0.0280 - 1s/epoch - 1ms/step\n",
      "283/283 - 0s - loss: 0.0280 - 167ms/epoch - 590us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "283/283 - 0s - loss: 0.0280 - 165ms/epoch - 582us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1367/1415 [===========================>..] - ETA: 0s - loss: 4.6158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 16:12:08.740901: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 16:12:08.874297: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "   1/1415 [..............................] - ETA: 7:28 - loss: 18.3420Epoch 1/28\n",
      "  61/1415 [>.............................] - ETA: 1s - loss: 14.8328  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 16:12:09.091728: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415/1415 [==============================] - 3s 2ms/step - loss: 4.0910 - val_loss: 2.5961\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 4.5479 - val_loss: 3.0225\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 5.0327 - val_loss: 2.4627\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 5.0013 - val_loss: 2.6408\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 3.7844 - val_loss: 2.3622\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 3.9943 - val_loss: 2.3082\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 2.6220 - val_loss: 2.6683\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 2.6522 - val_loss: 2.7233\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 5.4153 - val_loss: 3.7681\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 3.6360 - val_loss: 2.4978\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 4.6210 - val_loss: 2.5245\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 3.2118 - val_loss: 2.6269\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 3.2108 - val_loss: 2.8996\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6361 - val_loss: 2.6347\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7142 - val_loss: 2.7208\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 3.2846 - val_loss: 2.5980\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7503 - val_loss: 2.6271\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7847 - val_loss: 2.7545\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 3.7199 - val_loss: 2.5481\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 3.9276 - val_loss: 2.6204\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7947 - val_loss: 2.8343\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7034 - val_loss: 2.7275\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 3.1734 - val_loss: 2.7624\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7607 - val_loss: 2.6511\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7390 - val_loss: 2.6918\n",
      "1307/1415 [==========================>...] - ETA: 0s - loss: 3.1842Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 3.2009 - val_loss: 4.4090\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7906 - val_loss: 2.9250\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7481 - val_loss: 2.7291\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 3.1502 - val_loss: 2.8544\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7906 - val_loss: 2.6340\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.9320 - val_loss: 2.9054\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.9083 - val_loss: 3.2500\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7787 - val_loss: 2.6463\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.8352 - val_loss: 2.6325\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7365 - val_loss: 2.7353\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7931 - val_loss: 2.6714\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.8463 - val_loss: 2.7316\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7611 - val_loss: 2.6998\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7277 - val_loss: 2.7508\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7995 - val_loss: 2.8045\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7426 - val_loss: 2.8421\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 3.0653 - val_loss: 2.8401\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7360 - val_loss: 2.6336\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7119 - val_loss: 2.6294\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.8904 - val_loss: 2.9486\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7117 - val_loss: 3.0096\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.8041 - val_loss: 2.8431\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7711 - val_loss: 2.7219\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7756 - val_loss: 2.8085\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.8951 - val_loss: 3.0064\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7829 - val_loss: 2.8042\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6959 - val_loss: 2.6486\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7483 - val_loss: 2.6347\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7943 - val_loss: 2.8188\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.9646 - val_loss: 2.7856\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.8032 - val_loss: 2.8235\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7174 - val_loss: 2.6508\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7250 - val_loss: 2.7748\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7178 - val_loss: 2.8743\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.8113 - val_loss: 2.7949\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7416 - val_loss: 2.9739\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7216 - val_loss: 2.6309\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7043 - val_loss: 2.7618\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6833 - val_loss: 2.7551\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7824 - val_loss: 2.7483\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7781 - val_loss: 3.1247\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6948 - val_loss: 2.9617\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7168 - val_loss: 3.0043\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7324 - val_loss: 2.6998\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.8438 - val_loss: 2.7449\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7374 - val_loss: 2.7188\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7295 - val_loss: 2.6287\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7700 - val_loss: 2.7905\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6837 - val_loss: 2.7993\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 2.7998 - val_loss: 2.8500\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7717 - val_loss: 2.7042\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7159 - val_loss: 2.7564\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6937 - val_loss: 2.6173\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6820 - val_loss: 2.6195\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.8905 - val_loss: 3.4489\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7288 - val_loss: 2.8081\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6927 - val_loss: 2.9540\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6960 - val_loss: 2.6543\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6870 - val_loss: 2.6911\n",
      " 460/1415 [========>.....................] - ETA: 1s - loss: 2.6962Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.8158 - val_loss: 2.7297\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6951 - val_loss: 2.7432\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7034 - val_loss: 2.6347\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7164 - val_loss: 2.7902\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6650 - val_loss: 2.6419\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7546 - val_loss: 2.6641\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7120 - val_loss: 2.6635\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6909 - val_loss: 2.7552\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7146 - val_loss: 2.6244\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6792 - val_loss: 2.7222\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7581 - val_loss: 2.6596\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7032 - val_loss: 2.6667\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6914 - val_loss: 2.7050\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7787 - val_loss: 2.7899\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6908 - val_loss: 2.7927\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7486 - val_loss: 2.6954\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6895 - val_loss: 2.7474\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7058 - val_loss: 2.6426\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7429 - val_loss: 2.6349\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6601 - val_loss: 2.6031\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.8111 - val_loss: 2.6727\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7094 - val_loss: 2.7016\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6827 - val_loss: 2.6357\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6758 - val_loss: 2.6196\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6954 - val_loss: 2.6152\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7426 - val_loss: 3.0775\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7241 - val_loss: 2.7893\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6931 - val_loss: 2.7031\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6897 - val_loss: 2.5915\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6697 - val_loss: 2.9266\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7442 - val_loss: 2.6495\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6778 - val_loss: 2.6084\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6824 - val_loss: 2.6313\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7156 - val_loss: 2.6341\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6898 - val_loss: 2.6608\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7354 - val_loss: 2.6840\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7122 - val_loss: 2.6411\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6756 - val_loss: 2.6567\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6977 - val_loss: 2.7563\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7415 - val_loss: 2.6113\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7189 - val_loss: 2.7191\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7172 - val_loss: 2.7397\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7046 - val_loss: 2.6152\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6984 - val_loss: 2.6385\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6769 - val_loss: 2.5759\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7268 - val_loss: 2.6421\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7008 - val_loss: 2.6291\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7656 - val_loss: 3.0003\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6931 - val_loss: 2.6528\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6708 - val_loss: 2.6029\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7096 - val_loss: 2.6772\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7223 - val_loss: 2.6329\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6842 - val_loss: 2.6165\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.6837 - val_loss: 2.6778\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7011 - val_loss: 2.6538\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 2.6840 - val_loss: 2.5947\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1132/1132 - 3s - loss: 2.7821 - val_loss: 2.6461 - 3s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1132/1132 - 3s - loss: 2.6908 - val_loss: 2.8985 - 3s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 2.7262 - val_loss: 2.6531 - 1s/epoch - 967us/step\n",
      "Epoch 3/28\n",
      "Epoch 1/28\n",
      "1132/1132 - 2s - loss: 2.6903 - val_loss: 2.8019 - 2s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 2.7099 - val_loss: 2.6348 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 2.7047 - val_loss: 2.6164 - 1s/epoch - 928us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 2.7176 - val_loss: 2.6579 - 1s/epoch - 893us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 3s - loss: 2.7200 - val_loss: 2.7799 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 2.7119 - val_loss: 2.6816 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 2.7119 - val_loss: 2.6421 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 2.6954 - val_loss: 2.6233 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 2.7193 - val_loss: 2.6792 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 2.7243 - val_loss: 2.6583 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 2.7251 - val_loss: 2.6474 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 2.7202 - val_loss: 2.6470 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 2.6898 - val_loss: 2.6260 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 2.7274 - val_loss: 2.6953 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 2.7220 - val_loss: 2.7441 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 2.6917 - val_loss: 2.6256 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 2.6536 - val_loss: 2.6478 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 2.6958 - val_loss: 2.6097 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 2.7049 - val_loss: 3.0897 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 2.7522 - val_loss: 2.7984 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 2s - loss: 2.6886 - val_loss: 2.6652 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 2s - loss: 2.6608 - val_loss: 2.6442 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 2s - loss: 2.6727 - val_loss: 2.6191 - 2s/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 2s - loss: 2.7289 - val_loss: 2.7014 - 2s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 2s - loss: 2.6829 - val_loss: 2.8912 - 2s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 2.7285 - val_loss: 2.6634 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 2.6472 - val_loss: 2.5846 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 2.6683 - val_loss: 2.6749 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 2.7401 - val_loss: 2.6516 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 2.7443 - val_loss: 2.7845 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 2.7030 - val_loss: 2.6301 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 2.6602 - val_loss: 2.6552 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 2s - loss: 2.6863 - val_loss: 2.6018 - 2s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 2.7116 - val_loss: 2.6564 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 2.7008 - val_loss: 2.9535 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 2.6889 - val_loss: 2.6204 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 2s - loss: 2.6785 - val_loss: 2.6077 - 2s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 2s - loss: 2.6752 - val_loss: 2.8921 - 2s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 2s - loss: 2.7110 - val_loss: 2.7101 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 2s - loss: 2.7037 - val_loss: 2.6439 - 2s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 2s - loss: 2.6813 - val_loss: 2.6575 - 2s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 2s - loss: 2.6662 - val_loss: 2.7203 - 2s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 2s - loss: 2.6851 - val_loss: 2.6152 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 2s - loss: 2.7240 - val_loss: 2.6485 - 2s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 2s - loss: 2.6922 - val_loss: 2.6767 - 2s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 2s - loss: 2.7071 - val_loss: 2.6306 - 2s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 2.6777 - val_loss: 2.6357 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 2.6697 - val_loss: 2.7408 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 2.6975 - val_loss: 2.6247 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 2.7304 - val_loss: 2.7159 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 2.6878 - val_loss: 2.8003 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 2.6727 - val_loss: 2.7051 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 2.6845 - val_loss: 2.5917 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 2.6971 - val_loss: 2.6975 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 2.7932 - val_loss: 2.9475 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 2.6964 - val_loss: 2.6276 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 2.6852 - val_loss: 2.6415 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 2.6722 - val_loss: 2.6647 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 2.7001 - val_loss: 2.6289 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 2.7447 - val_loss: 2.6527 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 2.7215 - val_loss: 2.7014 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 2.6499 - val_loss: 2.5987 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 2.6935 - val_loss: 2.6034 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 2.7177 - val_loss: 2.7460 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 2.7656 - val_loss: 2.7569 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 2.7056 - val_loss: 2.6286 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 2.6692 - val_loss: 2.5864 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 2s - loss: 2.6828 - val_loss: 2.6188 - 2s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 2s - loss: 2.6571 - val_loss: 2.6765 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 2s - loss: 2.7074 - val_loss: 2.8195 - 2s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 2s - loss: 2.6979 - val_loss: 2.6808 - 2s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 2s - loss: 2.6538 - val_loss: 2.6824 - 2s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 2.7043 - val_loss: 2.6759 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 2.7141 - val_loss: 2.6972 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 2.6910 - val_loss: 2.6211 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 2.7101 - val_loss: 2.6314 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 2.6686 - val_loss: 2.6247 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 2.7218 - val_loss: 3.2337 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 2.7087 - val_loss: 2.6860 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 2.6690 - val_loss: 2.6464 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 2.7128 - val_loss: 2.7693 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 2s - loss: 2.6462 - val_loss: 2.6024 - 2s/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 2s - loss: 2.6902 - val_loss: 2.6188 - 2s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 2s - loss: 2.7197 - val_loss: 2.7160 - 2s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 2s - loss: 2.6661 - val_loss: 2.7952 - 2s/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 2s - loss: 2.7328 - val_loss: 2.6096 - 2s/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 2.6716 - val_loss: 2.6043 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 2.6975 - val_loss: 2.7036 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 2.7255 - val_loss: 2.6317 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 2.7533 - val_loss: 2.6350 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 2.6907 - val_loss: 2.6327 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 2.6616 - val_loss: 2.7056 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 2.7552 - val_loss: 2.6038 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 2.7372 - val_loss: 2.6632 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 2s - loss: 2.6675 - val_loss: 2.6506 - 2s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 2.7112 - val_loss: 2.7069 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 2.6613 - val_loss: 2.6307 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 2.7005 - val_loss: 2.6263 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 2.7029 - val_loss: 2.6599 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 2.6924 - val_loss: 2.6369 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 2.7302 - val_loss: 2.6915 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 2.6706 - val_loss: 2.6458 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 2.7279 - val_loss: 2.6773 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 2.8435 - val_loss: 2.7962 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 2s - loss: 2.6745 - val_loss: 2.6200 - 2s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 2s - loss: 2.7186 - val_loss: 2.5991 - 2s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 2.6637 - val_loss: 2.6109 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 2.7308 - val_loss: 2.6729 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 2.7300 - val_loss: 2.6926 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 2.7016 - val_loss: 2.7772 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 2.7243 - val_loss: 2.6390 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 2.6466 - val_loss: 2.5788 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 2.7005 - val_loss: 2.8484 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 2.7036 - val_loss: 2.6891 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 2s - loss: 2.6778 - val_loss: 2.9304 - 2s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 2s - loss: 2.7287 - val_loss: 2.9688 - 2s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 2s - loss: 2.6532 - val_loss: 3.0311 - 2s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 2s - loss: 2.7012 - val_loss: 2.6148 - 2s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 2s - loss: 2.7232 - val_loss: 2.6611 - 2s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 2.6976 - val_loss: 2.8419 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 2.6701 - val_loss: 3.1771 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 2.6600 - val_loss: 2.5753 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 2.7014 - val_loss: 2.8077 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 2.7212 - val_loss: 2.6605 - 1s/epoch - 1ms/step\n",
      "283/283 - 0s - loss: 2.3613 - 224ms/epoch - 790us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time= 1.7min\n",
      "1132/1132 - 1s - loss: 2.7011 - val_loss: 2.7621 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 2.6824 - val_loss: 2.7226 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 2.6544 - val_loss: 2.6048 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 2.6891 - val_loss: 2.7424 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 2.7010 - val_loss: 2.6205 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 2.6850 - val_loss: 2.7320 - 1s/epoch - 1ms/step\n",
      "283/283 - 0s - loss: 2.6048 - 190ms/epoch - 672us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time= 1.8min\n",
      "1132/1132 - 1s - loss: 2.6559 - val_loss: 2.6490 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 2.6938 - val_loss: 2.6254 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 2.6901 - val_loss: 2.6285 - 1s/epoch - 985us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 2.6465 - val_loss: 2.6093 - 1s/epoch - 977us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 2.6954 - val_loss: 2.7272 - 1s/epoch - 962us/step\n",
      "283/283 - 0s - loss: 2.3720 - 182ms/epoch - 642us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time= 1.8min\n",
      "1132/1132 - 1s - loss: 2.6903 - val_loss: 2.7052 - 1s/epoch - 970us/step\n",
      "283/283 - 0s - loss: 2.5726 - 169ms/epoch - 595us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time= 1.8min\n",
      "1132/1132 - 1s - loss: 2.6564 - val_loss: 2.5790 - 978ms/epoch - 864us/step\n",
      "283/283 - 0s - loss: 2.3943 - 144ms/epoch - 508us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time= 1.8min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "  62/1415 [>.............................] - ETA: 1s - loss: 16.9149  Epoch 1/28\n",
      " 143/1415 [==>...........................] - ETA: 0s - loss: 16.0030Epoch 1/28\n",
      "1354/1415 [===========================>..] - ETA: 0s - loss: 7.8631"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 16:13:58.307825: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 7.5663 - val_loss: 1.4198\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 7.5080 - val_loss: 1.3790\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 7.5485 - val_loss: 1.3950\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 7.5915 - val_loss: 1.3747\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 1s 837us/step - loss: 0.4355 - val_loss: 0.1533\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 850us/step - loss: 0.4212 - val_loss: 0.1557\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 833us/step - loss: 0.4242 - val_loss: 0.1561\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 831us/step - loss: 0.4118 - val_loss: 0.1577\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 806us/step - loss: 0.1194 - val_loss: 0.0993\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.1207 - val_loss: 0.0992\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 798us/step - loss: 0.1277 - val_loss: 0.1099\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 805us/step - loss: 0.1251 - val_loss: 0.1022\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 7.5406 - val_loss: 1.3645\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 1s 835us/step - loss: 0.0901 - val_loss: 0.0824\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 814us/step - loss: 0.0906 - val_loss: 0.0835\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 805us/step - loss: 0.0996 - val_loss: 0.0904\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 804us/step - loss: 0.0894 - val_loss: 0.0789\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 803us/step - loss: 0.4022 - val_loss: 0.1402\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 964us/step - loss: 0.0763 - val_loss: 0.0712\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 945us/step - loss: 0.0781 - val_loss: 0.0723\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 928us/step - loss: 0.0821 - val_loss: 0.0741\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 926us/step - loss: 0.0740 - val_loss: 0.0702\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 928us/step - loss: 0.1151 - val_loss: 0.1007\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.0680 - val_loss: 0.0649\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 808us/step - loss: 0.0682 - val_loss: 0.0641\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 809us/step - loss: 0.0686 - val_loss: 0.0639\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 813us/step - loss: 0.0683 - val_loss: 0.0662\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 815us/step - loss: 0.0932 - val_loss: 0.0869\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 871us/step - loss: 0.0623 - val_loss: 0.0598\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 829us/step - loss: 0.0610 - val_loss: 0.0586\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 862us/step - loss: 0.0605 - val_loss: 0.0563\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 834us/step - loss: 0.0833 - val_loss: 0.0802\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 843us/step - loss: 0.0645 - val_loss: 0.0626\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 855us/step - loss: 0.0585 - val_loss: 0.0571\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 821us/step - loss: 0.0568 - val_loss: 0.0551\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 836us/step - loss: 0.0530 - val_loss: 0.0497\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 822us/step - loss: 0.0776 - val_loss: 0.0753\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 825us/step - loss: 0.0610 - val_loss: 0.0590\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 847us/step - loss: 0.0562 - val_loss: 0.0550\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 903us/step - loss: 0.0542 - val_loss: 0.0534\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 990us/step - loss: 0.0475 - val_loss: 0.0456\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 983us/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 979us/step - loss: 0.0574 - val_loss: 0.0555\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 956us/step - loss: 0.0542 - val_loss: 0.0531\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 829us/step - loss: 0.0528 - val_loss: 0.0521\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 799us/step - loss: 0.0443 - val_loss: 0.0432\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 782us/step - loss: 0.0693 - val_loss: 0.0677\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 786us/step - loss: 0.0538 - val_loss: 0.0518\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 818us/step - loss: 0.0522 - val_loss: 0.0511\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 800us/step - loss: 0.0515 - val_loss: 0.0508\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 800us/step - loss: 0.0662 - val_loss: 0.0648\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.0500 - val_loss: 0.0485\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 814us/step - loss: 0.0426 - val_loss: 0.0418\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 785us/step - loss: 0.0504 - val_loss: 0.0499\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.0502 - val_loss: 0.0491\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 783us/step - loss: 0.0636 - val_loss: 0.0624\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 783us/step - loss: 0.0480 - val_loss: 0.0471\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 792us/step - loss: 0.0412 - val_loss: 0.0402\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 835us/step - loss: 0.0495 - val_loss: 0.0490\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 862us/step - loss: 0.0483 - val_loss: 0.0472\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 831us/step - loss: 0.0613 - val_loss: 0.0601\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 844us/step - loss: 0.0466 - val_loss: 0.0458\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 862us/step - loss: 0.0393 - val_loss: 0.0379\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 782us/step - loss: 0.0486 - val_loss: 0.0482\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 776us/step - loss: 0.0592 - val_loss: 0.0581\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 799us/step - loss: 0.0463 - val_loss: 0.0452\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 788us/step - loss: 0.0452 - val_loss: 0.0443\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 800us/step - loss: 0.0372 - val_loss: 0.0367\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 787us/step - loss: 0.0478 - val_loss: 0.0474\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 784us/step - loss: 0.0573 - val_loss: 0.0564\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 782us/step - loss: 0.0438 - val_loss: 0.0430\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 800us/step - loss: 0.0444 - val_loss: 0.0434\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 790us/step - loss: 0.0365 - val_loss: 0.0362\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 787us/step - loss: 0.0471 - val_loss: 0.0468\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 789us/step - loss: 0.0556 - val_loss: 0.0547\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 788us/step - loss: 0.0422 - val_loss: 0.0410\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 801us/step - loss: 0.0425 - val_loss: 0.0413\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 796us/step - loss: 0.0360 - val_loss: 0.0356\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 856us/step - loss: 0.0464 - val_loss: 0.0461\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 859us/step - loss: 0.0538 - val_loss: 0.0527\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 866us/step - loss: 0.0399 - val_loss: 0.0390\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 885us/step - loss: 0.0400 - val_loss: 0.0387\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 880us/step - loss: 0.0355 - val_loss: 0.0352\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 820us/step - loss: 0.0459 - val_loss: 0.0456\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 814us/step - loss: 0.0514 - val_loss: 0.0500\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 813us/step - loss: 0.0388 - val_loss: 0.0385\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 833us/step - loss: 0.0385 - val_loss: 0.0382\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 821us/step - loss: 0.0350 - val_loss: 0.0347\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 785us/step - loss: 0.0453 - val_loss: 0.0450\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 785us/step - loss: 0.0490 - val_loss: 0.0486\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 782us/step - loss: 0.0384 - val_loss: 0.0381\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 803us/step - loss: 0.0380 - val_loss: 0.0378\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 789us/step - loss: 0.0345 - val_loss: 0.0344\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 782us/step - loss: 0.0448 - val_loss: 0.0446\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 784us/step - loss: 0.0481 - val_loss: 0.0478\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 786us/step - loss: 0.0379 - val_loss: 0.0377\n",
      "  78/1415 [>.............................] - ETA: 0s - loss: 0.0478Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 797us/step - loss: 0.0376 - val_loss: 0.0374\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 791us/step - loss: 0.0341 - val_loss: 0.0337\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 856us/step - loss: 0.0443 - val_loss: 0.0440\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 860us/step - loss: 0.0474 - val_loss: 0.0470\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 855us/step - loss: 0.0376 - val_loss: 0.0374\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 860us/step - loss: 0.0336 - val_loss: 0.0335\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 870us/step - loss: 0.0373 - val_loss: 0.0371\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 788us/step - loss: 0.0438 - val_loss: 0.0435\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 782us/step - loss: 0.0467 - val_loss: 0.0463\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 783us/step - loss: 0.0372 - val_loss: 0.0370\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 794us/step - loss: 0.0333 - val_loss: 0.0330\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 803us/step - loss: 0.0369 - val_loss: 0.0367\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 781us/step - loss: 0.0433 - val_loss: 0.0430\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 779us/step - loss: 0.0460 - val_loss: 0.0456\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 782us/step - loss: 0.0369 - val_loss: 0.0367\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 792us/step - loss: 0.0330 - val_loss: 0.0329\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 792us/step - loss: 0.0366 - val_loss: 0.0364\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 851us/step - loss: 0.0428 - val_loss: 0.0426\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 846us/step - loss: 0.0454 - val_loss: 0.0450\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 847us/step - loss: 0.0366 - val_loss: 0.0364\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 856us/step - loss: 0.0327 - val_loss: 0.0325\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 871us/step - loss: 0.0363 - val_loss: 0.0362\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 797us/step - loss: 0.0424 - val_loss: 0.0421\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 796us/step - loss: 0.0447 - val_loss: 0.0443\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 793us/step - loss: 0.0363 - val_loss: 0.0361\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 795us/step - loss: 0.0324 - val_loss: 0.0323\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 829us/step - loss: 0.0360 - val_loss: 0.0359\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 808us/step - loss: 0.0419 - val_loss: 0.0417\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 808us/step - loss: 0.0440 - val_loss: 0.0436\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 821us/step - loss: 0.0360 - val_loss: 0.0358\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 820us/step - loss: 0.0322 - val_loss: 0.0320\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 808us/step - loss: 0.0358 - val_loss: 0.0356\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 783us/step - loss: 0.0415 - val_loss: 0.0412\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 779us/step - loss: 0.0433 - val_loss: 0.0429\n",
      "1280/1415 [==========================>...] - ETA: 0s - loss: 0.0320Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 779us/step - loss: 0.0357 - val_loss: 0.0356\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 790us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 800us/step - loss: 0.0355 - val_loss: 0.0354\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 854us/step - loss: 0.0410 - val_loss: 0.0408\n",
      "1415/1415 [==============================] - 1s 853us/step - loss: 0.0426 - val_loss: 0.0421\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 847us/step - loss: 0.0355 - val_loss: 0.0353\n",
      "1415/1415 [==============================] - 1s 838us/step - loss: 0.0317 - val_loss: 0.0315\n",
      "1415/1415 [==============================] - 1s 844us/step - loss: 0.0353 - val_loss: 0.0351\n",
      "1415/1415 [==============================] - 1s 671us/step - loss: 0.0418 - val_loss: 0.0414\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 603us/step - loss: 0.0411 - val_loss: 0.0407\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1132/1132 - 2s - loss: 0.0407 - val_loss: 0.0405 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 2s - loss: 0.0353 - val_loss: 0.0351 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 2s - loss: 0.0315 - val_loss: 0.0314 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 2s - loss: 0.0350 - val_loss: 0.0349 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 0.0403 - val_loss: 0.0401 - 798ms/epoch - 705us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0351 - val_loss: 0.0350 - 905ms/epoch - 799us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0314 - val_loss: 0.0312 - 924ms/epoch - 816us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0349 - val_loss: 0.0348 - 939ms/epoch - 830us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0400 - val_loss: 0.0398 - 910ms/epoch - 804us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 2s - loss: 0.0404 - val_loss: 0.0401 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 0.0349 - val_loss: 0.0348 - 842ms/epoch - 744us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0312 - val_loss: 0.0311 - 848ms/epoch - 749us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0347 - val_loss: 0.0346 - 862ms/epoch - 761us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0396 - val_loss: 0.0394 - 857ms/epoch - 757us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0399 - val_loss: 0.0396 - 863ms/epoch - 762us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0347 - val_loss: 0.0346 - 855ms/epoch - 756us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0311 - val_loss: 0.0309 - 868ms/epoch - 767us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0345 - val_loss: 0.0344 - 875ms/epoch - 773us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0392 - val_loss: 0.0390 - 865ms/epoch - 764us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0395 - val_loss: 0.0392 - 863ms/epoch - 762us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0346 - val_loss: 0.0345 - 866ms/epoch - 765us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0309 - val_loss: 0.0308 - 876ms/epoch - 774us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0344 - val_loss: 0.0343 - 896ms/epoch - 791us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0388 - val_loss: 0.0386 - 857ms/epoch - 757us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0391 - val_loss: 0.0388 - 867ms/epoch - 766us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0344 - val_loss: 0.0343 - 877ms/epoch - 775us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0308 - val_loss: 0.0306 - 870ms/epoch - 768us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0342 - val_loss: 0.0341 - 879ms/epoch - 777us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0385 - val_loss: 0.0383 - 867ms/epoch - 766us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0386 - val_loss: 0.0383 - 866ms/epoch - 765us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0343 - val_loss: 0.0341 - 863ms/epoch - 763us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0306 - val_loss: 0.0305 - 870ms/epoch - 769us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0341 - val_loss: 0.0340 - 876ms/epoch - 774us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0382 - val_loss: 0.0380 - 848ms/epoch - 749us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0381 - val_loss: 0.0377 - 851ms/epoch - 752us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0341 - val_loss: 0.0340 - 855ms/epoch - 755us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0305 - val_loss: 0.0303 - 866ms/epoch - 765us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0339 - val_loss: 0.0338 - 877ms/epoch - 775us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0379 - val_loss: 0.0378 - 853ms/epoch - 753us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0374 - val_loss: 0.0369 - 852ms/epoch - 753us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0339 - val_loss: 0.0338 - 858ms/epoch - 758us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0304 - val_loss: 0.0302 - 869ms/epoch - 768us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0338 - val_loss: 0.0337 - 874ms/epoch - 772us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0376 - val_loss: 0.0375 - 852ms/epoch - 753us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0365 - val_loss: 0.0360 - 849ms/epoch - 750us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0338 - val_loss: 0.0337 - 856ms/epoch - 756us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0302 - val_loss: 0.0301 - 869ms/epoch - 768us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0336 - val_loss: 0.0336 - 873ms/epoch - 771us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0373 - val_loss: 0.0371 - 853ms/epoch - 753us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0353 - val_loss: 0.0344 - 852ms/epoch - 753us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0336 - val_loss: 0.0335 - 851ms/epoch - 752us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0301 - val_loss: 0.0300 - 955ms/epoch - 843us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0335 - val_loss: 0.0334 - 953ms/epoch - 842us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0370 - val_loss: 0.0368 - 933ms/epoch - 824us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0337 - val_loss: 0.0330 - 932ms/epoch - 823us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0335 - val_loss: 0.0334 - 936ms/epoch - 826us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0300 - val_loss: 0.0299 - 865ms/epoch - 764us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0334 - val_loss: 0.0333 - 886ms/epoch - 782us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0367 - val_loss: 0.0364 - 862ms/epoch - 762us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0329 - val_loss: 0.0327 - 859ms/epoch - 759us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0334 - val_loss: 0.0332 - 865ms/epoch - 764us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0299 - val_loss: 0.0297 - 872ms/epoch - 771us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0332 - val_loss: 0.0332 - 882ms/epoch - 779us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0363 - val_loss: 0.0360 - 866ms/epoch - 765us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0326 - val_loss: 0.0325 - 860ms/epoch - 760us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0332 - val_loss: 0.0331 - 858ms/epoch - 758us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0298 - val_loss: 0.0296 - 867ms/epoch - 766us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0331 - val_loss: 0.0330 - 874ms/epoch - 772us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0358 - val_loss: 0.0355 - 855ms/epoch - 755us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0325 - val_loss: 0.0323 - 859ms/epoch - 759us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0331 - val_loss: 0.0330 - 862ms/epoch - 762us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0297 - val_loss: 0.0295 - 877ms/epoch - 774us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0330 - val_loss: 0.0329 - 883ms/epoch - 780us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0351 - val_loss: 0.0347 - 866ms/epoch - 765us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0323 - val_loss: 0.0322 - 853ms/epoch - 753us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0329 - val_loss: 0.0328 - 887ms/epoch - 783us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0295 - val_loss: 0.0294 - 902ms/epoch - 796us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0329 - val_loss: 0.0328 - 906ms/epoch - 800us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0342 - val_loss: 0.0335 - 892ms/epoch - 788us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0322 - val_loss: 0.0321 - 895ms/epoch - 791us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0328 - val_loss: 0.0327 - 866ms/epoch - 765us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0294 - val_loss: 0.0293 - 876ms/epoch - 774us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0327 - val_loss: 0.0327 - 901ms/epoch - 796us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0328 - val_loss: 0.0321 - 865ms/epoch - 764us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0320 - val_loss: 0.0319 - 869ms/epoch - 767us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0327 - val_loss: 0.0326 - 869ms/epoch - 768us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0293 - val_loss: 0.0292 - 874ms/epoch - 772us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0326 - val_loss: 0.0326 - 873ms/epoch - 771us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0320 - val_loss: 0.0319 - 872ms/epoch - 770us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0319 - val_loss: 0.0318 - 861ms/epoch - 761us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0325 - val_loss: 0.0324 - 872ms/epoch - 770us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0292 - val_loss: 0.0291 - 977ms/epoch - 863us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0325 - val_loss: 0.0325 - 967ms/epoch - 854us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0318 - val_loss: 0.0317 - 957ms/epoch - 846us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0317 - val_loss: 0.0316 - 929ms/epoch - 821us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0324 - val_loss: 0.0323 - 959ms/epoch - 847us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0290 - 881ms/epoch - 778us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0324 - val_loss: 0.0324 - 893ms/epoch - 788us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0317 - val_loss: 0.0316 - 854ms/epoch - 755us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0316 - val_loss: 0.0314 - 870ms/epoch - 769us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0322 - val_loss: 0.0321 - 855ms/epoch - 755us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0290 - val_loss: 0.0289 - 866ms/epoch - 765us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0323 - val_loss: 0.0323 - 874ms/epoch - 772us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0315 - val_loss: 0.0315 - 853ms/epoch - 753us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0314 - val_loss: 0.0313 - 848ms/epoch - 749us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0321 - val_loss: 0.0320 - 857ms/epoch - 757us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0290 - val_loss: 0.0288 - 870ms/epoch - 768us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0322 - val_loss: 0.0322 - 883ms/epoch - 780us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0314 - val_loss: 0.0314 - 861ms/epoch - 761us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0312 - val_loss: 0.0310 - 861ms/epoch - 761us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0320 - val_loss: 0.0318 - 859ms/epoch - 759us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0289 - val_loss: 0.0287 - 863ms/epoch - 762us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0321 - val_loss: 0.0321 - 885ms/epoch - 782us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0313 - val_loss: 0.0313 - 864ms/epoch - 763us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0310 - val_loss: 0.0309 - 863ms/epoch - 762us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0318 - val_loss: 0.0317 - 879ms/epoch - 776us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0288 - val_loss: 0.0287 - 891ms/epoch - 787us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0312 - val_loss: 0.0312 - 862ms/epoch - 762us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0320 - val_loss: 0.0320 - 881ms/epoch - 778us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0308 - val_loss: 0.0306 - 873ms/epoch - 771us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0317 - val_loss: 0.0316 - 856ms/epoch - 756us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0286 - 918ms/epoch - 811us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0311 - val_loss: 0.0311 - 906ms/epoch - 800us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0319 - val_loss: 0.0319 - 925ms/epoch - 817us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0305 - val_loss: 0.0302 - 899ms/epoch - 794us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0315 - val_loss: 0.0315 - 914ms/epoch - 807us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0286 - val_loss: 0.0285 - 959ms/epoch - 848us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0310 - val_loss: 0.0311 - 943ms/epoch - 833us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0318 - val_loss: 0.0318 - 964ms/epoch - 852us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0299 - val_loss: 0.0293 - 944ms/epoch - 834us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0314 - val_loss: 0.0313 - 1s/epoch - 885us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0284 - 975ms/epoch - 861us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0309 - val_loss: 0.0310 - 964ms/epoch - 852us/step\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0282 - 963ms/epoch - 851us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0317 - val_loss: 0.0317 - 986ms/epoch - 871us/step\n",
      "Epoch 28/28\n",
      "283/283 - 0s - loss: 0.0309 - 191ms/epoch - 677us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time= 1.2min\n",
      "1132/1132 - 1s - loss: 0.0313 - val_loss: 0.0312 - 907ms/epoch - 802us/step\n",
      "283/283 - 0s - loss: 0.0312 - 175ms/epoch - 617us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time= 1.2min\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0283 - 827ms/epoch - 731us/step\n",
      "283/283 - 0s - loss: 0.0283 - 165ms/epoch - 583us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time= 1.2min\n",
      "1132/1132 - 1s - loss: 0.0282 - val_loss: 0.0280 - 788ms/epoch - 696us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0316 - val_loss: 0.0316 - 798ms/epoch - 705us/step\n",
      "283/283 - 0s - loss: 0.0316 - 156ms/epoch - 551us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time= 1.2min\n",
      "1132/1132 - 1s - loss: 0.0280 - val_loss: 0.0280 - 655ms/epoch - 579us/step\n",
      "283/283 - 0s - loss: 0.0279 - 145ms/epoch - 512us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time= 1.2min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "   1/5658 [..............................] - ETA: 19:45 - loss: 2.4409Epoch 1/28\n",
      " 134/5658 [..............................] - ETA: 4s - loss: 2.7963Epoch 1/28\n",
      "5658/5658 [==============================] - 9s 2ms/step - loss: 1.5829 - val_loss: 0.7607\n",
      "Epoch 2/28\n",
      "5658/5658 [==============================] - 9s 2ms/step - loss: 1.8681 - val_loss: 1.0734\n",
      "Epoch 2/28\n",
      "5658/5658 [==============================] - 9s 2ms/step - loss: 1.7783 - val_loss: 0.9014\n",
      "Epoch 2/28\n",
      "5658/5658 [==============================] - 9s 2ms/step - loss: 1.7790 - val_loss: 0.7850\n",
      "Epoch 2/28\n",
      "5658/5658 [==============================] - 9s 2ms/step - loss: 1.6099 - val_loss: 0.7824\n",
      "Epoch 2/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9712 - val_loss: 0.6710\n",
      "Epoch 3/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 1.0145 - val_loss: 0.8185\n",
      "Epoch 3/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9326 - val_loss: 0.7092\n",
      "Epoch 3/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9834 - val_loss: 1.0060\n",
      "Epoch 3/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 1.1122 - val_loss: 0.8570\n",
      "Epoch 3/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9687 - val_loss: 2.1920\n",
      "Epoch 4/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9755 - val_loss: 0.7295\n",
      "Epoch 4/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9647 - val_loss: 2.1715\n",
      "Epoch 4/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9757 - val_loss: 0.6960\n",
      "Epoch 4/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9095 - val_loss: 0.7502\n",
      "Epoch 4/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9441 - val_loss: 0.8950\n",
      "Epoch 5/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9348 - val_loss: 0.9324\n",
      "Epoch 5/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9215 - val_loss: 0.8923\n",
      "Epoch 5/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9414 - val_loss: 0.8116\n",
      "Epoch 5/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 1.0161 - val_loss: 1.1807\n",
      "Epoch 5/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8984 - val_loss: 0.6993\n",
      "Epoch 6/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9133 - val_loss: 0.6716\n",
      "Epoch 6/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9469 - val_loss: 0.8308\n",
      "Epoch 6/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9268 - val_loss: 0.8164\n",
      "Epoch 6/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9693 - val_loss: 0.7021\n",
      "Epoch 6/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9282 - val_loss: 0.7707\n",
      "Epoch 7/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9477 - val_loss: 0.9909\n",
      "Epoch 7/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9156 - val_loss: 0.6841\n",
      "Epoch 7/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9265 - val_loss: 0.8062\n",
      "Epoch 7/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9322 - val_loss: 0.9610\n",
      "Epoch 7/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9245 - val_loss: 0.6787\n",
      "Epoch 8/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9604 - val_loss: 1.6743\n",
      "Epoch 8/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9783 - val_loss: 0.6799\n",
      "Epoch 8/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9436 - val_loss: 0.7371\n",
      "Epoch 8/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9001 - val_loss: 0.7687\n",
      "Epoch 8/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9371 - val_loss: 1.1208\n",
      "Epoch 9/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9060 - val_loss: 0.6801\n",
      "Epoch 9/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9003 - val_loss: 0.6733\n",
      "Epoch 9/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9031 - val_loss: 0.7163\n",
      "Epoch 9/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9240 - val_loss: 1.0665\n",
      "Epoch 9/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9405 - val_loss: 1.2869\n",
      "Epoch 10/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9164 - val_loss: 1.3045\n",
      "Epoch 10/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8958 - val_loss: 0.7046\n",
      "Epoch 10/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9529 - val_loss: 0.6980\n",
      "Epoch 10/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9300 - val_loss: 1.1277\n",
      "Epoch 10/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9863 - val_loss: 0.6796\n",
      "Epoch 11/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9024 - val_loss: 1.0366\n",
      "Epoch 11/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9046 - val_loss: 0.6836\n",
      "Epoch 11/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9262 - val_loss: 1.1032\n",
      "Epoch 11/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9098 - val_loss: 0.6849\n",
      "Epoch 11/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9315 - val_loss: 1.0349\n",
      "Epoch 12/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9216 - val_loss: 0.8898\n",
      "Epoch 12/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9081 - val_loss: 0.7457\n",
      "Epoch 12/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9236 - val_loss: 0.6839\n",
      "Epoch 12/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9539 - val_loss: 0.7049\n",
      "Epoch 12/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9210 - val_loss: 0.7374\n",
      "Epoch 13/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9257 - val_loss: 0.7507\n",
      " 458/5658 [=>............................] - ETA: 4s - loss: 0.8653Epoch 13/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9867 - val_loss: 0.8139\n",
      "Epoch 13/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9495 - val_loss: 0.6723\n",
      "Epoch 13/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9063 - val_loss: 1.1412\n",
      "Epoch 13/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9188 - val_loss: 0.7088\n",
      "Epoch 14/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9117 - val_loss: 0.7502\n",
      "Epoch 14/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8805 - val_loss: 1.5192\n",
      "Epoch 14/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9262 - val_loss: 1.0397\n",
      "Epoch 14/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9142 - val_loss: 0.7523\n",
      "Epoch 14/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9457 - val_loss: 0.8170\n",
      "5462/5658 [===========================>..] - ETA: 0s - loss: 0.9113Epoch 15/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9247 - val_loss: 0.6718\n",
      "Epoch 15/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9947 - val_loss: 0.9171\n",
      "Epoch 15/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8840 - val_loss: 1.1860\n",
      "Epoch 15/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9194 - val_loss: 0.8677\n",
      "Epoch 15/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8940 - val_loss: 0.7304\n",
      "Epoch 16/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9200 - val_loss: 1.2416\n",
      "Epoch 16/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8968 - val_loss: 0.7843\n",
      "Epoch 16/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9161 - val_loss: 0.7622\n",
      "Epoch 16/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9009 - val_loss: 0.7015\n",
      "Epoch 16/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9774 - val_loss: 0.8856\n",
      "Epoch 17/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8968 - val_loss: 0.7541\n",
      "Epoch 17/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9136 - val_loss: 0.9284\n",
      "Epoch 17/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9113 - val_loss: 0.7427\n",
      "Epoch 17/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9211 - val_loss: 1.0375\n",
      "Epoch 17/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9112 - val_loss: 0.7488\n",
      "Epoch 18/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9396 - val_loss: 0.6965\n",
      "Epoch 18/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9096 - val_loss: 0.8842\n",
      "Epoch 18/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9119 - val_loss: 0.7103\n",
      "Epoch 18/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9003 - val_loss: 0.9272\n",
      "Epoch 18/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9182 - val_loss: 0.6865\n",
      "Epoch 19/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9323 - val_loss: 0.6822\n",
      "Epoch 19/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9267 - val_loss: 0.8100\n",
      "Epoch 19/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9110 - val_loss: 0.7067\n",
      "Epoch 19/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9344 - val_loss: 0.7920\n",
      "Epoch 19/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8992 - val_loss: 1.4516\n",
      "Epoch 20/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9380 - val_loss: 0.6716\n",
      "Epoch 20/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9213 - val_loss: 0.6928\n",
      "Epoch 20/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9541 - val_loss: 0.8575\n",
      "Epoch 20/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8619 - val_loss: 0.8367\n",
      "Epoch 20/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9360 - val_loss: 0.8943\n",
      "Epoch 21/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9106 - val_loss: 0.7262\n",
      "Epoch 21/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9546 - val_loss: 0.7867\n",
      "Epoch 21/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9048 - val_loss: 0.6726\n",
      "Epoch 21/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9502 - val_loss: 1.6948\n",
      "Epoch 21/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8875 - val_loss: 0.7756\n",
      "Epoch 22/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8955 - val_loss: 0.7334\n",
      "Epoch 22/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9669 - val_loss: 0.8361\n",
      "Epoch 22/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9158 - val_loss: 0.7104\n",
      "Epoch 22/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9213 - val_loss: 0.9522\n",
      "Epoch 22/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9328 - val_loss: 0.9738\n",
      "Epoch 23/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9274 - val_loss: 0.6890\n",
      "Epoch 23/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9321 - val_loss: 3.3521\n",
      "Epoch 23/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8956 - val_loss: 0.6752\n",
      "Epoch 23/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8964 - val_loss: 0.6916\n",
      "Epoch 23/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8928 - val_loss: 1.4020\n",
      "Epoch 24/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9276 - val_loss: 0.7094\n",
      "Epoch 24/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9110 - val_loss: 0.7419\n",
      "Epoch 24/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9246 - val_loss: 0.7846\n",
      "Epoch 24/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9163 - val_loss: 0.6983\n",
      "Epoch 24/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8963 - val_loss: 0.7083\n",
      "Epoch 25/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8923 - val_loss: 2.0215\n",
      "Epoch 25/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9548 - val_loss: 0.6764\n",
      " 108/5658 [..............................] - ETA: 5s - loss: 0.9109Epoch 25/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9167 - val_loss: 0.7507\n",
      " 480/5658 [=>............................] - ETA: 4s - loss: 0.9907Epoch 25/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8938 - val_loss: 0.7403\n",
      "Epoch 25/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9159 - val_loss: 0.8493\n",
      "Epoch 26/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8819 - val_loss: 0.6817\n",
      "Epoch 26/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9475 - val_loss: 2.7636\n",
      "Epoch 26/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9225 - val_loss: 1.2546\n",
      "Epoch 26/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8916 - val_loss: 0.6770\n",
      "Epoch 26/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8989 - val_loss: 0.7512\n",
      "Epoch 27/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9051 - val_loss: 0.6851\n",
      "Epoch 27/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9178 - val_loss: 0.7074\n",
      "Epoch 27/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8853 - val_loss: 0.8310\n",
      "Epoch 27/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9584 - val_loss: 1.4571\n",
      "Epoch 27/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9557 - val_loss: 0.8830\n",
      "Epoch 28/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9524 - val_loss: 0.8325\n",
      "Epoch 28/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9593 - val_loss: 0.7378\n",
      "Epoch 28/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9407 - val_loss: 1.2718\n",
      "Epoch 28/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9052 - val_loss: 0.6770\n",
      "Epoch 28/28\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.8986 - val_loss: 0.8542\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9075 - val_loss: 0.9107\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9023 - val_loss: 1.5172\n",
      "5658/5658 [==============================] - 7s 1ms/step - loss: 0.9256 - val_loss: 0.9470\n",
      "5658/5658 [==============================] - 6s 1ms/step - loss: 0.8988 - val_loss: 0.7729\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "4527/4527 - 6s - loss: 0.9344 - val_loss: 0.7779 - 6s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "4527/4527 - 7s - loss: 0.9620 - val_loss: 1.3291 - 7s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "4527/4527 - 7s - loss: 0.9094 - val_loss: 0.6914 - 7s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "4527/4527 - 7s - loss: 0.8907 - val_loss: 0.7570 - 7s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "4527/4527 - 7s - loss: 0.9620 - val_loss: 0.6774 - 7s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "4527/4527 - 5s - loss: 0.9281 - val_loss: 1.6176 - 5s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "4527/4527 - 5s - loss: 0.8554 - val_loss: 0.7246 - 5s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "4527/4527 - 5s - loss: 0.9057 - val_loss: 0.7381 - 5s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "4527/4527 - 5s - loss: 0.8999 - val_loss: 0.6777 - 5s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "4527/4527 - 5s - loss: 0.9216 - val_loss: 0.8838 - 5s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "4527/4527 - 5s - loss: 0.8996 - val_loss: 0.9716 - 5s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "4527/4527 - 5s - loss: 0.9185 - val_loss: 0.7769 - 5s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "4527/4527 - 5s - loss: 0.9249 - val_loss: 0.8469 - 5s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "4527/4527 - 5s - loss: 0.8798 - val_loss: 1.1253 - 5s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "4527/4527 - 5s - loss: 0.9016 - val_loss: 1.1449 - 5s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "4527/4527 - 5s - loss: 0.9436 - val_loss: 0.7203 - 5s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "4527/4527 - 5s - loss: 0.8984 - val_loss: 0.8984 - 5s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "4527/4527 - 5s - loss: 0.8975 - val_loss: 0.6972 - 5s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "4527/4527 - 5s - loss: 0.9115 - val_loss: 0.7300 - 5s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "4527/4527 - 5s - loss: 0.8712 - val_loss: 0.7235 - 5s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "4527/4527 - 5s - loss: 0.9297 - val_loss: 1.7994 - 5s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "4527/4527 - 5s - loss: 0.9135 - val_loss: 0.6724 - 5s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "4527/4527 - 5s - loss: 0.8858 - val_loss: 0.7519 - 5s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "4527/4527 - 5s - loss: 0.8858 - val_loss: 1.2927 - 5s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "4527/4527 - 5s - loss: 0.9329 - val_loss: 0.6963 - 5s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "4527/4527 - 5s - loss: 0.9207 - val_loss: 0.7133 - 5s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "4527/4527 - 5s - loss: 0.9633 - val_loss: 1.3546 - 5s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "4527/4527 - 5s - loss: 0.8509 - val_loss: 0.6766 - 5s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "4527/4527 - 5s - loss: 0.8752 - val_loss: 0.7552 - 5s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "4527/4527 - 5s - loss: 0.9398 - val_loss: 0.7436 - 5s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "4527/4527 - 5s - loss: 0.9142 - val_loss: 0.7015 - 5s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "4527/4527 - 5s - loss: 0.9220 - val_loss: 0.7286 - 5s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "4527/4527 - 5s - loss: 0.8836 - val_loss: 0.8112 - 5s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "4527/4527 - 5s - loss: 0.8759 - val_loss: 1.0158 - 5s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "4527/4527 - 5s - loss: 0.8879 - val_loss: 1.2671 - 5s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "4527/4527 - 5s - loss: 0.9009 - val_loss: 1.0884 - 5s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "4527/4527 - 5s - loss: 0.9431 - val_loss: 0.9255 - 5s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "4527/4527 - 5s - loss: 0.9232 - val_loss: 0.7211 - 5s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "4527/4527 - 5s - loss: 0.9413 - val_loss: 1.3434 - 5s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "4527/4527 - 5s - loss: 0.8961 - val_loss: 1.2707 - 5s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "4527/4527 - 5s - loss: 0.9285 - val_loss: 0.7512 - 5s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "4527/4527 - 5s - loss: 0.8670 - val_loss: 1.3261 - 5s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "4527/4527 - 5s - loss: 0.8838 - val_loss: 0.7933 - 5s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "4527/4527 - 5s - loss: 0.9125 - val_loss: 3.9074 - 5s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "4527/4527 - 5s - loss: 0.8834 - val_loss: 1.6079 - 5s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "4527/4527 - 5s - loss: 0.8661 - val_loss: 0.7737 - 5s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "4527/4527 - 5s - loss: 0.9000 - val_loss: 0.6867 - 5s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "4527/4527 - 5s - loss: 0.9019 - val_loss: 0.6750 - 5s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "4527/4527 - 5s - loss: 0.9262 - val_loss: 0.7105 - 5s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "4527/4527 - 5s - loss: 0.8936 - val_loss: 0.8379 - 5s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "4527/4527 - 5s - loss: 0.9086 - val_loss: 0.7955 - 5s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "4527/4527 - 5s - loss: 0.8656 - val_loss: 0.6844 - 5s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "4527/4527 - 5s - loss: 0.9563 - val_loss: 1.3955 - 5s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "4527/4527 - 5s - loss: 0.9383 - val_loss: 0.9664 - 5s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "4527/4527 - 5s - loss: 0.8859 - val_loss: 0.6878 - 5s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "4527/4527 - 5s - loss: 0.9208 - val_loss: 0.7026 - 5s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "4527/4527 - 5s - loss: 0.9261 - val_loss: 0.7430 - 5s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "4527/4527 - 5s - loss: 0.8819 - val_loss: 0.7314 - 5s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "4527/4527 - 5s - loss: 0.9126 - val_loss: 0.7275 - 5s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "4527/4527 - 5s - loss: 0.9500 - val_loss: 2.3595 - 5s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "4527/4527 - 5s - loss: 0.9162 - val_loss: 0.6912 - 5s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "4527/4527 - 5s - loss: 0.9184 - val_loss: 0.9206 - 5s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "4527/4527 - 5s - loss: 0.8898 - val_loss: 0.6906 - 5s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "4527/4527 - 5s - loss: 0.8610 - val_loss: 0.9666 - 5s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "4527/4527 - 5s - loss: 0.8789 - val_loss: 0.7320 - 5s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "4527/4527 - 5s - loss: 0.8817 - val_loss: 0.8119 - 5s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "4527/4527 - 5s - loss: 0.9140 - val_loss: 0.7108 - 5s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "4527/4527 - 5s - loss: 0.9301 - val_loss: 1.1235 - 5s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "4527/4527 - 5s - loss: 0.9194 - val_loss: 0.6859 - 5s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "4527/4527 - 5s - loss: 0.9388 - val_loss: 0.7287 - 5s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "4527/4527 - 5s - loss: 0.9185 - val_loss: 0.6780 - 5s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "4527/4527 - 5s - loss: 0.9005 - val_loss: 0.8982 - 5s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "4527/4527 - 5s - loss: 0.8870 - val_loss: 0.7188 - 5s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "4527/4527 - 5s - loss: 0.9201 - val_loss: 0.6925 - 5s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "4527/4527 - 5s - loss: 0.9228 - val_loss: 0.9173 - 5s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "4527/4527 - 5s - loss: 0.8768 - val_loss: 0.6711 - 5s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "4527/4527 - 5s - loss: 0.8815 - val_loss: 0.7419 - 5s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "4527/4527 - 5s - loss: 0.9669 - val_loss: 0.8496 - 5s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "4527/4527 - 5s - loss: 0.8838 - val_loss: 0.8104 - 5s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "4527/4527 - 5s - loss: 0.9403 - val_loss: 0.7090 - 5s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "4527/4527 - 5s - loss: 0.9506 - val_loss: 0.6945 - 5s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "4527/4527 - 5s - loss: 0.9250 - val_loss: 0.6926 - 5s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "4527/4527 - 5s - loss: 0.9175 - val_loss: 0.8844 - 5s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "4527/4527 - 5s - loss: 0.9758 - val_loss: 0.7938 - 5s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "4527/4527 - 5s - loss: 0.8940 - val_loss: 1.4717 - 5s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "4527/4527 - 5s - loss: 0.9151 - val_loss: 0.8897 - 5s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "4527/4527 - 5s - loss: 0.9277 - val_loss: 1.5160 - 5s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "4527/4527 - 5s - loss: 0.9074 - val_loss: 1.5136 - 5s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "4527/4527 - 5s - loss: 0.9010 - val_loss: 0.8243 - 5s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "4527/4527 - 5s - loss: 0.8946 - val_loss: 0.7239 - 5s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "4527/4527 - 5s - loss: 0.9297 - val_loss: 0.6735 - 5s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "4527/4527 - 5s - loss: 0.8910 - val_loss: 1.4626 - 5s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "4527/4527 - 5s - loss: 0.8843 - val_loss: 0.7749 - 5s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "4527/4527 - 5s - loss: 0.9518 - val_loss: 2.4283 - 5s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "4527/4527 - 5s - loss: 0.9078 - val_loss: 0.6943 - 5s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "4527/4527 - 5s - loss: 0.8741 - val_loss: 0.6760 - 5s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "4527/4527 - 5s - loss: 0.8753 - val_loss: 0.7636 - 5s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "4527/4527 - 5s - loss: 0.8864 - val_loss: 1.0707 - 5s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "4527/4527 - 5s - loss: 0.9398 - val_loss: 0.6778 - 5s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "4527/4527 - 5s - loss: 0.9176 - val_loss: 0.6722 - 5s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "4527/4527 - 5s - loss: 0.8878 - val_loss: 1.0626 - 5s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "4527/4527 - 5s - loss: 0.9151 - val_loss: 0.9792 - 5s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "4527/4527 - 5s - loss: 0.8987 - val_loss: 0.7568 - 5s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "4527/4527 - 5s - loss: 0.9384 - val_loss: 0.8227 - 5s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "4527/4527 - 5s - loss: 0.9190 - val_loss: 1.5071 - 5s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "4527/4527 - 5s - loss: 0.9248 - val_loss: 1.2646 - 5s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "4527/4527 - 5s - loss: 0.8718 - val_loss: 0.7178 - 5s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "4527/4527 - 5s - loss: 0.9098 - val_loss: 0.6917 - 5s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "4527/4527 - 5s - loss: 0.8746 - val_loss: 1.0122 - 5s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "4527/4527 - 5s - loss: 0.8997 - val_loss: 0.8876 - 5s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "4527/4527 - 5s - loss: 0.9234 - val_loss: 0.7014 - 5s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "4527/4527 - 5s - loss: 0.8935 - val_loss: 0.6749 - 5s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "4527/4527 - 5s - loss: 0.8944 - val_loss: 0.8590 - 5s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "4527/4527 - 5s - loss: 0.8665 - val_loss: 0.8942 - 5s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "4527/4527 - 6s - loss: 0.8995 - val_loss: 0.7238 - 6s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "4527/4527 - 5s - loss: 0.9360 - val_loss: 0.6996 - 5s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "4527/4527 - 5s - loss: 0.9042 - val_loss: 0.8628 - 5s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "4527/4527 - 5s - loss: 0.9361 - val_loss: 0.6738 - 5s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "4527/4527 - 5s - loss: 0.8921 - val_loss: 0.7967 - 5s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "4527/4527 - 5s - loss: 0.9462 - val_loss: 0.6732 - 5s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "4527/4527 - 5s - loss: 0.9170 - val_loss: 0.6904 - 5s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "4527/4527 - 5s - loss: 0.8659 - val_loss: 1.0187 - 5s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "4527/4527 - 5s - loss: 0.9311 - val_loss: 1.3466 - 5s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "4527/4527 - 5s - loss: 0.9192 - val_loss: 0.6715 - 5s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "4527/4527 - 5s - loss: 0.9028 - val_loss: 0.7213 - 5s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "4527/4527 - 5s - loss: 0.9646 - val_loss: 0.7134 - 5s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "4527/4527 - 5s - loss: 0.9195 - val_loss: 1.2425 - 5s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "4527/4527 - 5s - loss: 0.8848 - val_loss: 0.8447 - 5s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "4527/4527 - 5s - loss: 0.9014 - val_loss: 0.8185 - 5s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "4527/4527 - 5s - loss: 0.9260 - val_loss: 0.6712 - 5s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "4527/4527 - 5s - loss: 0.9872 - val_loss: 0.6731 - 5s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "4527/4527 - 5s - loss: 0.8775 - val_loss: 0.7114 - 5s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "4527/4527 - 5s - loss: 0.8929 - val_loss: 0.6955 - 5s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "4527/4527 - 5s - loss: 0.8799 - val_loss: 0.9413 - 5s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "4527/4527 - 5s - loss: 0.9158 - val_loss: 0.6830 - 5s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "4527/4527 - 5s - loss: 0.9157 - val_loss: 0.7006 - 5s/epoch - 1ms/step\n",
      "1132/1132 - 1s - loss: 0.5217 - 614ms/epoch - 543us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 5.9min\n",
      "4527/4527 - 5s - loss: 0.9113 - val_loss: 1.1010 - 5s/epoch - 1ms/step\n",
      "4527/4527 - 5s - loss: 0.8833 - val_loss: 0.6727 - 5s/epoch - 1ms/step\n",
      "1132/1132 - 1s - loss: 0.5493 - 525ms/epoch - 464us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 5.9min\n",
      "1132/1132 - 1s - loss: 0.4646 - 529ms/epoch - 467us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 5.9min\n",
      "4527/4527 - 5s - loss: 0.8644 - val_loss: 0.7021 - 5s/epoch - 1ms/step\n",
      "4527/4527 - 5s - loss: 0.8930 - val_loss: 1.0424 - 5s/epoch - 1ms/step\n",
      "1132/1132 - 0s - loss: 0.5126 - 401ms/epoch - 355us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 5.9min\n",
      "1132/1132 - 0s - loss: 0.9466 - 390ms/epoch - 345us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 5.9min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "  48/2829 [..............................] - ETA: 2s - loss: 5.0185  Epoch 1/28\n",
      "Epoch 1/28\n",
      "2829/2829 [==============================] - 5s 2ms/step - loss: 2.0712 - val_loss: 0.7244\n",
      "Epoch 2/28\n",
      "2829/2829 [==============================] - 5s 2ms/step - loss: 1.9095 - val_loss: 0.7548\n",
      "Epoch 2/28\n",
      "2829/2829 [==============================] - 6s 2ms/step - loss: 2.0892 - val_loss: 0.6708\n",
      "Epoch 2/28\n",
      "2829/2829 [==============================] - 5s 2ms/step - loss: 2.6734 - val_loss: 9.1193\n",
      "Epoch 2/28\n",
      "2829/2829 [==============================] - 5s 2ms/step - loss: 2.3928 - val_loss: 1.0419\n",
      "Epoch 2/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 1.1693 - val_loss: 0.7965\n",
      "Epoch 3/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 1.0785 - val_loss: 0.7282\n",
      "Epoch 3/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.9776 - val_loss: 0.6796\n",
      "Epoch 3/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8231 - val_loss: 0.7271\n",
      "Epoch 3/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8236 - val_loss: 0.8364\n",
      "Epoch 3/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8090 - val_loss: 1.1353\n",
      "Epoch 4/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8054 - val_loss: 0.7031\n",
      "Epoch 4/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8092 - val_loss: 0.7310\n",
      "Epoch 4/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8869 - val_loss: 0.6893\n",
      "Epoch 4/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9310 - val_loss: 1.8581\n",
      "Epoch 4/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9060 - val_loss: 0.8803\n",
      "Epoch 5/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9826 - val_loss: 0.8832\n",
      "Epoch 5/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 1.0160 - val_loss: 0.7285\n",
      "Epoch 5/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9659 - val_loss: 0.8067\n",
      "Epoch 5/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8674 - val_loss: 0.6738\n",
      "Epoch 5/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9490 - val_loss: 0.7359\n",
      "Epoch 6/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8058 - val_loss: 0.6711\n",
      "Epoch 6/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9049 - val_loss: 0.9233\n",
      "Epoch 6/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9371 - val_loss: 0.7871\n",
      "Epoch 6/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9791 - val_loss: 0.6884\n",
      "Epoch 6/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9299 - val_loss: 0.7070\n",
      "Epoch 7/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8781 - val_loss: 0.7722\n",
      "Epoch 7/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8151 - val_loss: 1.0118\n",
      "Epoch 7/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8517 - val_loss: 0.8994\n",
      "Epoch 7/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8143 - val_loss: 0.6712\n",
      "Epoch 7/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8205 - val_loss: 0.7096\n",
      "Epoch 8/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8391 - val_loss: 0.6795\n",
      "Epoch 8/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8753 - val_loss: 0.8468\n",
      "Epoch 8/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8910 - val_loss: 0.7025\n",
      "Epoch 8/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.7947 - val_loss: 0.9969\n",
      "Epoch 8/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8403 - val_loss: 0.6802\n",
      "Epoch 9/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.7956 - val_loss: 0.6710\n",
      "Epoch 9/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8343 - val_loss: 0.7801\n",
      "Epoch 9/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8915 - val_loss: 2.2113\n",
      "Epoch 9/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8628 - val_loss: 0.6714\n",
      "Epoch 9/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8520 - val_loss: 0.7123\n",
      "Epoch 10/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8145 - val_loss: 0.9789\n",
      "Epoch 10/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9022 - val_loss: 1.2334\n",
      "Epoch 10/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8263 - val_loss: 1.2178\n",
      "Epoch 10/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8243 - val_loss: 0.7129\n",
      "Epoch 10/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8066 - val_loss: 0.6736\n",
      "Epoch 11/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8177 - val_loss: 0.6891\n",
      "Epoch 11/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8818 - val_loss: 0.9654\n",
      "Epoch 11/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8158 - val_loss: 0.6977\n",
      "Epoch 11/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8649 - val_loss: 0.6905\n",
      "Epoch 11/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.7951 - val_loss: 0.9321\n",
      "Epoch 12/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8621 - val_loss: 0.9204\n",
      "Epoch 12/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8637 - val_loss: 0.7106\n",
      "Epoch 12/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8834 - val_loss: 0.9210\n",
      "Epoch 12/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8843 - val_loss: 0.8555\n",
      "Epoch 12/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8667 - val_loss: 0.7102\n",
      "Epoch 13/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8655 - val_loss: 1.6669\n",
      "Epoch 13/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8633 - val_loss: 0.6955\n",
      "Epoch 13/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8307 - val_loss: 0.8685\n",
      "Epoch 13/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8690 - val_loss: 1.0140\n",
      "Epoch 13/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8328 - val_loss: 0.6966\n",
      "Epoch 14/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8976 - val_loss: 0.6767\n",
      "Epoch 14/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8556 - val_loss: 0.7006\n",
      "Epoch 14/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8520 - val_loss: 0.7489\n",
      "Epoch 14/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8314 - val_loss: 0.6918\n",
      "Epoch 14/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8223 - val_loss: 0.6763\n",
      "Epoch 15/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8430 - val_loss: 0.9418\n",
      "Epoch 15/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8803 - val_loss: 0.7175\n",
      "Epoch 15/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8552 - val_loss: 0.7696\n",
      "Epoch 15/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8066 - val_loss: 0.6744\n",
      "Epoch 15/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8703 - val_loss: 0.7291\n",
      "Epoch 16/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8378 - val_loss: 0.6719\n",
      "Epoch 16/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8144 - val_loss: 0.8075\n",
      "Epoch 16/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8622 - val_loss: 1.4297\n",
      "Epoch 16/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8245 - val_loss: 0.7926\n",
      "Epoch 16/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8747 - val_loss: 0.6866\n",
      "Epoch 17/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8114 - val_loss: 0.8047\n",
      "2763/2829 [============================>.] - ETA: 0s - loss: 0.8385Epoch 17/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8390 - val_loss: 0.8675\n",
      "Epoch 17/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8375 - val_loss: 0.7821\n",
      "Epoch 17/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8683 - val_loss: 0.6737\n",
      "Epoch 17/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8548 - val_loss: 0.7883\n",
      "Epoch 18/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8652 - val_loss: 0.8417\n",
      "Epoch 18/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8403 - val_loss: 0.7114\n",
      "Epoch 18/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8350 - val_loss: 0.6957\n",
      "Epoch 18/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8529 - val_loss: 0.9170\n",
      "Epoch 18/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8441 - val_loss: 0.7583\n",
      "Epoch 19/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8788 - val_loss: 0.7451\n",
      "Epoch 19/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8255 - val_loss: 0.8269\n",
      "Epoch 19/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8999 - val_loss: 0.8967\n",
      "Epoch 19/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9073 - val_loss: 0.6796\n",
      "Epoch 19/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8639 - val_loss: 0.7408\n",
      "Epoch 20/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8266 - val_loss: 0.9426\n",
      "Epoch 20/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8139 - val_loss: 0.7690\n",
      "Epoch 20/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8348 - val_loss: 0.6899\n",
      "Epoch 20/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8453 - val_loss: 0.7969\n",
      "Epoch 20/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8166 - val_loss: 0.9002\n",
      "Epoch 21/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8280 - val_loss: 0.7288\n",
      "Epoch 21/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8250 - val_loss: 1.3851\n",
      "Epoch 21/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8286 - val_loss: 0.7244\n",
      "Epoch 21/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8731 - val_loss: 0.7142\n",
      "Epoch 21/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8663 - val_loss: 0.8975\n",
      "Epoch 22/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8606 - val_loss: 0.9621\n",
      "Epoch 22/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.7971 - val_loss: 1.0080\n",
      "Epoch 22/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8833 - val_loss: 0.6953\n",
      "Epoch 22/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8185 - val_loss: 0.6789\n",
      "Epoch 22/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8622 - val_loss: 0.9750\n",
      "Epoch 23/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8769 - val_loss: 0.7308\n",
      "Epoch 23/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8349 - val_loss: 0.7761\n",
      "Epoch 23/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8363 - val_loss: 0.8782\n",
      "Epoch 23/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9401 - val_loss: 0.6726\n",
      "Epoch 23/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8365 - val_loss: 0.6859\n",
      "Epoch 24/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8819 - val_loss: 0.8311\n",
      "Epoch 24/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8700 - val_loss: 0.6943\n",
      "Epoch 24/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8391 - val_loss: 0.7005\n",
      "Epoch 24/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8487 - val_loss: 1.0244\n",
      "Epoch 24/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8355 - val_loss: 0.8681\n",
      "Epoch 25/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8246 - val_loss: 1.0000\n",
      "Epoch 25/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8407 - val_loss: 1.3190\n",
      "Epoch 25/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8408 - val_loss: 0.7390\n",
      "Epoch 25/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8584 - val_loss: 0.6883\n",
      "Epoch 25/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8489 - val_loss: 0.7247\n",
      "Epoch 26/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8723 - val_loss: 0.6726\n",
      "Epoch 26/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8124 - val_loss: 0.6995\n",
      "Epoch 26/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8289 - val_loss: 0.8784\n",
      "Epoch 26/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8666 - val_loss: 1.2849\n",
      "Epoch 26/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8458 - val_loss: 0.7312\n",
      "Epoch 27/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.9040 - val_loss: 0.7488\n",
      "Epoch 27/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8544 - val_loss: 0.6899\n",
      "Epoch 27/28\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.9172 - val_loss: 0.6820\n",
      "Epoch 27/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8028 - val_loss: 0.7955\n",
      "Epoch 27/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8220 - val_loss: 0.7186\n",
      "Epoch 28/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8055 - val_loss: 0.7066\n",
      "2681/2829 [===========================>..] - ETA: 0s - loss: 0.8757Epoch 28/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8228 - val_loss: 1.2636\n",
      "Epoch 28/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8525 - val_loss: 0.6834\n",
      "Epoch 28/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8717 - val_loss: 0.7217\n",
      "Epoch 28/28\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8909 - val_loss: 0.6721\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8220 - val_loss: 0.6918\n",
      "2829/2829 [==============================] - 4s 1ms/step - loss: 0.8268 - val_loss: 0.6737\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8926 - val_loss: 0.8103\n",
      "2829/2829 [==============================] - 3s 1ms/step - loss: 0.8570 - val_loss: 1.4439\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "2264/2264 - 4s - loss: 0.8375 - val_loss: 1.0269 - 4s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "2264/2264 - 4s - loss: 0.8041 - val_loss: 0.6738 - 4s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "2264/2264 - 5s - loss: 0.8603 - val_loss: 0.7166 - 5s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "2264/2264 - 5s - loss: 0.8464 - val_loss: 0.7368 - 5s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "2264/2264 - 5s - loss: 0.8682 - val_loss: 0.7532 - 5s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "2264/2264 - 3s - loss: 0.8497 - val_loss: 0.7417 - 3s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "2264/2264 - 3s - loss: 0.8339 - val_loss: 1.4135 - 3s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "2264/2264 - 3s - loss: 0.8012 - val_loss: 0.7441 - 3s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "2264/2264 - 3s - loss: 0.8480 - val_loss: 0.6743 - 3s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "2264/2264 - 3s - loss: 0.8387 - val_loss: 0.7249 - 3s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "2264/2264 - 3s - loss: 0.8010 - val_loss: 0.9547 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "2264/2264 - 3s - loss: 0.8972 - val_loss: 0.9318 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "2264/2264 - 3s - loss: 0.8178 - val_loss: 0.7662 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "2264/2264 - 3s - loss: 0.9021 - val_loss: 0.6892 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "2264/2264 - 3s - loss: 0.8249 - val_loss: 0.8536 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "2264/2264 - 3s - loss: 0.8680 - val_loss: 0.6818 - 3s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "2264/2264 - 3s - loss: 0.8751 - val_loss: 0.6916 - 3s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "2264/2264 - 3s - loss: 0.8194 - val_loss: 0.6803 - 3s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "2264/2264 - 3s - loss: 0.8612 - val_loss: 0.7134 - 3s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "2264/2264 - 3s - loss: 0.8014 - val_loss: 1.3374 - 3s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "2264/2264 - 3s - loss: 0.8556 - val_loss: 0.7231 - 3s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "2264/2264 - 3s - loss: 0.8246 - val_loss: 0.7844 - 3s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "2264/2264 - 3s - loss: 0.8137 - val_loss: 0.8196 - 3s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "2264/2264 - 3s - loss: 0.8151 - val_loss: 0.6801 - 3s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "2264/2264 - 3s - loss: 0.8743 - val_loss: 0.8677 - 3s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "2264/2264 - 3s - loss: 0.7975 - val_loss: 0.8325 - 3s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "2264/2264 - 3s - loss: 0.8251 - val_loss: 0.7307 - 3s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "2264/2264 - 3s - loss: 0.8397 - val_loss: 0.8844 - 3s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "2264/2264 - 3s - loss: 0.8456 - val_loss: 1.2048 - 3s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "2264/2264 - 3s - loss: 0.8411 - val_loss: 0.8042 - 3s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "2264/2264 - 3s - loss: 0.8429 - val_loss: 0.8478 - 3s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "2264/2264 - 3s - loss: 0.8445 - val_loss: 0.8448 - 3s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "2264/2264 - 3s - loss: 0.8191 - val_loss: 0.9245 - 3s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "2264/2264 - 3s - loss: 0.8597 - val_loss: 0.6853 - 3s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "2264/2264 - 3s - loss: 0.8140 - val_loss: 0.6869 - 3s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "2264/2264 - 3s - loss: 0.8148 - val_loss: 1.1117 - 3s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "2264/2264 - 3s - loss: 0.8462 - val_loss: 0.7776 - 3s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "2264/2264 - 3s - loss: 0.8340 - val_loss: 0.7941 - 3s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "2264/2264 - 3s - loss: 0.8514 - val_loss: 1.3925 - 3s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "2264/2264 - 3s - loss: 0.8564 - val_loss: 0.6939 - 3s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "2264/2264 - 3s - loss: 0.8713 - val_loss: 0.6852 - 3s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "2264/2264 - 3s - loss: 0.8373 - val_loss: 0.8009 - 3s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "2264/2264 - 3s - loss: 0.8639 - val_loss: 0.7129 - 3s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "2264/2264 - 3s - loss: 0.8390 - val_loss: 0.7596 - 3s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "2264/2264 - 3s - loss: 0.9179 - val_loss: 0.6920 - 3s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "2264/2264 - 3s - loss: 0.8120 - val_loss: 0.8651 - 3s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "2264/2264 - 3s - loss: 0.8320 - val_loss: 0.6726 - 3s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "2264/2264 - 3s - loss: 0.8390 - val_loss: 0.6781 - 3s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "2264/2264 - 3s - loss: 0.7855 - val_loss: 0.8848 - 3s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "2264/2264 - 3s - loss: 0.7972 - val_loss: 0.8344 - 3s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "2264/2264 - 3s - loss: 0.8640 - val_loss: 1.6544 - 3s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "2264/2264 - 3s - loss: 0.9357 - val_loss: 0.7101 - 3s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "2264/2264 - 3s - loss: 0.8210 - val_loss: 1.0700 - 3s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "2264/2264 - 3s - loss: 0.8254 - val_loss: 0.7555 - 3s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "2264/2264 - 3s - loss: 0.8073 - val_loss: 0.8672 - 3s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "2264/2264 - 3s - loss: 0.8954 - val_loss: 1.1290 - 3s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "2264/2264 - 3s - loss: 0.8752 - val_loss: 1.1773 - 3s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "2264/2264 - 3s - loss: 0.8283 - val_loss: 0.6737 - 3s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "2264/2264 - 3s - loss: 0.8473 - val_loss: 1.0557 - 3s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "2264/2264 - 3s - loss: 0.8483 - val_loss: 1.1014 - 3s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "2264/2264 - 3s - loss: 0.8145 - val_loss: 0.6878 - 3s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "2264/2264 - 3s - loss: 0.8250 - val_loss: 0.6954 - 3s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "2264/2264 - 3s - loss: 0.8554 - val_loss: 0.9043 - 3s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "2264/2264 - 3s - loss: 0.8273 - val_loss: 1.1602 - 3s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "2264/2264 - 3s - loss: 0.8753 - val_loss: 0.7735 - 3s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "2264/2264 - 3s - loss: 0.8175 - val_loss: 1.5901 - 3s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "2264/2264 - 3s - loss: 0.8546 - val_loss: 0.7175 - 3s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "2264/2264 - 3s - loss: 0.8165 - val_loss: 0.7020 - 3s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "2264/2264 - 3s - loss: 0.8581 - val_loss: 0.6827 - 3s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "2264/2264 - 3s - loss: 0.8482 - val_loss: 0.9186 - 3s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "2264/2264 - 3s - loss: 0.8527 - val_loss: 0.7120 - 3s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "2264/2264 - 3s - loss: 0.8252 - val_loss: 0.6714 - 3s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "2264/2264 - 3s - loss: 0.8095 - val_loss: 0.8255 - 3s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "2264/2264 - 3s - loss: 0.7994 - val_loss: 0.8033 - 3s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "2264/2264 - 3s - loss: 0.8135 - val_loss: 1.3264 - 3s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "2264/2264 - 3s - loss: 0.8748 - val_loss: 0.6770 - 3s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "2264/2264 - 3s - loss: 0.8227 - val_loss: 0.7339 - 3s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "2264/2264 - 3s - loss: 0.8220 - val_loss: 0.6824 - 3s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "2264/2264 - 3s - loss: 0.8451 - val_loss: 1.1601 - 3s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "2264/2264 - 3s - loss: 0.9162 - val_loss: 0.6864 - 3s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "2264/2264 - 3s - loss: 0.8516 - val_loss: 0.9241 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "2264/2264 - 3s - loss: 0.8159 - val_loss: 0.6815 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "2264/2264 - 3s - loss: 0.8438 - val_loss: 0.6778 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "2264/2264 - 3s - loss: 0.8215 - val_loss: 0.9417 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "2264/2264 - 3s - loss: 0.8270 - val_loss: 0.6907 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "2264/2264 - 3s - loss: 0.8096 - val_loss: 0.7128 - 3s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "2264/2264 - 3s - loss: 0.8554 - val_loss: 0.6855 - 3s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "2264/2264 - 3s - loss: 0.8220 - val_loss: 1.2341 - 3s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "2264/2264 - 3s - loss: 0.8237 - val_loss: 0.6736 - 3s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "2264/2264 - 3s - loss: 0.8239 - val_loss: 0.7633 - 3s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "2264/2264 - 3s - loss: 0.9109 - val_loss: 0.7261 - 3s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "2264/2264 - 3s - loss: 0.8242 - val_loss: 0.7287 - 3s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "2264/2264 - 3s - loss: 0.7856 - val_loss: 0.7487 - 3s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "2264/2264 - 3s - loss: 0.8381 - val_loss: 0.6720 - 3s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "2264/2264 - 3s - loss: 0.8706 - val_loss: 1.0139 - 3s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "2264/2264 - 3s - loss: 0.8483 - val_loss: 0.6984 - 3s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "2264/2264 - 3s - loss: 0.8624 - val_loss: 2.1594 - 3s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "2264/2264 - 3s - loss: 0.8130 - val_loss: 1.0958 - 3s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "2264/2264 - 3s - loss: 0.8034 - val_loss: 0.8150 - 3s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "2264/2264 - 3s - loss: 0.9068 - val_loss: 1.2419 - 3s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "2264/2264 - 3s - loss: 0.8263 - val_loss: 0.7268 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "2264/2264 - 3s - loss: 0.8458 - val_loss: 0.8464 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "2264/2264 - 3s - loss: 0.8550 - val_loss: 2.0415 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "2264/2264 - 3s - loss: 0.8398 - val_loss: 1.2323 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "2264/2264 - 3s - loss: 0.8058 - val_loss: 0.9608 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "2264/2264 - 3s - loss: 0.8946 - val_loss: 0.8654 - 3s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "2264/2264 - 3s - loss: 0.7999 - val_loss: 0.6750 - 3s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "2264/2264 - 3s - loss: 0.8320 - val_loss: 0.6792 - 3s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "2264/2264 - 3s - loss: 0.8144 - val_loss: 0.7714 - 3s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "2264/2264 - 3s - loss: 0.8104 - val_loss: 0.7712 - 3s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "2264/2264 - 3s - loss: 0.8433 - val_loss: 0.6908 - 3s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "2264/2264 - 3s - loss: 0.8334 - val_loss: 0.8877 - 3s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "2264/2264 - 3s - loss: 0.7996 - val_loss: 0.9791 - 3s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "2264/2264 - 3s - loss: 0.8806 - val_loss: 0.6858 - 3s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "2264/2264 - 3s - loss: 0.7923 - val_loss: 1.3679 - 3s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "2264/2264 - 3s - loss: 0.8261 - val_loss: 0.9186 - 3s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "2264/2264 - 3s - loss: 0.8398 - val_loss: 0.8463 - 3s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "2264/2264 - 3s - loss: 0.8833 - val_loss: 0.6995 - 3s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "2264/2264 - 3s - loss: 0.7984 - val_loss: 0.7216 - 3s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "2264/2264 - 3s - loss: 0.7886 - val_loss: 0.7134 - 3s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "2264/2264 - 3s - loss: 0.9262 - val_loss: 0.7888 - 3s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "2264/2264 - 3s - loss: 0.8818 - val_loss: 1.4716 - 3s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "2264/2264 - 3s - loss: 0.8763 - val_loss: 1.0345 - 3s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "2264/2264 - 3s - loss: 0.8506 - val_loss: 0.7629 - 3s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "2264/2264 - 3s - loss: 0.8939 - val_loss: 1.2327 - 3s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "2264/2264 - 3s - loss: 0.8086 - val_loss: 1.1104 - 3s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "2264/2264 - 3s - loss: 0.7967 - val_loss: 0.9964 - 3s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "2264/2264 - 3s - loss: 0.8219 - val_loss: 0.6941 - 3s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "2264/2264 - 3s - loss: 0.8503 - val_loss: 0.6801 - 3s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "2264/2264 - 3s - loss: 0.8268 - val_loss: 0.7100 - 3s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "2264/2264 - 3s - loss: 0.8481 - val_loss: 0.8391 - 3s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "2264/2264 - 3s - loss: 0.8459 - val_loss: 0.7453 - 3s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "2264/2264 - 3s - loss: 0.8669 - val_loss: 0.6943 - 3s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "2264/2264 - 3s - loss: 0.8168 - val_loss: 0.9499 - 3s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "2264/2264 - 3s - loss: 0.8097 - val_loss: 0.7129 - 3s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "2264/2264 - 3s - loss: 0.8215 - val_loss: 0.9163 - 3s/epoch - 1ms/step\n",
      "2264/2264 - 3s - loss: 0.8199 - val_loss: 0.8342 - 3s/epoch - 1ms/step\n",
      "566/566 - 0s - loss: 0.6320 - 320ms/epoch - 565us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 3.2min\n",
      "2264/2264 - 3s - loss: 0.8473 - val_loss: 0.6805 - 3s/epoch - 1ms/step\n",
      "2264/2264 - 3s - loss: 0.8534 - val_loss: 0.7325 - 3s/epoch - 1ms/step\n",
      "566/566 - 0s - loss: 0.7139 - 282ms/epoch - 499us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 3.2min\n",
      "566/566 - 0s - loss: 0.4240 - 278ms/epoch - 491us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 3.2min\n",
      "566/566 - 0s - loss: 0.5877 - 261ms/epoch - 460us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 3.2min\n",
      "2264/2264 - 3s - loss: 0.8268 - val_loss: 0.7560 - 3s/epoch - 1ms/step\n",
      "566/566 - 0s - loss: 0.6075 - 215ms/epoch - 381us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 3.2min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "   1/1415 [..............................] - ETA: 4:50 - loss: 17.6281Epoch 1/28\n",
      "1359/1415 [===========================>..] - ETA: 0s - loss: 1.1611"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 16:24:20.230030: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 16:24:20.231053: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 1.1046 - val_loss: 0.1205\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 1.0890 - val_loss: 0.1140\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 1.1206 - val_loss: 0.1238\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1027 - val_loss: 0.0873\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0957 - val_loss: 0.0863\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1038 - val_loss: 0.0909\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 1.2069 - val_loss: 0.1166\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 1.1261 - val_loss: 0.1294\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0821 - val_loss: 0.0779\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0826 - val_loss: 0.0781\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0845 - val_loss: 0.0768\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1134 - val_loss: 0.0975\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0987 - val_loss: 0.0870\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0752 - val_loss: 0.0723\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0757 - val_loss: 0.0725\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0736 - val_loss: 0.0694\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0846 - val_loss: 0.0779\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0815 - val_loss: 0.0767\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0687 - val_loss: 0.0631\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0713 - val_loss: 0.0691\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0651 - val_loss: 0.0613\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0756 - val_loss: 0.0765\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0735 - val_loss: 0.0680\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0618 - val_loss: 0.0599\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0676 - val_loss: 0.0650\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0602 - val_loss: 0.0586\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0711 - val_loss: 0.0691\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0647 - val_loss: 0.0623\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0593 - val_loss: 0.0578\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0613 - val_loss: 0.0581\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0580 - val_loss: 0.0568\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0675 - val_loss: 0.0652\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0613 - val_loss: 0.0598\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0573 - val_loss: 0.0562\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0574 - val_loss: 0.0562\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0563 - val_loss: 0.0555\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0623 - val_loss: 0.0582\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0588 - val_loss: 0.0574\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0557 - val_loss: 0.0546\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0557 - val_loss: 0.0554\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0549 - val_loss: 0.0547\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0571 - val_loss: 0.0560\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0569 - val_loss: 0.0556\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0544 - val_loss: 0.0538\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0543 - val_loss: 0.0539\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0538 - val_loss: 0.0532\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0554 - val_loss: 0.0545\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0554 - val_loss: 0.0546\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0532 - val_loss: 0.0523\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0532 - val_loss: 0.0525\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0526 - val_loss: 0.0523\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0542 - val_loss: 0.0532\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0541 - val_loss: 0.0532\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0521 - val_loss: 0.0532\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0522 - val_loss: 0.0516\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0517 - val_loss: 0.0514\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0530 - val_loss: 0.0523\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0529 - val_loss: 0.0523\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0512 - val_loss: 0.0504\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0513 - val_loss: 0.0510\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0509 - val_loss: 0.0512\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0520 - val_loss: 0.0512\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0519 - val_loss: 0.0512\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0501 - val_loss: 0.0498\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0504 - val_loss: 0.0499\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0503 - val_loss: 0.0496\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0509 - val_loss: 0.0506\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0509 - val_loss: 0.0504\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0489 - val_loss: 0.0480\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0494 - val_loss: 0.0487\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0496 - val_loss: 0.0495\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0499 - val_loss: 0.0491\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0501 - val_loss: 0.0495\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0471 - val_loss: 0.0458\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0480 - val_loss: 0.0468\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0489 - val_loss: 0.0486\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0485 - val_loss: 0.0473\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0493 - val_loss: 0.0491\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0452 - val_loss: 0.0446\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0461 - val_loss: 0.0452\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0482 - val_loss: 0.0477\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0463 - val_loss: 0.0453\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0486 - val_loss: 0.0486\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0443 - val_loss: 0.0439\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0448 - val_loss: 0.0444\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0475 - val_loss: 0.0469\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0449 - val_loss: 0.0443\n",
      " 884/1415 [=================>............] - ETA: 0s - loss: 0.0464Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0477 - val_loss: 0.0471\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0437 - val_loss: 0.0433\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0441 - val_loss: 0.0438\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0461 - val_loss: 0.0451\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0443 - val_loss: 0.0440\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0465 - val_loss: 0.0454\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0433 - val_loss: 0.0429\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0437 - val_loss: 0.0434\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0441 - val_loss: 0.0433\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0438 - val_loss: 0.0433\n",
      "1126/1415 [======================>.......] - ETA: 0s - loss: 0.0429Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0447 - val_loss: 0.0438\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0429 - val_loss: 0.0427\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0432 - val_loss: 0.0431\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0431 - val_loss: 0.0428\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0434 - val_loss: 0.0430\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0434 - val_loss: 0.0427\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0425 - val_loss: 0.0423\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0429 - val_loss: 0.0427\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0426 - val_loss: 0.0422\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0429 - val_loss: 0.0426\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0428 - val_loss: 0.0425\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0422 - val_loss: 0.0424\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0425 - val_loss: 0.0426\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0422 - val_loss: 0.0419\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0425 - val_loss: 0.0428\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0424 - val_loss: 0.0419\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0418 - val_loss: 0.0418\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0422 - val_loss: 0.0423\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0418 - val_loss: 0.0415\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0421 - val_loss: 0.0418\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0421 - val_loss: 0.0418\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0414 - val_loss: 0.0411\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0418 - val_loss: 0.0417\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0414 - val_loss: 0.0411\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0417 - val_loss: 0.0413\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0418 - val_loss: 0.0416\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0409 - val_loss: 0.0406\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0413 - val_loss: 0.0409\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0410 - val_loss: 0.0406\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0413 - val_loss: 0.0408\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0415 - val_loss: 0.0412\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0406 - val_loss: 0.0403\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0409 - val_loss: 0.0405\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0406 - val_loss: 0.0405\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0409 - val_loss: 0.0408\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0412 - val_loss: 0.0409\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0403 - val_loss: 0.0400\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0406 - val_loss: 0.0403\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0403 - val_loss: 0.0400\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0406 - val_loss: 0.0404\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 985us/step - loss: 0.0409 - val_loss: 0.0410\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 994us/step - loss: 0.0403 - val_loss: 0.0401\n",
      "1415/1415 [==============================] - 1s 989us/step - loss: 0.0407 - val_loss: 0.0403\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1132/1132 - 3s - loss: 0.0400 - val_loss: 0.0399 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.0403 - val_loss: 0.0401 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.0401 - val_loss: 0.0399 - 3s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 0.0398 - val_loss: 0.0395 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0401 - val_loss: 0.0399 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0399 - val_loss: 0.0397 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0396 - val_loss: 0.0394 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 3s - loss: 0.0400 - val_loss: 0.0399 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 0.0399 - val_loss: 0.0396 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0396 - val_loss: 0.0396 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 3s - loss: 0.0404 - val_loss: 0.0404 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 2s - loss: 0.0394 - val_loss: 0.0391 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 2s - loss: 0.0398 - val_loss: 0.0397 - 2s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 2s - loss: 0.0395 - val_loss: 0.0393 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 2s - loss: 0.0397 - val_loss: 0.0399 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0402 - val_loss: 0.0400 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0393 - val_loss: 0.0391 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0396 - val_loss: 0.0394 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0393 - val_loss: 0.0392 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0395 - val_loss: 0.0393 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0400 - val_loss: 0.0397 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0391 - val_loss: 0.0390 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 2s - loss: 0.0394 - val_loss: 0.0396 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 2s - loss: 0.0393 - val_loss: 0.0392 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 2s - loss: 0.0391 - val_loss: 0.0389 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 2s - loss: 0.0398 - val_loss: 0.0396 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0389 - val_loss: 0.0390 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0392 - val_loss: 0.0390 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0391 - val_loss: 0.0391 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0389 - val_loss: 0.0388 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0396 - val_loss: 0.0393 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0388 - val_loss: 0.0388 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0390 - val_loss: 0.0387 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0388 - val_loss: 0.0386 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0389 - val_loss: 0.0387 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 2s - loss: 0.0393 - val_loss: 0.0389 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0386 - val_loss: 0.0383 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0389 - val_loss: 0.0388 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0386 - val_loss: 0.0383 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0387 - val_loss: 0.0387 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0390 - val_loss: 0.0387 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0384 - val_loss: 0.0385 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0387 - val_loss: 0.0387 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0384 - val_loss: 0.0384 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0386 - val_loss: 0.0390 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0388 - val_loss: 0.0389 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0383 - val_loss: 0.0382 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0385 - val_loss: 0.0382 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0383 - val_loss: 0.0382 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0385 - val_loss: 0.0383 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0387 - val_loss: 0.0385 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0381 - val_loss: 0.0379 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0383 - val_loss: 0.0380 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0382 - val_loss: 0.0379 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0383 - val_loss: 0.0383 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0385 - val_loss: 0.0382 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0380 - val_loss: 0.0380 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0382 - val_loss: 0.0383 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0382 - val_loss: 0.0383 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0380 - val_loss: 0.0380 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 2s - loss: 0.0384 - val_loss: 0.0381 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0379 - val_loss: 0.0377 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0381 - val_loss: 0.0379 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0380 - val_loss: 0.0379 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0379 - val_loss: 0.0376 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0382 - val_loss: 0.0380 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0378 - val_loss: 0.0377 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0380 - val_loss: 0.0381 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0379 - val_loss: 0.0377 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0377 - val_loss: 0.0376 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 2s - loss: 0.0381 - val_loss: 0.0378 - 2s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 2s - loss: 0.0376 - val_loss: 0.0374 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 2s - loss: 0.0377 - val_loss: 0.0376 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 2s - loss: 0.0378 - val_loss: 0.0377 - 2s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 2s - loss: 0.0376 - val_loss: 0.0375 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0379 - val_loss: 0.0377 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0375 - val_loss: 0.0372 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0376 - val_loss: 0.0375 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0375 - val_loss: 0.0373 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0377 - val_loss: 0.0377 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0378 - val_loss: 0.0375 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 2s - loss: 0.0374 - val_loss: 0.0372 - 2s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0374 - val_loss: 0.0374 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 2s - loss: 0.0374 - val_loss: 0.0372 - 2s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 2s - loss: 0.0376 - val_loss: 0.0373 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0377 - val_loss: 0.0376 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0373 - val_loss: 0.0371 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0373 - val_loss: 0.0374 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0373 - val_loss: 0.0370 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0375 - val_loss: 0.0372 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 2s - loss: 0.0375 - val_loss: 0.0372 - 2s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0371 - val_loss: 0.0372 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 2s - loss: 0.0371 - val_loss: 0.0370 - 2s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 2s - loss: 0.0372 - val_loss: 0.0370 - 2s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0374 - val_loss: 0.0372 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0374 - val_loss: 0.0371 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0370 - val_loss: 0.0369 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0370 - val_loss: 0.0375 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0370 - val_loss: 0.0369 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0372 - val_loss: 0.0371 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0373 - val_loss: 0.0373 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 2s - loss: 0.0369 - val_loss: 0.0370 - 2s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 2s - loss: 0.0369 - val_loss: 0.0367 - 2s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 2s - loss: 0.0369 - val_loss: 0.0369 - 2s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 2s - loss: 0.0371 - val_loss: 0.0370 - 2s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0372 - val_loss: 0.0370 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0368 - val_loss: 0.0366 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0368 - val_loss: 0.0367 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0368 - val_loss: 0.0366 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0370 - val_loss: 0.0369 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 2s - loss: 0.0371 - val_loss: 0.0370 - 2s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0367 - val_loss: 0.0364 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0367 - val_loss: 0.0368 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0367 - val_loss: 0.0363 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0369 - val_loss: 0.0370 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0370 - val_loss: 0.0368 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0366 - val_loss: 0.0365 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0366 - val_loss: 0.0366 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0368 - val_loss: 0.0372 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 2s - loss: 0.0365 - val_loss: 0.0367 - 2s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0369 - val_loss: 0.0365 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 2s - loss: 0.0365 - val_loss: 0.0365 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 2s - loss: 0.0365 - val_loss: 0.0365 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0364 - val_loss: 0.0364 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 2s - loss: 0.0367 - val_loss: 0.0364 - 2s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0368 - val_loss: 0.0365 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0364 - val_loss: 0.0366 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0364 - val_loss: 0.0364 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 2s - loss: 0.0363 - val_loss: 0.0363 - 2s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 2s - loss: 0.0366 - val_loss: 0.0365 - 2s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 2s - loss: 0.0367 - val_loss: 0.0367 - 2s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0363 - val_loss: 0.0361 - 1s/epoch - 1ms/step\n",
      "1132/1132 - 2s - loss: 0.0363 - val_loss: 0.0362 - 2s/epoch - 1ms/step\n",
      "1132/1132 - 1s - loss: 0.0362 - val_loss: 0.0362 - 1s/epoch - 1ms/step\n",
      "1132/1132 - 1s - loss: 0.0365 - val_loss: 0.0364 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "283/283 - 0s - loss: 0.0361 - 199ms/epoch - 704us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "283/283 - 0s - loss: 0.0361 - 194ms/epoch - 684us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "1132/1132 - 1s - loss: 0.0366 - val_loss: 0.0364 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "283/283 - 0s - loss: 0.0361 - 178ms/epoch - 630us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "1132/1132 - 1s - loss: 0.0363 - val_loss: 0.0365 - 1s/epoch - 902us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0365 - val_loss: 0.0365 - 1s/epoch - 894us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0363 - val_loss: 0.0361 - 994ms/epoch - 878us/step\n",
      "1132/1132 - 1s - loss: 0.0364 - val_loss: 0.0361 - 991ms/epoch - 876us/step\n",
      "283/283 - 0s - loss: 0.0361 - 152ms/epoch - 536us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "283/283 - 0s - loss: 0.0359 - 161ms/epoch - 569us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "   1/1415 [..............................] - ETA: 4:44 - loss: 2.7766Epoch 1/28\n",
      "1378/1415 [============================>.] - ETA: 0s - loss: 2.0773"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 16:26:08.527443: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 2.1002 - val_loss: 2.5436\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 2.3113 - val_loss: 3.3037\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 2.0861 - val_loss: 2.1858\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 2.0658 - val_loss: 1.0888\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.7878 - val_loss: 0.7091\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.1849 - val_loss: 0.6724\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.3040 - val_loss: 0.7961\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 1.7825 - val_loss: 0.7350\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 1.8143 - val_loss: 2.6892\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7453 - val_loss: 0.6978\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7368 - val_loss: 0.7341\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7365 - val_loss: 0.6707\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7438 - val_loss: 0.6918\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 2.2151 - val_loss: 1.3790\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7293 - val_loss: 0.7649\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7427 - val_loss: 0.6829\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7382 - val_loss: 0.8621\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7514 - val_loss: 0.7849\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7518 - val_loss: 0.8402\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9422 - val_loss: 0.7563\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7576 - val_loss: 0.6856\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7670 - val_loss: 0.6713\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7666 - val_loss: 0.9508\n",
      "1030/1415 [====================>.........] - ETA: 0s - loss: 0.7375Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7461 - val_loss: 0.6965\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7409 - val_loss: 0.6734\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8243 - val_loss: 0.7251\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 1.1039 - val_loss: 0.6918\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7390 - val_loss: 0.7181\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8061 - val_loss: 0.6962\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8175 - val_loss: 0.7265\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8076 - val_loss: 0.8245\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8283 - val_loss: 1.1777\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9900 - val_loss: 0.6712\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8204 - val_loss: 0.8814\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8509 - val_loss: 0.7085\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 1.2200 - val_loss: 1.0768\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8695 - val_loss: 0.7545\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8058 - val_loss: 0.7136\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8252 - val_loss: 0.6735\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8903 - val_loss: 1.1096\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8749 - val_loss: 0.7827\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8509 - val_loss: 0.7340\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 1.0487 - val_loss: 0.6967\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8907 - val_loss: 0.6936\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8996 - val_loss: 1.1233\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8370 - val_loss: 0.6899\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8134 - val_loss: 0.7501\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8396 - val_loss: 0.7463\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8687 - val_loss: 0.6746\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8459 - val_loss: 0.7548\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8069 - val_loss: 0.7289\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7972 - val_loss: 0.7496\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7708 - val_loss: 0.8411\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8922 - val_loss: 0.9798\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7650 - val_loss: 0.6779\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8001 - val_loss: 0.7134\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7679 - val_loss: 0.7040\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8129 - val_loss: 1.6387\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7987 - val_loss: 0.7547\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7495 - val_loss: 0.8799\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7803 - val_loss: 1.0679\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8563 - val_loss: 0.8264\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8311 - val_loss: 0.7014\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7785 - val_loss: 0.9249\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7926 - val_loss: 1.1722\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7808 - val_loss: 0.7015\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7516 - val_loss: 0.6787\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7565 - val_loss: 0.9546\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8437 - val_loss: 0.6714\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8116 - val_loss: 0.7490\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8025 - val_loss: 0.8932\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8081 - val_loss: 0.6801\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8271 - val_loss: 0.6970\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7756 - val_loss: 1.1206\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8625 - val_loss: 0.7558\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7755 - val_loss: 0.7064\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7801 - val_loss: 0.7153\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8423 - val_loss: 0.6718\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7821 - val_loss: 0.9351\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7518 - val_loss: 0.8109\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7713 - val_loss: 0.8058\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7815 - val_loss: 0.8198\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8223 - val_loss: 0.7578\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8432 - val_loss: 0.7435\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7662 - val_loss: 0.8241\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7811 - val_loss: 0.6720\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7803 - val_loss: 0.6785\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8060 - val_loss: 0.9256\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7823 - val_loss: 0.8446\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7727 - val_loss: 0.6947\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7885 - val_loss: 0.6792\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8093 - val_loss: 0.7705\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8473 - val_loss: 1.0405\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7981 - val_loss: 0.8534\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7881 - val_loss: 0.7123\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7855 - val_loss: 0.6743\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7554 - val_loss: 0.7276\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7525 - val_loss: 0.7637\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7825 - val_loss: 0.6834\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8112 - val_loss: 0.7523\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7338 - val_loss: 1.2469\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7422 - val_loss: 0.7444\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7669 - val_loss: 0.6891\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7912 - val_loss: 0.9891\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7769 - val_loss: 0.6819\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7805 - val_loss: 0.9574\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7628 - val_loss: 0.8494\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8303 - val_loss: 0.7354\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8797 - val_loss: 0.7935\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8324 - val_loss: 0.9288\n",
      "1078/1415 [=====================>........] - ETA: 0s - loss: 0.7364Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8663 - val_loss: 1.0155\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8108 - val_loss: 0.6866\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7728 - val_loss: 1.2556\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7470 - val_loss: 0.6711\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8338 - val_loss: 0.8153\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8964 - val_loss: 0.6767\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8192 - val_loss: 0.6969\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8433 - val_loss: 0.6993\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7984 - val_loss: 0.6849\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8367 - val_loss: 0.6722\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8235 - val_loss: 0.6918\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7533 - val_loss: 1.1461\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7684 - val_loss: 0.7311\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8021 - val_loss: 0.7229\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9018 - val_loss: 0.7327\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7683 - val_loss: 0.6723\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7980 - val_loss: 1.0647\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7956 - val_loss: 0.7289\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7844 - val_loss: 1.0454\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8045 - val_loss: 0.7101\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7746 - val_loss: 0.6891\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7552 - val_loss: 0.7374\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7875 - val_loss: 0.6954\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7370 - val_loss: 0.6748\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7854 - val_loss: 0.7121\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7855 - val_loss: 0.6849\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.7713 - val_loss: 1.2429\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8044 - val_loss: 0.8562\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.8132 - val_loss: 0.7555\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 850us/step - loss: 0.7611 - val_loss: 0.7538\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1132/1132 - 3s - loss: 0.7516 - val_loss: 0.8126 - 3s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.8343 - val_loss: 0.6713 - 3s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.7797 - val_loss: 0.6950 - 3s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.8161 - val_loss: 0.7139 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 0.7567 - val_loss: 0.7409 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.8219 - val_loss: 0.7899 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.7415 - val_loss: 0.8300 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 3s - loss: 0.7806 - val_loss: 0.6712 - 3s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 0.7556 - val_loss: 1.0132 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.9070 - val_loss: 0.7161 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.8173 - val_loss: 0.6843 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.7645 - val_loss: 0.8124 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.7923 - val_loss: 0.7094 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.7751 - val_loss: 0.7562 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.7887 - val_loss: 0.6770 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.8005 - val_loss: 0.7843 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.8001 - val_loss: 0.8695 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.7777 - val_loss: 0.8860 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.7644 - val_loss: 0.6833 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.7856 - val_loss: 0.6861 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.7642 - val_loss: 0.6887 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.7539 - val_loss: 0.7044 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.7673 - val_loss: 0.7605 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 2s - loss: 0.7733 - val_loss: 0.6745 - 2s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.7665 - val_loss: 0.6726 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.7840 - val_loss: 0.7470 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.7735 - val_loss: 0.6710 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.7994 - val_loss: 0.6922 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.8514 - val_loss: 0.7806 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.7320 - val_loss: 0.6771 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.8632 - val_loss: 0.6841 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.7832 - val_loss: 0.6724 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.7703 - val_loss: 0.7286 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.7506 - val_loss: 0.9381 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.7750 - val_loss: 0.6950 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.7509 - val_loss: 0.7226 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.7939 - val_loss: 0.7409 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.8736 - val_loss: 0.8180 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.7987 - val_loss: 1.0397 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.7907 - val_loss: 0.9963 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.8469 - val_loss: 0.6999 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.8636 - val_loss: 0.7395 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.8367 - val_loss: 0.6749 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.8222 - val_loss: 0.6880 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.8137 - val_loss: 1.8076 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.8213 - val_loss: 0.8824 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.7557 - val_loss: 0.6927 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.7672 - val_loss: 0.6866 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.7847 - val_loss: 0.6864 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.7597 - val_loss: 0.8676 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.8172 - val_loss: 0.7018 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.8398 - val_loss: 1.9494 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.8202 - val_loss: 0.7496 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.7702 - val_loss: 0.6745 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.9665 - val_loss: 1.8658 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.7318 - val_loss: 0.7039 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.7605 - val_loss: 0.8740 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.7670 - val_loss: 0.7074 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.8029 - val_loss: 0.8993 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.7882 - val_loss: 0.7326 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.8610 - val_loss: 0.8437 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.7431 - val_loss: 0.6722 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.8148 - val_loss: 0.8279 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.7604 - val_loss: 0.8477 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.8147 - val_loss: 0.8350 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.7877 - val_loss: 0.8320 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.7779 - val_loss: 1.0612 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.8202 - val_loss: 2.6997 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.8255 - val_loss: 0.7197 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.7990 - val_loss: 0.9844 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.7530 - val_loss: 0.7388 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.7664 - val_loss: 1.3653 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.9337 - val_loss: 0.7641 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.7455 - val_loss: 0.6720 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.7968 - val_loss: 0.6717 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.7967 - val_loss: 0.6713 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.8363 - val_loss: 0.7062 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.8090 - val_loss: 0.8379 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.7601 - val_loss: 0.6970 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.7889 - val_loss: 0.7233 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.8186 - val_loss: 0.9210 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 2s - loss: 0.7852 - val_loss: 0.9817 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 2s - loss: 0.8035 - val_loss: 0.7126 - 2s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 2s - loss: 0.8267 - val_loss: 0.6791 - 2s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.7605 - val_loss: 0.6738 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.7634 - val_loss: 0.6921 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.8109 - val_loss: 0.6959 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.7573 - val_loss: 0.6759 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.7661 - val_loss: 0.6722 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.7793 - val_loss: 0.6792 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.7588 - val_loss: 0.7254 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.7762 - val_loss: 0.6899 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.7645 - val_loss: 0.9141 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.7823 - val_loss: 0.7610 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.8002 - val_loss: 0.7391 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.7526 - val_loss: 0.7838 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.8930 - val_loss: 0.8094 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.7959 - val_loss: 0.7386 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.8092 - val_loss: 0.8111 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.7594 - val_loss: 0.7539 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.7714 - val_loss: 0.7930 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.7893 - val_loss: 0.8267 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.7503 - val_loss: 0.6861 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.8636 - val_loss: 0.8352 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.8388 - val_loss: 0.8193 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.7504 - val_loss: 1.1543 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.8477 - val_loss: 0.7791 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.9531 - val_loss: 3.0119 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.8356 - val_loss: 0.7046 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.7568 - val_loss: 0.8073 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.7687 - val_loss: 0.9742 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.8088 - val_loss: 0.7039 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.8035 - val_loss: 0.7261 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.7801 - val_loss: 0.7211 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.7734 - val_loss: 1.1602 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.8164 - val_loss: 0.7277 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.8174 - val_loss: 0.6734 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.7868 - val_loss: 0.8475 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.9220 - val_loss: 0.7166 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.8362 - val_loss: 0.6903 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.7923 - val_loss: 0.7169 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.8098 - val_loss: 0.6754 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 2s - loss: 0.8309 - val_loss: 0.7276 - 2s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 2s - loss: 0.7627 - val_loss: 0.6721 - 2s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.7719 - val_loss: 0.6885 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.7666 - val_loss: 0.6840 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.7605 - val_loss: 0.8973 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.8034 - val_loss: 0.7223 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.8156 - val_loss: 0.7675 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.8526 - val_loss: 0.6822 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.8333 - val_loss: 0.6773 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.7905 - val_loss: 0.7572 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.7478 - val_loss: 0.7568 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.7860 - val_loss: 0.7238 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.7570 - val_loss: 0.7179 - 1s/epoch - 1ms/step\n",
      "283/283 - 0s - loss: 0.4051 - 206ms/epoch - 729us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.8min\n",
      "1132/1132 - 1s - loss: 0.7303 - val_loss: 0.7714 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.8173 - val_loss: 1.4149 - 1s/epoch - 1ms/step\n",
      "283/283 - 0s - loss: 0.9112 - 187ms/epoch - 661us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.8min\n",
      "1132/1132 - 1s - loss: 0.7587 - val_loss: 0.7053 - 1s/epoch - 1ms/step\n",
      "1132/1132 - 1s - loss: 0.7706 - val_loss: 0.6779 - 1s/epoch - 1ms/step\n",
      "283/283 - 0s - loss: 0.4342 - 174ms/epoch - 613us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.8min\n",
      "283/283 - 0s - loss: 0.5425 - 178ms/epoch - 630us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.8min\n",
      "1132/1132 - 1s - loss: 0.8157 - val_loss: 0.7951 - 993ms/epoch - 877us/step\n",
      "283/283 - 0s - loss: 0.6650 - 144ms/epoch - 508us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.7min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "   1/1415 [..............................] - ETA: 4:17 - loss: 3.0321Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 1.6299 - val_loss: 1.2116\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 1.6221 - val_loss: 1.2031\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 1.6725 - val_loss: 1.2409\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 1.6426 - val_loss: 1.2193\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 1.6252 - val_loss: 1.2051\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 1s 804us/step - loss: 0.9288 - val_loss: 0.6942\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 803us/step - loss: 0.9223 - val_loss: 0.6894\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 798us/step - loss: 0.9512 - val_loss: 0.7110\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 793us/step - loss: 0.9346 - val_loss: 0.6986\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 798us/step - loss: 0.9238 - val_loss: 0.6906\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 817us/step - loss: 0.5343 - val_loss: 0.4015\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 819us/step - loss: 0.5308 - val_loss: 0.3990\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 806us/step - loss: 0.5472 - val_loss: 0.4113\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 806us/step - loss: 0.5377 - val_loss: 0.4041\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 817us/step - loss: 0.5316 - val_loss: 0.3996\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 792us/step - loss: 0.3110 - val_loss: 0.2357\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 794us/step - loss: 0.3092 - val_loss: 0.2344\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 778us/step - loss: 0.3183 - val_loss: 0.2411\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 785us/step - loss: 0.3129 - val_loss: 0.2371\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 791us/step - loss: 0.3096 - val_loss: 0.2346\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 870us/step - loss: 0.1843 - val_loss: 0.1415\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 866us/step - loss: 0.1835 - val_loss: 0.1410\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 861us/step - loss: 0.1885 - val_loss: 0.1445\n",
      "   1/1415 [..............................] - ETA: 0s - loss: 0.1394Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 909us/step - loss: 0.1854 - val_loss: 0.1424\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 921us/step - loss: 0.1835 - val_loss: 0.1409\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 836us/step - loss: 0.1146 - val_loss: 0.0895\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 843us/step - loss: 0.1122 - val_loss: 0.0878\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 843us/step - loss: 0.1120 - val_loss: 0.0877\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 784us/step - loss: 0.1130 - val_loss: 0.0884\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 791us/step - loss: 0.1118 - val_loss: 0.0875\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 787us/step - loss: 0.0725 - val_loss: 0.0581\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 800us/step - loss: 0.0712 - val_loss: 0.0573\n",
      "1415/1415 [==============================] - 1s 800us/step - loss: 0.0711 - val_loss: 0.0571\n",
      "Epoch 8/28\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 789us/step - loss: 0.0716 - val_loss: 0.0575\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 796us/step - loss: 0.0709 - val_loss: 0.0570\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 797us/step - loss: 0.0484 - val_loss: 0.0401\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 806us/step - loss: 0.0476 - val_loss: 0.0395\n",
      "1415/1415 [==============================] - 1s 797us/step - loss: 0.0480 - val_loss: 0.0398\n",
      "Epoch 9/28\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 806us/step - loss: 0.0478 - val_loss: 0.0398\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 858us/step - loss: 0.0475 - val_loss: 0.0395\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 846us/step - loss: 0.0345 - val_loss: 0.0297\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 846us/step - loss: 0.0341 - val_loss: 0.0294\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 847us/step - loss: 0.0344 - val_loss: 0.0296\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 853us/step - loss: 0.0344 - val_loss: 0.0296\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 793us/step - loss: 0.0340 - val_loss: 0.0293\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 798us/step - loss: 0.0265 - val_loss: 0.0236\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 797us/step - loss: 0.0264 - val_loss: 0.0236\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 803us/step - loss: 0.0266 - val_loss: 0.0237\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 814us/step - loss: 0.0262 - val_loss: 0.0234\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 806us/step - loss: 0.0262 - val_loss: 0.0234\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 779us/step - loss: 0.0217 - val_loss: 0.0200\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 779us/step - loss: 0.0218 - val_loss: 0.0201\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 787us/step - loss: 0.0219 - val_loss: 0.0202\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 788us/step - loss: 0.0216 - val_loss: 0.0199\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 793us/step - loss: 0.0216 - val_loss: 0.0199\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 779us/step - loss: 0.0189 - val_loss: 0.0178\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 794us/step - loss: 0.0190 - val_loss: 0.0179\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 788us/step - loss: 0.0188 - val_loss: 0.0178\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 797us/step - loss: 0.0192 - val_loss: 0.0181\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 854us/step - loss: 0.0188 - val_loss: 0.0177\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 860us/step - loss: 0.0172 - val_loss: 0.0164\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 881us/step - loss: 0.0173 - val_loss: 0.0166\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 881us/step - loss: 0.0171 - val_loss: 0.0164\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 882us/step - loss: 0.0174 - val_loss: 0.0167\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 814us/step - loss: 0.0171 - val_loss: 0.0164\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 788us/step - loss: 0.0160 - val_loss: 0.0156\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 794us/step - loss: 0.0162 - val_loss: 0.0157\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 795us/step - loss: 0.0161 - val_loss: 0.0156\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 801us/step - loss: 0.0164 - val_loss: 0.0158\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 801us/step - loss: 0.0160 - val_loss: 0.0156\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 976us/step - loss: 0.0153 - val_loss: 0.0149\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0155 - val_loss: 0.0151\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 992us/step - loss: 0.0153 - val_loss: 0.0150\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 989us/step - loss: 0.0156 - val_loss: 0.0152\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 985us/step - loss: 0.0153 - val_loss: 0.0149\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0148 - val_loss: 0.0145\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0149 - val_loss: 0.0146\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0148 - val_loss: 0.0145\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0151 - val_loss: 0.0148\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0148 - val_loss: 0.0145\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 839us/step - loss: 0.0144 - val_loss: 0.0141\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 844us/step - loss: 0.0145 - val_loss: 0.0142\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 846us/step - loss: 0.0144 - val_loss: 0.0142\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 844us/step - loss: 0.0147 - val_loss: 0.0144\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 896us/step - loss: 0.0144 - val_loss: 0.0142\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 905us/step - loss: 0.0141 - val_loss: 0.0139\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 898us/step - loss: 0.0142 - val_loss: 0.0140\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 917us/step - loss: 0.0143 - val_loss: 0.0141\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 920us/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 871us/step - loss: 0.0141 - val_loss: 0.0139\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 861us/step - loss: 0.0138 - val_loss: 0.0136\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 863us/step - loss: 0.0140 - val_loss: 0.0137\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 864us/step - loss: 0.0141 - val_loss: 0.0139\n",
      "1415/1415 [==============================] - 1s 863us/step - loss: 0.0139 - val_loss: 0.0137\n",
      "Epoch 20/28\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 892us/step - loss: 0.0139 - val_loss: 0.0136\n",
      " 410/1415 [=======>......................] - ETA: 0s - loss: 0.0138Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 800us/step - loss: 0.0136 - val_loss: 0.0134\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 811us/step - loss: 0.0137 - val_loss: 0.0135\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 809us/step - loss: 0.0138 - val_loss: 0.0136\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 812us/step - loss: 0.0137 - val_loss: 0.0135\n",
      "  77/1415 [>.............................] - ETA: 0s - loss: 0.0136Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 789us/step - loss: 0.0136 - val_loss: 0.0135\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 780us/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 780us/step - loss: 0.0135 - val_loss: 0.0133\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 784us/step - loss: 0.0136 - val_loss: 0.0134\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 785us/step - loss: 0.0135 - val_loss: 0.0133\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 807us/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 809us/step - loss: 0.0132 - val_loss: 0.0131\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 812us/step - loss: 0.0133 - val_loss: 0.0132\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 818us/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 819us/step - loss: 0.0133 - val_loss: 0.0131\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 852us/step - loss: 0.0133 - val_loss: 0.0131\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 875us/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 875us/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 884us/step - loss: 0.0133 - val_loss: 0.0131\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 887us/step - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 812us/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 799us/step - loss: 0.0129 - val_loss: 0.0127\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 817us/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 813us/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 819us/step - loss: 0.0129 - val_loss: 0.0128\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 792us/step - loss: 0.0128 - val_loss: 0.0126\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 791us/step - loss: 0.0128 - val_loss: 0.0127\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 800us/step - loss: 0.0129 - val_loss: 0.0128\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 805us/step - loss: 0.0128 - val_loss: 0.0127\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 790us/step - loss: 0.0128 - val_loss: 0.0126\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 781us/step - loss: 0.0126 - val_loss: 0.0125\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 782us/step - loss: 0.0127 - val_loss: 0.0125\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 859us/step - loss: 0.0128 - val_loss: 0.0126\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 861us/step - loss: 0.0127 - val_loss: 0.0125\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 877us/step - loss: 0.0127 - val_loss: 0.0125\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 866us/step - loss: 0.0125 - val_loss: 0.0123\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 864us/step - loss: 0.0126 - val_loss: 0.0124\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 811us/step - loss: 0.0127 - val_loss: 0.0125\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 810us/step - loss: 0.0126 - val_loss: 0.0124\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 796us/step - loss: 0.0125 - val_loss: 0.0124\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 788us/step - loss: 0.0124 - val_loss: 0.0123\n",
      "1415/1415 [==============================] - 1s 777us/step - loss: 0.0124 - val_loss: 0.0123\n",
      "1415/1415 [==============================] - 1s 790us/step - loss: 0.0125 - val_loss: 0.0124\n",
      "1415/1415 [==============================] - 1s 789us/step - loss: 0.0124 - val_loss: 0.0123\n",
      "1415/1415 [==============================] - 1s 764us/step - loss: 0.0124 - val_loss: 0.0123\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1132/1132 - 2s - loss: 0.0123 - val_loss: 0.0122 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 2s - loss: 0.0124 - val_loss: 0.0123 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 2s - loss: 0.0123 - val_loss: 0.0122 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 2s - loss: 0.0123 - val_loss: 0.0122 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 2s - loss: 0.0123 - val_loss: 0.0122 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 0.0122 - val_loss: 0.0121 - 922ms/epoch - 815us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0123 - val_loss: 0.0122 - 950ms/epoch - 839us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0122 - val_loss: 0.0121 - 935ms/epoch - 826us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0122 - val_loss: 0.0121 - 881ms/epoch - 779us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0122 - val_loss: 0.0121 - 902ms/epoch - 797us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0121 - val_loss: 0.0120 - 896ms/epoch - 792us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0122 - val_loss: 0.0121 - 913ms/epoch - 806us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0122 - val_loss: 0.0120 - 891ms/epoch - 787us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0121 - val_loss: 0.0120 - 902ms/epoch - 797us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0121 - val_loss: 0.0120 - 868ms/epoch - 766us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0121 - val_loss: 0.0119 - 860ms/epoch - 759us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0121 - val_loss: 0.0120 - 850ms/epoch - 751us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0121 - val_loss: 0.0120 - 862ms/epoch - 762us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0120 - val_loss: 0.0119 - 855ms/epoch - 755us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0121 - val_loss: 0.0119 - 864ms/epoch - 763us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0120 - val_loss: 0.0118 - 849ms/epoch - 750us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0120 - val_loss: 0.0118 - 853ms/epoch - 753us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0121 - val_loss: 0.0119 - 854ms/epoch - 754us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0120 - val_loss: 0.0119 - 858ms/epoch - 758us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0120 - val_loss: 0.0118 - 861ms/epoch - 760us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0119 - val_loss: 0.0118 - 851ms/epoch - 752us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0119 - val_loss: 0.0118 - 850ms/epoch - 751us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0120 - val_loss: 0.0118 - 855ms/epoch - 755us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0119 - val_loss: 0.0118 - 858ms/epoch - 758us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0119 - val_loss: 0.0118 - 862ms/epoch - 762us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0118 - val_loss: 0.0117 - 855ms/epoch - 755us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0118 - val_loss: 0.0117 - 851ms/epoch - 752us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0119 - val_loss: 0.0118 - 860ms/epoch - 760us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0118 - val_loss: 0.0117 - 859ms/epoch - 759us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0118 - val_loss: 0.0117 - 863ms/epoch - 762us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0118 - val_loss: 0.0116 - 855ms/epoch - 756us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0118 - val_loss: 0.0116 - 869ms/epoch - 767us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0118 - val_loss: 0.0117 - 872ms/epoch - 771us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0117 - val_loss: 0.0116 - 867ms/epoch - 766us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0118 - val_loss: 0.0116 - 872ms/epoch - 770us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0117 - val_loss: 0.0115 - 866ms/epoch - 765us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0117 - val_loss: 0.0116 - 876ms/epoch - 774us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0118 - val_loss: 0.0116 - 880ms/epoch - 778us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0117 - val_loss: 0.0116 - 869ms/epoch - 768us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0117 - val_loss: 0.0116 - 963ms/epoch - 851us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0116 - val_loss: 0.0115 - 1s/epoch - 903us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0116 - val_loss: 0.0115 - 1s/epoch - 893us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0117 - val_loss: 0.0115 - 1s/epoch - 910us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0116 - val_loss: 0.0115 - 1s/epoch - 900us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0116 - val_loss: 0.0115 - 927ms/epoch - 819us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0116 - val_loss: 0.0114 - 849ms/epoch - 750us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0115 - val_loss: 0.0114 - 850ms/epoch - 751us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0116 - val_loss: 0.0116 - 858ms/epoch - 758us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0115 - val_loss: 0.0114 - 857ms/epoch - 757us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0116 - val_loss: 0.0116 - 870ms/epoch - 768us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0115 - val_loss: 0.0113 - 866ms/epoch - 765us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0115 - val_loss: 0.0113 - 870ms/epoch - 769us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0115 - val_loss: 0.0114 - 878ms/epoch - 776us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0115 - val_loss: 0.0114 - 863ms/epoch - 763us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0115 - val_loss: 0.0114 - 863ms/epoch - 762us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0114 - val_loss: 0.0113 - 856ms/epoch - 756us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0114 - val_loss: 0.0113 - 851ms/epoch - 752us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0115 - val_loss: 0.0114 - 862ms/epoch - 761us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0114 - val_loss: 0.0117 - 857ms/epoch - 757us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0114 - val_loss: 0.0114 - 858ms/epoch - 758us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0114 - val_loss: 0.0115 - 850ms/epoch - 751us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0114 - val_loss: 0.0112 - 847ms/epoch - 748us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0114 - val_loss: 0.0113 - 861ms/epoch - 760us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0114 - val_loss: 0.0113 - 863ms/epoch - 763us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0114 - val_loss: 0.0113 - 861ms/epoch - 761us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0113 - val_loss: 0.0111 - 852ms/epoch - 753us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0113 - val_loss: 0.0112 - 886ms/epoch - 783us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0114 - val_loss: 0.0113 - 894ms/epoch - 789us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0113 - val_loss: 0.0112 - 886ms/epoch - 783us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0113 - val_loss: 0.0112 - 886ms/epoch - 783us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0112 - val_loss: 0.0111 - 875ms/epoch - 773us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0112 - val_loss: 0.0111 - 855ms/epoch - 755us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0113 - val_loss: 0.0112 - 869ms/epoch - 767us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0112 - val_loss: 0.0112 - 856ms/epoch - 757us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0113 - val_loss: 0.0111 - 866ms/epoch - 765us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0112 - val_loss: 0.0111 - 857ms/epoch - 757us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0112 - val_loss: 0.0111 - 858ms/epoch - 758us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0112 - val_loss: 0.0111 - 863ms/epoch - 762us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0112 - val_loss: 0.0111 - 854ms/epoch - 754us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0112 - val_loss: 0.0112 - 861ms/epoch - 760us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0111 - val_loss: 0.0110 - 852ms/epoch - 753us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0111 - val_loss: 0.0111 - 953ms/epoch - 842us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0112 - val_loss: 0.0111 - 953ms/epoch - 842us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0111 - val_loss: 0.0110 - 958ms/epoch - 846us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0112 - val_loss: 0.0111 - 941ms/epoch - 831us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0111 - val_loss: 0.0109 - 941ms/epoch - 831us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0111 - val_loss: 0.0109 - 874ms/epoch - 772us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0111 - val_loss: 0.0110 - 884ms/epoch - 781us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0111 - val_loss: 0.0110 - 875ms/epoch - 773us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0111 - val_loss: 0.0110 - 873ms/epoch - 771us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0110 - val_loss: 0.0109 - 862ms/epoch - 761us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0110 - val_loss: 0.0109 - 848ms/epoch - 749us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0111 - val_loss: 0.0110 - 858ms/epoch - 758us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0110 - val_loss: 0.0109 - 855ms/epoch - 755us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0110 - val_loss: 0.0109 - 854ms/epoch - 754us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0110 - val_loss: 0.0108 - 851ms/epoch - 752us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0110 - val_loss: 0.0108 - 849ms/epoch - 750us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0110 - val_loss: 0.0110 - 860ms/epoch - 760us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0110 - val_loss: 0.0109 - 855ms/epoch - 756us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0110 - val_loss: 0.0109 - 860ms/epoch - 759us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0109 - val_loss: 0.0108 - 843ms/epoch - 745us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0109 - val_loss: 0.0109 - 844ms/epoch - 745us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0110 - val_loss: 0.0109 - 855ms/epoch - 755us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0109 - val_loss: 0.0108 - 859ms/epoch - 759us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0109 - val_loss: 0.0108 - 865ms/epoch - 764us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0109 - val_loss: 0.0108 - 858ms/epoch - 758us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0109 - val_loss: 0.0107 - 857ms/epoch - 757us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0109 - val_loss: 0.0108 - 860ms/epoch - 759us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0109 - val_loss: 0.0108 - 864ms/epoch - 763us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0109 - val_loss: 0.0108 - 861ms/epoch - 761us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0108 - val_loss: 0.0107 - 850ms/epoch - 751us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0108 - val_loss: 0.0107 - 850ms/epoch - 751us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0109 - val_loss: 0.0108 - 855ms/epoch - 755us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0108 - val_loss: 0.0107 - 861ms/epoch - 761us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0109 - val_loss: 0.0107 - 861ms/epoch - 761us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0108 - val_loss: 0.0107 - 849ms/epoch - 750us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0108 - val_loss: 0.0106 - 847ms/epoch - 748us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0108 - val_loss: 0.0107 - 861ms/epoch - 760us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0108 - val_loss: 0.0107 - 857ms/epoch - 757us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0108 - val_loss: 0.0107 - 859ms/epoch - 759us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0106 - 852ms/epoch - 752us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0106 - 853ms/epoch - 753us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0108 - val_loss: 0.0107 - 856ms/epoch - 756us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0107 - 867ms/epoch - 766us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0108 - val_loss: 0.0106 - 868ms/epoch - 766us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0106 - 874ms/epoch - 772us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0106 - 862ms/epoch - 761us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0106 - 870ms/epoch - 769us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0107 - 963ms/epoch - 850us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0106 - 945ms/epoch - 834us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0105 - 935ms/epoch - 826us/step\n",
      "283/283 - 0s - loss: 0.0100 - 184ms/epoch - 650us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time= 1.2min\n",
      "1132/1132 - 1s - loss: 0.0106 - val_loss: 0.0105 - 930ms/epoch - 822us/step\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0106 - 927ms/epoch - 819us/step\n",
      "283/283 - 0s - loss: 0.0101 - 177ms/epoch - 627us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time= 1.2min\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0106 - 834ms/epoch - 737us/step\n",
      "283/283 - 0s - loss: 0.0102 - 164ms/epoch - 579us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time= 1.2min\n",
      "283/283 - 0s - loss: 0.0102 - 186ms/epoch - 657us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time= 1.2min\n",
      "1132/1132 - 1s - loss: 0.0107 - val_loss: 0.0106 - 826ms/epoch - 730us/step\n",
      "283/283 - 0s - loss: 0.0101 - 143ms/epoch - 507us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time= 1.2min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "   1/1415 [..............................] - ETA: 4:32 - loss: 19.6157Epoch 1/28\n",
      "1371/1415 [============================>.] - ETA: 0s - loss: 7.3585"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 16:29:07.256267: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 7.1455 - val_loss: 0.8402\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 7.0037 - val_loss: 0.8116\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 7.0276 - val_loss: 0.8165\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 7.1592 - val_loss: 0.8156\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 1s 781us/step - loss: 0.3213 - val_loss: 0.1736\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 782us/step - loss: 0.3422 - val_loss: 0.2072\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 776us/step - loss: 0.3314 - val_loss: 0.1912\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 783us/step - loss: 0.3417 - val_loss: 0.1964\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 779us/step - loss: 0.1600 - val_loss: 0.1494\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 779us/step - loss: 0.1800 - val_loss: 0.1620\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 774us/step - loss: 0.1728 - val_loss: 0.1591\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 782us/step - loss: 0.1655 - val_loss: 0.1483\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 7.1524 - val_loss: 0.8107\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 1s 798us/step - loss: 0.1408 - val_loss: 0.1323\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.1519 - val_loss: 0.1444\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 801us/step - loss: 0.1490 - val_loss: 0.1421\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 807us/step - loss: 0.1406 - val_loss: 0.1346\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.3396 - val_loss: 0.2056\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 863us/step - loss: 0.1277 - val_loss: 0.1255\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 867us/step - loss: 0.1393 - val_loss: 0.1366\n",
      "1326/1415 [===========================>..] - ETA: 0s - loss: 0.1306Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 864us/step - loss: 0.1353 - val_loss: 0.1295\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 868us/step - loss: 0.1303 - val_loss: 0.1253\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 863us/step - loss: 0.1766 - val_loss: 0.1551\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 809us/step - loss: 0.1223 - val_loss: 0.1198\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 818us/step - loss: 0.1321 - val_loss: 0.1282\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 820us/step - loss: 0.1228 - val_loss: 0.1155\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 822us/step - loss: 0.1200 - val_loss: 0.1158\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 814us/step - loss: 0.1440 - val_loss: 0.1353\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 894us/step - loss: 0.1174 - val_loss: 0.1154\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 892us/step - loss: 0.1235 - val_loss: 0.1173\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 889us/step - loss: 0.1079 - val_loss: 0.1019\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 904us/step - loss: 0.1141 - val_loss: 0.1123\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 900us/step - loss: 0.1299 - val_loss: 0.1251\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 805us/step - loss: 0.1110 - val_loss: 0.1071\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 798us/step - loss: 0.1116 - val_loss: 0.1065\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 800us/step - loss: 0.0988 - val_loss: 0.0967\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 865us/step - loss: 0.1221 - val_loss: 0.1199\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 877us/step - loss: 0.1106 - val_loss: 0.1081\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 879us/step - loss: 0.1037 - val_loss: 0.1021\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 880us/step - loss: 0.1044 - val_loss: 0.1046\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 875us/step - loss: 0.0959 - val_loss: 0.0948\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 803us/step - loss: 0.1179 - val_loss: 0.1157\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 812us/step - loss: 0.1059 - val_loss: 0.1034\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 819us/step - loss: 0.1012 - val_loss: 0.1007\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 819us/step - loss: 0.1011 - val_loss: 0.0988\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 817us/step - loss: 0.0939 - val_loss: 0.0930\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 819us/step - loss: 0.1140 - val_loss: 0.1120\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 822us/step - loss: 0.1021 - val_loss: 0.1008\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 807us/step - loss: 0.0993 - val_loss: 0.0985\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 805us/step - loss: 0.0969 - val_loss: 0.0948\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 808us/step - loss: 0.0921 - val_loss: 0.0908\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 803us/step - loss: 0.1100 - val_loss: 0.1073\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 807us/step - loss: 0.0988 - val_loss: 0.0967\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 807us/step - loss: 0.0977 - val_loss: 0.0965\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 806us/step - loss: 0.0944 - val_loss: 0.0936\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 797us/step - loss: 0.0906 - val_loss: 0.0898\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 797us/step - loss: 0.1046 - val_loss: 0.1010\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 806us/step - loss: 0.0948 - val_loss: 0.0932\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 862us/step - loss: 0.0961 - val_loss: 0.0952\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 862us/step - loss: 0.0928 - val_loss: 0.0926\n",
      "1250/1415 [=========================>....] - ETA: 0s - loss: 0.0928Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 858us/step - loss: 0.0892 - val_loss: 0.0886\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 855us/step - loss: 0.0984 - val_loss: 0.0970\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 864us/step - loss: 0.0928 - val_loss: 0.0926\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 806us/step - loss: 0.0948 - val_loss: 0.0943\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 809us/step - loss: 0.0914 - val_loss: 0.0910\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 801us/step - loss: 0.0879 - val_loss: 0.0870\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.0962 - val_loss: 0.0950\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 808us/step - loss: 0.0913 - val_loss: 0.0905\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 813us/step - loss: 0.0935 - val_loss: 0.0929\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 816us/step - loss: 0.0902 - val_loss: 0.0898\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 810us/step - loss: 0.0868 - val_loss: 0.0862\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.0947 - val_loss: 0.0940\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 809us/step - loss: 0.0900 - val_loss: 0.0892\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 799us/step - loss: 0.0923 - val_loss: 0.0914\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 814us/step - loss: 0.0890 - val_loss: 0.0898\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 796us/step - loss: 0.0857 - val_loss: 0.0851\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 799us/step - loss: 0.0933 - val_loss: 0.0924\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 807us/step - loss: 0.0887 - val_loss: 0.0878\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 882us/step - loss: 0.0911 - val_loss: 0.0903\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 872us/step - loss: 0.0879 - val_loss: 0.0872\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 869us/step - loss: 0.0847 - val_loss: 0.0844\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 864us/step - loss: 0.0920 - val_loss: 0.0912\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 880us/step - loss: 0.0876 - val_loss: 0.0868\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 824us/step - loss: 0.0892 - val_loss: 0.0890\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 832us/step - loss: 0.0870 - val_loss: 0.0862\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 824us/step - loss: 0.0838 - val_loss: 0.0835\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 818us/step - loss: 0.0907 - val_loss: 0.0897\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 834us/step - loss: 0.0866 - val_loss: 0.0866\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 811us/step - loss: 0.0860 - val_loss: 0.0850\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 811us/step - loss: 0.0861 - val_loss: 0.0858\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 807us/step - loss: 0.0829 - val_loss: 0.0824\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 806us/step - loss: 0.0895 - val_loss: 0.0886\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 812us/step - loss: 0.0856 - val_loss: 0.0855\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.0843 - val_loss: 0.0836\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.0852 - val_loss: 0.0844\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 802us/step - loss: 0.0821 - val_loss: 0.0818\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 798us/step - loss: 0.0872 - val_loss: 0.0856\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 805us/step - loss: 0.0847 - val_loss: 0.0844\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 872us/step - loss: 0.0835 - val_loss: 0.0829\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 875us/step - loss: 0.0844 - val_loss: 0.0842\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 867us/step - loss: 0.0814 - val_loss: 0.0809\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 868us/step - loss: 0.0841 - val_loss: 0.0828\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 874us/step - loss: 0.0839 - val_loss: 0.0836\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 824us/step - loss: 0.0828 - val_loss: 0.0820\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 814us/step - loss: 0.0837 - val_loss: 0.0843\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 825us/step - loss: 0.0807 - val_loss: 0.0805\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 813us/step - loss: 0.0827 - val_loss: 0.0821\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 840us/step - loss: 0.0831 - val_loss: 0.0822\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 805us/step - loss: 0.0821 - val_loss: 0.0822\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 804us/step - loss: 0.0830 - val_loss: 0.0821\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 883us/step - loss: 0.0800 - val_loss: 0.0824\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 902us/step - loss: 0.0818 - val_loss: 0.0811\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 965us/step - loss: 0.0824 - val_loss: 0.0825\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 968us/step - loss: 0.0815 - val_loss: 0.0875\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0823 - val_loss: 0.0816\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 904us/step - loss: 0.0794 - val_loss: 0.0786\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 879us/step - loss: 0.0810 - val_loss: 0.0804\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 867us/step - loss: 0.0817 - val_loss: 0.0824\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0809 - val_loss: 0.0805\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0816 - val_loss: 0.0828\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 994us/step - loss: 0.0788 - val_loss: 0.0786\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 985us/step - loss: 0.0803 - val_loss: 0.0798\n",
      "  74/1415 [>.............................] - ETA: 0s - loss: 0.0815Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 961us/step - loss: 0.0810 - val_loss: 0.0802\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 813us/step - loss: 0.0803 - val_loss: 0.0798\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 811us/step - loss: 0.0810 - val_loss: 0.0809\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 810us/step - loss: 0.0782 - val_loss: 0.0776\n",
      " 210/1415 [===>..........................] - ETA: 0s - loss: 0.0801Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 812us/step - loss: 0.0797 - val_loss: 0.0797\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 820us/step - loss: 0.0803 - val_loss: 0.0797\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 817us/step - loss: 0.0798 - val_loss: 0.0801\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 812us/step - loss: 0.0804 - val_loss: 0.0797\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 813us/step - loss: 0.0777 - val_loss: 0.0813\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 810us/step - loss: 0.0790 - val_loss: 0.0782\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 815us/step - loss: 0.0797 - val_loss: 0.0795\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 815us/step - loss: 0.0792 - val_loss: 0.0787\n",
      "1415/1415 [==============================] - 1s 808us/step - loss: 0.0799 - val_loss: 0.0806\n",
      "1415/1415 [==============================] - 1s 800us/step - loss: 0.0772 - val_loss: 0.0766\n",
      "1415/1415 [==============================] - 1s 797us/step - loss: 0.0784 - val_loss: 0.0780\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 772us/step - loss: 0.0792 - val_loss: 0.0784\n",
      "1415/1415 [==============================] - 1s 643us/step - loss: 0.0779 - val_loss: 0.0781\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 688us/step - loss: 0.0774 - val_loss: 0.0778\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1132/1132 - 2s - loss: 0.0788 - val_loss: 0.0782 - 2s/epoch - 2ms/step\n",
      "1132/1132 - 2s - loss: 0.0793 - val_loss: 0.0789 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "Epoch 2/28\n",
      "1132/1132 - 2s - loss: 0.0768 - val_loss: 0.0761 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.0787 - val_loss: 0.0788 - 3s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 0.0785 - val_loss: 0.0779 - 945ms/epoch - 835us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0789 - val_loss: 0.0789 - 951ms/epoch - 840us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0764 - val_loss: 0.0759 - 941ms/epoch - 832us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0783 - val_loss: 0.0778 - 845ms/epoch - 747us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0781 - val_loss: 0.0773 - 843ms/epoch - 745us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0785 - val_loss: 0.0780 - 847ms/epoch - 748us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0760 - val_loss: 0.0758 - 840ms/epoch - 742us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 2s - loss: 0.0768 - val_loss: 0.0762 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 0.0779 - val_loss: 0.0778 - 869ms/epoch - 767us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0777 - val_loss: 0.0775 - 876ms/epoch - 774us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0781 - val_loss: 0.0776 - 881ms/epoch - 778us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0757 - val_loss: 0.0760 - 878ms/epoch - 775us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0764 - val_loss: 0.0757 - 890ms/epoch - 786us/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0775 - val_loss: 0.0771 - 891ms/epoch - 787us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0773 - val_loss: 0.0767 - 884ms/epoch - 781us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0777 - val_loss: 0.0772 - 893ms/epoch - 789us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0754 - val_loss: 0.0773 - 889ms/epoch - 785us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0760 - val_loss: 0.0757 - 884ms/epoch - 781us/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0771 - val_loss: 0.0772 - 895ms/epoch - 790us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0770 - val_loss: 0.0768 - 891ms/epoch - 787us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0773 - val_loss: 0.0769 - 889ms/epoch - 785us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0751 - val_loss: 0.0751 - 874ms/epoch - 772us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0757 - val_loss: 0.0752 - 879ms/epoch - 776us/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0768 - val_loss: 0.0761 - 874ms/epoch - 772us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0767 - val_loss: 0.0769 - 879ms/epoch - 776us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0770 - val_loss: 0.0767 - 884ms/epoch - 781us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0747 - val_loss: 0.0742 - 872ms/epoch - 770us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0753 - val_loss: 0.0748 - 877ms/epoch - 774us/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0764 - val_loss: 0.0771 - 891ms/epoch - 787us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0763 - val_loss: 0.0768 - 889ms/epoch - 785us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0767 - val_loss: 0.0764 - 895ms/epoch - 791us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0745 - val_loss: 0.0736 - 887ms/epoch - 784us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0750 - val_loss: 0.0743 - 889ms/epoch - 785us/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0761 - val_loss: 0.0754 - 887ms/epoch - 784us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0760 - val_loss: 0.0759 - 883ms/epoch - 780us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0763 - val_loss: 0.0763 - 883ms/epoch - 780us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0741 - val_loss: 0.0743 - 874ms/epoch - 772us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0746 - val_loss: 0.0747 - 888ms/epoch - 784us/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 1s - loss: 0.0757 - val_loss: 0.0757 - 886ms/epoch - 783us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0757 - val_loss: 0.0749 - 897ms/epoch - 792us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0760 - val_loss: 0.0776 - 893ms/epoch - 789us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0738 - val_loss: 0.0735 - 982ms/epoch - 868us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0743 - val_loss: 0.0752 - 974ms/epoch - 860us/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0754 - val_loss: 0.0746 - 982ms/epoch - 867us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0754 - val_loss: 0.0752 - 979ms/epoch - 865us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0758 - val_loss: 0.0752 - 990ms/epoch - 874us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0736 - val_loss: 0.0733 - 885ms/epoch - 782us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0740 - val_loss: 0.0733 - 943ms/epoch - 833us/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0750 - val_loss: 0.0746 - 969ms/epoch - 856us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0752 - val_loss: 0.0743 - 954ms/epoch - 843us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0754 - val_loss: 0.0749 - 958ms/epoch - 846us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0733 - val_loss: 0.0726 - 936ms/epoch - 827us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0737 - val_loss: 0.0753 - 887ms/epoch - 784us/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0748 - val_loss: 0.0740 - 888ms/epoch - 785us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0748 - val_loss: 0.0744 - 876ms/epoch - 774us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0751 - val_loss: 0.0749 - 885ms/epoch - 782us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0730 - val_loss: 0.0753 - 879ms/epoch - 777us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0734 - val_loss: 0.0728 - 879ms/epoch - 776us/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0745 - val_loss: 0.0740 - 883ms/epoch - 780us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0746 - val_loss: 0.0743 - 879ms/epoch - 776us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0748 - val_loss: 0.0743 - 893ms/epoch - 789us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0728 - val_loss: 0.0721 - 883ms/epoch - 780us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0731 - val_loss: 0.0731 - 891ms/epoch - 787us/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0742 - val_loss: 0.0751 - 880ms/epoch - 777us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0743 - val_loss: 0.0752 - 879ms/epoch - 777us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0745 - val_loss: 0.0759 - 892ms/epoch - 788us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0725 - val_loss: 0.0722 - 876ms/epoch - 774us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0728 - val_loss: 0.0732 - 875ms/epoch - 773us/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0739 - val_loss: 0.0745 - 1s/epoch - 885us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0740 - val_loss: 0.0738 - 996ms/epoch - 880us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0743 - val_loss: 0.0743 - 1s/epoch - 896us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0723 - val_loss: 0.0715 - 1s/epoch - 900us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0725 - val_loss: 0.0720 - 981ms/epoch - 867us/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0737 - val_loss: 0.0730 - 909ms/epoch - 803us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0738 - val_loss: 0.0745 - 899ms/epoch - 794us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0740 - val_loss: 0.0733 - 904ms/epoch - 799us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0720 - val_loss: 0.0717 - 888ms/epoch - 785us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0722 - val_loss: 0.0729 - 885ms/epoch - 782us/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0733 - val_loss: 0.0728 - 980ms/epoch - 866us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0735 - val_loss: 0.0743 - 969ms/epoch - 856us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0737 - val_loss: 0.0745 - 976ms/epoch - 862us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0717 - val_loss: 0.0711 - 992ms/epoch - 876us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0720 - val_loss: 0.0720 - 991ms/epoch - 875us/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 1s - loss: 0.0731 - val_loss: 0.0723 - 914ms/epoch - 807us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0733 - val_loss: 0.0731 - 909ms/epoch - 803us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0735 - val_loss: 0.0735 - 912ms/epoch - 805us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0715 - val_loss: 0.0716 - 908ms/epoch - 802us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0718 - val_loss: 0.0723 - 880ms/epoch - 778us/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0729 - val_loss: 0.0722 - 881ms/epoch - 778us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0731 - val_loss: 0.0725 - 883ms/epoch - 780us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0732 - val_loss: 0.0728 - 885ms/epoch - 782us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0713 - val_loss: 0.0707 - 880ms/epoch - 777us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0715 - val_loss: 0.0712 - 878ms/epoch - 776us/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0726 - val_loss: 0.0718 - 883ms/epoch - 780us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0729 - val_loss: 0.0720 - 878ms/epoch - 775us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0730 - val_loss: 0.0723 - 890ms/epoch - 786us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0712 - val_loss: 0.0710 - 878ms/epoch - 775us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0713 - val_loss: 0.0708 - 886ms/epoch - 783us/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0725 - val_loss: 0.0720 - 892ms/epoch - 788us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0726 - val_loss: 0.0724 - 889ms/epoch - 785us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0728 - val_loss: 0.0726 - 895ms/epoch - 790us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0709 - val_loss: 0.0710 - 881ms/epoch - 778us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0711 - val_loss: 0.0706 - 873ms/epoch - 771us/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0723 - val_loss: 0.0713 - 882ms/epoch - 779us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0724 - val_loss: 0.0718 - 878ms/epoch - 776us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0725 - val_loss: 0.0724 - 885ms/epoch - 782us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0708 - val_loss: 0.0700 - 878ms/epoch - 775us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0709 - val_loss: 0.0714 - 882ms/epoch - 779us/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0720 - val_loss: 0.0718 - 884ms/epoch - 781us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0721 - val_loss: 0.0722 - 880ms/epoch - 777us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0706 - val_loss: 0.0696 - 874ms/epoch - 772us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0723 - val_loss: 0.0721 - 885ms/epoch - 782us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0706 - val_loss: 0.0703 - 880ms/epoch - 777us/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0718 - val_loss: 0.0710 - 883ms/epoch - 780us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0720 - val_loss: 0.0723 - 876ms/epoch - 773us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0703 - val_loss: 0.0698 - 883ms/epoch - 780us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0721 - val_loss: 0.0715 - 891ms/epoch - 787us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0703 - val_loss: 0.0703 - 885ms/epoch - 782us/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0716 - val_loss: 0.0716 - 902ms/epoch - 797us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0718 - val_loss: 0.0715 - 905ms/epoch - 799us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0702 - val_loss: 0.0702 - 912ms/epoch - 806us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0719 - val_loss: 0.0712 - 902ms/epoch - 797us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0702 - val_loss: 0.0702 - 903ms/epoch - 797us/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0714 - val_loss: 0.0710 - 891ms/epoch - 787us/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0717 - val_loss: 0.0710 - 882ms/epoch - 779us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0717 - val_loss: 0.0716 - 882ms/epoch - 779us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0700 - val_loss: 0.0733 - 889ms/epoch - 785us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0701 - val_loss: 0.0710 - 877ms/epoch - 775us/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0712 - val_loss: 0.0711 - 962ms/epoch - 850us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0714 - val_loss: 0.0705 - 969ms/epoch - 856us/step\n",
      "1132/1132 - 1s - loss: 0.0698 - val_loss: 0.0717 - 962ms/epoch - 850us/step\n",
      "1132/1132 - 1s - loss: 0.0715 - val_loss: 0.0745 - 965ms/epoch - 852us/step\n",
      "283/283 - 0s - loss: 0.0679 - 197ms/epoch - 695us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time= 1.2min\n",
      "1132/1132 - 1s - loss: 0.0698 - val_loss: 0.0690 - 953ms/epoch - 842us/step\n",
      "Epoch 27/28\n",
      "283/283 - 0s - loss: 0.0682 - 195ms/epoch - 689us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time= 1.2min\n",
      "283/283 - 0s - loss: 0.0716 - 196ms/epoch - 693us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time= 1.2min\n",
      "1132/1132 - 1s - loss: 0.0709 - val_loss: 0.0701 - 813ms/epoch - 718us/step\n",
      "283/283 - 0s - loss: 0.0673 - 160ms/epoch - 566us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time= 1.2min\n",
      "1132/1132 - 1s - loss: 0.0697 - val_loss: 0.0692 - 703ms/epoch - 621us/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 1s - loss: 0.0696 - val_loss: 0.0692 - 640ms/epoch - 565us/step\n",
      "283/283 - 0s - loss: 0.0670 - 158ms/epoch - 558us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time= 1.2min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 1.0577 - val_loss: 0.0871\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 1.0611 - val_loss: 0.0961\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 1.0734 - val_loss: 0.0985\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 1.0759 - val_loss: 0.0912\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 1.0659 - val_loss: 0.0928\n",
      "Epoch 2/28\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0752 - val_loss: 0.0601\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0714 - val_loss: 0.0616\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0684 - val_loss: 0.0568\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0855 - val_loss: 0.0744\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0762 - val_loss: 0.0613\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0549 - val_loss: 0.0516\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0578 - val_loss: 0.0537\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0562 - val_loss: 0.0506\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0539 - val_loss: 0.0511\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0644 - val_loss: 0.0567\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0496 - val_loss: 0.0475\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0516 - val_loss: 0.0492\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0489 - val_loss: 0.0471\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0493 - val_loss: 0.0474\n",
      "  51/1415 [>.............................] - ETA: 1s - loss: 0.0489Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0538 - val_loss: 0.0511\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0462 - val_loss: 0.0448\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0477 - val_loss: 0.0462\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0458 - val_loss: 0.0445\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0461 - val_loss: 0.0444\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0492 - val_loss: 0.0473\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0437 - val_loss: 0.0425\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0434 - val_loss: 0.0422\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0450 - val_loss: 0.0439\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0436 - val_loss: 0.0423\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0459 - val_loss: 0.0446\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0416 - val_loss: 0.0407\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0416 - val_loss: 0.0405\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0415 - val_loss: 0.0405\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0430 - val_loss: 0.0417\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0434 - val_loss: 0.0424\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0400 - val_loss: 0.0390\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0401 - val_loss: 0.0393\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0400 - val_loss: 0.0393\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0413 - val_loss: 0.0403\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0414 - val_loss: 0.0405\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0387 - val_loss: 0.0380\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0388 - val_loss: 0.0382\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0388 - val_loss: 0.0383\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0399 - val_loss: 0.0394\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0398 - val_loss: 0.0389\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0376 - val_loss: 0.0371\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0378 - val_loss: 0.0371\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0378 - val_loss: 0.0374\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0388 - val_loss: 0.0382\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0375 - val_loss: 0.0364\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0366 - val_loss: 0.0361\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0369 - val_loss: 0.0368\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0369 - val_loss: 0.0365\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0378 - val_loss: 0.0373\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0360 - val_loss: 0.0356\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0358 - val_loss: 0.0355\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0361 - val_loss: 0.0356\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0362 - val_loss: 0.0357\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0370 - val_loss: 0.0364\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0352 - val_loss: 0.0349\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0352 - val_loss: 0.0348\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0355 - val_loss: 0.0350\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0355 - val_loss: 0.0352\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0345 - val_loss: 0.0341\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0362 - val_loss: 0.0357\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0346 - val_loss: 0.0343\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0349 - val_loss: 0.0347\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0349 - val_loss: 0.0346\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0356 - val_loss: 0.0352\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0339 - val_loss: 0.0337\n",
      " 143/1415 [==>...........................] - ETA: 1s - loss: 0.0346Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0340 - val_loss: 0.0337\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0344 - val_loss: 0.0341\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0344 - val_loss: 0.0340\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0350 - val_loss: 0.0346\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0334 - val_loss: 0.0333\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0336 - val_loss: 0.0333\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0339 - val_loss: 0.0336\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0340 - val_loss: 0.0338\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0330 - val_loss: 0.0328\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0345 - val_loss: 0.0344\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0331 - val_loss: 0.0331\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0335 - val_loss: 0.0332\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0336 - val_loss: 0.0335\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0326 - val_loss: 0.0324\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0341 - val_loss: 0.0338\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0328 - val_loss: 0.0327\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0331 - val_loss: 0.0328\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0332 - val_loss: 0.0331\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0337 - val_loss: 0.0334\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0323 - val_loss: 0.0322\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0324 - val_loss: 0.0324\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0328 - val_loss: 0.0325\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0329 - val_loss: 0.0325\n",
      "  51/1415 [>.............................] - ETA: 1s - loss: 0.0325Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0333 - val_loss: 0.0329\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0321 - val_loss: 0.0318\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0325 - val_loss: 0.0323\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0325 - val_loss: 0.0324\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0317 - val_loss: 0.0316\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0330 - val_loss: 0.0326\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0318 - val_loss: 0.0316\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0322 - val_loss: 0.0320\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0323 - val_loss: 0.0322\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0326 - val_loss: 0.0326\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0314 - val_loss: 0.0313\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0316 - val_loss: 0.0314\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0319 - val_loss: 0.0318\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0324 - val_loss: 0.0322\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0312 - val_loss: 0.0311\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0313 - val_loss: 0.0312\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0317 - val_loss: 0.0313\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0318 - val_loss: 0.0318\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0310 - val_loss: 0.0309\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0321 - val_loss: 0.0318\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0311 - val_loss: 0.0309\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0314 - val_loss: 0.0312\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0315 - val_loss: 0.0313\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0308 - val_loss: 0.0307\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0318 - val_loss: 0.0316\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0309 - val_loss: 0.0308\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0312 - val_loss: 0.0311\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0313 - val_loss: 0.0311\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0306 - val_loss: 0.0305\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0316 - val_loss: 0.0314\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0307 - val_loss: 0.0307\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0310 - val_loss: 0.0310\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0311 - val_loss: 0.0309\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0304 - val_loss: 0.0303\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0314 - val_loss: 0.0311\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0305 - val_loss: 0.0304\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0308 - val_loss: 0.0308\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0309 - val_loss: 0.0306\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0302 - val_loss: 0.0302\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0312 - val_loss: 0.0313\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0303 - val_loss: 0.0302\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0306 - val_loss: 0.0305\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0308 - val_loss: 0.0307\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0301 - val_loss: 0.0301\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0310 - val_loss: 0.0309\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "1132/1132 - 3s - loss: 0.0302 - val_loss: 0.0300 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.0305 - val_loss: 0.0303 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.0308 - val_loss: 0.0306 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.0299 - val_loss: 0.0298 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 3s - loss: 0.0306 - val_loss: 0.0304 - 3s/epoch - 3ms/step\n",
      "Epoch 2/28\n",
      "1132/1132 - 1s - loss: 0.0300 - val_loss: 0.0298 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0304 - val_loss: 0.0303 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0307 - val_loss: 0.0305 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0298 - val_loss: 0.0297 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0305 - val_loss: 0.0303 - 1s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1132/1132 - 1s - loss: 0.0299 - val_loss: 0.0297 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0305 - val_loss: 0.0307 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0302 - val_loss: 0.0301 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0297 - val_loss: 0.0296 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0304 - val_loss: 0.0303 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1132/1132 - 1s - loss: 0.0298 - val_loss: 0.0296 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0301 - val_loss: 0.0300 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0296 - val_loss: 0.0296 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 2s - loss: 0.0304 - val_loss: 0.0303 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 1s - loss: 0.0302 - val_loss: 0.0301 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1132/1132 - 2s - loss: 0.0297 - val_loss: 0.0294 - 2s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 1s - loss: 0.0300 - val_loss: 0.0300 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 2s - loss: 0.0303 - val_loss: 0.0301 - 2s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 2s - loss: 0.0295 - val_loss: 0.0295 - 2s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 2s - loss: 0.0301 - val_loss: 0.0300 - 2s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1132/1132 - 2s - loss: 0.0296 - val_loss: 0.0294 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 2s - loss: 0.0299 - val_loss: 0.0299 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 2s - loss: 0.0302 - val_loss: 0.0302 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 2s - loss: 0.0294 - val_loss: 0.0293 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 2s - loss: 0.0300 - val_loss: 0.0299 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1132/1132 - 1s - loss: 0.0295 - val_loss: 0.0294 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 2s - loss: 0.0298 - val_loss: 0.0298 - 2s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 2s - loss: 0.0300 - val_loss: 0.0299 - 2s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 2s - loss: 0.0293 - val_loss: 0.0295 - 2s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 2s - loss: 0.0299 - val_loss: 0.0299 - 2s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1132/1132 - 2s - loss: 0.0294 - val_loss: 0.0292 - 2s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0299 - val_loss: 0.0297 - 1s/epoch - 1ms/step\n",
      "1132/1132 - 1s - loss: 0.0297 - val_loss: 0.0297 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0292 - val_loss: 0.0292 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0298 - val_loss: 0.0297 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1132/1132 - 1s - loss: 0.0293 - val_loss: 0.0292 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0296 - val_loss: 0.0295 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0298 - val_loss: 0.0299 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0292 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0297 - val_loss: 0.0297 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1132/1132 - 1s - loss: 0.0292 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0295 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0297 - val_loss: 0.0295 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0296 - val_loss: 0.0296 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0289 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0294 - val_loss: 0.0294 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0296 - val_loss: 0.0295 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0290 - val_loss: 0.0289 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 1s - loss: 0.0295 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1132/1132 - 2s - loss: 0.0290 - val_loss: 0.0289 - 2s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 2s - loss: 0.0293 - val_loss: 0.0293 - 2s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 2s - loss: 0.0296 - val_loss: 0.0296 - 2s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 2s - loss: 0.0289 - val_loss: 0.0288 - 2s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 2s - loss: 0.0295 - val_loss: 0.0295 - 2s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1132/1132 - 1s - loss: 0.0289 - val_loss: 0.0287 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 2s - loss: 0.0293 - val_loss: 0.0292 - 2s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0295 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 1s - loss: 0.0294 - val_loss: 0.0292 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 2s - loss: 0.0289 - val_loss: 0.0288 - 2s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1132/1132 - 2s - loss: 0.0288 - val_loss: 0.0286 - 2s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0292 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0294 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0293 - val_loss: 0.0292 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 1s - loss: 0.0288 - val_loss: 0.0288 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1132/1132 - 2s - loss: 0.0288 - val_loss: 0.0287 - 2s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0293 - val_loss: 0.0293 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0292 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0287 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 2s - loss: 0.0290 - val_loss: 0.0289 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 2s - loss: 0.0292 - val_loss: 0.0291 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 2s - loss: 0.0291 - val_loss: 0.0290 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 2s - loss: 0.0287 - val_loss: 0.0286 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1132/1132 - 2s - loss: 0.0286 - val_loss: 0.0284 - 2s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 2s - loss: 0.0290 - val_loss: 0.0290 - 2s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0290 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0291 - val_loss: 0.0291 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 1s - loss: 0.0286 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1132/1132 - 2s - loss: 0.0285 - val_loss: 0.0285 - 2s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 2s - loss: 0.0289 - val_loss: 0.0288 - 2s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 2s - loss: 0.0290 - val_loss: 0.0291 - 2s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 2s - loss: 0.0290 - val_loss: 0.0289 - 2s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 2s - loss: 0.0285 - val_loss: 0.0284 - 2s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0283 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 2s - loss: 0.0288 - val_loss: 0.0287 - 2s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 2s - loss: 0.0290 - val_loss: 0.0289 - 2s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 2s - loss: 0.0289 - val_loss: 0.0289 - 2s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 2s - loss: 0.0285 - val_loss: 0.0285 - 2s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0283 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0288 - val_loss: 0.0288 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0289 - val_loss: 0.0287 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0288 - val_loss: 0.0287 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0284 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 2s - loss: 0.0287 - val_loss: 0.0286 - 2s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0288 - val_loss: 0.0290 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0288 - val_loss: 0.0287 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0281 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1132/1132 - 1s - loss: 0.0283 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0286 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 2s - loss: 0.0288 - val_loss: 0.0286 - 2s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 1s - loss: 0.0283 - val_loss: 0.0284 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1132/1132 - 2s - loss: 0.0282 - val_loss: 0.0280 - 2s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0286 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 1s - loss: 0.0287 - val_loss: 0.0285 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 2s - loss: 0.0287 - val_loss: 0.0286 - 2s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 2s - loss: 0.0283 - val_loss: 0.0282 - 2s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1132/1132 - 2s - loss: 0.0282 - val_loss: 0.0280 - 2s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0285 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0286 - val_loss: 0.0285 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 2s - loss: 0.0287 - val_loss: 0.0287 - 2s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0282 - val_loss: 0.0282 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1132/1132 - 1s - loss: 0.0281 - val_loss: 0.0281 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0284 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0285 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0282 - val_loss: 0.0283 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 2s - loss: 0.0286 - val_loss: 0.0285 - 2s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1132/1132 - 1s - loss: 0.0281 - val_loss: 0.0278 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0284 - val_loss: 0.0285 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 2s - loss: 0.0285 - val_loss: 0.0283 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0281 - val_loss: 0.0281 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 1s - loss: 0.0285 - val_loss: 0.0286 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1132/1132 - 2s - loss: 0.0280 - val_loss: 0.0280 - 2s/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 2s - loss: 0.0284 - val_loss: 0.0283 - 2s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 2s - loss: 0.0284 - val_loss: 0.0285 - 2s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 2s - loss: 0.0281 - val_loss: 0.0280 - 2s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 2s - loss: 0.0285 - val_loss: 0.0284 - 2s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1132/1132 - 2s - loss: 0.0280 - val_loss: 0.0279 - 2s/epoch - 1ms/step\n",
      "283/283 - 0s - loss: 0.0279 - 234ms/epoch - 828us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "1132/1132 - 2s - loss: 0.0283 - val_loss: 0.0281 - 2s/epoch - 1ms/step\n",
      "1132/1132 - 2s - loss: 0.0284 - val_loss: 0.0283 - 2s/epoch - 1ms/step\n",
      "1132/1132 - 2s - loss: 0.0280 - val_loss: 0.0280 - 2s/epoch - 1ms/step\n",
      "1132/1132 - 2s - loss: 0.0284 - val_loss: 0.0283 - 2s/epoch - 1ms/step\n",
      "283/283 - 0s - loss: 0.0281 - 197ms/epoch - 697us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "283/283 - 0s - loss: 0.0283 - 191ms/epoch - 675us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "283/283 - 0s - loss: 0.0280 - 184ms/epoch - 650us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "283/283 - 0s - loss: 0.0283 - 181ms/epoch - 640us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time= 1.8min\n",
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 1.6379 - val_loss: 1.2152\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 1s 613us/step - loss: 0.9315 - val_loss: 0.6962\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 661us/step - loss: 0.5359 - val_loss: 0.4028\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 652us/step - loss: 0.3120 - val_loss: 0.2365\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 663us/step - loss: 0.1850 - val_loss: 0.1420\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 653us/step - loss: 0.1128 - val_loss: 0.0883\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 604us/step - loss: 0.0716 - val_loss: 0.0575\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 688us/step - loss: 0.0480 - val_loss: 0.0398\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 645us/step - loss: 0.0344 - val_loss: 0.0297\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 660us/step - loss: 0.0265 - val_loss: 0.0237\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 575us/step - loss: 0.0218 - val_loss: 0.0201\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 694us/step - loss: 0.0190 - val_loss: 0.0179\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 661us/step - loss: 0.0173 - val_loss: 0.0166\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 641us/step - loss: 0.0162 - val_loss: 0.0157\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 654us/step - loss: 0.0155 - val_loss: 0.0151\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 582us/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 663us/step - loss: 0.0146 - val_loss: 0.0143\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 664us/step - loss: 0.0142 - val_loss: 0.0140\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 652us/step - loss: 0.0140 - val_loss: 0.0139\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 595us/step - loss: 0.0138 - val_loss: 0.0135\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 639us/step - loss: 0.0136 - val_loss: 0.0134\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 680us/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 659us/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 641us/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 603us/step - loss: 0.0129 - val_loss: 0.0127\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 649us/step - loss: 0.0127 - val_loss: 0.0126\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 657us/step - loss: 0.0126 - val_loss: 0.0124\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 657us/step - loss: 0.0125 - val_loss: 0.0124\n",
      "Epoch 1/28\n",
      "1415/1415 - 2s - loss: 0.0124 - val_loss: 0.0122 - 2s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 - 1s - loss: 0.0123 - val_loss: 0.0125 - 803ms/epoch - 567us/step\n",
      "Epoch 3/28\n",
      "1415/1415 - 1s - loss: 0.0122 - val_loss: 0.0120 - 737ms/epoch - 521us/step\n",
      "Epoch 4/28\n",
      "1415/1415 - 1s - loss: 0.0120 - val_loss: 0.0119 - 778ms/epoch - 550us/step\n",
      "Epoch 5/28\n",
      "1415/1415 - 1s - loss: 0.0119 - val_loss: 0.0118 - 728ms/epoch - 515us/step\n",
      "Epoch 6/28\n",
      "1415/1415 - 1s - loss: 0.0119 - val_loss: 0.0117 - 731ms/epoch - 517us/step\n",
      "Epoch 7/28\n",
      "1415/1415 - 1s - loss: 0.0118 - val_loss: 0.0116 - 727ms/epoch - 514us/step\n",
      "Epoch 8/28\n",
      "1415/1415 - 1s - loss: 0.0117 - val_loss: 0.0116 - 731ms/epoch - 517us/step\n",
      "Epoch 9/28\n",
      "1415/1415 - 1s - loss: 0.0116 - val_loss: 0.0115 - 723ms/epoch - 511us/step\n",
      "Epoch 10/28\n",
      "1415/1415 - 1s - loss: 0.0115 - val_loss: 0.0114 - 716ms/epoch - 506us/step\n",
      "Epoch 11/28\n",
      "1415/1415 - 1s - loss: 0.0114 - val_loss: 0.0113 - 718ms/epoch - 507us/step\n",
      "Epoch 12/28\n",
      "1415/1415 - 1s - loss: 0.0114 - val_loss: 0.0112 - 718ms/epoch - 508us/step\n",
      "Epoch 13/28\n",
      "1415/1415 - 1s - loss: 0.0113 - val_loss: 0.0111 - 721ms/epoch - 510us/step\n",
      "Epoch 14/28\n",
      "1415/1415 - 1s - loss: 0.0112 - val_loss: 0.0111 - 716ms/epoch - 506us/step\n",
      "Epoch 15/28\n",
      "1415/1415 - 1s - loss: 0.0112 - val_loss: 0.0110 - 810ms/epoch - 572us/step\n",
      "Epoch 16/28\n",
      "1415/1415 - 1s - loss: 0.0111 - val_loss: 0.0110 - 718ms/epoch - 508us/step\n",
      "Epoch 17/28\n",
      "1415/1415 - 1s - loss: 0.0110 - val_loss: 0.0109 - 719ms/epoch - 508us/step\n",
      "Epoch 18/28\n",
      "1415/1415 - 1s - loss: 0.0110 - val_loss: 0.0108 - 730ms/epoch - 516us/step\n",
      "Epoch 19/28\n",
      "1415/1415 - 1s - loss: 0.0109 - val_loss: 0.0108 - 719ms/epoch - 508us/step\n",
      "Epoch 20/28\n",
      "1415/1415 - 1s - loss: 0.0108 - val_loss: 0.0107 - 719ms/epoch - 508us/step\n",
      "Epoch 21/28\n",
      "1415/1415 - 1s - loss: 0.0108 - val_loss: 0.0107 - 717ms/epoch - 507us/step\n",
      "Epoch 22/28\n",
      "1415/1415 - 1s - loss: 0.0107 - val_loss: 0.0106 - 726ms/epoch - 513us/step\n",
      "Epoch 23/28\n",
      "1415/1415 - 1s - loss: 0.0107 - val_loss: 0.0105 - 717ms/epoch - 507us/step\n",
      "Epoch 24/28\n",
      "1415/1415 - 1s - loss: 0.0106 - val_loss: 0.0105 - 842ms/epoch - 595us/step\n",
      "Epoch 25/28\n",
      "1415/1415 - 1s - loss: 0.0106 - val_loss: 0.0105 - 720ms/epoch - 509us/step\n",
      "Epoch 26/28\n",
      "1415/1415 - 1s - loss: 0.0105 - val_loss: 0.0104 - 734ms/epoch - 519us/step\n",
      "Epoch 27/28\n",
      "1415/1415 - 1s - loss: 0.0105 - val_loss: 0.0104 - 754ms/epoch - 533us/step\n",
      "Epoch 28/28\n",
      "1415/1415 - 1s - loss: 0.0104 - val_loss: 0.0103 - 863ms/epoch - 610us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x3448b7490&gt;,\n",
       "              n_iter=10, n_jobs=-1, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x3448b7490&gt;,\n",
       "              n_iter=10, n_jobs=-1, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x3448b7490&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x3448b7490&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x3448b7490>,\n",
       "              n_iter=10, n_jobs=-1, random_state=42,\n",
       "              search_spaces={'activation': ['relu', 'tanh'],\n",
       "                             'batch_size': [16, 32, 64],\n",
       "                             'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
       "                             'learning_rate': [0.001, 0.01, 0.1],\n",
       "                             'optimizer': ['sgd', 'adam']},\n",
       "              verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define base model function for bayes search\n",
    "def create_model(activation='relu', optimizer='adam', learning_rate=0.001, kernel_regularizer=None, batch_size=32):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)),\n",
    "        Dense(128, activation=activation, kernel_regularizer=kernel_regularizer), \n",
    "        Dense(64, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)  # Output layer with 1 neuron for regression\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=epochs, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'optimizer': ['sgd', 'adam'],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=epochs, verbose=2)\n",
    "\n",
    "# perform bayes search hyperparameter tuning\n",
    "bayes_search = BayesSearchCV(estimator=keras_reg, search_spaces=param_dist, n_iter=10, cv=5, verbose=2, random_state=42, error_score='raise', n_jobs=-1)\n",
    "bayes_search.fit(Xtrain, ytrain, sample_weight=wtrain, validation_data=(Xval, yval, wval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106.658198</td>\n",
       "      <td>0.370504</td>\n",
       "      <td>0.221495</td>\n",
       "      <td>0.024929</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.028800</td>\n",
       "      <td>-0.027909</td>\n",
       "      <td>-0.028003</td>\n",
       "      <td>-0.027963</td>\n",
       "      <td>-0.028090</td>\n",
       "      <td>-0.028153</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105.886285</td>\n",
       "      <td>0.684409</td>\n",
       "      <td>0.202457</td>\n",
       "      <td>0.025194</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-2.394302</td>\n",
       "      <td>-2.604819</td>\n",
       "      <td>-2.361294</td>\n",
       "      <td>-2.372018</td>\n",
       "      <td>-2.572588</td>\n",
       "      <td>-2.461004</td>\n",
       "      <td>0.105303</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71.668892</td>\n",
       "      <td>0.612557</td>\n",
       "      <td>0.183377</td>\n",
       "      <td>0.016142</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.027891</td>\n",
       "      <td>-0.030940</td>\n",
       "      <td>-0.031576</td>\n",
       "      <td>-0.031198</td>\n",
       "      <td>-0.028302</td>\n",
       "      <td>-0.029981</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>354.996373</td>\n",
       "      <td>1.103403</td>\n",
       "      <td>0.510601</td>\n",
       "      <td>0.085780</td>\n",
       "      <td>tanh</td>\n",
       "      <td>16</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 16, 'kern...</td>\n",
       "      <td>-0.521653</td>\n",
       "      <td>-0.512634</td>\n",
       "      <td>-0.549299</td>\n",
       "      <td>-0.946596</td>\n",
       "      <td>-0.464610</td>\n",
       "      <td>-0.598958</td>\n",
       "      <td>0.175952</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191.185697</td>\n",
       "      <td>0.319664</td>\n",
       "      <td>0.288745</td>\n",
       "      <td>0.035541</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 32, 'kern...</td>\n",
       "      <td>-0.607476</td>\n",
       "      <td>-0.631992</td>\n",
       "      <td>-0.713870</td>\n",
       "      <td>-0.587702</td>\n",
       "      <td>-0.423994</td>\n",
       "      <td>-0.593007</td>\n",
       "      <td>0.094795</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105.615029</td>\n",
       "      <td>0.129771</td>\n",
       "      <td>0.193015</td>\n",
       "      <td>0.018038</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.035928</td>\n",
       "      <td>-0.036116</td>\n",
       "      <td>-0.036105</td>\n",
       "      <td>-0.036124</td>\n",
       "      <td>-0.036072</td>\n",
       "      <td>-0.036069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>104.978380</td>\n",
       "      <td>0.692380</td>\n",
       "      <td>0.194093</td>\n",
       "      <td>0.021535</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.405105</td>\n",
       "      <td>-0.664988</td>\n",
       "      <td>-0.911213</td>\n",
       "      <td>-0.434243</td>\n",
       "      <td>-0.542505</td>\n",
       "      <td>-0.591611</td>\n",
       "      <td>0.184114</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71.845226</td>\n",
       "      <td>0.266802</td>\n",
       "      <td>0.186086</td>\n",
       "      <td>0.016325</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.010107</td>\n",
       "      <td>-0.010080</td>\n",
       "      <td>-0.010025</td>\n",
       "      <td>-0.010248</td>\n",
       "      <td>-0.010191</td>\n",
       "      <td>-0.010130</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>72.727813</td>\n",
       "      <td>0.477158</td>\n",
       "      <td>0.196715</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.068188</td>\n",
       "      <td>-0.067336</td>\n",
       "      <td>-0.067859</td>\n",
       "      <td>-0.071594</td>\n",
       "      <td>-0.066987</td>\n",
       "      <td>-0.068393</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>109.130262</td>\n",
       "      <td>0.151374</td>\n",
       "      <td>0.221905</td>\n",
       "      <td>0.020398</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.028274</td>\n",
       "      <td>-0.027864</td>\n",
       "      <td>-0.028144</td>\n",
       "      <td>-0.028327</td>\n",
       "      <td>-0.027992</td>\n",
       "      <td>-0.028120</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     106.658198      0.370504         0.221495        0.024929   \n",
       "1     105.886285      0.684409         0.202457        0.025194   \n",
       "2      71.668892      0.612557         0.183377        0.016142   \n",
       "3     354.996373      1.103403         0.510601        0.085780   \n",
       "4     191.185697      0.319664         0.288745        0.035541   \n",
       "5     105.615029      0.129771         0.193015        0.018038   \n",
       "6     104.978380      0.692380         0.194093        0.021535   \n",
       "7      71.845226      0.266802         0.186086        0.016325   \n",
       "8      72.727813      0.477158         0.196715        0.018382   \n",
       "9     109.130262      0.151374         0.221905        0.020398   \n",
       "\n",
       "  param_activation param_batch_size param_kernel_regularizer  \\\n",
       "0             relu               64                    l1_l2   \n",
       "1             tanh               64                       l1   \n",
       "2             relu               64                       l1   \n",
       "3             tanh               16                       l2   \n",
       "4             tanh               32                       l2   \n",
       "5             tanh               64                       l1   \n",
       "6             tanh               64                       l2   \n",
       "7             tanh               64                       l2   \n",
       "8             tanh               64                    l1_l2   \n",
       "9             relu               64                    l1_l2   \n",
       "\n",
       "  param_learning_rate param_optimizer  \\\n",
       "0               0.001            adam   \n",
       "1                 0.1            adam   \n",
       "2                0.01             sgd   \n",
       "3                 0.1            adam   \n",
       "4                 0.1            adam   \n",
       "5               0.001            adam   \n",
       "6                 0.1            adam   \n",
       "7                 0.1             sgd   \n",
       "8                0.01             sgd   \n",
       "9               0.001            adam   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.028800   \n",
       "1  {'activation': 'tanh', 'batch_size': 64, 'kern...          -2.394302   \n",
       "2  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.027891   \n",
       "3  {'activation': 'tanh', 'batch_size': 16, 'kern...          -0.521653   \n",
       "4  {'activation': 'tanh', 'batch_size': 32, 'kern...          -0.607476   \n",
       "5  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.035928   \n",
       "6  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.405105   \n",
       "7  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.010107   \n",
       "8  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.068188   \n",
       "9  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.028274   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0          -0.027909          -0.028003          -0.027963          -0.028090   \n",
       "1          -2.604819          -2.361294          -2.372018          -2.572588   \n",
       "2          -0.030940          -0.031576          -0.031198          -0.028302   \n",
       "3          -0.512634          -0.549299          -0.946596          -0.464610   \n",
       "4          -0.631992          -0.713870          -0.587702          -0.423994   \n",
       "5          -0.036116          -0.036105          -0.036124          -0.036072   \n",
       "6          -0.664988          -0.911213          -0.434243          -0.542505   \n",
       "7          -0.010080          -0.010025          -0.010248          -0.010191   \n",
       "8          -0.067336          -0.067859          -0.071594          -0.066987   \n",
       "9          -0.027864          -0.028144          -0.028327          -0.027992   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0        -0.028153        0.000329                3  \n",
       "1        -2.461004        0.105303               10  \n",
       "2        -0.029981        0.001558                4  \n",
       "3        -0.598958        0.175952                9  \n",
       "4        -0.593007        0.094795                8  \n",
       "5        -0.036069        0.000073                5  \n",
       "6        -0.591611        0.184114                7  \n",
       "7        -0.010130        0.000080                1  \n",
       "8        -0.068393        0.001653                6  \n",
       "9        -0.028120        0.000173                2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bayes_search_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "\n",
    "display(bayes_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 10058\n",
      "max_resources_: 90524\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 12\n",
      "n_resources: 10058\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_37274/419890925.py:26: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=epochs, verbose=2)\n",
      "2024-06-10 18:45:08.490966: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 18:45:08.571545: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 18:45:08.613074: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 18:45:08.655317: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 18:45:08.697838: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 18:45:08.742832: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 18:45:08.777204: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 18:45:08.802807: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-06-10 18:45:08.842800: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "   1/1415 [..............................] - ETA: 5:59 - loss: 18.2167Epoch 1/28\n",
      "  40/1415 [..............................] - ETA: 1s - loss: 16.8645  Epoch 1/28\n",
      "Epoch 1/28\n",
      "   1/1415 [..............................] - ETA: 6:41 - loss: 18.0780"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 18:45:08.952440: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 157/1415 [==>...........................] - ETA: 1s - loss: 15.7360Epoch 1/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 7.4424 - val_loss: 1.4588\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 7.5038 - val_loss: 1.4401\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 7.5258 - val_loss: 1.4809\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 7.5045 - val_loss: 1.4706\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 7.5340 - val_loss: 1.4295\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 1.0818 - val_loss: 0.1181\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 1.0689 - val_loss: 0.1256\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 1.0470 - val_loss: 0.1164\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.1138 - val_loss: 0.1184\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.0751 - val_loss: 0.1233\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.5163 - val_loss: 0.2077\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.4978 - val_loss: 0.2038\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.5212 - val_loss: 0.2238\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.5332 - val_loss: 0.2285\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.5141 - val_loss: 0.2102\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1026 - val_loss: 0.0874\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1155 - val_loss: 0.1067\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1057 - val_loss: 0.0941\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1011 - val_loss: 0.0890\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1071 - val_loss: 0.0982\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1566 - val_loss: 0.1248\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1675 - val_loss: 0.1417\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1766 - val_loss: 0.1493\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1824 - val_loss: 0.1566\n",
      "1399/1415 [============================>.] - ETA: 0s - loss: 0.1587Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1584 - val_loss: 0.1322\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0785 - val_loss: 0.0713\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0961 - val_loss: 0.0841\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0847 - val_loss: 0.0802\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0839 - val_loss: 0.0785\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0876 - val_loss: 0.0789\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1122 - val_loss: 0.1023\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1383 - val_loss: 0.1305\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1305 - val_loss: 0.1231\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1416 - val_loss: 0.1280\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1236 - val_loss: 0.1172\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0673 - val_loss: 0.0649\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0969 - val_loss: 0.0930\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0763 - val_loss: 0.0728\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0772 - val_loss: 0.0737\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0760 - val_loss: 0.0727\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1262 - val_loss: 0.1217\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1175 - val_loss: 0.1109\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0757 - val_loss: 0.0724\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1195 - val_loss: 0.1121\n",
      " 269/1415 [====>.........................] - ETA: 2s - loss: 0.0724Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1130 - val_loss: 0.1095\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0907 - val_loss: 0.0881\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0634 - val_loss: 0.0616\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1165 - val_loss: 0.1111\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1060 - val_loss: 0.1014\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0689 - val_loss: 0.0639\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1068 - val_loss: 0.1035\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1061 - val_loss: 0.0994\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0707 - val_loss: 0.0682\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0702 - val_loss: 0.0648\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0704 - val_loss: 0.0662\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0863 - val_loss: 0.0867\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1050 - val_loss: 0.0983\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0986 - val_loss: 0.0973\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0607 - val_loss: 0.0592\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1001 - val_loss: 0.0961\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0932 - val_loss: 0.0871\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0625 - val_loss: 0.0606\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0641 - val_loss: 0.0603\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0621 - val_loss: 0.0600\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0626 - val_loss: 0.0601\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0828 - val_loss: 0.0818\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0923 - val_loss: 0.0876\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0944 - val_loss: 0.0937\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0929 - val_loss: 0.0894\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0826 - val_loss: 0.0795\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0585 - val_loss: 0.0581\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0598 - val_loss: 0.0584\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0593 - val_loss: 0.0582\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0592 - val_loss: 0.0591\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0592 - val_loss: 0.0586\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0806 - val_loss: 0.0808\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0863 - val_loss: 0.0845\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0911 - val_loss: 0.0896\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0861 - val_loss: 0.0825\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0787 - val_loss: 0.0780\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0568 - val_loss: 0.0558\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0577 - val_loss: 0.0565\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0574 - val_loss: 0.0561\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0789 - val_loss: 0.0778\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0573 - val_loss: 0.0561\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0573 - val_loss: 0.0563\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0838 - val_loss: 0.0824\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0807 - val_loss: 0.0797\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0875 - val_loss: 0.0850\n",
      " 645/1415 [============>.................] - ETA: 1s - loss: 0.0565Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0770 - val_loss: 0.0761\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0554 - val_loss: 0.0545\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0771 - val_loss: 0.0765\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0557 - val_loss: 0.0547\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0556 - val_loss: 0.0555\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0561 - val_loss: 0.0549\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0558 - val_loss: 0.0558\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0810 - val_loss: 0.0792\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0829 - val_loss: 0.0806\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0787 - val_loss: 0.0777\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0758 - val_loss: 0.0748\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0749 - val_loss: 0.0739\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0541 - val_loss: 0.0536\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0782 - val_loss: 0.0765\n",
      " 405/1415 [=======>......................] - ETA: 1s - loss: 0.0533Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0542 - val_loss: 0.0537\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0773 - val_loss: 0.0765\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0781 - val_loss: 0.0756\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0547 - val_loss: 0.0540\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0544 - val_loss: 0.0535\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0544 - val_loss: 0.0535\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0747 - val_loss: 0.0736\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0530 - val_loss: 0.0527\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0761 - val_loss: 0.0778\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0760 - val_loss: 0.0754\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0741 - val_loss: 0.0748\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0530 - val_loss: 0.0521\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0532 - val_loss: 0.0528\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0535 - val_loss: 0.0529\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0737 - val_loss: 0.0763\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0532 - val_loss: 0.0524\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0693 - val_loss: 0.0675\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0521 - val_loss: 0.0516\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0751 - val_loss: 0.0755\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0749 - val_loss: 0.0744\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0709 - val_loss: 0.0695\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0727 - val_loss: 0.0724\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0518 - val_loss: 0.0509\n",
      " 384/1415 [=======>......................] - ETA: 1s - loss: 0.0742Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0523 - val_loss: 0.0516\n",
      " 470/1415 [========>.....................] - ETA: 1s - loss: 0.0745Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0524 - val_loss: 0.0518\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0522 - val_loss: 0.0513\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0674 - val_loss: 0.0665\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0743 - val_loss: 0.0744\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0737 - val_loss: 0.0726\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0678 - val_loss: 0.0664\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0512 - val_loss: 0.0507\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0718 - val_loss: 0.0707\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0505 - val_loss: 0.0499\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0514 - val_loss: 0.0505\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0515 - val_loss: 0.0509\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0512 - val_loss: 0.0506\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0667 - val_loss: 0.0666\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0725 - val_loss: 0.0718\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0735 - val_loss: 0.0729\n",
      " 793/1415 [===============>..............] - ETA: 0s - loss: 0.0661Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0655 - val_loss: 0.0658\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0708 - val_loss: 0.0699\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0504 - val_loss: 0.0500\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0486 - val_loss: 0.0471\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0505 - val_loss: 0.0500\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0506 - val_loss: 0.0500\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0660 - val_loss: 0.0673\n",
      "  55/1415 [>.............................] - ETA: 1s - loss: 0.0498Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0501 - val_loss: 0.0494\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0728 - val_loss: 0.0759\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0699 - val_loss: 0.0688\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0638 - val_loss: 0.0635\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0497 - val_loss: 0.0491\n",
      "1176/1415 [=======================>......] - ETA: 0s - loss: 0.0500Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0654 - val_loss: 0.0648\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0462 - val_loss: 0.0456\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0498 - val_loss: 0.0490\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0499 - val_loss: 0.0493\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0484 - val_loss: 0.0471\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0704 - val_loss: 0.0700\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0721 - val_loss: 0.0822\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0631 - val_loss: 0.0630\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0689 - val_loss: 0.0677\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0489 - val_loss: 0.0486\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0647 - val_loss: 0.0641\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0451 - val_loss: 0.0449\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0492 - val_loss: 0.0491\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0694 - val_loss: 0.0703\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0490 - val_loss: 0.0487\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0713 - val_loss: 0.0707\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0624 - val_loss: 0.0618\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0676 - val_loss: 0.0664\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0461 - val_loss: 0.0450\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0642 - val_loss: 0.0634\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0480 - val_loss: 0.0476\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0683 - val_loss: 0.0670\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0445 - val_loss: 0.0448\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0487 - val_loss: 0.0484\n",
      "1370/1415 [============================>.] - ETA: 0s - loss: 0.0449Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0707 - val_loss: 0.0702\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0483 - val_loss: 0.0478\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0619 - val_loss: 0.0636\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0657 - val_loss: 0.0640\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0449 - val_loss: 0.0443\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0636 - val_loss: 0.0662\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0468 - val_loss: 0.0458\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0671 - val_loss: 0.0660\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0700 - val_loss: 0.0690\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0614 - val_loss: 0.0617\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0634 - val_loss: 0.0627\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0480 - val_loss: 0.0478\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0440 - val_loss: 0.0437\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0473 - val_loss: 0.0465\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0443 - val_loss: 0.0440\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0632 - val_loss: 0.0626\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0656 - val_loss: 0.0642\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0448 - val_loss: 0.0438\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0693 - val_loss: 0.0692\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0608 - val_loss: 0.0602\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0613 - val_loss: 0.0608\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0473 - val_loss: 0.0471\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0435 - val_loss: 0.0431\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0457 - val_loss: 0.0447\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0438 - val_loss: 0.0436\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0626 - val_loss: 0.0619\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0635 - val_loss: 0.0619\n",
      " 723/1415 [==============>...............] - ETA: 1s - loss: 0.0466Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0437 - val_loss: 0.0436\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0685 - val_loss: 0.0684\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0603 - val_loss: 0.0611\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0604 - val_loss: 0.0598\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0463 - val_loss: 0.0456\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0431 - val_loss: 0.0429\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0621 - val_loss: 0.0618\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0441 - val_loss: 0.0435\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0434 - val_loss: 0.0433\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0610 - val_loss: 0.0663\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0675 - val_loss: 0.0676\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0599 - val_loss: 0.0603\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0599 - val_loss: 0.0593\n",
      "1005/1415 [====================>.........] - ETA: 0s - loss: 0.0427Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0432 - val_loss: 0.0428\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0616 - val_loss: 0.0620\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0426 - val_loss: 0.0421\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0445 - val_loss: 0.0441\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0432 - val_loss: 0.0428\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0590 - val_loss: 0.0579\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0430 - val_loss: 0.0428\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0662 - val_loss: 0.0654\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0594 - val_loss: 0.0600\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0594 - val_loss: 0.0587\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0428 - val_loss: 0.0424\n",
      "1086/1415 [======================>.......] - ETA: 0s - loss: 0.0432Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0423 - val_loss: 0.0419\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0431 - val_loss: 0.0428\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0428 - val_loss: 0.0424\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0583 - val_loss: 0.0574\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0426 - val_loss: 0.0422\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0641 - val_loss: 0.0629\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0589 - val_loss: 0.0591\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0589 - val_loss: 0.0588\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0608 - val_loss: 0.0612\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0424 - val_loss: 0.0428\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0578 - val_loss: 0.0568\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0419 - val_loss: 0.0415\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0423 - val_loss: 0.0422\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0619 - val_loss: 0.0601\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0584 - val_loss: 0.0575\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0587 - val_loss: 0.0590\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0422 - val_loss: 0.0420\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0603 - val_loss: 0.0597\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0421 - val_loss: 0.0420\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0573 - val_loss: 0.0569\n",
      "1060/1415 [=====================>........] - ETA: 0s - loss: 0.0420Epoch 1/28\n",
      "1335/1415 [===========================>..] - ETA: 0s - loss: 0.0580252/252 - 0s - loss: 0.0602 - 285ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "1152/1415 [=======================>......] - ETA: 0s - loss: 0.0419252/252 - 0s - loss: 0.0602 - 288ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0422 - val_loss: 0.0421\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0602 - val_loss: 0.0611\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0580 - val_loss: 0.0576\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0415 - val_loss: 0.0415\n",
      "Epoch 25/28\n",
      "1324/1415 [===========================>..] - ETA: 0s - loss: 0.0419252/252 - 0s - loss: 0.0606 - 256ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0582 - val_loss: 0.0578\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0420 - val_loss: 0.0418\n",
      "Epoch 25/28\n",
      " 109/1415 [=>............................] - ETA: 3s - loss: 0.0418252/252 - 0s - loss: 0.0599 - 349ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 184/1415 [==>...........................] - ETA: 2s - loss: 0.0418252/252 - 0s - loss: 0.0572 - 462ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0419 - val_loss: 0.0420\n",
      "Epoch 25/28\n",
      "  32/1415 [..............................] - ETA: 2s - loss: 0.0420252/252 - 0s - loss: 0.0598 - 296ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 113/1415 [=>............................] - ETA: 1s - loss: 0.0419252/252 - 0s - loss: 0.0574 - 272ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      " 450/1415 [========>.....................] - ETA: 1s - loss: 0.0417Epoch 1/28\n",
      " 464/1415 [========>.....................] - ETA: 1s - loss: 0.0417Epoch 1/28\n",
      " 486/1415 [=========>....................] - ETA: 1s - loss: 0.0417252/252 - 0s - loss: 0.0597 - 354ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 553/1415 [==========>...................] - ETA: 1s - loss: 0.0417252/252 - 0s - loss: 0.0572 - 358ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "Epoch 1/28\n",
      " 582/1415 [===========>..................] - ETA: 1s - loss: 0.0417252/252 - 0s - loss: 0.0598 - 270ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      " 684/1415 [=============>................] - ETA: 1s - loss: 0.0417252/252 - 0s - loss: 0.0598 - 265ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0577 - 340ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      " 744/1415 [==============>...............] - ETA: 1s - loss: 0.0417252/252 - 0s - loss: 0.0574 - 277ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 810/1415 [================>.............] - ETA: 1s - loss: 0.0417252/252 - 0s - loss: 0.0596 - 326ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "252/252 - 0s - loss: 0.0585 - 405ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 837/1415 [================>.............] - ETA: 0s - loss: 0.0417252/252 - 0s - loss: 0.0596 - 279ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0417 - val_loss: 0.0417\n",
      "Epoch 26/28\n",
      " 607/1415 [===========>..................] - ETA: 1s - loss: 0.0416252/252 - 0s - loss: 0.0578 - 271ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      " 916/1415 [==================>...........] - ETA: 0s - loss: 0.0417252/252 - 0s - loss: 0.0578 - 295ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "252/252 - 0s - loss: 0.0600 - 216ms/epoch - 858us/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0579 - 196ms/epoch - 776us/step\n",
      "Epoch 3/28\n",
      "1032/1415 [====================>.........] - ETA: 0s - loss: 0.0416252/252 - 0s - loss: 0.0595 - 335ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0594 - 213ms/epoch - 844us/step\n",
      "Epoch 5/28\n",
      " 183/1415 [==>...........................] - ETA: 2s - loss: 0.0416252/252 - 0s - loss: 0.0576 - 349ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 800/1415 [===============>..............] - ETA: 1s - loss: 0.0416252/252 - 0s - loss: 0.0582 - 215ms/epoch - 853us/step\n",
      "Epoch 4/28\n",
      " 257/1415 [====>.........................] - ETA: 2s - loss: 0.0415252/252 - 0s - loss: 0.0570 - 458ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1159/1415 [=======================>......] - ETA: 0s - loss: 0.0416252/252 - 0s - loss: 0.0594 - 338ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 345/1415 [======>.......................] - ETA: 2s - loss: 0.0416252/252 - 0s - loss: 0.0595 - 387ms/epoch - 2ms/step\n",
      " 959/1415 [===================>..........] - ETA: 0s - loss: 0.0415Epoch 6/28\n",
      "1224/1415 [========================>.....] - ETA: 0s - loss: 0.0416252/252 - 0s - loss: 0.0575 - 401ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      " 366/1415 [======>.......................] - ETA: 2s - loss: 0.0416252/252 - 0s - loss: 0.0583 - 418ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "1289/1415 [==========================>...] - ETA: 0s - loss: 0.0416252/252 - 0s - loss: 0.0575 - 290ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1355/1415 [===========================>..] - ETA: 0s - loss: 0.0416252/252 - 0s - loss: 0.0594 - 309ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1389/1415 [============================>.] - ETA: 0s - loss: 0.0416252/252 - 0s - loss: 0.0581 - 240ms/epoch - 953us/step\n",
      "Epoch 6/28\n",
      "1141/1415 [=======================>......] - ETA: 0s - loss: 0.0415252/252 - 0s - loss: 0.0596 - 309ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0572 - 288ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 613/1415 [===========>..................] - ETA: 1s - loss: 0.0415252/252 - 0s - loss: 0.0568 - 273ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0418 - val_loss: 0.0418\n",
      "Epoch 26/28\n",
      "1287/1415 [==========================>...] - ETA: 0s - loss: 0.0415252/252 - 0s - loss: 0.0594 - 272ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0412 - val_loss: 0.0409\n",
      "Epoch 26/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0405252/252 - 0s - loss: 0.0577 - 278ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 745/1415 [==============>...............] - ETA: 1s - loss: 0.0415252/252 - 0s - loss: 0.0575 - 286ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0594 - 295ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0416 - val_loss: 0.0414\n",
      "Epoch 26/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.0409252/252 - 0s - loss: 0.0570 - 294ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 140/1415 [=>............................] - ETA: 2s - loss: 0.0410252/252 - 0s - loss: 0.0592 - 307ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 174/1415 [==>...........................] - ETA: 2s - loss: 0.0409252/252 - 0s - loss: 0.0574 - 280ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      " 124/1415 [=>............................] - ETA: 2s - loss: 0.0412252/252 - 0s - loss: 0.0578 - 334ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      " 208/1415 [===>..........................] - ETA: 2s - loss: 0.0410252/252 - 0s - loss: 0.0591 - 329ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 175/1415 [==>...........................] - ETA: 2s - loss: 0.0411252/252 - 0s - loss: 0.0568 - 305ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1051/1415 [=====================>........] - ETA: 0s - loss: 0.0415252/252 - 0s - loss: 0.0593 - 287ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0415 - val_loss: 0.0412\n",
      "Epoch 26/28\n",
      "1081/1415 [=====================>........] - ETA: 0s - loss: 0.0415252/252 - 0s - loss: 0.0570 - 279ms/epoch - 1ms/step\n",
      "   1/1415 [..............................] - ETA: 3s - loss: 0.0406Epoch 9/28\n",
      " 412/1415 [=======>......................] - ETA: 1s - loss: 0.0415252/252 - 0s - loss: 0.0579 - 272ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 365/1415 [======>.......................] - ETA: 1s - loss: 0.0410252/252 - 0s - loss: 0.0595 - 265ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 411/1415 [=======>......................] - ETA: 1s - loss: 0.0410252/252 - 0s - loss: 0.0571 - 291ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 509/1415 [=========>....................] - ETA: 1s - loss: 0.0409252/252 - 0s - loss: 0.0571 - 282ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0591 - 342ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 207/1415 [===>..........................] - ETA: 2s - loss: 0.0413252/252 - 0s - loss: 0.0579 - 347ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 644/1415 [============>.................] - ETA: 1s - loss: 0.0415252/252 - 0s - loss: 0.0589 - 380ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      " 612/1415 [===========>..................] - ETA: 1s - loss: 0.0409252/252 - 0s - loss: 0.0572 - 307ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 793/1415 [===============>..............] - ETA: 0s - loss: 0.0415252/252 - 0s - loss: 0.0574 - 337ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 725/1415 [==============>...............] - ETA: 1s - loss: 0.0409252/252 - 0s - loss: 0.0592 - 344ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 824/1415 [================>.............] - ETA: 0s - loss: 0.0415252/252 - 0s - loss: 0.0576 - 312ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 793/1415 [===============>..............] - ETA: 1s - loss: 0.0409252/252 - 0s - loss: 0.0590 - 315ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 826/1415 [================>.............] - ETA: 0s - loss: 0.0409252/252 - 0s - loss: 0.0566 - 347ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 814/1415 [================>.............] - ETA: 1s - loss: 0.0413252/252 - 0s - loss: 0.0592 - 278ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 848/1415 [================>.............] - ETA: 0s - loss: 0.0413252/252 - 0s - loss: 0.0571 - 341ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 943/1415 [==================>...........] - ETA: 0s - loss: 0.0409252/252 - 0s - loss: 0.0573 - 310ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0414 - val_loss: 0.0413\n",
      "Epoch 27/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0407252/252 - 0s - loss: 0.0589 - 269ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 648/1415 [============>.................] - ETA: 1s - loss: 0.0412252/252 - 0s - loss: 0.0564 - 278ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 116/1415 [=>............................] - ETA: 2s - loss: 0.0415252/252 - 0s - loss: 0.0572 - 228ms/epoch - 906us/step\n",
      "Epoch 13/28\n",
      " 724/1415 [==============>...............] - ETA: 1s - loss: 0.0412252/252 - 0s - loss: 0.0589 - 311ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1037/1415 [====================>.........] - ETA: 0s - loss: 0.0409252/252 - 0s - loss: 0.0569 - 280ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 201/1415 [===>..........................] - ETA: 1s - loss: 0.0414252/252 - 0s - loss: 0.0587 - 293ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 252/1415 [====>.........................] - ETA: 1s - loss: 0.0414252/252 - 0s - loss: 0.0565 - 360ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 285/1415 [=====>........................] - ETA: 1s - loss: 0.0414252/252 - 0s - loss: 0.0575 - 240ms/epoch - 954us/step\n",
      "Epoch 14/28\n",
      " 377/1415 [======>.......................] - ETA: 1s - loss: 0.0413252/252 - 0s - loss: 0.0589 - 470ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "1379/1415 [============================>.] - ETA: 0s - loss: 0.0415252/252 - 1s - loss: 0.0567 - 517ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1252/1415 [=========================>....] - ETA: 0s - loss: 0.0409252/252 - 0s - loss: 0.0567 - 347ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "252/252 - 0s - loss: 0.0587 - 481ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1320/1415 [==========================>...] - ETA: 0s - loss: 0.0408252/252 - 0s - loss: 0.0574 - 421ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 604/1415 [===========>..................] - ETA: 1s - loss: 0.0412252/252 - 0s - loss: 0.0588 - 350ms/epoch - 1ms/step\n",
      "1401/1415 [============================>.] - ETA: 0s - loss: 0.0412Epoch 21/28\n",
      "1144/1415 [=======================>......] - ETA: 0s - loss: 0.0411252/252 - 0s - loss: 0.0566 - 291ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0567 - 304ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "252/252 - 0s - loss: 0.0589 - 308ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0415 - val_loss: 0.0414\n",
      "Epoch 27/28\n",
      "  17/1415 [..............................] - ETA: 4s - loss: 0.0417252/252 - 0s - loss: 0.0572 - 302ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "  85/1415 [>.............................] - ETA: 2s - loss: 0.0415252/252 - 0s - loss: 0.0592 - 227ms/epoch - 901us/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0570 - 228ms/epoch - 905us/step\n",
      "Epoch 16/28\n",
      "1350/1415 [===========================>..] - ETA: 0s - loss: 0.0411252/252 - 0s - loss: 0.0565 - 271ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 159/1415 [==>...........................] - ETA: 2s - loss: 0.0414252/252 - 0s - loss: 0.0587 - 283ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0408 - val_loss: 0.0405\n",
      "Epoch 27/28\n",
      " 231/1415 [===>..........................] - ETA: 1s - loss: 0.0415252/252 - 0s - loss: 0.0574 - 289ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0412 - val_loss: 0.0411\n",
      "Epoch 27/28\n",
      " 301/1415 [=====>........................] - ETA: 1s - loss: 0.0415252/252 - 0s - loss: 0.0567 - 287ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "252/252 - 0s - loss: 0.0589 - 310ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "  81/1415 [>.............................] - ETA: 2s - loss: 0.0408252/252 - 0s - loss: 0.0565 - 288ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 397/1415 [=======>......................] - ETA: 1s - loss: 0.0414252/252 - 0s - loss: 0.0586 - 357ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 208/1415 [===>..........................] - ETA: 2s - loss: 0.0406252/252 - 0s - loss: 0.0570 - 338ms/epoch - 1ms/step\n",
      "1102/1415 [======================>.......] - ETA: 0s - loss: 0.0411Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0411 - val_loss: 0.0409\n",
      "252/252 - 0s - loss: 0.0569 - 326ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "Epoch 18/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0404252/252 - 0s - loss: 0.0587 - 317ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 510/1415 [=========>....................] - ETA: 1s - loss: 0.0413252/252 - 0s - loss: 0.0565 - 299ms/epoch - 1ms/step\n",
      " 308/1415 [=====>........................] - ETA: 1s - loss: 0.0409Epoch 21/28\n",
      " 337/1415 [======>.......................] - ETA: 1s - loss: 0.0409252/252 - 0s - loss: 0.0585 - 269ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 106/1415 [=>............................] - ETA: 2s - loss: 0.0408252/252 - 0s - loss: 0.0569 - 299ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 178/1415 [==>...........................] - ETA: 2s - loss: 0.0408252/252 - 0s - loss: 0.0588 - 305ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1332/1415 [===========================>..] - ETA: 0s - loss: 0.0411252/252 - 0s - loss: 0.0564 - 326ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 483/1415 [=========>....................] - ETA: 1s - loss: 0.0405252/252 - 0s - loss: 0.0565 - 323ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 238/1415 [====>.........................] - ETA: 2s - loss: 0.0408252/252 - 0s - loss: 0.0584 - 314ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 302/1415 [=====>........................] - ETA: 1s - loss: 0.0407252/252 - 0s - loss: 0.0571 - 304ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 829/1415 [================>.............] - ETA: 0s - loss: 0.0413252/252 - 0s - loss: 0.0564 - 274ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 865/1415 [=================>............] - ETA: 0s - loss: 0.0413252/252 - 0s - loss: 0.0589 - 335ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 674/1415 [=============>................] - ETA: 1s - loss: 0.0405252/252 - 0s - loss: 0.0562 - 301ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 710/1415 [==============>...............] - ETA: 1s - loss: 0.0405252/252 - 0s - loss: 0.0584 - 318ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 955/1415 [===================>..........] - ETA: 0s - loss: 0.0413252/252 - 0s - loss: 0.0567 - 290ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0411 - val_loss: 0.0408\n",
      "Epoch 28/28\n",
      " 545/1415 [==========>...................] - ETA: 1s - loss: 0.0408252/252 - 0s - loss: 0.0565 - 319ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "  51/1415 [>.............................] - ETA: 2s - loss: 0.0409252/252 - 0s - loss: 0.0589 - 314ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "  76/1415 [>.............................] - ETA: 2s - loss: 0.0410252/252 - 0s - loss: 0.0563 - 322ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 104/1415 [=>............................] - ETA: 2s - loss: 0.0409252/252 - 0s - loss: 0.0583 - 325ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 939/1415 [==================>...........] - ETA: 0s - loss: 0.0408252/252 - 0s - loss: 0.0570 - 317ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1019/1415 [====================>.........] - ETA: 0s - loss: 0.0405252/252 - 0s - loss: 0.0563 - 314ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1242/1415 [=========================>....] - ETA: 0s - loss: 0.0412252/252 - 0s - loss: 0.0585 - 302ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 267/1415 [====>.........................] - ETA: 1s - loss: 0.0409252/252 - 0s - loss: 0.0556 - 312ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 293/1415 [=====>........................] - ETA: 1s - loss: 0.0409252/252 - 0s - loss: 0.0581 - 303ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1115/1415 [======================>.......] - ETA: 0s - loss: 0.0408252/252 - 0s - loss: 0.0569 - 297ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 380/1415 [=======>......................] - ETA: 1s - loss: 0.0410252/252 - 0s - loss: 0.0563 - 268ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1209/1415 [========================>.....] - ETA: 0s - loss: 0.0408252/252 - 0s - loss: 0.0584 - 310ms/epoch - 1ms/step\n",
      "1243/1415 [=========================>....] - ETA: 0s - loss: 0.0408252/252 - 0s - loss: 0.0560 - 323ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 473/1415 [=========>....................] - ETA: 1s - loss: 0.0409252/252 - 0s - loss: 0.0582 - 308ms/epoch - 1ms/step\n",
      "1278/1415 [==========================>...] - ETA: 0s - loss: 0.0408Epoch 24/28\n",
      " 503/1415 [=========>....................] - ETA: 1s - loss: 0.0409252/252 - 0s - loss: 0.0568 - 288ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1355/1415 [===========================>..] - ETA: 0s - loss: 0.040863/63 - 0s - loss: 0.0571 - 229ms/epoch - 4ms/step\n",
      "252/252 - 0s - loss: 0.0562 - 350ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 614/1415 [============>.................] - ETA: 1s - loss: 0.0409252/252 - 0s - loss: 0.0560 - 309ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      " 652/1415 [============>.................] - ETA: 1s - loss: 0.0408252/252 - 0s - loss: 0.0581 - 304ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0412 - val_loss: 0.0409\n",
      "Epoch 28/28\n",
      " 690/1415 [=============>................] - ETA: 1s - loss: 0.0408252/252 - 0s - loss: 0.0569 - 304ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1239/1415 [=========================>....] - ETA: 0s - loss: 0.0407252/252 - 0s - loss: 0.0564 - 255ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 803/1415 [================>.............] - ETA: 1s - loss: 0.0408252/252 - 0s - loss: 0.0559 - 331ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1, optimizer=sgd; total time= 1.4min\n",
      " 839/1415 [================>.............] - ETA: 0s - loss: 0.0408252/252 - 0s - loss: 0.0558 - 289ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 872/1415 [=================>............] - ETA: 0s - loss: 0.0408252/252 - 0s - loss: 0.0580 - 312ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0404 - val_loss: 0.0404\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0408 - val_loss: 0.0405\n",
      "Epoch 28/28\n",
      "   1/1415 [..............................] - ETA: 3s - loss: 0.0402252/252 - 0s - loss: 0.0567 - 320ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "  54/1415 [>.............................] - ETA: 2s - loss: 0.0405252/252 - 0s - loss: 0.0563 - 338ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 111/1415 [=>............................] - ETA: 2s - loss: 0.0405252/252 - 0s - loss: 0.0561 - 317ms/epoch - 1ms/step\n",
      " 146/1415 [==>...........................] - ETA: 2s - loss: 0.0405252/252 - 0s - loss: 0.0578 - 309ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      " 179/1415 [==>...........................] - ETA: 2s - loss: 0.0405252/252 - 0s - loss: 0.0563 - 310ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      " 248/1415 [====>.........................] - ETA: 1s - loss: 0.0404252/252 - 0s - loss: 0.0559 - 307ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "63/63 - 0s - loss: 0.0518 - 194ms/epoch - 3ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0407 - val_loss: 0.0407\n",
      "Epoch 28/28\n",
      "1254/1415 [=========================>....] - ETA: 0s - loss: 0.0408252/252 - 0s - loss: 0.0580 - 280ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1313/1415 [==========================>...] - ETA: 0s - loss: 0.0407252/252 - 0s - loss: 0.0529 - 194ms/epoch - 769us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1, optimizer=sgd; total time= 1.4min\n",
      "252/252 - 0s - loss: 0.0563 - 335ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1369/1415 [============================>.] - ETA: 0s - loss: 0.0407252/252 - 0s - loss: 0.0563 - 333ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 227/1415 [===>..........................] - ETA: 2s - loss: 0.0405252/252 - 0s - loss: 0.0576 - 335ms/epoch - 1ms/step\n",
      " 818/1415 [================>.............] - ETA: 0s - loss: 0.0409252/252 - 0s - loss: 0.0566 - 351ms/epoch - 1ms/step\n",
      " 632/1415 [============>.................] - ETA: 1s - loss: 0.040563/63 - 0s - loss: 0.0566 - 111ms/epoch - 2ms/step\n",
      " 870/1415 [=================>............] - ETA: 0s - loss: 0.0409252/252 - 0s - loss: 0.0561 - 449ms/epoch - 2ms/step\n",
      " 673/1415 [=============>................] - ETA: 1s - loss: 0.040263/63 - 0s - loss: 0.0536 - 218ms/epoch - 3ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0407 - val_loss: 0.0409\n",
      " 777/1415 [===============>..............] - ETA: 1s - loss: 0.040263/63 - 0s - loss: 0.0551 - 148ms/epoch - 2ms/step\n",
      " 809/1415 [================>.............] - ETA: 1s - loss: 0.0402252/252 - 0s - loss: 0.0570 - 309ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1, optimizer=sgd; total time= 1.5min\n",
      " 839/1415 [================>.............] - ETA: 1s - loss: 0.0402252/252 - 0s - loss: 0.0534 - 287ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1, optimizer=sgd; total time= 1.5min\n",
      " 677/1415 [=============>................] - ETA: 1s - loss: 0.0404252/252 - 0s - loss: 0.0554 - 304ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1, optimizer=sgd; total time= 1.5min\n",
      "1281/1415 [==========================>...] - ETA: 0s - loss: 0.0401Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0408 - val_loss: 0.0410\n",
      "1145/1415 [=======================>......] - ETA: 0s - loss: 0.0404252/252 - 0s - loss: 0.0407 - 299ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0404 - val_loss: 0.0403\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0401 - val_loss: 0.0400\n",
      "1347/1415 [===========================>..] - ETA: 0s - loss: 0.0404252/252 - 0s - loss: 0.0406 - 318ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1401/1415 [============================>.] - ETA: 0s - loss: 0.0403Epoch 1/28\n",
      "252/252 - 0s - loss: 0.0405 - 441ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0403 - val_loss: 0.0402\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "252/252 - 0s - loss: 0.0408 - 398ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "252/252 - 0s - loss: 0.0400 - 252ms/epoch - 1000us/step\n",
      "Epoch 2/28\n",
      "252/252 - 0s - loss: 0.0405 - 305ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "252/252 - 0s - loss: 0.0403 - 331ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "252/252 - 0s - loss: 0.0407 - 243ms/epoch - 966us/step\n",
      "Epoch 3/28\n",
      "252/252 - 0s - loss: 0.0405 - 299ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0402 - 230ms/epoch - 911us/step\n",
      "Epoch 6/28\n",
      "Epoch 3/28\n",
      "252/252 - 0s - loss: 0.0400 - 322ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "Epoch 1/28\n",
      "252/252 - 0s - loss: 0.0406 - 314ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0404 - 301ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0402 - 330ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0399 - 335ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0402 - 332ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "252/252 - 0s - loss: 0.0403 - 319ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0405 - 418ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "252/252 - 0s - loss: 0.0401 - 357ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "252/252 - 0s - loss: 0.0399 - 391ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "252/252 - 0s - loss: 0.0401 - 351ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "252/252 - 0s - loss: 0.0403 - 357ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0404 - 361ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "252/252 - 0s - loss: 0.0401 - 329ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "252/252 - 0s - loss: 0.0398 - 356ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "252/252 - 0s - loss: 0.0400 - 317ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0402 - 343ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0400 - 361ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0403 - 399ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0397 - 319ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0401 - 353ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "252/252 - 0s - loss: 0.0400 - 320ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0401 - 392ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0404 - 369ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0398 - 423ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0400 - 377ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "252/252 - 0s - loss: 0.0400 - 306ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0401 - 338ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0404 - 354ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0397 - 336ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0399 - 347ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0400 - 357ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0401 - 340ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "252/252 - 0s - loss: 0.0402 - 338ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0396 - 339ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0399 - 331ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0399 - 324ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0400 - 395ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0402 - 342ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0399 - 362ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0396 - 424ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0401 - 282ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0399 - 398ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0401 - 314ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0395 - 424ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0399 - 478ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0400 - 476ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0398 - 467ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "252/252 - 0s - loss: 0.0401 - 495ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "252/252 - 0s - loss: 0.0398 - 376ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0396 - 419ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "252/252 - 0s - loss: 0.0398 - 311ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "Epoch 1/28\n",
      "252/252 - 0s - loss: 0.0400 - 401ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "252/252 - 0s - loss: 0.0400 - 404ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0397 - 382ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0395 - 374ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0400 - 259ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "252/252 - 0s - loss: 0.0398 - 362ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "   1/1415 [..............................] - ETA: 9:20 - loss: 2.9068252/252 - 0s - loss: 0.0400 - 323ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "  56/1415 [>.............................] - ETA: 3s - loss: 2.1761252/252 - 0s - loss: 0.0395 - 318ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0397 - 336ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 131/1415 [=>............................] - ETA: 2s - loss: 2.1165252/252 - 0s - loss: 0.0398 - 372ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 205/1415 [===>..........................] - ETA: 1s - loss: 2.0774252/252 - 0s - loss: 0.0399 - 412ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      " 265/1415 [====>.........................] - ETA: 1s - loss: 2.0489252/252 - 0s - loss: 0.0400 - 387ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "Epoch 1/28\n",
      " 459/1415 [========>.....................] - ETA: 1s - loss: 1.9664252/252 - 0s - loss: 0.0395 - 418ms/epoch - 2ms/step\n",
      "252/252 - 0s - loss: 0.0396 - 409ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "Epoch 14/28\n",
      "  22/1415 [..............................] - ETA: 3s - loss: 2.2020  252/252 - 0s - loss: 0.0397 - 373ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 523/1415 [==========>...................] - ETA: 1s - loss: 1.9410252/252 - 0s - loss: 0.0399 - 390ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 578/1415 [===========>..................] - ETA: 1s - loss: 1.9196252/252 - 0s - loss: 0.0399 - 381ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      " 686/1415 [=============>................] - ETA: 0s - loss: 1.8790252/252 - 0s - loss: 0.0396 - 366ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 175/1415 [==>...........................] - ETA: 2s - loss: 2.0822252/252 - 0s - loss: 0.0394 - 378ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 725/1415 [==============>...............] - ETA: 0s - loss: 1.8647Epoch 1/28\n",
      " 206/1415 [===>..........................] - ETA: 2s - loss: 2.0677252/252 - 0s - loss: 0.0397 - 354ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 757/1415 [===============>..............] - ETA: 0s - loss: 1.8532252/252 - 0s - loss: 0.0398 - 338ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 237/1415 [====>.........................] - ETA: 2s - loss: 2.0535Epoch 1/28\n",
      " 824/1415 [================>.............] - ETA: 0s - loss: 1.8293252/252 - 0s - loss: 0.0399 - 356ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 394/1415 [=======>......................] - ETA: 1s - loss: 1.9869252/252 - 0s - loss: 0.0394 - 354ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "252/252 - 0s - loss: 0.0397 - 372ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 419/1415 [=======>......................] - ETA: 1s - loss: 1.9768252/252 - 0s - loss: 0.0396 - 370ms/epoch - 1ms/step\n",
      " 981/1415 [===================>..........] - ETA: 0s - loss: 1.7752Epoch 19/28\n",
      "   1/1415 [..............................] - ETA: 7:22 - loss: 3.1240252/252 - 0s - loss: 0.0399 - 368ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "Epoch 1/28\n",
      " 310/1415 [=====>........................] - ETA: 1s - loss: 2.0402252/252 - 0s - loss: 0.0398 - 444ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "   9/1415 [..............................] - ETA: 11s - loss: 2.3295 252/252 - 0s - loss: 0.0395 - 421ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0396 - 415ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "  18/1415 [..............................] - ETA: 9s - loss: 2.2370 252/252 - 0s - loss: 0.0396 - 409ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 185/1415 [==>...........................] - ETA: 2s - loss: 2.1055252/252 - 0s - loss: 0.0398 - 411ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 284/1415 [=====>........................] - ETA: 2s - loss: 2.0579252/252 - 0s - loss: 0.0398 - 387ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 377/1415 [======>.......................] - ETA: 2s - loss: 2.0172252/252 - 0s - loss: 0.0393 - 389ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 937/1415 [==================>...........] - ETA: 0s - loss: 1.7863252/252 - 0s - loss: 0.0396 - 413ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      " 410/1415 [=======>......................] - ETA: 1s - loss: 2.0034252/252 - 0s - loss: 0.0397 - 383ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "252/252 - 0s - loss: 0.0396 - 410ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      " 516/1415 [=========>....................] - ETA: 1s - loss: 1.9604252/252 - 0s - loss: 0.0399 - 386ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      " 617/1415 [============>.................] - ETA: 1s - loss: 1.9212252/252 - 0s - loss: 0.0393 - 373ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 410/1415 [=======>......................] - ETA: 2s - loss: 1.9741252/252 - 0s - loss: 0.0396 - 369ms/epoch - 1ms/step\n",
      " 643/1415 [============>.................] - ETA: 1s - loss: 1.9113Epoch 19/28\n",
      "1205/1415 [========================>.....] - ETA: 0s - loss: 1.6989252/252 - 0s - loss: 0.0396 - 359ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 660/1415 [============>.................] - ETA: 1s - loss: 1.9050252/252 - 0s - loss: 0.0397 - 381ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      " 784/1415 [===============>..............] - ETA: 1s - loss: 1.8592252/252 - 0s - loss: 0.0398 - 378ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 910/1415 [==================>...........] - ETA: 0s - loss: 1.8146252/252 - 0s - loss: 0.0392 - 386ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0395 - 394ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 941/1415 [==================>...........] - ETA: 0s - loss: 1.8039252/252 - 0s - loss: 0.0395 - 382ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "252/252 - 0s - loss: 0.0397 - 371ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1049/1415 [=====================>........] - ETA: 0s - loss: 1.7673252/252 - 0s - loss: 0.0397 - 363ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1141/1415 [=======================>......] - ETA: 0s - loss: 1.7371252/252 - 0s - loss: 0.0393 - 370ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 997/1415 [====================>.........] - ETA: 0s - loss: 1.7608252/252 - 0s - loss: 0.0396 - 336ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0395 - 360ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "Epoch 21/28\n",
      "1173/1415 [=======================>......] - ETA: 0s - loss: 1.7268252/252 - 0s - loss: 0.0397 - 351ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1290/1415 [==========================>...] - ETA: 0s - loss: 1.6897252/252 - 0s - loss: 0.0397 - 343ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1386/1415 [============================>.] - ETA: 0s - loss: 1.6602252/252 - 0s - loss: 0.0392 - 420ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "1226/1415 [========================>.....] - ETA: 0s - loss: 1.6871252/252 - 0s - loss: 0.0395 - 430ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0395 - 431ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "252/252 - 0s - loss: 0.0396 - 438ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "1339/1415 [===========================>..] - ETA: 0s - loss: 1.6525252/252 - 0s - loss: 0.0397 - 440ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "1404/1415 [============================>.] - ETA: 0s - loss: 1.6330252/252 - 0s - loss: 0.0391 - 347ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "252/252 - 0s - loss: 0.0393 - 339ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0394 - 343ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "252/252 - 0s - loss: 0.0396 - 368ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0396 - 327ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "63/63 - 0s - loss: 0.0392 - 99ms/epoch - 2ms/step\n",
      "252/252 - 0s - loss: 0.0395 - 332ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "252/252 - 0s - loss: 0.0393 - 331ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "252/252 - 0s - loss: 0.0391 - 386ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0393 - 159ms/epoch - 630us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x10cfcc790>; total time= 1.6min\n",
      "252/252 - 0s - loss: 0.0395 - 384ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "252/252 - 0s - loss: 0.0391 - 264ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "252/252 - 0s - loss: 0.0394 - 416ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "252/252 - 0s - loss: 0.0393 - 416ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 1.6381 - val_loss: 1.2160\n",
      "Epoch 2/28\n",
      "  33/1415 [..............................] - ETA: 2s - loss: 1.2078252/252 - 0s - loss: 0.0395 - 290ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 175/1415 [==>...........................] - ETA: 1s - loss: 1.1750252/252 - 0s - loss: 0.0391 - 363ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 206/1415 [===>..........................] - ETA: 1s - loss: 1.1679252/252 - 0s - loss: 0.0393 - 282ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 234/1415 [===>..........................] - ETA: 1s - loss: 1.1616252/252 - 0s - loss: 0.0394 - 346ms/epoch - 1ms/step\n",
      " 295/1415 [=====>........................] - ETA: 1s - loss: 1.1479252/252 - 0s - loss: 0.0395 - 327ms/epoch - 1ms/step\n",
      " 326/1415 [=====>........................] - ETA: 1s - loss: 1.141063/63 - 0s - loss: 0.0388 - 156ms/epoch - 2ms/step\n",
      " 369/1415 [======>.......................] - ETA: 1s - loss: 1.1316252/252 - 0s - loss: 0.0391 - 291ms/epoch - 1ms/step\n",
      " 416/1415 [=======>......................] - ETA: 1s - loss: 1.1214252/252 - 0s - loss: 0.0392 - 345ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "63/63 - 0s - loss: 0.0389 - 219ms/epoch - 3ms/step\n",
      " 444/1415 [========>.....................] - ETA: 1s - loss: 1.115463/63 - 0s - loss: 0.0390 - 138ms/epoch - 2ms/step\n",
      "252/252 - 0s - loss: 0.0391 - 178ms/epoch - 707us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x12710c790>; total time= 1.7min\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 1.6349 - val_loss: 1.2145\n",
      "Epoch 2/28\n",
      " 602/1415 [===========>..................] - ETA: 1s - loss: 1.0823252/252 - 0s - loss: 0.0390 - 228ms/epoch - 905us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x10d1fc790>; total time= 1.7min\n",
      " 649/1415 [============>.................] - ETA: 1s - loss: 1.0727252/252 - 0s - loss: 0.0388 - 232ms/epoch - 921us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1162a8790>; total time= 1.7min\n",
      " 715/1415 [==============>...............] - ETA: 0s - loss: 1.0594252/252 - 0s - loss: 0.0392 - 420ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 1.6477 - val_loss: 1.2234\n",
      "Epoch 2/28\n",
      "1024/1415 [====================>.........] - ETA: 0s - loss: 1.0002252/252 - 0s - loss: 0.0392 - 325ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 1.6299 - val_loss: 1.2107\n",
      "Epoch 2/28\n",
      "  69/1415 [>.............................] - ETA: 2s - loss: 1.194563/63 - 0s - loss: 0.0388 - 126ms/epoch - 2ms/step\n",
      " 836/1415 [================>.............] - ETA: 0s - loss: 1.0346252/252 - 0s - loss: 0.0388 - 243ms/epoch - 962us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1625d7790>; total time= 1.7min\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.6517 - val_loss: 1.2255\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9320 - val_loss: 0.6966\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9310 - val_loss: 0.6959\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9377 - val_loss: 0.7007\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9281 - val_loss: 0.6938\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9392 - val_loss: 0.7018\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.5361 - val_loss: 0.4029\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.5356 - val_loss: 0.4024\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.5393 - val_loss: 0.4053\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.5342 - val_loss: 0.4016\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.5401 - val_loss: 0.4059\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.3121 - val_loss: 0.2365\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.3116 - val_loss: 0.2360\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.3139 - val_loss: 0.2378\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.3111 - val_loss: 0.2358\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.3143 - val_loss: 0.2381\n",
      "Epoch 5/28\n",
      " 262/1415 [====>.........................] - ETA: 1s - loss: 0.2273Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1850 - val_loss: 0.1420\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1845 - val_loss: 0.1416\n",
      "Epoch 6/28\n",
      " 596/1415 [===========>..................] - ETA: 1s - loss: 0.4525Epoch 1/28\n",
      "1011/1415 [====================>.........] - ETA: 0s - loss: 0.1200Epoch 1/28\n",
      " 291/1415 [=====>........................] - ETA: 1s - loss: 0.1349Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1845 - val_loss: 0.1417\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1859 - val_loss: 0.1426\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1862 - val_loss: 0.1429\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1127 - val_loss: 0.0882\n",
      "Epoch 7/28\n",
      "1052/1415 [=====================>........] - ETA: 0s - loss: 0.1188Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1123 - val_loss: 0.0878\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1132 - val_loss: 0.0885\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1124 - val_loss: 0.0880\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1133 - val_loss: 0.0886\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0715 - val_loss: 0.0575\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0711 - val_loss: 0.0571\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.2030 - val_loss: 0.0170\n",
      "1379/1415 [============================>.] - ETA: 0s - loss: 0.0722Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0713 - val_loss: 0.0573\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0718 - val_loss: 0.0576\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.1889 - val_loss: 0.0174\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.2099 - val_loss: 0.0172\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0717 - val_loss: 0.0575\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.1813 - val_loss: 0.0166\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0479 - val_loss: 0.0400\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0476 - val_loss: 0.0395\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0478 - val_loss: 0.0397\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0153 - val_loss: 0.0132\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1930 - val_loss: 0.0201\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0480 - val_loss: 0.0398\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0480 - val_loss: 0.0399\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0157 - val_loss: 0.0214\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0152 - val_loss: 0.0133\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0152 - val_loss: 0.0137\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0344 - val_loss: 0.0297\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0340 - val_loss: 0.0293\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0343 - val_loss: 0.0295\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0342 - val_loss: 0.0295\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0343 - val_loss: 0.0295\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0155 - val_loss: 0.0132\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0127 - val_loss: 0.0116\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0265 - val_loss: 0.0236\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0133 - val_loss: 0.0117\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0129 - val_loss: 0.0313\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0127 - val_loss: 0.0129\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0262 - val_loss: 0.0234\n",
      " 279/1415 [====>.........................] - ETA: 1s - loss: 0.0121Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0264 - val_loss: 0.0236\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0264 - val_loss: 0.0236\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0264 - val_loss: 0.0236\n",
      "1332/1415 [===========================>..] - ETA: 0s - loss: 0.0131Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0218 - val_loss: 0.0201\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0131 - val_loss: 0.0113\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0118 - val_loss: 0.0104\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0116 - val_loss: 0.0212\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0116 - val_loss: 0.0103\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0216 - val_loss: 0.0199\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0217 - val_loss: 0.0200\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0217 - val_loss: 0.0200\n",
      "  61/1415 [>.............................] - ETA: 1s - loss: 0.0197Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0218 - val_loss: 0.0200\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0190 - val_loss: 0.0180\n",
      " 798/1415 [===============>..............] - ETA: 1s - loss: 0.0107Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0105 - val_loss: 0.0094\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0116 - val_loss: 0.0112\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0188 - val_loss: 0.0178\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0106 - val_loss: 0.0149\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0189 - val_loss: 0.0178\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0190 - val_loss: 0.0180\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0190 - val_loss: 0.0179\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0173 - val_loss: 0.0166\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0099 - val_loss: 0.0087\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0171 - val_loss: 0.0164\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0100 - val_loss: 0.0089\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0096 - val_loss: 0.0087\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0100 - val_loss: 0.0101\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0173 - val_loss: 0.0165\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0172 - val_loss: 0.0165\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0172 - val_loss: 0.0165\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0162 - val_loss: 0.0157\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0161 - val_loss: 0.0155\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0093 - val_loss: 0.0082\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0161 - val_loss: 0.0156\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0162 - val_loss: 0.0157\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0162 - val_loss: 0.0156\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0155 - val_loss: 0.0151\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0096 - val_loss: 0.0083\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0093 - val_loss: 0.0080\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0153 - val_loss: 0.0150\n",
      "1403/1415 [============================>.] - ETA: 0s - loss: 0.0087Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0154 - val_loss: 0.0150\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0150 - val_loss: 0.0147\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0154 - val_loss: 0.0151\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0155 - val_loss: 0.0151\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0089 - val_loss: 0.0079\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0088 - val_loss: 0.0095\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0092 - val_loss: 0.0082\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0148 - val_loss: 0.0146\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0086 - val_loss: 0.0117\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0149 - val_loss: 0.0146\n",
      "1141/1415 [=======================>......] - ETA: 0s - loss: 0.0087Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0149 - val_loss: 0.0146\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0146 - val_loss: 0.0143\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0090 - val_loss: 0.0106\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0086 - val_loss: 0.0077\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0086 - val_loss: 0.0092\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0145 - val_loss: 0.0142\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0145 - val_loss: 0.0143\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0145 - val_loss: 0.0142\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0143 - val_loss: 0.0140\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0146 - val_loss: 0.0143\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0081 - val_loss: 0.0083\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0086 - val_loss: 0.0075\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0081 - val_loss: 0.0091\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0082 - val_loss: 0.0077\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0142 - val_loss: 0.0139\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0142 - val_loss: 0.0140\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0143 - val_loss: 0.0140\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0142 - val_loss: 0.0139\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0140 - val_loss: 0.0138\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0080 - val_loss: 0.0126\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0084 - val_loss: 0.0075\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0082 - val_loss: 0.0069\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0139 - val_loss: 0.0137\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0082 - val_loss: 0.0074\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0139 - val_loss: 0.0137\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0139 - val_loss: 0.0138\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0138 - val_loss: 0.0135\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0140 - val_loss: 0.0138\n",
      "   1/1415 [..............................] - ETA: 3s - loss: 0.0135Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0078 - val_loss: 0.0071\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0137 - val_loss: 0.0135\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0078 - val_loss: 0.0074\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0079 - val_loss: 0.0070\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0077 - val_loss: 0.0071\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0137 - val_loss: 0.0135\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0137 - val_loss: 0.0135\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0138 - val_loss: 0.0136\n",
      " 740/1415 [==============>...............] - ETA: 0s - loss: 0.0136Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0136 - val_loss: 0.0134\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0135 - val_loss: 0.0133\n",
      " 934/1415 [==================>...........] - ETA: 0s - loss: 0.0134Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0076 - val_loss: 0.0073\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0075 - val_loss: 0.0067\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0075 - val_loss: 0.0065\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0135 - val_loss: 0.0133\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0135 - val_loss: 0.0134\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0136 - val_loss: 0.0134\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0133 - val_loss: 0.0131\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0073 - val_loss: 0.0067\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0075 - val_loss: 0.0065\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0133 - val_loss: 0.0132\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0134 - val_loss: 0.0133\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0071 - val_loss: 0.0095\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0132 - val_loss: 0.0131\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0132 - val_loss: 0.0131\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0072 - val_loss: 0.0068\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0071 - val_loss: 0.0062\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0072 - val_loss: 0.0063\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0070 - val_loss: 0.0065\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "1352/1415 [===========================>..] - ETA: 0s - loss: 0.0069Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0129 - val_loss: 0.0127\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0072 - val_loss: 0.0079\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0069 - val_loss: 0.0066\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0072 - val_loss: 0.0075\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0129 - val_loss: 0.0127\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0129 - val_loss: 0.0127\n",
      " 841/1415 [================>.............] - ETA: 1s - loss: 0.0071Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0129 - val_loss: 0.0127\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0129 - val_loss: 0.0128\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0128 - val_loss: 0.0127\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0118Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0071 - val_loss: 0.0062\n",
      " 767/1415 [===============>..............] - ETA: 0s - loss: 0.0128Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0069 - val_loss: 0.0064\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0127 - val_loss: 0.0125\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0070 - val_loss: 0.0060\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0128 - val_loss: 0.0126\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0127 - val_loss: 0.0126\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0126 - val_loss: 0.0125\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0128 - val_loss: 0.0126\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0068 - val_loss: 0.0059\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0069 - val_loss: 0.0064\n",
      " 816/1415 [================>.............] - ETA: 0s - loss: 0.0126Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0126 - val_loss: 0.0124\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0069 - val_loss: 0.0062\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0067 - val_loss: 0.0057\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0126 - val_loss: 0.0125\n",
      "1012/1415 [====================>.........] - ETA: 0s - loss: 0.0067Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0126 - val_loss: 0.0124\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0125 - val_loss: 0.0124\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0127 - val_loss: 0.0125\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0066 - val_loss: 0.0107\n",
      "Epoch 20/28\n",
      " 167/1415 [==>...........................] - ETA: 1s - loss: 0.0066Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0125 - val_loss: 0.0123\n",
      "1407/1415 [============================>.] - ETA: 0s - loss: 0.0067252/252 - 0s - loss: 0.0123 - 296ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 19/28\n",
      " 515/1415 [=========>....................] - ETA: 1s - loss: 0.0065252/252 - 0s - loss: 0.0123 - 209ms/epoch - 829us/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0067 - val_loss: 0.0065\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0067 - val_loss: 0.0059\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0067 - val_loss: 0.0075\n",
      "Epoch 20/28\n",
      " 699/1415 [=============>................] - ETA: 1s - loss: 0.0064252/252 - 0s - loss: 0.0123 - 317ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0125 - val_loss: 0.0123\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0125 - val_loss: 0.0123\n",
      " 895/1415 [=================>............] - ETA: 0s - loss: 0.0064Epoch 1/28\n",
      " 927/1415 [==================>...........] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0123 - 361ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0125 - val_loss: 0.0124\n",
      "1134/1415 [=======================>......] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0123 - 308ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 605/1415 [===========>..................] - ETA: 1s - loss: 0.0067252/252 - 0s - loss: 0.0123 - 414ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 802/1415 [================>.............] - ETA: 0s - loss: 0.0066252/252 - 0s - loss: 0.0123 - 251ms/epoch - 996us/step\n",
      "Epoch 3/28\n",
      " 708/1415 [==============>...............] - ETA: 1s - loss: 0.0064252/252 - 0s - loss: 0.0122 - 315ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 956/1415 [===================>..........] - ETA: 0s - loss: 0.0064Epoch 1/28\n",
      "1021/1415 [====================>.........] - ETA: 0s - loss: 0.0065Epoch 1/28\n",
      " 884/1415 [=================>............] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0123 - 272ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1054/1415 [=====================>........] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0122 - 281ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      " 946/1415 [===================>..........] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0123 - 239ms/epoch - 950us/step\n",
      "Epoch 2/28\n",
      "1013/1415 [====================>.........] - ETA: 0s - loss: 0.0065Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 21/28\n",
      "1045/1415 [=====================>........] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0123 - 291ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "  52/1415 [>.............................] - ETA: 4s - loss: 0.0062252/252 - 0s - loss: 0.0122 - 404ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "252/252 - 0s - loss: 0.0122 - 389ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "1230/1415 [=========================>....] - ETA: 0s - loss: 0.0066252/252 - 0s - loss: 0.0123 - 467ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "1292/1415 [==========================>...] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0122 - 293ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 235/1415 [===>..........................] - ETA: 2s - loss: 0.0066252/252 - 0s - loss: 0.0123 - 454ms/epoch - 2ms/step\n",
      "252/252 - 1s - loss: 0.0124 - 507ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "Epoch 3/28\n",
      "1326/1415 [===========================>..] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0122 - 327ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1368/1415 [============================>.] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0123 - 312ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1405/1415 [============================>.] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0122 - 246ms/epoch - 978us/step\n",
      "Epoch 4/28\n",
      " 394/1415 [=======>......................] - ETA: 1s - loss: 0.0065252/252 - 0s - loss: 0.0122 - 299ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0122 - 272ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 426/1415 [========>.....................] - ETA: 1s - loss: 0.0064252/252 - 0s - loss: 0.0124 - 325ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 20/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.0064252/252 - 0s - loss: 0.0122 - 229ms/epoch - 910us/step\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0080\n",
      "Epoch 21/28\n",
      "  68/1415 [>.............................] - ETA: 2s - loss: 0.0061252/252 - 0s - loss: 0.0122 - 255ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 21/28\n",
      "  63/1415 [>.............................] - ETA: 2s - loss: 0.0074252/252 - 0s - loss: 0.0122 - 293ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0121 - 286ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0124 - 262ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 21/28\n",
      " 144/1415 [==>...........................] - ETA: 2s - loss: 0.0069252/252 - 0s - loss: 0.0122 - 327ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 175/1415 [==>...........................] - ETA: 2s - loss: 0.0068252/252 - 0s - loss: 0.0122 - 278ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 305/1415 [=====>........................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0124 - 331ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 369/1415 [======>.......................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0121 - 424ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0121 - 427ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 400/1415 [=======>......................] - ETA: 1s - loss: 0.0061252/252 - 0s - loss: 0.0122 - 332ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 424/1415 [=======>......................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0122 - 339ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 482/1415 [=========>....................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0123 - 297ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 540/1415 [==========>...................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0121 - 310ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 507/1415 [=========>....................] - ETA: 1s - loss: 0.0068252/252 - 0s - loss: 0.0121 - 346ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 572/1415 [===========>..................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0122 - 317ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      " 567/1415 [===========>..................] - ETA: 1s - loss: 0.0069252/252 - 0s - loss: 0.0122 - 320ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      " 550/1415 [==========>...................] - ETA: 1s - loss: 0.0067252/252 - 0s - loss: 0.0123 - 330ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 708/1415 [==============>...............] - ETA: 1s - loss: 0.0065252/252 - 0s - loss: 0.0121 - 300ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0121 - 277ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 760/1415 [===============>..............] - ETA: 1s - loss: 0.0065252/252 - 0s - loss: 0.0122 - 350ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 726/1415 [==============>...............] - ETA: 1s - loss: 0.0067252/252 - 0s - loss: 0.0121 - 319ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 813/1415 [================>.............] - ETA: 1s - loss: 0.0067252/252 - 0s - loss: 0.0121 - 276ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0123 - 365ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      " 766/1415 [===============>..............] - ETA: 1s - loss: 0.0067252/252 - 0s - loss: 0.0121 - 316ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 939/1415 [==================>...........] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0121 - 273ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 901/1415 [==================>...........] - ETA: 0s - loss: 0.0066252/252 - 0s - loss: 0.0122 - 324ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 966/1415 [===================>..........] - ETA: 0s - loss: 0.0066252/252 - 0s - loss: 0.0123 - 251ms/epoch - 997us/step\n",
      "Epoch 9/28\n",
      "1067/1415 [=====================>........] - ETA: 0s - loss: 0.0064252/252 - 0s - loss: 0.0121 - 331ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1028/1415 [====================>.........] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0121 - 316ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1012/1415 [====================>.........] - ETA: 0s - loss: 0.0066252/252 - 0s - loss: 0.0121 - 300ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1087/1415 [======================>.......] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0121 - 339ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 22/28\n",
      "1187/1415 [========================>.....] - ETA: 0s - loss: 0.0064252/252 - 0s - loss: 0.0123 - 359ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1219/1415 [========================>.....] - ETA: 0s - loss: 0.0064252/252 - 0s - loss: 0.0120 - 340ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1174/1415 [=======================>......] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0121 - 342ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1205/1415 [========================>.....] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0121 - 286ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1356/1415 [===========================>..] - ETA: 0s - loss: 0.0064252/252 - 0s - loss: 0.0121 - 329ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1367/1415 [===========================>..] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0122 - 304ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1395/1415 [============================>.] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0120 - 313ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1300/1415 [==========================>...] - ETA: 0s - loss: 0.0063252/252 - 0s - loss: 0.0120 - 312ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1330/1415 [===========================>..] - ETA: 0s - loss: 0.0063252/252 - 0s - loss: 0.0121 - 328ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1364/1415 [===========================>..] - ETA: 0s - loss: 0.0063252/252 - 0s - loss: 0.0121 - 341ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 395/1415 [=======>......................] - ETA: 1s - loss: 0.0065252/252 - 0s - loss: 0.0122 - 297ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 437/1415 [========>.....................] - ETA: 1s - loss: 0.0065252/252 - 0s - loss: 0.0120 - 282ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 21/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.0127252/252 - 0s - loss: 0.0120 - 287ms/epoch - 1ms/step\n",
      " 473/1415 [=========>....................] - ETA: 1s - loss: 0.0065Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0121 - 228ms/epoch - 905us/step\n",
      "Epoch 14/28\n",
      " 510/1415 [=========>....................] - ETA: 1s - loss: 0.0065252/252 - 0s - loss: 0.0121 - 270ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 22/28\n",
      "  42/1415 [..............................] - ETA: 3s - loss: 0.0061252/252 - 0s - loss: 0.0122 - 265ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 22/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.0058252/252 - 0s - loss: 0.0120 - 239ms/epoch - 948us/step\n",
      "Epoch 17/28\n",
      " 633/1415 [============>.................] - ETA: 1s - loss: 0.0065252/252 - 0s - loss: 0.0120 - 299ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 22/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0068252/252 - 0s - loss: 0.0120 - 406ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "  37/1415 [..............................] - ETA: 2s - loss: 0.0061252/252 - 0s - loss: 0.0120 - 459ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 114/1415 [=>............................] - ETA: 2s - loss: 0.0060252/252 - 0s - loss: 0.0120 - 387ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "252/252 - 0s - loss: 0.0122 - 465ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      " 178/1415 [==>...........................] - ETA: 2s - loss: 0.0059252/252 - 0s - loss: 0.0120 - 472ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 432/1415 [========>.....................] - ETA: 1s - loss: 0.0068252/252 - 0s - loss: 0.0120 - 305ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0120 - 367ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 307/1415 [=====>........................] - ETA: 1s - loss: 0.0067252/252 - 0s - loss: 0.0122 - 312ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 436/1415 [========>.....................] - ETA: 2s - loss: 0.0065252/252 - 0s - loss: 0.0120 - 354ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 352/1415 [======>.......................] - ETA: 1s - loss: 0.0066252/252 - 0s - loss: 0.0119 - 316ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 487/1415 [=========>....................] - ETA: 1s - loss: 0.0066252/252 - 0s - loss: 0.0120 - 261ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 408/1415 [=======>......................] - ETA: 1s - loss: 0.0065252/252 - 0s - loss: 0.0120 - 329ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 463/1415 [========>.....................] - ETA: 1s - loss: 0.0064252/252 - 0s - loss: 0.0121 - 306ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 496/1415 [=========>....................] - ETA: 1s - loss: 0.0064252/252 - 0s - loss: 0.0120 - 329ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 526/1415 [==========>...................] - ETA: 1s - loss: 0.0064252/252 - 0s - loss: 0.0119 - 303ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 621/1415 [============>.................] - ETA: 1s - loss: 0.0063252/252 - 0s - loss: 0.0120 - 306ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 550/1415 [==========>...................] - ETA: 1s - loss: 0.0064252/252 - 0s - loss: 0.0120 - 273ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 767/1415 [===============>..............] - ETA: 1s - loss: 0.0063252/252 - 0s - loss: 0.0119 - 325ms/epoch - 1ms/step\n",
      " 817/1415 [================>.............] - ETA: 1s - loss: 0.0065Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0121 - 372ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 711/1415 [==============>...............] - ETA: 1s - loss: 0.0063252/252 - 0s - loss: 0.0119 - 330ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1411/1415 [============================>.] - ETA: 0s - loss: 0.0062252/252 - 0s - loss: 0.0120 - 349ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 774/1415 [===============>..............] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0120 - 344ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 827/1415 [================>.............] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0121 - 240ms/epoch - 954us/step\n",
      "Epoch 18/28\n",
      " 885/1415 [=================>............] - ETA: 0s - loss: 0.0062252/252 - 0s - loss: 0.0119 - 347ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 916/1415 [==================>...........] - ETA: 0s - loss: 0.0062252/252 - 0s - loss: 0.0119 - 335ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1146/1415 [=======================>......] - ETA: 0s - loss: 0.0066252/252 - 0s - loss: 0.0120 - 280ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 952/1415 [===================>..........] - ETA: 0s - loss: 0.0062252/252 - 0s - loss: 0.0120 - 319ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0062 - val_loss: 0.0059\n",
      "Epoch 23/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.0064252/252 - 0s - loss: 0.0121 - 288ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "  58/1415 [>.............................] - ETA: 2s - loss: 0.0058252/252 - 0s - loss: 0.0119 - 307ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "  85/1415 [>.............................] - ETA: 2s - loss: 0.0062252/252 - 0s - loss: 0.0119 - 312ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1330/1415 [===========================>..] - ETA: 0s - loss: 0.0065252/252 - 0s - loss: 0.0119 - 317ms/epoch - 1ms/step\n",
      "1122/1415 [======================>.......] - ETA: 0s - loss: 0.0062Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0119 - 311ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1179/1415 [=======================>......] - ETA: 0s - loss: 0.0062252/252 - 0s - loss: 0.0121 - 352ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1358/1415 [===========================>..] - ETA: 0s - loss: 0.0064252/252 - 0s - loss: 0.0119 - 271ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1263/1415 [=========================>....] - ETA: 0s - loss: 0.0062252/252 - 0s - loss: 0.0119 - 294ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 252/1415 [====>.........................] - ETA: 2s - loss: 0.0062252/252 - 0s - loss: 0.0119 - 326ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0119 - 323ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 325/1415 [=====>........................] - ETA: 2s - loss: 0.0062252/252 - 0s - loss: 0.0121 - 267ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 362/1415 [======>.......................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0119 - 284ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 397/1415 [=======>......................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0118 - 273ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0119 - 244ms/epoch - 968us/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0063\n",
      "Epoch 22/28\n",
      "   1/1415 [..............................] - ETA: 3s - loss: 0.0070252/252 - 0s - loss: 0.0119 - 266ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "  44/1415 [..............................] - ETA: 3s - loss: 0.0080252/252 - 0s - loss: 0.0120 - 283ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 23/28\n",
      " 516/1415 [=========>....................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0119 - 243ms/epoch - 963us/step\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 23/28\n",
      " 546/1415 [==========>...................] - ETA: 1s - loss: 0.006163/63 - 0s - loss: 0.0114 - 234ms/epoch - 4ms/step\n",
      " 610/1415 [===========>..................] - ETA: 1s - loss: 0.0061252/252 - 0s - loss: 0.0119 - 313ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 23/28\n",
      " 209/1415 [===>..........................] - ETA: 2s - loss: 0.0065252/252 - 0s - loss: 0.0119 - 359ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 668/1415 [=============>................] - ETA: 1s - loss: 0.0061252/252 - 0s - loss: 0.0120 - 275ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 125/1415 [=>............................] - ETA: 2s - loss: 0.0062252/252 - 0s - loss: 0.0118 - 372ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      " 249/1415 [====>.........................] - ETA: 1s - loss: 0.0063252/252 - 0s - loss: 0.0119 - 298ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 374/1415 [======>.......................] - ETA: 1s - loss: 0.0063252/252 - 0s - loss: 0.0113 - 423ms/epoch - 2ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=sgd; total time= 1.5min\n",
      " 400/1415 [=======>......................] - ETA: 1s - loss: 0.0063252/252 - 0s - loss: 0.0119 - 305ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 831/1415 [================>.............] - ETA: 1s - loss: 0.0060252/252 - 0s - loss: 0.0120 - 307ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 853/1415 [=================>............] - ETA: 1s - loss: 0.0060252/252 - 0s - loss: 0.0118 - 251ms/epoch - 996us/step\n",
      " 259/1415 [====>.........................] - ETA: 2s - loss: 0.0060Epoch 28/28\n",
      " 942/1415 [==================>...........] - ETA: 0s - loss: 0.0060252/252 - 0s - loss: 0.0118 - 322ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 547/1415 [==========>...................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0119 - 299ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1005/1415 [====================>.........] - ETA: 0s - loss: 0.0060252/252 - 0s - loss: 0.0120 - 303ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 627/1415 [============>.................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0118 - 425ms/epoch - 2ms/step\n",
      "1152/1415 [=======================>......] - ETA: 0s - loss: 0.0060252/252 - 0s - loss: 0.0118 - 456ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 619/1415 [============>.................] - ETA: 1s - loss: 0.0062252/252 - 0s - loss: 0.0118 - 410ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 749/1415 [==============>...............] - ETA: 1s - loss: 0.0063252/252 - 0s - loss: 0.0120 - 391ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      " 650/1415 [============>.................] - ETA: 1s - loss: 0.006263/63 - 0s - loss: 0.0115 - 223ms/epoch - 4ms/step\n",
      "1320/1415 [==========================>...] - ETA: 0s - loss: 0.0064252/252 - 0s - loss: 0.0118 - 258ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1385/1415 [============================>.] - ETA: 0s - loss: 0.0063252/252 - 0s - loss: 0.0112 - 257ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=sgd; total time= 1.5min\n",
      "252/252 - 0s - loss: 0.0120 - 317ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "252/252 - 0s - loss: 0.0118 - 346ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1011/1415 [====================>.........] - ETA: 0s - loss: 0.0062252/252 - 0s - loss: 0.0118 - 339ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0118 - 243ms/epoch - 963us/step\n",
      "1041/1415 [=====================>........] - ETA: 0s - loss: 0.0062252/252 - 0s - loss: 0.0119 - 313ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1179/1415 [=======================>......] - ETA: 0s - loss: 0.006363/63 - 0s - loss: 0.0112 - 139ms/epoch - 2ms/step\n",
      "1206/1415 [========================>.....] - ETA: 0s - loss: 0.006363/63 - 0s - loss: 0.0112 - 173ms/epoch - 3ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0063 - val_loss: 0.0058\n",
      "Epoch 24/28\n",
      "1226/1415 [========================>.....] - ETA: 0s - loss: 0.0063252/252 - 0s - loss: 0.0119 - 296ms/epoch - 1ms/step\n",
      "1208/1415 [========================>.....] - ETA: 0s - loss: 0.0062252/252 - 0s - loss: 0.0113 - 195ms/epoch - 773us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=sgd; total time= 1.5min\n",
      "1309/1415 [==========================>...] - ETA: 0s - loss: 0.006363/63 - 0s - loss: 0.0110 - 118ms/epoch - 2ms/step\n",
      "252/252 - 0s - loss: 0.0112 - 341ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=sgd; total time= 1.5min\n",
      "1361/1415 [===========================>..] - ETA: 0s - loss: 0.0063252/252 - 0s - loss: 0.0113 - 111ms/epoch - 441us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=sgd; total time= 1.5min\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0061 - val_loss: 0.0054\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0062 - val_loss: 0.0055\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0060 - val_loss: 0.0056\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0060 - val_loss: 0.0054\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0062 - val_loss: 0.0054\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0060 - val_loss: 0.0068\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0062 - val_loss: 0.0069\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0059 - val_loss: 0.0057\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0059 - val_loss: 0.0081\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0059 - val_loss: 0.0078\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0062 - val_loss: 0.0057\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0062 - val_loss: 0.0054\n",
      "Epoch 25/28\n",
      " 986/1415 [===================>..........] - ETA: 0s - loss: 0.0061Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 27/28\n",
      " 396/1415 [=======>......................] - ETA: 1s - loss: 0.0057Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0060 - val_loss: 0.0063\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0059 - val_loss: 0.0057\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0059 - val_loss: 0.0051\n",
      "Epoch 27/28\n",
      " 196/1415 [===>..........................] - ETA: 1s - loss: 16.9193Epoch 1/28\n",
      " 145/1415 [==>...........................] - ETA: 1s - loss: 0.0065Epoch 1/28\n",
      " 208/1415 [===>..........................] - ETA: 1s - loss: 0.0063Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0057 - val_loss: 0.0059\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0059 - val_loss: 0.0049\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0058 - val_loss: 0.0060\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0060 - val_loss: 0.0055\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0058 - val_loss: 0.0066\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 7.0672 - val_loss: 0.8302\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0060 - val_loss: 0.0056\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 7.1922 - val_loss: 0.8122\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 7.0690 - val_loss: 0.7848\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0057 - val_loss: 0.0054\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0057 - val_loss: 0.0070\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 7.1398 - val_loss: 0.8233\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0061 - val_loss: 0.0050\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 7.1196 - val_loss: 0.8286\n",
      "Epoch 2/28\n",
      " 436/1415 [========>.....................] - ETA: 1s - loss: 0.5238Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0059 - val_loss: 0.0068\n",
      " 776/1415 [===============>..............] - ETA: 0s - loss: 0.4224252/252 - 0s - loss: 0.0071 - 383ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 908/1415 [==================>...........] - ETA: 0s - loss: 0.3963Epoch 1/28\n",
      "1042/1415 [=====================>........] - ETA: 0s - loss: 0.3743Epoch 1/28\n",
      "1086/1415 [======================>.......] - ETA: 0s - loss: 0.3681252/252 - 0s - loss: 0.0065 - 428ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      " 927/1415 [==================>...........] - ETA: 0s - loss: 0.4131252/252 - 0s - loss: 0.0069 - 362ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "1197/1415 [========================>.....] - ETA: 0s - loss: 0.3539Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.3439 - val_loss: 0.1964\n",
      "1153/1415 [=======================>......] - ETA: 0s - loss: 0.3339Epoch 3/28\n",
      "1169/1415 [=======================>......] - ETA: 0s - loss: 0.3321252/252 - 0s - loss: 0.0070 - 403ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      " 141/1415 [=>............................] - ETA: 1s - loss: 0.1930252/252 - 1s - loss: 0.0062 - 548ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1240/1415 [=========================>....] - ETA: 0s - loss: 0.3246252/252 - 0s - loss: 0.0086 - 399ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "1362/1415 [===========================>..] - ETA: 0s - loss: 0.3131252/252 - 1s - loss: 0.0069 - 565ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1273/1415 [=========================>....] - ETA: 0s - loss: 0.3408252/252 - 0s - loss: 0.0072 - 375ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1134/1415 [=======================>......] - ETA: 0s - loss: 0.0057252/252 - 0s - loss: 0.0075 - 401ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      " 372/1415 [======>.......................] - ETA: 1s - loss: 0.1869252/252 - 0s - loss: 0.0064 - 390ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.3305 - val_loss: 0.1949\n",
      "Epoch 3/28\n",
      " 468/1415 [========>.....................] - ETA: 1s - loss: 0.1852252/252 - 0s - loss: 0.0062 - 356ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      " 539/1415 [==========>...................] - ETA: 1s - loss: 0.1838252/252 - 0s - loss: 0.0056 - 388ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.3088 - val_loss: 0.1951\n",
      "Epoch 3/28\n",
      " 597/1415 [===========>..................] - ETA: 1s - loss: 0.1829252/252 - 0s - loss: 0.0056 - 373ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 234/1415 [===>..........................] - ETA: 1s - loss: 0.1885252/252 - 0s - loss: 0.0063 - 413ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.3453 - val_loss: 0.1978\n",
      "Epoch 3/28\n",
      " 290/1415 [=====>........................] - ETA: 1s - loss: 0.1870252/252 - 0s - loss: 0.0054 - 337ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.3255 - val_loss: 0.1839\n",
      "Epoch 3/28\n",
      " 385/1415 [=======>......................] - ETA: 1s - loss: 0.1848252/252 - 0s - loss: 0.0063 - 380ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      " 122/1415 [=>............................] - ETA: 1s - loss: 0.1830252/252 - 0s - loss: 0.0069 - 369ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 254/1415 [====>.........................] - ETA: 1s - loss: 0.1922252/252 - 0s - loss: 0.0057 - 409ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0057 - val_loss: 0.0051\n",
      " 170/1415 [==>...........................] - ETA: 1s - loss: 0.1819252/252 - 0s - loss: 0.0063 - 384ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      " 296/1415 [=====>........................] - ETA: 1s - loss: 0.1800252/252 - 0s - loss: 0.0061 - 381ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      " 354/1415 [======>.......................] - ETA: 1s - loss: 0.1793252/252 - 0s - loss: 0.0059 - 389ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      " 612/1415 [===========>..................] - ETA: 1s - loss: 0.1904252/252 - 0s - loss: 0.0073 - 387ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      " 842/1415 [================>.............] - ETA: 0s - loss: 0.1737252/252 - 0s - loss: 0.0080 - 380ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "1255/1415 [=========================>....] - ETA: 0s - loss: 0.1738252/252 - 0s - loss: 0.0058 - 439ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "1396/1415 [============================>.] - ETA: 0s - loss: 0.1722252/252 - 0s - loss: 0.0065 - 496ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0060 - 490ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1155/1415 [=======================>......] - ETA: 0s - loss: 0.1681252/252 - 1s - loss: 0.0055 - 558ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1023/1415 [====================>.........] - ETA: 0s - loss: 0.1876252/252 - 0s - loss: 0.0058 - 413ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      "1259/1415 [=========================>....] - ETA: 0s - loss: 0.1663Epoch 1/28\n",
      "1364/1415 [===========================>..] - ETA: 0s - loss: 0.1647252/252 - 0s - loss: 0.0059 - 411ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "1176/1415 [=======================>......] - ETA: 0s - loss: 0.1865252/252 - 0s - loss: 0.0062 - 451ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1719 - val_loss: 0.1561\n",
      "Epoch 4/28\n",
      " 997/1415 [====================>.........] - ETA: 0s - loss: 0.1736252/252 - 0s - loss: 0.0059 - 410ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1067/1415 [=====================>........] - ETA: 0s - loss: 0.1731252/252 - 0s - loss: 0.0065 - 443ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      " 227/1415 [===>..........................] - ETA: 1s - loss: 0.1537252/252 - 0s - loss: 0.0062 - 478ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "252/252 - 0s - loss: 0.0068 - 412ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 297/1415 [=====>........................] - ETA: 1s - loss: 0.1536252/252 - 0s - loss: 0.0058 - 450ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      " 325/1415 [=====>........................] - ETA: 1s - loss: 0.1541252/252 - 0s - loss: 0.0062 - 402ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1641 - val_loss: 0.1448\n",
      "Epoch 4/28\n",
      " 412/1415 [=======>......................] - ETA: 1s - loss: 0.1535252/252 - 0s - loss: 0.0060 - 409ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      " 476/1415 [=========>....................] - ETA: 1s - loss: 0.1526252/252 - 0s - loss: 0.0061 - 388ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      " 519/1415 [==========>...................] - ETA: 1s - loss: 0.1525252/252 - 0s - loss: 0.0071 - 410ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1847 - val_loss: 0.1743\n",
      "Epoch 4/28\n",
      " 581/1415 [===========>..................] - ETA: 1s - loss: 0.1523252/252 - 0s - loss: 0.0069 - 419ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 285/1415 [=====>........................] - ETA: 1s - loss: 0.1424252/252 - 0s - loss: 0.0059 - 378ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 701/1415 [=============>................] - ETA: 0s - loss: 0.1515252/252 - 0s - loss: 0.0065 - 391ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1716 - val_loss: 0.1548\n",
      "Epoch 4/28\n",
      " 238/1415 [====>.........................] - ETA: 1s - loss: 0.1728252/252 - 0s - loss: 0.0063 - 402ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      " 798/1415 [===============>..............] - ETA: 0s - loss: 0.1507252/252 - 0s - loss: 0.0058 - 373ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 502/1415 [=========>....................] - ETA: 1s - loss: 0.1415252/252 - 0s - loss: 0.0070 - 366ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1706 - val_loss: 0.1622\n",
      "Epoch 4/28\n",
      " 882/1415 [=================>............] - ETA: 0s - loss: 0.1503252/252 - 0s - loss: 0.0069 - 413ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      " 955/1415 [===================>..........] - ETA: 0s - loss: 0.1499252/252 - 0s - loss: 0.0068 - 405ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      " 198/1415 [===>..........................] - ETA: 1s - loss: 0.1609252/252 - 0s - loss: 0.0055 - 407ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      " 737/1415 [==============>...............] - ETA: 0s - loss: 0.1402252/252 - 0s - loss: 0.0058 - 409ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 767/1415 [===============>..............] - ETA: 0s - loss: 0.1402252/252 - 0s - loss: 0.0063 - 387ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1128/1415 [======================>.......] - ETA: 0s - loss: 0.1489252/252 - 0s - loss: 0.0058 - 407ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      " 893/1415 [=================>............] - ETA: 0s - loss: 0.1399252/252 - 0s - loss: 0.0058 - 394ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 469/1415 [========>.....................] - ETA: 1s - loss: 0.1601252/252 - 0s - loss: 0.0059 - 400ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0061 - 415ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "1012/1415 [====================>.........] - ETA: 0s - loss: 0.1397252/252 - 0s - loss: 0.0060 - 407ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1384/1415 [============================>.] - ETA: 0s - loss: 0.1475252/252 - 0s - loss: 0.0065 - 465ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 655/1415 [============>.................] - ETA: 1s - loss: 0.1592252/252 - 0s - loss: 0.0057 - 446ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "1214/1415 [========================>.....] - ETA: 0s - loss: 0.1390252/252 - 0s - loss: 0.0052 - 427ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 785/1415 [===============>..............] - ETA: 1s - loss: 0.1497252/252 - 0s - loss: 0.0057 - 461ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      " 800/1415 [===============>..............] - ETA: 1s - loss: 0.1496252/252 - 0s - loss: 0.0062 - 450ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      " 905/1415 [==================>...........] - ETA: 0s - loss: 0.1492252/252 - 0s - loss: 0.0060 - 430ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      " 940/1415 [==================>...........] - ETA: 0s - loss: 0.1492252/252 - 0s - loss: 0.0057 - 485ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1474 - val_loss: 0.1388\n",
      "Epoch 5/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.1320252/252 - 0s - loss: 0.0057 - 489ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "  35/1415 [..............................] - ETA: 2s - loss: 0.1379252/252 - 0s - loss: 0.0052 - 498ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1062/1415 [=====================>........] - ETA: 0s - loss: 0.1487252/252 - 0s - loss: 0.0062 - 494ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 193/1415 [===>..........................] - ETA: 1s - loss: 0.1378252/252 - 1s - loss: 0.0064 - 536ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 225/1415 [===>..........................] - ETA: 1s - loss: 0.1377252/252 - 0s - loss: 0.0063 - 440ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1386 - val_loss: 0.1347\n",
      "Epoch 5/28\n",
      "1290/1415 [==========================>...] - ETA: 0s - loss: 0.1475252/252 - 0s - loss: 0.0063 - 436ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "  38/1415 [..............................] - ETA: 1s - loss: 0.1340252/252 - 0s - loss: 0.0103 - 386ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0060 - 432ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      " 402/1415 [=======>......................] - ETA: 1s - loss: 0.1370252/252 - 0s - loss: 0.0056 - 328ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 186/1415 [==>...........................] - ETA: 1s - loss: 0.1345252/252 - 0s - loss: 0.0061 - 362ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1615 - val_loss: 0.1476\n",
      "Epoch 5/28\n",
      " 275/1415 [====>.........................] - ETA: 1s - loss: 0.1343252/252 - 0s - loss: 0.0062 - 386ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      " 314/1415 [=====>........................] - ETA: 1s - loss: 0.1339252/252 - 0s - loss: 0.0055 - 384ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0056 - 398ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      " 133/1415 [=>............................] - ETA: 2s - loss: 0.1484252/252 - 0s - loss: 0.0071 - 373ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1557 - val_loss: 0.1480\n",
      "Epoch 5/28\n",
      " 171/1415 [==>...........................] - ETA: 1s - loss: 0.1473252/252 - 0s - loss: 0.0059 - 376ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1469 - val_loss: 0.1415\n",
      "Epoch 5/28\n",
      "  40/1415 [..............................] - ETA: 6s - loss: 0.1404 252/252 - 1s - loss: 0.0054 - 568ms/epoch - 2ms/step\n",
      " 141/1415 [=>............................] - ETA: 3s - loss: 0.1478Epoch 19/28\n",
      "  65/1415 [>.............................] - ETA: 5s - loss: 0.1406252/252 - 1s - loss: 0.0054 - 553ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 1s - loss: 0.0062 - 549ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      " 972/1415 [===================>..........] - ETA: 0s - loss: 0.1343252/252 - 1s - loss: 0.0061 - 650ms/epoch - 3ms/step\n",
      "Epoch 18/28\n",
      "252/252 - 1s - loss: 0.0058 - 581ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      " 790/1415 [===============>..............] - ETA: 1s - loss: 0.1332252/252 - 0s - loss: 0.0063 - 477ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 816/1415 [================>.............] - ETA: 1s - loss: 0.1332252/252 - 1s - loss: 0.0063 - 506ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1171/1415 [=======================>......] - ETA: 0s - loss: 0.1337252/252 - 1s - loss: 0.0059 - 511ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "1270/1415 [=========================>....] - ETA: 0s - loss: 0.1333252/252 - 0s - loss: 0.0062 - 436ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      " 948/1415 [===================>..........] - ETA: 0s - loss: 0.1330252/252 - 0s - loss: 0.0069 - 441ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 567/1415 [===========>..................] - ETA: 1s - loss: 0.1398252/252 - 0s - loss: 0.0081 - 420ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0057 - 386ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 580/1415 [===========>..................] - ETA: 1s - loss: 0.1397252/252 - 0s - loss: 0.0062 - 406ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 644/1415 [============>.................] - ETA: 1s - loss: 0.1395252/252 - 0s - loss: 0.0056 - 386ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "252/252 - 0s - loss: 0.0059 - 382ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 806/1415 [================>.............] - ETA: 1s - loss: 0.1390252/252 - 0s - loss: 0.0051 - 427ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0067 - 381ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      " 848/1415 [================>.............] - ETA: 1s - loss: 0.1389252/252 - 0s - loss: 0.0053 - 447ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1327 - val_loss: 0.1269\n",
      "Epoch 6/28\n",
      " 902/1415 [==================>...........] - ETA: 0s - loss: 0.1386252/252 - 0s - loss: 0.0072 - 398ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0057 - 406ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      " 239/1415 [====>.........................] - ETA: 1s - loss: 0.1245252/252 - 0s - loss: 0.0055 - 406ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "1116/1415 [======================>.......] - ETA: 0s - loss: 0.1377252/252 - 0s - loss: 0.0056 - 445ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "1127/1415 [======================>.......] - ETA: 0s - loss: 0.1377252/252 - 0s - loss: 0.0054 - 460ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1321 - val_loss: 0.1294\n",
      "Epoch 6/28\n",
      "1262/1415 [=========================>....] - ETA: 0s - loss: 0.1409252/252 - 0s - loss: 0.0059 - 396ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 300/1415 [=====>........................] - ETA: 1s - loss: 0.1243252/252 - 0s - loss: 0.0056 - 428ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "1362/1415 [===========================>..] - ETA: 0s - loss: 0.1368252/252 - 1s - loss: 0.0059 - 517ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 491/1415 [=========>....................] - ETA: 1s - loss: 0.1246252/252 - 1s - loss: 0.0052 - 509ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1364 - val_loss: 0.1281\n",
      "Epoch 6/28\n",
      "  37/1415 [..............................] - ETA: 1s - loss: 0.1292252/252 - 1s - loss: 0.0071 - 519ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      " 546/1415 [==========>...................] - ETA: 1s - loss: 0.1245252/252 - 1s - loss: 0.0058 - 528ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 311/1415 [=====>........................] - ETA: 2s - loss: 0.1283252/252 - 1s - loss: 0.0059 - 518ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      " 457/1415 [========>.....................] - ETA: 1s - loss: 0.1284252/252 - 0s - loss: 0.0055 - 418ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "252/252 - 0s - loss: 0.0052 - 415ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1405 - val_loss: 0.1352\n",
      "Epoch 6/28\n",
      "  46/1415 [..............................] - ETA: 1s - loss: 0.1352252/252 - 0s - loss: 0.0055 - 422ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      " 532/1415 [==========>...................] - ETA: 1s - loss: 0.1283252/252 - 0s - loss: 0.0057 - 404ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1366 - val_loss: 0.1315\n",
      "Epoch 6/28\n",
      " 903/1415 [==================>...........] - ETA: 0s - loss: 0.1236252/252 - 0s - loss: 0.0058 - 489ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 471/1415 [========>.....................] - ETA: 1s - loss: 0.1271252/252 - 0s - loss: 0.0062 - 391ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      " 797/1415 [===============>..............] - ETA: 1s - loss: 0.1280252/252 - 0s - loss: 0.0050 - 435ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      " 279/1415 [====>.........................] - ETA: 1s - loss: 0.1338252/252 - 0s - loss: 0.0058 - 385ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      " 819/1415 [================>.............] - ETA: 0s - loss: 0.1280252/252 - 0s - loss: 0.0052 - 422ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      " 274/1415 [====>.........................] - ETA: 1s - loss: 0.1298252/252 - 0s - loss: 0.0102 - 354ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 473/1415 [=========>....................] - ETA: 1s - loss: 0.1290252/252 - 0s - loss: 0.0059 - 440ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      " 513/1415 [=========>....................] - ETA: 1s - loss: 0.1288252/252 - 0s - loss: 0.0054 - 383ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "1080/1415 [=====================>........] - ETA: 0s - loss: 0.1278252/252 - 0s - loss: 0.0058 - 462ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 550/1415 [==========>...................] - ETA: 1s - loss: 0.1285252/252 - 0s - loss: 0.0059 - 436ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "1139/1415 [=======================>......] - ETA: 0s - loss: 0.1278252/252 - 0s - loss: 0.0058 - 433ms/epoch - 2ms/step\n",
      "1274/1415 [==========================>...] - ETA: 0s - loss: 0.1276252/252 - 0s - loss: 0.0058 - 388ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1322/1415 [===========================>..] - ETA: 0s - loss: 0.127663/63 - 0s - loss: 0.0052 - 242ms/epoch - 4ms/step\n",
      "252/252 - 0s - loss: 0.0053 - 396ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "1363/1415 [===========================>..] - ETA: 0s - loss: 0.1275252/252 - 0s - loss: 0.0093 - 396ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "252/252 - 0s - loss: 0.0055 - 407ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1222 - val_loss: 0.1180\n",
      "Epoch 7/28\n",
      "1256/1415 [=========================>....] - ETA: 0s - loss: 0.1251252/252 - 0s - loss: 0.0059 - 369ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1063/1415 [=====================>........] - ETA: 0s - loss: 0.1322252/252 - 0s - loss: 0.0065 - 391ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0058 - 353ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "252/252 - 0s - loss: 0.0051 - 399ms/epoch - 2ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x13a7435b0>; total time= 1.7min\n",
      "1098/1415 [======================>.......] - ETA: 0s - loss: 0.1321252/252 - 0s - loss: 0.0051 - 419ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1275 - val_loss: 0.1266\n",
      "Epoch 7/28\n",
      " 115/1415 [=>............................] - ETA: 1s - loss: 0.1241252/252 - 0s - loss: 0.0057 - 384ms/epoch - 2ms/step\n",
      "1303/1415 [==========================>...] - ETA: 0s - loss: 0.131563/63 - 0s - loss: 0.0072 - 296ms/epoch - 5ms/step\n",
      "1302/1415 [==========================>...] - ETA: 0s - loss: 0.1264252/252 - 0s - loss: 0.0049 - 376ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1342/1415 [===========================>..] - ETA: 0s - loss: 0.1315252/252 - 0s - loss: 0.0057 - 393ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1249 - val_loss: 0.1224\n",
      "Epoch 7/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.127363/63 - 0s - loss: 0.0073 - 190ms/epoch - 3ms/step\n",
      "  86/1415 [>.............................] - ETA: 1s - loss: 0.1224252/252 - 0s - loss: 0.0070 - 242ms/epoch - 959us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1547c35b0>; total time= 1.7min\n",
      "63/63 - 0s - loss: 0.0048 - 198ms/epoch - 3ms/step\n",
      " 128/1415 [=>............................] - ETA: 1s - loss: 0.1226252/252 - 0s - loss: 0.0057 - 316ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 252/1415 [====>.........................] - ETA: 1s - loss: 0.1220252/252 - 0s - loss: 0.0072 - 261ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16dc835b0>; total time= 1.7min\n",
      "252/252 - 0s - loss: 0.0050 - 202ms/epoch - 802us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16a1035b0>; total time= 1.7min\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1312 - val_loss: 0.1267\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1261 - val_loss: 0.1216\n",
      "Epoch 7/28\n",
      " 392/1415 [=======>......................] - ETA: 1s - loss: 0.1219252/252 - 0s - loss: 0.0049 - 411ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      " 626/1415 [============>.................] - ETA: 1s - loss: 0.1214252/252 - 0s - loss: 0.0065 - 254ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 587/1415 [===========>..................] - ETA: 0s - loss: 0.1249252/252 - 0s - loss: 0.0049 - 243ms/epoch - 966us/step\n",
      "Epoch 26/28\n",
      " 820/1415 [================>.............] - ETA: 0s - loss: 0.1241252/252 - 0s - loss: 0.0049 - 256ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1160 - val_loss: 0.1143\n",
      "Epoch 8/28\n",
      "1061/1415 [=====================>........] - ETA: 0s - loss: 0.1232252/252 - 0s - loss: 0.0075 - 244ms/epoch - 969us/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1236 - val_loss: 0.1219\n",
      "Epoch 8/28\n",
      "  82/1415 [>.............................] - ETA: 1s - loss: 0.1203252/252 - 0s - loss: 0.0052 - 277ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1207 - val_loss: 0.1188\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1178 - val_loss: 0.1143\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1221 - val_loss: 0.1179\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1125 - val_loss: 0.1108\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1205 - val_loss: 0.1190\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1172 - val_loss: 0.1166\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1125 - val_loss: 0.1107\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1158 - val_loss: 0.1132\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1095 - val_loss: 0.1078\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1177 - val_loss: 0.1168\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1127 - val_loss: 0.1092\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1093 - val_loss: 0.1075\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1104 - val_loss: 0.1071\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1065 - val_loss: 0.1054\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1154 - val_loss: 0.1146\n",
      "Epoch 11/28\n",
      "1356/1415 [===========================>..] - ETA: 0s - loss: 0.1065Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1074 - val_loss: 0.1057\n",
      "Epoch 11/28\n",
      "  55/1415 [>.............................] - ETA: 1s - loss: 14.9675  Epoch 1/28\n",
      " 477/1415 [=========>....................] - ETA: 1s - loss: 0.1137Epoch 1/28\n",
      " 694/1415 [=============>................] - ETA: 0s - loss: 0.1043Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1063 - val_loss: 0.1046\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1033 - val_loss: 0.1007\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1037 - val_loss: 0.1021\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1132 - val_loss: 0.1119\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1049 - val_loss: 0.1039\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0995 - val_loss: 0.0991\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1029 - val_loss: 0.1011\n",
      "Epoch 12/28\n",
      " 636/1415 [============>.................] - ETA: 0s - loss: 0.098163/63 - 0s - loss: 0.0056 - 112ms/epoch - 2ms/step\n",
      " 806/1415 [================>.............] - ETA: 0s - loss: 0.0981252/252 - 0s - loss: 0.0056 - 208ms/epoch - 825us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1768a65b0>; total time= 1.9min\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1006 - val_loss: 0.0983\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1112 - val_loss: 0.1103\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.1029 - val_loss: 0.1019\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0976 - val_loss: 0.0966\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0982 - val_loss: 0.0959\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.1802 - val_loss: 0.1304\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 1.1600 - val_loss: 0.1422\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.2415 - val_loss: 0.1430\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.1811 - val_loss: 0.1382\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0963 - val_loss: 0.0932\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1092 - val_loss: 0.1082\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.1009 - val_loss: 0.0998\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0954 - val_loss: 0.0935\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0926 - val_loss: 0.0921\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1182 - val_loss: 0.1089\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1303 - val_loss: 0.1201\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1295 - val_loss: 0.1189\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1208 - val_loss: 0.1083\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0911 - val_loss: 0.0894\n",
      "  39/1415 [..............................] - ETA: 1s - loss: 0.1101Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1073 - val_loss: 0.1064\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0987 - val_loss: 0.0973\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0916 - val_loss: 0.0896\n",
      "  87/1415 [>.............................] - ETA: 2s - loss: 0.0980Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0905 - val_loss: 0.0894\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1039 - val_loss: 0.0982\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1033 - val_loss: 0.0987\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0891 - val_loss: 0.0883\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1090 - val_loss: 0.0977\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1118 - val_loss: 0.1015\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1053 - val_loss: 0.1037\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0960 - val_loss: 0.0941\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0893 - val_loss: 0.0895\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0892 - val_loss: 0.0892\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0944 - val_loss: 0.0904\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0949 - val_loss: 0.0904\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1024 - val_loss: 0.1002\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0917 - val_loss: 0.0891\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0879 - val_loss: 0.0879\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0948 - val_loss: 0.0910\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0881 - val_loss: 0.0878\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0978 - val_loss: 0.0927\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0881 - val_loss: 0.0872\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0881 - val_loss: 0.0854\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0870 - val_loss: 0.0856\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0868 - val_loss: 0.0861\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0971 - val_loss: 0.0946\n",
      "Epoch 18/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0912Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0867 - val_loss: 0.0810\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0870 - val_loss: 0.0862\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0881 - val_loss: 0.0840\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0857 - val_loss: 0.0801\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0870 - val_loss: 0.0865\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0929 - val_loss: 0.0905\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0833 - val_loss: 0.0806\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0858 - val_loss: 0.0853\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0857 - val_loss: 0.0848\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0860 - val_loss: 0.0853\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0795 - val_loss: 0.0769\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0860 - val_loss: 0.0851\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0798 - val_loss: 0.0764\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0782 - val_loss: 0.0764\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0882 - val_loss: 0.0855\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0849 - val_loss: 0.0842\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0848 - val_loss: 0.0847\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0781 - val_loss: 0.0733\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0851 - val_loss: 0.0846\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0760 - val_loss: 0.0733\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0850 - val_loss: 0.0848\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0748 - val_loss: 0.0728\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0750 - val_loss: 0.0734\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 1.2191 - val_loss: 0.1392\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0855 - val_loss: 0.0851\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0839 - val_loss: 0.0835\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0840 - val_loss: 0.0836\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0843 - val_loss: 0.0843\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0842 - val_loss: 0.0833\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0727 - val_loss: 0.0703\n",
      " 769/1415 [===============>..............] - ETA: 1s - loss: 0.0828Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0720 - val_loss: 0.0703TA: 0s - loss: 0.126\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0706 - val_loss: 0.0680\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1260 - val_loss: 0.1120\n",
      " 668/1415 [=============>................] - ETA: 1s - loss: 0.0703Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0846 - val_loss: 0.0851\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0832 - val_loss: 0.0838\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0831 - val_loss: 0.0836\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0835 - val_loss: 0.0828\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0696 - val_loss: 0.0674\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0834 - val_loss: 0.0826\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0697 - val_loss: 0.0675\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0837 - val_loss: 0.0841\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0694 - val_loss: 0.0672\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0663 - val_loss: 0.0646\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1000 - val_loss: 0.0912\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0825 - val_loss: 0.0837\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0823 - val_loss: 0.0816\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0828 - val_loss: 0.0819\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0664 - val_loss: 0.0641\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0827 - val_loss: 0.0825\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0665 - val_loss: 0.0652\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0830 - val_loss: 0.0824\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0815 - val_loss: 0.0808\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0667 - val_loss: 0.0648\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0817 - val_loss: 0.0812\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0638 - val_loss: 0.0626\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0821 - val_loss: 0.0824\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0886 - val_loss: 0.0849\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0630 - val_loss: 0.0614\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0820 - val_loss: 0.0814\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0634 - val_loss: 0.0619\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0823 - val_loss: 0.0823\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0808 - val_loss: 0.0800\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0814 - val_loss: 0.0810\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0811 - val_loss: 0.0808\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0636 - val_loss: 0.0615\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0618 - val_loss: 0.0604\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0827 - val_loss: 0.0794\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0813 - val_loss: 0.0821\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0589 - val_loss: 0.0556\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0815 - val_loss: 0.0809\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0615 - val_loss: 0.0609\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0808 - val_loss: 0.0815\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0801 - val_loss: 0.0795\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0805 - val_loss: 0.0803\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0609 - val_loss: 0.0602\n",
      " 912/1415 [==================>...........] - ETA: 0s - loss: 0.0556Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0585 - val_loss: 0.0548\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0782 - val_loss: 0.0764\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0807 - val_loss: 0.0805\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0553 - val_loss: 0.0544\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0808 - val_loss: 0.0803\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0802 - val_loss: 0.0800\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0601 - val_loss: 0.0590\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0794 - val_loss: 0.0791\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0800 - val_loss: 0.0808\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0593 - val_loss: 0.0582\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0801 - val_loss: 0.0798\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0545 - val_loss: 0.0539\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0747 - val_loss: 0.0737\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0541 - val_loss: 0.0534\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0802 - val_loss: 0.0810\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0796 - val_loss: 0.0810\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0788 - val_loss: 0.0785\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0794 - val_loss: 0.0789\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0575 - val_loss: 0.0544\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0561 - val_loss: 0.0532\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0795 - val_loss: 0.0791\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0534 - val_loss: 0.0525\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0718 - val_loss: 0.0712\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0796 - val_loss: 0.0789\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0782 - val_loss: 0.0775\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0791 - val_loss: 0.0783\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0529 - val_loss: 0.0526\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0788 - val_loss: 0.0780\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0535 - val_loss: 0.0525............] - ETA: 1s - loss: 0.069\n",
      "Epoch 16/28\n",
      "1204/1415 [========================>.....] - ETA: 0s - loss: 0.0691Epoch 1/28\n",
      " 695/1415 [=============>................] - ETA: 1s - loss: 0.0521Epoch 1/28\n",
      " 729/1415 [==============>...............] - ETA: 1s - loss: 0.0521Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0790 - val_loss: 0.0788\n",
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0527 - val_loss: 0.0520\n",
      "Epoch 16/28\n",
      " 850/1415 [=================>............] - ETA: 0s - loss: 0.0521252/252 - 0s - loss: 0.0791 - 334ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      " 880/1415 [=================>............] - ETA: 0s - loss: 0.0521252/252 - 0s - loss: 0.0778 - 238ms/epoch - 946us/step\n",
      "Epoch 2/28\n",
      " 944/1415 [===================>..........] - ETA: 0s - loss: 0.0520252/252 - 0s - loss: 0.0784 - 301ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "252/252 - 0s - loss: 0.0788 - 403ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0523 - val_loss: 0.0515\n",
      "Epoch 16/28\n",
      "1046/1415 [=====================>........] - ETA: 0s - loss: 0.0520252/252 - 0s - loss: 0.0789 - 300ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      " 779/1415 [===============>..............] - ETA: 1s - loss: 0.0527252/252 - 0s - loss: 0.0789 - 218ms/epoch - 867us/step\n",
      "Epoch 3/28\n",
      " 274/1415 [====>.........................] - ETA: 1s - loss: 0.0521252/252 - 0s - loss: 0.0779 - 340ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0686 - val_loss: 0.0648\n",
      "Epoch 10/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0642252/252 - 0s - loss: 0.0783 - 308ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      " 114/1415 [=>............................] - ETA: 2s - loss: 0.0643252/252 - 0s - loss: 0.0788 - 355ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 145/1415 [==>...........................] - ETA: 2s - loss: 0.0644252/252 - 0s - loss: 0.0788 - 332ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 992/1415 [====================>.........] - ETA: 0s - loss: 0.0526252/252 - 0s - loss: 0.0774 - 345ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 196/1415 [===>..........................] - ETA: 2s - loss: 0.0642252/252 - 0s - loss: 0.0783 - 355ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1073/1415 [=====================>........] - ETA: 0s - loss: 0.0525Epoch 1/28\n",
      " 305/1415 [=====>........................] - ETA: 2s - loss: 0.0639252/252 - 0s - loss: 0.0787 - 367ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0775 - 282ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0786 - 305ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "Epoch 5/28\n",
      "Epoch 5/28\n",
      " 722/1415 [==============>...............] - ETA: 1s - loss: 0.0521252/252 - 0s - loss: 0.0781 - 362ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 549/1415 [==========>...................] - ETA: 1s - loss: 0.0517252/252 - 0s - loss: 0.0786 - 289ms/epoch - 1ms/step\n",
      " 793/1415 [===============>..............] - ETA: 1s - loss: 0.0520Epoch 6/28\n",
      " 494/1415 [=========>....................] - ETA: 1s - loss: 0.0635252/252 - 0s - loss: 0.0784 - 422ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0519 - val_loss: 0.0521\n",
      "Epoch 17/28\n",
      " 825/1415 [================>.............] - ETA: 1s - loss: 0.0521252/252 - 0s - loss: 0.0773 - 341ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 551/1415 [==========>...................] - ETA: 1s - loss: 0.0634252/252 - 0s - loss: 0.0785 - 411ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      " 572/1415 [===========>..................] - ETA: 1s - loss: 0.0634252/252 - 0s - loss: 0.0779 - 285ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 641/1415 [============>.................] - ETA: 1s - loss: 0.0634252/252 - 0s - loss: 0.0783 - 316ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      " 990/1415 [===================>..........] - ETA: 0s - loss: 0.0520252/252 - 0s - loss: 0.0773 - 442ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 1s - loss: 0.0785 - 511ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      " 201/1415 [===>..........................] - ETA: 3s - loss: 0.0514252/252 - 0s - loss: 0.0783 - 417ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      " 238/1415 [====>.........................] - ETA: 3s - loss: 0.0515252/252 - 0s - loss: 0.0781 - 475ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      " 324/1415 [=====>........................] - ETA: 2s - loss: 0.0514252/252 - 0s - loss: 0.0785 - 277ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0783 - 466ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      " 357/1415 [======>.......................] - ETA: 2s - loss: 0.0514252/252 - 0s - loss: 0.0770 - 354ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0524 - val_loss: 0.0518\n",
      "Epoch 17/28\n",
      "   1/1415 [..............................] - ETA: 4s - loss: 0.0510252/252 - 0s - loss: 0.0784 - 336ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "  34/1415 [..............................] - ETA: 2s - loss: 0.0520252/252 - 0s - loss: 0.0780 - 281ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1002/1415 [====================>.........] - ETA: 0s - loss: 0.0630252/252 - 0s - loss: 0.0783 - 306ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0784 - 312ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1062/1415 [=====================>........] - ETA: 0s - loss: 0.0629252/252 - 0s - loss: 0.0783 - 343ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 208/1415 [===>..........................] - ETA: 2s - loss: 0.0519252/252 - 0s - loss: 0.0772 - 384ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0778 - 323ms/epoch - 1ms/step\n",
      " 590/1415 [===========>..................] - ETA: 1s - loss: 0.0513Epoch 9/28\n",
      " 330/1415 [=====>........................] - ETA: 1s - loss: 0.0518252/252 - 0s - loss: 0.0781 - 337ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1217/1415 [========================>.....] - ETA: 0s - loss: 0.0628252/252 - 0s - loss: 0.0784 - 359ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 383/1415 [=======>......................] - ETA: 1s - loss: 0.0518252/252 - 0s - loss: 0.0782 - 323ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1368/1415 [============================>.] - ETA: 0s - loss: 0.0514252/252 - 0s - loss: 0.0776 - 319ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0769 - 337ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1370/1415 [============================>.] - ETA: 0s - loss: 0.0625252/252 - 0s - loss: 0.0779 - 401ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1383/1415 [============================>.] - ETA: 0s - loss: 0.0625252/252 - 0s - loss: 0.0780 - 410ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0518 - val_loss: 0.0511\n",
      "Epoch 17/28\n",
      "  77/1415 [>.............................] - ETA: 1s - loss: 0.0512252/252 - 0s - loss: 0.0781 - 432ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0776 - 419ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0770 - 434ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0514 - val_loss: 0.0508\n",
      "Epoch 17/28\n",
      "  55/1415 [>.............................] - ETA: 1s - loss: 0.0505252/252 - 0s - loss: 0.0781 - 379ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0779 - 409ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "  85/1415 [>.............................] - ETA: 1s - loss: 0.0507252/252 - 0s - loss: 0.0780 - 314ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1128/1415 [======================>.......] - ETA: 0s - loss: 0.0512252/252 - 0s - loss: 0.0774 - 338ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 285/1415 [=====>........................] - ETA: 1s - loss: 0.0512252/252 - 0s - loss: 0.0770 - 325ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0625 - val_loss: 0.0604\n",
      "Epoch 11/28\n",
      " 206/1415 [===>..........................] - ETA: 1s - loss: 0.0508252/252 - 0s - loss: 0.0778 - 260ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 235/1415 [===>..........................] - ETA: 1s - loss: 0.0507252/252 - 0s - loss: 0.0780 - 311ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 950/1415 [===================>..........] - ETA: 0s - loss: 0.0517252/252 - 0s - loss: 0.0778 - 327ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 286/1415 [=====>........................] - ETA: 1s - loss: 0.0508252/252 - 0s - loss: 0.0776 - 330ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "252/252 - 0s - loss: 0.0767 - 324ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 587/1415 [===========>..................] - ETA: 1s - loss: 0.0512252/252 - 0s - loss: 0.0779 - 335ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 428/1415 [========>.....................] - ETA: 1s - loss: 0.0507252/252 - 0s - loss: 0.0777 - 410ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      "1123/1415 [======================>.......] - ETA: 0s - loss: 0.0516252/252 - 0s - loss: 0.0781 - 314ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1150/1415 [=======================>......] - ETA: 0s - loss: 0.0515252/252 - 0s - loss: 0.0775 - 327ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 650/1415 [============>.................] - ETA: 1s - loss: 0.0511252/252 - 0s - loss: 0.0767 - 335ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 600/1415 [===========>..................] - ETA: 1s - loss: 0.0508252/252 - 0s - loss: 0.0776 - 290ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1307/1415 [==========================>...] - ETA: 0s - loss: 0.0515252/252 - 0s - loss: 0.0780 - 338ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 628/1415 [============>.................] - ETA: 1s - loss: 0.0507252/252 - 0s - loss: 0.0779 - 342ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1389/1415 [============================>.] - ETA: 0s - loss: 0.0515252/252 - 0s - loss: 0.0766 - 340ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0773 - 360ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0511 - val_loss: 0.0505\n",
      "Epoch 18/28\n",
      " 656/1415 [============>.................] - ETA: 1s - loss: 0.0602252/252 - 0s - loss: 0.0776 - 282ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0776 - 349ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 799/1415 [===============>..............] - ETA: 1s - loss: 0.0506252/252 - 0s - loss: 0.0778 - 346ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 836/1415 [================>.............] - ETA: 0s - loss: 0.0506252/252 - 0s - loss: 0.0766 - 306ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 718/1415 [==============>...............] - ETA: 1s - loss: 0.0602252/252 - 0s - loss: 0.0771 - 321ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0515 - val_loss: 0.0511\n",
      "Epoch 18/28\n",
      "1159/1415 [=======================>......] - ETA: 0s - loss: 0.0509252/252 - 0s - loss: 0.0776 - 451ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "252/252 - 0s - loss: 0.0775 - 450ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "  38/1415 [..............................] - ETA: 5s - loss: 0.0506252/252 - 0s - loss: 0.0776 - 457ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      " 383/1415 [=======>......................] - ETA: 2s - loss: 0.0504252/252 - 0s - loss: 0.0765 - 478ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "1056/1415 [=====================>........] - ETA: 0s - loss: 0.0506252/252 - 0s - loss: 0.0773 - 485ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "1190/1415 [========================>.....] - ETA: 0s - loss: 0.0505252/252 - 0s - loss: 0.0778 - 338ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1337/1415 [===========================>..] - ETA: 0s - loss: 0.0509252/252 - 0s - loss: 0.0775 - 346ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "1358/1415 [===========================>..] - ETA: 0s - loss: 0.0509252/252 - 0s - loss: 0.0775 - 378ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 278/1415 [====>.........................] - ETA: 2s - loss: 0.0510252/252 - 0s - loss: 0.0763 - 317ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "252/252 - 0s - loss: 0.0771 - 301ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 689/1415 [=============>................] - ETA: 1s - loss: 0.0503252/252 - 0s - loss: 0.0774 - 317ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0774 - 329ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1236/1415 [=========================>....] - ETA: 0s - loss: 0.0596252/252 - 0s - loss: 0.0774 - 375ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1271/1415 [=========================>....] - ETA: 0s - loss: 0.0595252/252 - 0s - loss: 0.0762 - 400ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      " 490/1415 [=========>....................] - ETA: 1s - loss: 0.0510252/252 - 0s - loss: 0.0770 - 410ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0508 - val_loss: 0.0502\n",
      "Epoch 18/28\n",
      " 593/1415 [===========>..................] - ETA: 1s - loss: 0.0509252/252 - 0s - loss: 0.0771 - 329ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1408/1415 [============================>.] - ETA: 0s - loss: 0.0593252/252 - 0s - loss: 0.0773 - 363ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 621/1415 [============>.................] - ETA: 1s - loss: 0.0508252/252 - 0s - loss: 0.0774 - 290ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 648/1415 [============>.................] - ETA: 1s - loss: 0.0508252/252 - 0s - loss: 0.0769 - 253ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 670/1415 [=============>................] - ETA: 1s - loss: 0.0508252/252 - 0s - loss: 0.0762 - 317ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 979/1415 [===================>..........] - ETA: 0s - loss: 0.0503252/252 - 0s - loss: 0.0772 - 200ms/epoch - 795us/step\n",
      "Epoch 21/28\n",
      "1011/1415 [====================>.........] - ETA: 0s - loss: 0.0503252/252 - 0s - loss: 0.0771 - 286ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0505 - val_loss: 0.0501\n",
      "Epoch 18/28\n",
      "1044/1415 [=====================>........] - ETA: 0s - loss: 0.0503252/252 - 0s - loss: 0.0772 - 272ms/epoch - 1ms/step\n",
      "  38/1415 [..............................] - ETA: 1s - loss: 0.0503Epoch 21/28\n",
      " 833/1415 [================>.............] - ETA: 1s - loss: 0.0508252/252 - 0s - loss: 0.0768 - 357ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 851/1415 [=================>............] - ETA: 1s - loss: 0.0507252/252 - 0s - loss: 0.0760 - 359ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 350/1415 [======>.......................] - ETA: 2s - loss: 0.0506252/252 - 0s - loss: 0.0771 - 371ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 914/1415 [==================>...........] - ETA: 0s - loss: 0.0507252/252 - 0s - loss: 0.0772 - 435ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0593 - val_loss: 0.0576\n",
      "Epoch 12/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.0563252/252 - 0s - loss: 0.0768 - 266ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0771 - 398ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 127/1415 [=>............................] - ETA: 1s - loss: 0.0575252/252 - 0s - loss: 0.0760 - 379ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 224/1415 [===>..........................] - ETA: 1s - loss: 0.0574252/252 - 0s - loss: 0.0769 - 423ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0772 - 437ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 257/1415 [====>.........................] - ETA: 1s - loss: 0.0574252/252 - 0s - loss: 0.0770 - 399ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 622/1415 [============>.................] - ETA: 1s - loss: 0.0504252/252 - 0s - loss: 0.0767 - 439ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 683/1415 [=============>................] - ETA: 1s - loss: 0.0504252/252 - 0s - loss: 0.0759 - 384ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 384/1415 [=======>......................] - ETA: 1s - loss: 0.0573252/252 - 0s - loss: 0.0770 - 305ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 412/1415 [=======>......................] - ETA: 1s - loss: 0.0573252/252 - 0s - loss: 0.0771 - 403ms/epoch - 2ms/step\n",
      " 760/1415 [===============>..............] - ETA: 1s - loss: 0.0503Epoch 24/28\n",
      " 413/1415 [=======>......................] - ETA: 1s - loss: 0.0573252/252 - 0s - loss: 0.0765 - 400ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      " 656/1415 [============>.................] - ETA: 1s - loss: 0.0498252/252 - 0s - loss: 0.0770 - 479ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0502 - val_loss: 0.0497\n",
      "Epoch 19/28\n",
      " 505/1415 [=========>....................] - ETA: 1s - loss: 0.0572252/252 - 0s - loss: 0.0758 - 485ms/epoch - 2ms/step\n",
      "  53/1415 [>.............................] - ETA: 1s - loss: 0.0495Epoch 24/28\n",
      " 571/1415 [===========>..................] - ETA: 1s - loss: 0.0571252/252 - 0s - loss: 0.0771 - 320ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "  57/1415 [>.............................] - ETA: 4s - loss: 0.0495252/252 - 0s - loss: 0.0769 - 491ms/epoch - 2ms/step\n",
      " 777/1415 [===============>..............] - ETA: 1s - loss: 0.0498Epoch 21/28\n",
      "  96/1415 [=>............................] - ETA: 3s - loss: 0.0498252/252 - 0s - loss: 0.0766 - 354ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 988/1415 [===================>..........] - ETA: 0s - loss: 0.0502252/252 - 0s - loss: 0.0769 - 314ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 746/1415 [==============>...............] - ETA: 1s - loss: 0.0570252/252 - 0s - loss: 0.0757 - 395ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "252/252 - 0s - loss: 0.0769 - 316ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 779/1415 [===============>..............] - ETA: 1s - loss: 0.0569252/252 - 0s - loss: 0.0768 - 332ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0506 - val_loss: 0.0501\n",
      "Epoch 19/28\n",
      " 821/1415 [================>.............] - ETA: 1s - loss: 0.0569252/252 - 0s - loss: 0.0769 - 379ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0765 - 399ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      " 906/1415 [==================>...........] - ETA: 0s - loss: 0.0568252/252 - 0s - loss: 0.0755 - 335ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 965/1415 [===================>..........] - ETA: 0s - loss: 0.0568252/252 - 0s - loss: 0.0767 - 392ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 993/1415 [====================>.........] - ETA: 0s - loss: 0.0568252/252 - 0s - loss: 0.0767 - 318ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1385/1415 [============================>.] - ETA: 0s - loss: 0.0500252/252 - 0s - loss: 0.0767 - 439ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "1012/1415 [====================>.........] - ETA: 0s - loss: 0.0568252/252 - 0s - loss: 0.0766 - 343ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1093/1415 [======================>.......] - ETA: 0s - loss: 0.0567252/252 - 0s - loss: 0.0770 - 287ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1128/1415 [======================>.......] - ETA: 0s - loss: 0.0567252/252 - 0s - loss: 0.0755 - 381ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1175/1415 [=======================>......] - ETA: 0s - loss: 0.0567252/252 - 0s - loss: 0.0766 - 316ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "252/252 - 0s - loss: 0.0768 - 333ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "252/252 - 0s - loss: 0.0765 - 308ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1266/1415 [=========================>....] - ETA: 0s - loss: 0.0566252/252 - 0s - loss: 0.0755 - 255ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0500 - val_loss: 0.0495\n",
      "Epoch 19/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.0522252/252 - 0s - loss: 0.0767 - 355ms/epoch - 1ms/step\n",
      "  42/1415 [..............................] - ETA: 1s - loss: 0.0503252/252 - 0s - loss: 0.0763 - 257ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0767 - 278ms/epoch - 1ms/step\n",
      "  74/1415 [>.............................] - ETA: 1s - loss: 0.0500252/252 - 0s - loss: 0.0767 - 342ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 921/1415 [==================>...........] - ETA: 0s - loss: 0.049663/63 - 0s - loss: 0.0717 - 209ms/epoch - 3ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0497 - val_loss: 0.0492\n",
      "Epoch 19/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.0519252/252 - 0s - loss: 0.0755 - 359ms/epoch - 1ms/step\n",
      " 223/1415 [===>..........................] - ETA: 1s - loss: 0.049963/63 - 0s - loss: 0.0732 - 265ms/epoch - 4ms/step\n",
      "  38/1415 [..............................] - ETA: 1s - loss: 0.049363/63 - 0s - loss: 0.0752 - 232ms/epoch - 4ms/step\n",
      "  75/1415 [>.............................] - ETA: 1s - loss: 0.0492252/252 - 0s - loss: 0.0763 - 299ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 167/1415 [==>...........................] - ETA: 1s - loss: 0.049263/63 - 0s - loss: 0.0726 - 263ms/epoch - 4ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0565 - val_loss: 0.0551\n",
      "Epoch 13/28\n",
      "252/252 - 0s - loss: 0.0732 - 307ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1_l2, optimizer=sgd; total time= 1.6min\n",
      " 233/1415 [===>..........................] - ETA: 1s - loss: 0.0491252/252 - 0s - loss: 0.0736 - 300ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1_l2, optimizer=sgd; total time= 1.6min\n",
      "252/252 - 0s - loss: 0.0735 - 287ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1_l2, optimizer=sgd; total time= 1.6min\n",
      " 281/1415 [====>.........................] - ETA: 2s - loss: 0.0491252/252 - 0s - loss: 0.0765 - 436ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 392/1415 [=======>......................] - ETA: 2s - loss: 0.0492252/252 - 0s - loss: 0.0724 - 495ms/epoch - 2ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1_l2, optimizer=sgd; total time= 1.6min\n",
      " 513/1415 [=========>....................] - ETA: 1s - loss: 0.0491252/252 - 0s - loss: 0.0764 - 357ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 649/1415 [============>.................] - ETA: 1s - loss: 0.0491252/252 - 0s - loss: 0.0763 - 234ms/epoch - 928us/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0495 - val_loss: 0.0489\n",
      "Epoch 20/28\n",
      "  33/1415 [..............................] - ETA: 4s - loss: 0.049263/63 - 0s - loss: 0.0729 - 187ms/epoch - 3ms/step\n",
      " 218/1415 [===>..........................] - ETA: 2s - loss: 0.0489252/252 - 0s - loss: 0.0732 - 337ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1_l2, optimizer=sgd; total time= 1.6min\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0498 - val_loss: 0.0492\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0493 - val_loss: 0.0491\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0489 - val_loss: 0.0483\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0549 - val_loss: 0.0540\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0491 - val_loss: 0.0485\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0487 - val_loss: 0.0482\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0486 - val_loss: 0.0484\n",
      "1182/1415 [========================>.....] - ETA: 0s - loss: 0.0538Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0483 - val_loss: 0.0477\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0537 - val_loss: 0.0534\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0481 - val_loss: 0.0477\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0484 - val_loss: 0.0479\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0479 - val_loss: 0.0476\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0477 - val_loss: 0.0472\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0526 - val_loss: 0.0526\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0475 - val_loss: 0.0479\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0478 - val_loss: 0.0480\n",
      "Epoch 23/28\n",
      " 131/1415 [=>............................] - ETA: 2s - loss: 0.0473Epoch 1/28\n",
      " 121/1415 [=>............................] - ETA: 3s - loss: 0.0474Epoch 1/28\n",
      "1173/1415 [=======================>......] - ETA: 0s - loss: 0.0472Epoch 1/28\n",
      "  94/1415 [>.............................] - ETA: 2s - loss: 16.5893Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0474 - val_loss: 0.0469\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0471 - val_loss: 0.0467\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0517 - val_loss: 0.0511\n",
      "Epoch 17/28\n",
      " 591/1415 [===========>..................] - ETA: 1s - loss: 0.0467Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0470 - val_loss: 0.0465\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0473 - val_loss: 0.0470\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0469 - val_loss: 0.0465\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0465 - val_loss: 0.0467\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0508 - val_loss: 0.0501\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 7.5923 - val_loss: 1.4080\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 7.5219 - val_loss: 1.3634\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 7.5458 - val_loss: 1.4097\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 7.5939 - val_loss: 1.4237\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0465 - val_loss: 0.0461\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0469 - val_loss: 0.0464\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0464 - val_loss: 0.0460\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0461 - val_loss: 0.0456\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.4320 - val_loss: 0.1685\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0500 - val_loss: 0.0496\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.3940 - val_loss: 0.1417\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.4426 - val_loss: 0.1702\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.4457 - val_loss: 0.1628\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 7.6059 - val_loss: 1.4163\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0461 - val_loss: 0.0458\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0464 - val_loss: 0.0483\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1373 - val_loss: 0.1154\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0457 - val_loss: 0.0457\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0459 - val_loss: 0.0455\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1321 - val_loss: 0.1037\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1195 - val_loss: 0.1047\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1238 - val_loss: 0.1003\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0492 - val_loss: 0.0489\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.4269 - val_loss: 0.1529\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0457 - val_loss: 0.0452\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0460 - val_loss: 0.0454\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0997 - val_loss: 0.0860\n",
      " 785/1415 [===============>..............] - ETA: 1s - loss: 0.0487Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0884 - val_loss: 0.0767\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0940 - val_loss: 0.0848\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0453 - val_loss: 0.0449\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0905 - val_loss: 0.0826\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0455 - val_loss: 0.0455\n",
      " 581/1415 [===========>..................] - ETA: 1s - loss: 0.0824Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1265 - val_loss: 0.1084\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0485 - val_loss: 0.0479\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0453 - val_loss: 0.0452\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0780 - val_loss: 0.0716\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0455 - val_loss: 0.0457\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0720 - val_loss: 0.0686\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0781 - val_loss: 0.0713\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0787 - val_loss: 0.0751\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0449 - val_loss: 0.0447\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0978 - val_loss: 0.0887\n",
      "  68/1415 [>.............................] - ETA: 2s - loss: 0.0449Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0451 - val_loss: 0.0448\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0479 - val_loss: 0.0472\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0688 - val_loss: 0.0668\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0449 - val_loss: 0.0446\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0659 - val_loss: 0.0631\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0677 - val_loss: 0.0637\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0451 - val_loss: 0.0447\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0724 - val_loss: 0.0698\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0830 - val_loss: 0.0775\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0445 - val_loss: 0.0448\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0448 - val_loss: 0.0444\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0473 - val_loss: 0.0470\n",
      "Epoch 23/28\n",
      "1064/1415 [=====================>........] - ETA: 0s - loss: 0.0612Epoch 1/28\n",
      " 913/1415 [==================>...........] - ETA: 0s - loss: 0.0743Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0653 - val_loss: 0.0640\n",
      "Epoch 8/28\n",
      "1358/1415 [===========================>..] - ETA: 0s - loss: 0.0604252/252 - 0s - loss: 0.0451 - 348ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0608 - val_loss: 0.0586\n",
      "Epoch 8/28\n",
      "1203/1415 [========================>.....] - ETA: 0s - loss: 0.0733252/252 - 0s - loss: 0.0454 - 352ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      " 146/1415 [==>...........................] - ETA: 1s - loss: 0.0585Epoch 1/28\n",
      "Epoch 1/28\n",
      "252/252 - 0s - loss: 0.0449 - 312ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0603 - val_loss: 0.0561\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0676 - val_loss: 0.0650\n",
      "Epoch 8/28\n",
      " 604/1415 [===========>..................] - ETA: 1s - loss: 0.0468252/252 - 0s - loss: 0.0452 - 424ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      " 399/1415 [=======>......................] - ETA: 1s - loss: 0.0581252/252 - 0s - loss: 0.0447 - 356ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 483/1415 [=========>....................] - ETA: 1s - loss: 0.0580252/252 - 0s - loss: 0.0445 - 461ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 622/1415 [============>.................] - ETA: 1s - loss: 0.0634252/252 - 0s - loss: 0.0447 - 476ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0726 - val_loss: 0.0679\n",
      "Epoch 7/28\n",
      " 383/1415 [=======>......................] - ETA: 1s - loss: 0.0555252/252 - 0s - loss: 0.0451 - 387ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      " 818/1415 [================>.............] - ETA: 0s - loss: 0.0633252/252 - 0s - loss: 0.0446 - 411ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      " 488/1415 [=========>....................] - ETA: 1s - loss: 0.0552252/252 - 0s - loss: 0.0445 - 415ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      " 426/1415 [========>.....................] - ETA: 1s - loss: 0.0645252/252 - 0s - loss: 0.0447 - 472ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "1107/1415 [======================>.......] - ETA: 0s - loss: 0.0468252/252 - 0s - loss: 0.0449 - 444ms/epoch - 2ms/step\n",
      " 565/1415 [==========>...................] - ETA: 1s - loss: 0.0642Epoch 5/28\n",
      "1104/1415 [======================>.......] - ETA: 0s - loss: 0.0573252/252 - 0s - loss: 0.0446 - 399ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "1192/1415 [========================>.....] - ETA: 0s - loss: 0.0572252/252 - 0s - loss: 0.0444 - 378ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0442 - 430ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      "1341/1415 [===========================>..] - ETA: 0s - loss: 0.0571252/252 - 0s - loss: 0.0451 - 446ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      " 957/1415 [===================>..........] - ETA: 0s - loss: 0.0544252/252 - 0s - loss: 0.0446 - 484ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      " 995/1415 [====================>.........] - ETA: 0s - loss: 0.0635252/252 - 0s - loss: 0.0440 - 475ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "1042/1415 [=====================>........] - ETA: 0s - loss: 0.0543252/252 - 0s - loss: 0.0446 - 495ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "1132/1415 [=======================>......] - ETA: 0s - loss: 0.0633252/252 - 0s - loss: 0.0450 - 451ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1197/1415 [========================>.....] - ETA: 0s - loss: 0.0632252/252 - 0s - loss: 0.0444 - 487ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0570 - val_loss: 0.0554\n",
      "Epoch 9/28\n",
      "1228/1415 [=========================>....] - ETA: 0s - loss: 0.0631252/252 - 0s - loss: 0.0440 - 414ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "1260/1415 [=========================>....] - ETA: 0s - loss: 0.0630252/252 - 0s - loss: 0.0444 - 430ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0628 - val_loss: 0.0616\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0468 - val_loss: 0.0463\n",
      "Epoch 24/28\n",
      "1131/1415 [======================>.......] - ETA: 0s - loss: 0.0653252/252 - 0s - loss: 0.0449 - 452ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1211/1415 [========================>.....] - ETA: 0s - loss: 0.0652252/252 - 0s - loss: 0.0445 - 435ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      " 304/1415 [=====>........................] - ETA: 2s - loss: 0.0554252/252 - 1s - loss: 0.0440 - 573ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1306/1415 [==========================>...] - ETA: 0s - loss: 0.0649252/252 - 1s - loss: 0.0443 - 632ms/epoch - 3ms/step\n",
      "Epoch 7/28\n",
      " 345/1415 [======>.......................] - ETA: 2s - loss: 0.0614252/252 - 1s - loss: 0.0449 - 561ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0536 - val_loss: 0.0505\n",
      "Epoch 9/28\n",
      " 302/1415 [=====>........................] - ETA: 2s - loss: 0.0464252/252 - 1s - loss: 0.0444 - 543ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 360/1415 [======>.......................] - ETA: 2s - loss: 0.0464252/252 - 0s - loss: 0.0441 - 416ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0628 - val_loss: 0.0601\n",
      "Epoch 9/28\n",
      " 665/1415 [=============>................] - ETA: 1s - loss: 0.0550252/252 - 0s - loss: 0.0442 - 424ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0647 - val_loss: 0.0615\n",
      "Epoch 8/28\n",
      " 170/1415 [==>...........................] - ETA: 2s - loss: 0.0600252/252 - 0s - loss: 0.0447 - 478ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 289/1415 [=====>........................] - ETA: 2s - loss: 0.0501252/252 - 0s - loss: 0.0443 - 481ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      " 741/1415 [==============>...............] - ETA: 1s - loss: 0.0611252/252 - 0s - loss: 0.0442 - 479ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      " 774/1415 [===============>..............] - ETA: 1s - loss: 0.0611252/252 - 0s - loss: 0.0441 - 439ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      " 471/1415 [========>.....................] - ETA: 1s - loss: 0.0594252/252 - 0s - loss: 0.0448 - 463ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      " 524/1415 [==========>...................] - ETA: 1s - loss: 0.0593252/252 - 0s - loss: 0.0441 - 419ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      " 587/1415 [===========>..................] - ETA: 1s - loss: 0.0499252/252 - 0s - loss: 0.0438 - 432ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 839/1415 [================>.............] - ETA: 1s - loss: 0.0464252/252 - 0s - loss: 0.0440 - 441ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 475/1415 [=========>....................] - ETA: 1s - loss: 0.0611252/252 - 0s - loss: 0.0448 - 377ms/epoch - 1ms/step\n",
      "1262/1415 [=========================>....] - ETA: 0s - loss: 0.0545Epoch 12/28\n",
      " 554/1415 [==========>...................] - ETA: 1s - loss: 0.0611252/252 - 0s - loss: 0.0445 - 432ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1009/1415 [====================>.........] - ETA: 0s - loss: 0.0463252/252 - 0s - loss: 0.0438 - 373ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 822/1415 [================>.............] - ETA: 1s - loss: 0.0496252/252 - 0s - loss: 0.0440 - 397ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "1147/1415 [=======================>......] - ETA: 0s - loss: 0.0463252/252 - 0s - loss: 0.0447 - 397ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1041/1415 [=====================>........] - ETA: 0s - loss: 0.0494252/252 - 0s - loss: 0.0442 - 413ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1224/1415 [========================>.....] - ETA: 0s - loss: 0.0463252/252 - 0s - loss: 0.0439 - 410ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1257/1415 [=========================>....] - ETA: 0s - loss: 0.0463252/252 - 0s - loss: 0.0441 - 381ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0543 - val_loss: 0.0529\n",
      "Epoch 10/28\n",
      "1360/1415 [===========================>..] - ETA: 0s - loss: 0.0463252/252 - 0s - loss: 0.0446 - 404ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0606 - val_loss: 0.0597\n",
      "Epoch 10/28\n",
      "1047/1415 [=====================>........] - ETA: 0s - loss: 0.0601252/252 - 0s - loss: 0.0441 - 465ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1111/1415 [======================>.......] - ETA: 0s - loss: 0.0600252/252 - 1s - loss: 0.0440 - 552ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "252/252 - 1s - loss: 0.0439 - 522ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 261/1415 [====>.........................] - ETA: 2s - loss: 0.0595252/252 - 1s - loss: 0.0444 - 544ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 322/1415 [=====>........................] - ETA: 2s - loss: 0.0595252/252 - 0s - loss: 0.0439 - 366ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0443 - 483ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0463 - val_loss: 0.0461\n",
      "Epoch 25/28\n",
      " 447/1415 [========>.....................] - ETA: 1s - loss: 0.0594252/252 - 0s - loss: 0.0438 - 473ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0580 - val_loss: 0.0561\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0491 - val_loss: 0.0476\n",
      "Epoch 10/28\n",
      "  64/1415 [>.............................] - ETA: 1s - loss: 0.0483252/252 - 0s - loss: 0.0445 - 476ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      " 215/1415 [===>..........................] - ETA: 2s - loss: 0.0459252/252 - 0s - loss: 0.0437 - 463ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0443 - 460ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      " 149/1415 [==>...........................] - ETA: 1s - loss: 0.0478252/252 - 0s - loss: 0.0438 - 406ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0595 - val_loss: 0.0569\n",
      "Epoch 9/28\n",
      " 381/1415 [=======>......................] - ETA: 2s - loss: 0.0560252/252 - 1s - loss: 0.0444 - 607ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      " 448/1415 [========>.....................] - ETA: 1s - loss: 0.0476252/252 - 1s - loss: 0.0437 - 672ms/epoch - 3ms/step\n",
      "Epoch 16/28\n",
      " 955/1415 [===================>..........] - ETA: 0s - loss: 0.0591252/252 - 1s - loss: 0.0441 - 692ms/epoch - 3ms/step\n",
      "Epoch 18/28\n",
      " 518/1415 [=========>....................] - ETA: 1s - loss: 0.0476252/252 - 1s - loss: 0.0438 - 712ms/epoch - 3ms/step\n",
      "Epoch 16/28\n",
      " 615/1415 [============>.................] - ETA: 1s - loss: 0.0474252/252 - 0s - loss: 0.0446 - 429ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      " 676/1415 [=============>................] - ETA: 1s - loss: 0.0474252/252 - 0s - loss: 0.0436 - 371ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 728/1415 [==============>...............] - ETA: 1s - loss: 0.0473252/252 - 0s - loss: 0.0440 - 443ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      " 759/1415 [===============>..............] - ETA: 1s - loss: 0.0473252/252 - 0s - loss: 0.0438 - 410ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      " 805/1415 [================>.............] - ETA: 1s - loss: 0.0459252/252 - 0s - loss: 0.0446 - 389ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      " 857/1415 [=================>............] - ETA: 1s - loss: 0.0459252/252 - 0s - loss: 0.0435 - 384ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      " 747/1415 [==============>...............] - ETA: 1s - loss: 0.0558252/252 - 0s - loss: 0.0441 - 385ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "1018/1415 [====================>.........] - ETA: 0s - loss: 0.0472252/252 - 0s - loss: 0.0440 - 408ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "1127/1415 [======================>.......] - ETA: 0s - loss: 0.0471252/252 - 0s - loss: 0.0445 - 429ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0588 - val_loss: 0.0579\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0521 - val_loss: 0.0508\n",
      "Epoch 11/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0501252/252 - 0s - loss: 0.0437 - 397ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "  89/1415 [>.............................] - ETA: 2s - loss: 0.0579252/252 - 0s - loss: 0.0439 - 435ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      " 138/1415 [=>............................] - ETA: 2s - loss: 0.0578252/252 - 0s - loss: 0.0436 - 400ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      " 248/1415 [====>.........................] - ETA: 1s - loss: 0.0578252/252 - 0s - loss: 0.0445 - 401ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      " 370/1415 [======>.......................] - ETA: 1s - loss: 0.0506252/252 - 0s - loss: 0.0436 - 409ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 462/1415 [========>.....................] - ETA: 1s - loss: 0.0505252/252 - 0s - loss: 0.0439 - 434ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 474/1415 [=========>....................] - ETA: 1s - loss: 0.0505252/252 - 0s - loss: 0.0438 - 444ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 566/1415 [===========>..................] - ETA: 1s - loss: 0.0504252/252 - 0s - loss: 0.0442 - 384ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0549 - val_loss: 0.0533\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0469 - val_loss: 0.0458\n",
      "Epoch 11/28\n",
      "  70/1415 [>.............................] - ETA: 0s - loss: 0.0463252/252 - 0s - loss: 0.0435 - 450ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      " 107/1415 [=>............................] - ETA: 1s - loss: 0.0462252/252 - 0s - loss: 0.0438 - 393ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 774/1415 [===============>..............] - ETA: 0s - loss: 0.0502252/252 - 0s - loss: 0.0436 - 464ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      " 795/1415 [===============>..............] - ETA: 0s - loss: 0.0575252/252 - 0s - loss: 0.0441 - 495ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0546 - val_loss: 0.0520\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0458 - val_loss: 0.0454\n",
      "Epoch 26/28\n",
      " 875/1415 [=================>............] - ETA: 0s - loss: 0.0575252/252 - 0s - loss: 0.0435 - 497ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 163/1415 [==>...........................] - ETA: 1s - loss: 0.0455252/252 - 1s - loss: 0.0438 - 539ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      " 185/1415 [==>...........................] - ETA: 2s - loss: 0.0455252/252 - 0s - loss: 0.0436 - 460ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 276/1415 [====>.........................] - ETA: 1s - loss: 0.0519252/252 - 0s - loss: 0.0444 - 482ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "1160/1415 [=======================>......] - ETA: 0s - loss: 0.0499252/252 - 0s - loss: 0.0435 - 435ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 681/1415 [=============>................] - ETA: 1s - loss: 0.0456252/252 - 0s - loss: 0.0439 - 457ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "1355/1415 [===========================>..] - ETA: 0s - loss: 0.0572252/252 - 0s - loss: 0.0436 - 468ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "1373/1415 [============================>.] - ETA: 0s - loss: 0.0497252/252 - 0s - loss: 0.0443 - 451ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "1400/1415 [============================>.] - ETA: 0s - loss: 0.0497252/252 - 0s - loss: 0.0434 - 403ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      " 741/1415 [==============>...............] - ETA: 1s - loss: 0.0512252/252 - 0s - loss: 0.0438 - 459ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      " 957/1415 [===================>..........] - ETA: 0s - loss: 0.0523252/252 - 1s - loss: 0.0436 - 615ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      " 716/1415 [==============>...............] - ETA: 1s - loss: 0.0455252/252 - 1s - loss: 0.0444 - 663ms/epoch - 3ms/step\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0571 - val_loss: 0.0563\n",
      "Epoch 12/28\n",
      "1090/1415 [======================>.......] - ETA: 0s - loss: 0.0453252/252 - 1s - loss: 0.0435 - 853ms/epoch - 3ms/step\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0497 - val_loss: 0.0482\n",
      "Epoch 12/28\n",
      " 810/1415 [================>.............] - ETA: 1s - loss: 0.0455252/252 - 1s - loss: 0.0438 - 802ms/epoch - 3ms/step\n",
      "Epoch 27/28\n",
      "1254/1415 [=========================>....] - ETA: 0s - loss: 0.0452252/252 - 1s - loss: 0.0435 - 1s/epoch - 4ms/step\n",
      "Epoch 25/28\n",
      "1301/1415 [==========================>...] - ETA: 0s - loss: 0.0451252/252 - 1s - loss: 0.0441 - 1s/epoch - 4ms/step\n",
      "Epoch 27/28\n",
      " 401/1415 [=======>......................] - ETA: 2s - loss: 0.0561252/252 - 1s - loss: 0.0434 - 962ms/epoch - 4ms/step\n",
      "Epoch 26/28\n",
      "1020/1415 [====================>.........] - ETA: 1s - loss: 0.0454252/252 - 1s - loss: 0.0439 - 920ms/epoch - 4ms/step\n",
      "Epoch 28/28\n",
      " 548/1415 [==========>...................] - ETA: 2s - loss: 0.0560252/252 - 1s - loss: 0.0434 - 520ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "1387/1415 [============================>.] - ETA: 0s - loss: 0.0502252/252 - 1s - loss: 0.0441 - 508ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      " 496/1415 [=========>....................] - ETA: 2s - loss: 0.0477252/252 - 0s - loss: 0.0433 - 397ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 755/1415 [===============>..............] - ETA: 1s - loss: 0.0559252/252 - 0s - loss: 0.0437 - 392ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0516 - val_loss: 0.0496\n",
      "Epoch 12/28\n",
      "  41/1415 [..............................] - ETA: 1s - loss: 0.0494252/252 - 0s - loss: 0.0435 - 405ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0450 - val_loss: 0.0440\n",
      "Epoch 12/28\n",
      " 692/1415 [=============>................] - ETA: 1s - loss: 0.0476252/252 - 0s - loss: 0.0441 - 366ms/epoch - 1ms/step\n",
      " 931/1415 [==================>...........] - ETA: 0s - loss: 0.055863/63 - 0s - loss: 0.0435 - 210ms/epoch - 3ms/step\n",
      " 140/1415 [=>............................] - ETA: 1s - loss: 0.0496252/252 - 0s - loss: 0.0432 - 380ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0502 - val_loss: 0.0482\n",
      "Epoch 11/28\n",
      " 126/1415 [=>............................] - ETA: 1s - loss: 0.0482252/252 - 0s - loss: 0.0435 - 352ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 932/1415 [==================>...........] - ETA: 0s - loss: 0.047463/63 - 0s - loss: 0.0434 - 276ms/epoch - 4ms/step\n",
      "1005/1415 [====================>.........] - ETA: 0s - loss: 0.0473252/252 - 0s - loss: 0.0433 - 335ms/epoch - 1ms/step\n",
      "1012/1415 [====================>.........] - ETA: 0s - loss: 0.0473252/252 - 0s - loss: 0.0432 - 376ms/epoch - 1ms/step\n",
      " 249/1415 [====>.........................] - ETA: 1s - loss: 0.0480[CV] END activation=tanh, kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1547c34f0>; total time= 1.9min\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0454 - val_loss: 0.0449\n",
      "Epoch 27/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0467252/252 - 0s - loss: 0.0436 - 242ms/epoch - 960us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x13a7434f0>; total time= 1.9min\n",
      " 104/1415 [=>............................] - ETA: 4s - loss: 0.0452252/252 - 1s - loss: 0.0433 - 687ms/epoch - 3ms/step\n",
      "1411/1415 [============================>.] - ETA: 0s - loss: 0.055563/63 - 1s - loss: 0.0429 - 585ms/epoch - 9ms/step\n",
      " 595/1415 [===========>..................] - ETA: 1s - loss: 0.047663/63 - 0s - loss: 0.0427 - 228ms/epoch - 4ms/step\n",
      " 891/1415 [=================>............] - ETA: 0s - loss: 0.0489252/252 - 0s - loss: 0.0433 - 140ms/epoch - 556us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16dc834f0>; total time= 1.9min\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0555 - val_loss: 0.0547\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0468 - val_loss: 0.0451\n",
      "Epoch 13/28\n",
      " 870/1415 [=================>............] - ETA: 0s - loss: 0.0474252/252 - 0s - loss: 0.0429 - 388ms/epoch - 2ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16a1034f0>; total time= 1.9min\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0429 - val_loss: 0.0415\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0484 - val_loss: 0.0469\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0468 - val_loss: 0.0455\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0451 - val_loss: 0.0446\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0541 - val_loss: 0.0535\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0437 - val_loss: 0.0421\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0402 - val_loss: 0.0391\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0454 - val_loss: 0.0436\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0450 - val_loss: 0.0442\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0414 - val_loss: 0.0408\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0529 - val_loss: 0.0523\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0446 - val_loss: 0.0444\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0387 - val_loss: 0.0381\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0423 - val_loss: 0.0409\n",
      "Epoch 15/28\n",
      " 382/1415 [=======>......................] - ETA: 1s - loss: 0.0381Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0438 - val_loss: 0.0432\n",
      "Epoch 14/28\n",
      " 776/1415 [===============>..............] - ETA: 0s - loss: 0.0380252/252 - 0s - loss: 0.0445 - 427ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0405 - val_loss: 0.0400\n",
      "Epoch 16/28\n",
      "  68/1415 [>.............................] - ETA: 2s - loss: 0.0400252/252 - 0s - loss: 0.0443 - 389ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0518 - val_loss: 0.0512\n",
      "Epoch 16/28\n",
      " 368/1415 [======>.......................] - ETA: 1s - loss: 0.0399252/252 - 0s - loss: 0.0443 - 436ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      "1389/1415 [============================>.] - ETA: 0s - loss: 0.0401252/252 - 0s - loss: 0.0442 - 405ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0378 - val_loss: 0.0373\n",
      "Epoch 16/28\n",
      "1324/1415 [===========================>..] - ETA: 0s - loss: 0.0429252/252 - 0s - loss: 0.0440 - 269ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0401 - val_loss: 0.0395\n",
      "Epoch 16/28\n",
      " 502/1415 [=========>....................] - ETA: 1s - loss: 0.0372252/252 - 0s - loss: 0.0440 - 386ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0428 - val_loss: 0.0423\n",
      "Epoch 15/28\n",
      " 154/1415 [==>...........................] - ETA: 0s - loss: 0.0422252/252 - 0s - loss: 0.0439 - 282ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1009/1415 [====================>.........] - ETA: 0s - loss: 0.0371252/252 - 0s - loss: 0.0440 - 399ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0397 - val_loss: 0.0393\n",
      "Epoch 17/28\n",
      "1208/1415 [========================>.....] - ETA: 0s - loss: 0.0370Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0507 - val_loss: 0.0501\n",
      "Epoch 17/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0500Epoch 1/28\n",
      " 219/1415 [===>..........................] - ETA: 1s - loss: 0.0393252/252 - 0s - loss: 0.0439 - 452ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      "  35/1415 [..............................] - ETA: 4s - loss: 14.8263Epoch 1/28\n",
      " 178/1415 [==>...........................] - ETA: 2s - loss: 6.6691252/252 - 0s - loss: 0.0439 - 437ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0370 - val_loss: 0.0365\n",
      "Epoch 17/28\n",
      " 664/1415 [=============>................] - ETA: 1s - loss: 0.0391Epoch 1/28\n",
      " 751/1415 [==============>...............] - ETA: 1s - loss: 0.0391252/252 - 1s - loss: 0.0437 - 542ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0393 - val_loss: 0.0391\n",
      "Epoch 17/28\n",
      " 250/1415 [====>.........................] - ETA: 2s - loss: 4.8824252/252 - 0s - loss: 0.0437 - 399ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0419 - val_loss: 0.0413\n",
      "Epoch 16/28\n",
      " 157/1415 [==>...........................] - ETA: 2s - loss: 7.3816252/252 - 0s - loss: 0.0437 - 482ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      " 415/1415 [=======>......................] - ETA: 1s - loss: 0.0412252/252 - 0s - loss: 0.0438 - 401ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0390 - val_loss: 0.0386\n",
      "Epoch 18/28\n",
      " 982/1415 [===================>..........] - ETA: 0s - loss: 1.3606252/252 - 1s - loss: 0.0438 - 535ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0496 - val_loss: 0.0491\n",
      "Epoch 18/28\n",
      "1164/1415 [=======================>......] - ETA: 0s - loss: 1.1643252/252 - 0s - loss: 0.0435 - 458ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0363 - val_loss: 0.0359\n",
      "Epoch 18/28\n",
      " 546/1415 [==========>...................] - ETA: 1s - loss: 0.0489252/252 - 1s - loss: 0.0434 - 546ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0387 - val_loss: 0.0383\n",
      "Epoch 18/28\n",
      "1361/1415 [===========================>..] - ETA: 0s - loss: 0.9981252/252 - 0s - loss: 0.0435 - 341ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0409 - val_loss: 0.0403\n",
      "Epoch 17/28\n",
      "1040/1415 [=====================>........] - ETA: 0s - loss: 0.0487252/252 - 0s - loss: 0.0435 - 341ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1023/1415 [====================>.........] - ETA: 0s - loss: 0.0357252/252 - 0s - loss: 0.0433 - 458ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0383 - val_loss: 0.0380\n",
      "Epoch 19/28\n",
      " 871/1415 [=================>............] - ETA: 0s - loss: 0.0383252/252 - 0s - loss: 0.0434 - 293ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0486 - val_loss: 0.0480\n",
      " 189/1415 [===>..........................] - ETA: 1s - loss: 0.0380Epoch 19/28\n",
      " 148/1415 [==>...........................] - ETA: 1s - loss: 0.0480252/252 - 0s - loss: 0.0435 - 279ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0356 - val_loss: 0.0352\n",
      "Epoch 19/28\n",
      " 508/1415 [=========>....................] - ETA: 0s - loss: 0.0479252/252 - 0s - loss: 0.0432 - 359ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 324/1415 [=====>........................] - ETA: 1s - loss: 0.0352252/252 - 0s - loss: 0.0434 - 337ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0382 - val_loss: 0.0380\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.9653 - val_loss: 0.0838\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0398 - val_loss: 0.0390\n",
      "1032/1415 [====================>.........] - ETA: 0s - loss: 0.0477Epoch 18/28\n",
      " 279/1415 [====>.........................] - ETA: 1s - loss: 0.0378252/252 - 0s - loss: 0.0432 - 387ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.9764 - val_loss: 0.0840\n",
      "Epoch 2/28\n",
      " 270/1415 [====>.........................] - ETA: 2s - loss: 0.0802252/252 - 1s - loss: 0.0433 - 544ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 7s 4ms/step - loss: 0.9749 - val_loss: 0.0889\n",
      "Epoch 2/28\n",
      "1215/1415 [========================>.....] - ETA: 0s - loss: 0.0351252/252 - 0s - loss: 0.0431 - 383ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0378 - val_loss: 0.0376\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0475 - val_loss: 0.0470\n",
      "Epoch 20/28\n",
      "   1/1415 [..............................] - ETA: 0s - loss: 0.0374Epoch 20/28\n",
      "1415/1415 [==============================] - 7s 4ms/step - loss: 0.9636 - val_loss: 0.0840\n",
      "Epoch 2/28\n",
      " 422/1415 [=======>......................] - ETA: 1s - loss: 0.0835252/252 - 0s - loss: 0.0432 - 469ms/epoch - 2ms/step\n",
      " 581/1415 [===========>..................] - ETA: 1s - loss: 0.082063/63 - 0s - loss: 0.0427 - 301ms/epoch - 5ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0351 - val_loss: 0.0349\n",
      "Epoch 20/28\n",
      " 742/1415 [==============>...............] - ETA: 0s - loss: 0.0375252/252 - 0s - loss: 0.0428 - 298ms/epoch - 1ms/step\n",
      " 202/1415 [===>..........................] - ETA: 1s - loss: 0.0349[CV] END activation=tanh, kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1768a64f0>; total time= 2.0min\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0377 - val_loss: 0.0373\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0383 - val_loss: 0.0373\n",
      " 240/1415 [====>.........................] - ETA: 1s - loss: 0.0373Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0699 - val_loss: 0.0615\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0374 - val_loss: 0.0371\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0736 - val_loss: 0.0660\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0466 - val_loss: 0.0461\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0761 - val_loss: 0.0673\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0728 - val_loss: 0.0642\n",
      " 312/1415 [=====>........................] - ETA: 1s - loss: 0.0658Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0346 - val_loss: 0.0344\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0372 - val_loss: 0.0369\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0362 - val_loss: 0.0351\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0569 - val_loss: 0.0525\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0370 - val_loss: 0.0367\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0457 - val_loss: 0.0453\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0613 - val_loss: 0.0573\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0614 - val_loss: 0.0564\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0343 - val_loss: 0.0341\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0584 - val_loss: 0.0541\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0366 - val_loss: 0.0362\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0350 - val_loss: 0.0346\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0366 - val_loss: 0.0364\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0449 - val_loss: 0.0444\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0538 - val_loss: 0.0510\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0472 - val_loss: 0.0425\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0339 - val_loss: 0.0337\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0360 - val_loss: 0.0356\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0531 - val_loss: 0.0504\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0344 - val_loss: 0.0340\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0501 - val_loss: 0.0450\n",
      "1258/1415 [=========================>....] - ETA: 0s - loss: 0.0362Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0362 - val_loss: 0.0359\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0441 - val_loss: 0.0437\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0413 - val_loss: 0.0402\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0487 - val_loss: 0.0466\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0336 - val_loss: 0.0334\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0353 - val_loss: 0.0346\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0480 - val_loss: 0.0461\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0339 - val_loss: 0.0336\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0417 - val_loss: 0.0403\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0358 - val_loss: 0.0355\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0434 - val_loss: 0.0428\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0333 - val_loss: 0.0331\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0340 - val_loss: 0.0330\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0330Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0394 - val_loss: 0.0387\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0450 - val_loss: 0.0434\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0334 - val_loss: 0.0332\n",
      " 350/1415 [======>.......................] - ETA: 1s - loss: 0.0327Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0445 - val_loss: 0.0427\n",
      "Epoch 7/28\n",
      "  57/1415 [>.............................] - ETA: 1s - loss: 0.0428Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0354 - val_loss: 0.0351\n",
      "1242/1415 [=========================>....] - ETA: 0s - loss: 0.0426Epoch 26/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0394 - val_loss: 0.0386\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0425 - val_loss: 0.0421\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0324 - val_loss: 0.0320\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0330 - val_loss: 0.0328\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0331 - val_loss: 0.0329\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0378 - val_loss: 0.0371\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0409 - val_loss: 0.0380\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0350 - val_loss: 0.0347\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0398 - val_loss: 0.0379\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.9835 - val_loss: 0.0851\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 0.0417 - val_loss: 0.0411\n",
      " 778/1415 [===============>..............] - ETA: 1s - loss: 0.0369Epoch 27/28\n",
      " 502/1415 [=========>....................] - ETA: 1s - loss: 0.0347Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0320 - val_loss: 0.0318\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0327 - val_loss: 0.0325\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0379 - val_loss: 0.0371\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0328 - val_loss: 0.0326\n",
      " 313/1415 [=====>........................] - ETA: 2s - loss: 0.0318Epoch 26/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0366 - val_loss: 0.0359TA: 0s - loss: 0.074\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0372 - val_loss: 0.0366\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0731 - val_loss: 0.0648\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0409 - val_loss: 0.0405\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0371 - val_loss: 0.0365\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0345 - val_loss: 0.0342\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0324 - val_loss: 0.0323\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0317 - val_loss: 0.0315\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0366 - val_loss: 0.0359\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0326 - val_loss: 0.0324\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0594 - val_loss: 0.0553\n",
      " 247/1415 [====>.........................] - ETA: 8s - loss: 0.0324Epoch 4/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0352 - val_loss: 0.0337\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0401 - val_loss: 0.0396\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0359 - val_loss: 0.0354\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0340 - val_loss: 0.0335 - loss: 0.053\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0358 - val_loss: 0.0352\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0322 - val_loss: 0.0320\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0314 - val_loss: 0.0313\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0524 - val_loss: 0.0498\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0323 - val_loss: 0.0321\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0356 - val_loss: 0.0351\n",
      "Epoch 10/28\n",
      " 278/1415 [====>.........................] - ETA: 1s - loss: 0.0322Epoch 1/28\n",
      " 619/1415 [============>.................] - ETA: 1s - loss: 0.0488Epoch 1/28\n",
      " 878/1415 [=================>............] - ETA: 1s - loss: 0.0350252/252 - 0s - loss: 0.0396 - 333ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      " 317/1415 [=====>........................] - ETA: 2s - loss: 0.0350Epoch 1/28\n",
      "Epoch 1/28\n",
      " 670/1415 [=============>................] - ETA: 1s - loss: 0.0321252/252 - 0s - loss: 0.0335 - 485ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 707/1415 [=============>................] - ETA: 1s - loss: 0.0321252/252 - 0s - loss: 0.0395 - 391ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      " 748/1415 [==============>...............] - ETA: 1s - loss: 0.0321252/252 - 0s - loss: 0.0321 - 440ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 815/1415 [================>.............] - ETA: 0s - loss: 0.0321252/252 - 1s - loss: 0.0313 - 521ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1252/1415 [=========================>....] - ETA: 0s - loss: 0.0349252/252 - 0s - loss: 0.0334 - 323ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1187/1415 [========================>.....] - ETA: 0s - loss: 0.0476252/252 - 0s - loss: 0.0393 - 275ms/epoch - 1ms/step\n",
      " 693/1415 [=============>................] - ETA: 1s - loss: 0.0349Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0334 - val_loss: 0.0331\n",
      "Epoch 11/28\n",
      "  77/1415 [>.............................] - ETA: 1s - loss: 0.0331252/252 - 0s - loss: 0.0320 - 407ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0350 - val_loss: 0.0345\n",
      "Epoch 11/28\n",
      "1044/1415 [=====================>........] - ETA: 0s - loss: 0.0321252/252 - 0s - loss: 0.0313 - 366ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      " 875/1415 [=================>............] - ETA: 1s - loss: 0.0349252/252 - 0s - loss: 0.0332 - 364ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 182/1415 [==>...........................] - ETA: 2s - loss: 0.0331252/252 - 0s - loss: 0.0392 - 397ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      " 217/1415 [===>..........................] - ETA: 1s - loss: 0.0331252/252 - 0s - loss: 0.0320 - 264ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0468 - val_loss: 0.0416\n",
      "Epoch 6/28\n",
      " 138/1415 [=>............................] - ETA: 3s - loss: 0.0345252/252 - 0s - loss: 0.0312 - 319ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 136/1415 [=>............................] - ETA: 1s - loss: 0.0415252/252 - 0s - loss: 0.0330 - 297ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 378/1415 [=======>......................] - ETA: 1s - loss: 0.0330252/252 - 0s - loss: 0.0390 - 306ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 178/1415 [==>...........................] - ETA: 1s - loss: 0.0414252/252 - 0s - loss: 0.0319 - 255ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0349 - val_loss: 0.0345\n",
      "Epoch 11/28\n",
      "1185/1415 [========================>.....] - ETA: 0s - loss: 0.0348252/252 - 0s - loss: 0.0312 - 333ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 164/1415 [==>...........................] - ETA: 1s - loss: 0.0344252/252 - 0s - loss: 0.0319 - 297ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1264/1415 [=========================>....] - ETA: 0s - loss: 0.0348252/252 - 0s - loss: 0.0388 - 328ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0327 - 382ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "1381/1415 [============================>.] - ETA: 0s - loss: 0.0348252/252 - 0s - loss: 0.0311 - 347ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.0319\n",
      " 625/1415 [============>.................] - ETA: 1s - loss: 0.0344252/252 - 0s - loss: 0.0318 - 348ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 717/1415 [==============>...............] - ETA: 0s - loss: 0.0410252/252 - 0s - loss: 0.0386 - 344ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0324 - 350ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 472/1415 [=========>....................] - ETA: 1s - loss: 0.0344252/252 - 0s - loss: 0.0311 - 364ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 526/1415 [==========>...................] - ETA: 1s - loss: 0.0343252/252 - 0s - loss: 0.0318 - 330ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      " 556/1415 [==========>...................] - ETA: 1s - loss: 0.0343252/252 - 0s - loss: 0.0384 - 349ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0321 - 351ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0348 - val_loss: 0.0345\n",
      "Epoch 11/28\n",
      "1099/1415 [======================>.......] - ETA: 0s - loss: 0.0407252/252 - 0s - loss: 0.0311 - 283ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0318 - 290ms/epoch - 1ms/step\n",
      " 641/1415 [============>.................] - ETA: 1s - loss: 0.0343252/252 - 0s - loss: 0.0382 - 263ms/epoch - 1ms/step\n",
      "Epoch 1/28\n",
      " 972/1415 [===================>..........] - ETA: 0s - loss: 0.0343252/252 - 0s - loss: 0.0317 - 235ms/epoch - 933us/step\n",
      "1075/1415 [=====================>........] - ETA: 0s - loss: 0.0329252/252 - 0s - loss: 0.0310 - 125ms/epoch - 495us/step\n",
      " 106/1415 [=>............................] - ETA: 1s - loss: 0.0343Epoch 9/28\n",
      "Epoch 9/28\n",
      "Epoch 10/28\n",
      "Epoch 9/28\n",
      " 769/1415 [===============>..............] - ETA: 2s - loss: 0.0343252/252 - 2s - loss: 0.0319 - 2s/epoch - 8ms/step\n",
      "Epoch 2/28\n",
      "1142/1415 [=======================>......] - ETA: 1s - loss: 0.0343252/252 - 1s - loss: 0.0310 - 797ms/epoch - 3ms/step\n",
      "Epoch 10/28\n",
      "1255/1415 [=========================>....] - ETA: 0s - loss: 0.0328252/252 - 1s - loss: 0.0312 - 802ms/epoch - 3ms/step\n",
      "Epoch 10/28\n",
      " 285/1415 [=====>........................] - ETA: 10s - loss: 0.0343252/252 - 1s - loss: 0.0379 - 807ms/epoch - 3ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 2s - loss: 0.0317 - 2s/epoch - 10ms/step\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0404 - val_loss: 0.0395\n",
      "Epoch 7/28\n",
      " 105/1415 [=>............................] - ETA: 3s - loss: 0.0394252/252 - 1s - loss: 0.0318 - 677ms/epoch - 3ms/step\n",
      "Epoch 3/28\n",
      "252/252 - 0s - loss: 0.0310 - 436ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      " 141/1415 [=>............................] - ETA: 2s - loss: 0.0394252/252 - 0s - loss: 0.0376 - 448ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0310 - 465ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0317 - 478ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      " 599/1415 [===========>..................] - ETA: 4s - loss: 0.0342252/252 - 0s - loss: 0.0318 - 455ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      " 635/1415 [============>.................] - ETA: 4s - loss: 0.0342252/252 - 0s - loss: 0.0373 - 441ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 669/1415 [=============>................] - ETA: 4s - loss: 0.0342252/252 - 0s - loss: 0.0310 - 478ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0309 - 480ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      " 718/1415 [==============>...............] - ETA: 3s - loss: 0.0342252/252 - 1s - loss: 0.0317 - 577ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0328 - val_loss: 0.0325\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0342 - val_loss: 0.0339\n",
      "Epoch 12/28\n",
      "  61/1415 [>.............................] - ETA: 5s - loss: 0.0325252/252 - 0s - loss: 0.0370 - 461ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "  75/1415 [>.............................] - ETA: 5s - loss: 0.0325252/252 - 1s - loss: 0.0317 - 561ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "1363/1415 [===========================>..] - ETA: 0s - loss: 0.0341252/252 - 1s - loss: 0.0309 - 623ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 134/1415 [=>............................] - ETA: 3s - loss: 0.0338252/252 - 1s - loss: 0.0309 - 731ms/epoch - 3ms/step\n",
      "Epoch 13/28\n",
      " 161/1415 [==>...........................] - ETA: 3s - loss: 0.0338252/252 - 1s - loss: 0.0316 - 657ms/epoch - 3ms/step\n",
      "Epoch 13/28\n",
      " 239/1415 [====>.........................] - ETA: 2s - loss: 0.0338252/252 - 1s - loss: 0.0367 - 517ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 959/1415 [===================>..........] - ETA: 0s - loss: 0.0389252/252 - 0s - loss: 0.0317 - 480ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "252/252 - 0s - loss: 0.0309 - 370ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 331/1415 [======>.......................] - ETA: 2s - loss: 0.0338252/252 - 0s - loss: 0.0308 - 415ms/epoch - 2ms/step\n",
      "1071/1415 [=====================>........] - ETA: 1s - loss: 0.0341Epoch 14/28\n",
      "1096/1415 [======================>.......] - ETA: 0s - loss: 0.0388252/252 - 0s - loss: 0.0316 - 386ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1159/1415 [=======================>......] - ETA: 0s - loss: 0.0388252/252 - 0s - loss: 0.0317 - 327ms/epoch - 1ms/step\n",
      " 437/1415 [========>.....................] - ETA: 2s - loss: 0.0324Epoch 7/28\n",
      "1142/1415 [=======================>......] - ETA: 1s - loss: 0.0341252/252 - 0s - loss: 0.0362 - 370ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1231/1415 [=========================>....] - ETA: 0s - loss: 0.0388252/252 - 0s - loss: 0.0308 - 395ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0341 - val_loss: 0.0339\n",
      "Epoch 12/28\n",
      " 548/1415 [==========>...................] - ETA: 2s - loss: 0.0324252/252 - 0s - loss: 0.0315 - 321ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 103/1415 [=>............................] - ETA: 1s - loss: 0.0338252/252 - 0s - loss: 0.0308 - 376ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 156/1415 [==>...........................] - ETA: 2s - loss: 0.0338252/252 - 0s - loss: 0.0358 - 327ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 611/1415 [===========>..................] - ETA: 1s - loss: 0.0337252/252 - 0s - loss: 0.0316 - 368ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1357/1415 [===========================>..] - ETA: 0s - loss: 0.0340252/252 - 0s - loss: 0.0308 - 301ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0387 - val_loss: 0.0381\n",
      "Epoch 8/28\n",
      " 284/1415 [=====>........................] - ETA: 1s - loss: 0.0337252/252 - 0s - loss: 0.0315 - 349ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 726/1415 [==============>...............] - ETA: 1s - loss: 0.0337252/252 - 0s - loss: 0.0308 - 330ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 797/1415 [===============>..............] - ETA: 1s - loss: 0.0337252/252 - 0s - loss: 0.0353 - 379ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "252/252 - 0s - loss: 0.0307 - 350ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 857/1415 [=================>............] - ETA: 1s - loss: 0.0323252/252 - 0s - loss: 0.0316 - 400ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      " 905/1415 [==================>...........] - ETA: 1s - loss: 0.0337252/252 - 0s - loss: 0.0315 - 378ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      " 485/1415 [=========>....................] - ETA: 1s - loss: 0.0337252/252 - 0s - loss: 0.0307 - 391ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0340 - val_loss: 0.0337\n",
      "Epoch 12/28\n",
      " 530/1415 [==========>...................] - ETA: 1s - loss: 0.0337252/252 - 0s - loss: 0.0315 - 288ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 981/1415 [===================>..........] - ETA: 0s - loss: 0.0337252/252 - 0s - loss: 0.0350 - 351ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0307 - 363ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1063/1415 [=====================>........] - ETA: 0s - loss: 0.0336252/252 - 0s - loss: 0.0314 - 303ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 643/1415 [============>.................] - ETA: 1s - loss: 0.0336252/252 - 0s - loss: 0.0307 - 302ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1188/1415 [========================>.....] - ETA: 0s - loss: 0.0323252/252 - 0s - loss: 0.0315 - 319ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 730/1415 [==============>...............] - ETA: 1s - loss: 0.0336252/252 - 0s - loss: 0.0349 - 332ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 675/1415 [=============>................] - ETA: 1s - loss: 0.0376252/252 - 0s - loss: 0.0306 - 361ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1259/1415 [=========================>....] - ETA: 0s - loss: 0.0336252/252 - 0s - loss: 0.0307 - 378ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 828/1415 [================>.............] - ETA: 1s - loss: 0.0336252/252 - 0s - loss: 0.0314 - 429ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0348 - 261ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 875/1415 [=================>............] - ETA: 1s - loss: 0.0336252/252 - 0s - loss: 0.0315 - 405ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1324/1415 [===========================>..] - ETA: 0s - loss: 0.0336252/252 - 0s - loss: 0.0306 - 349ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 928/1415 [==================>...........] - ETA: 0s - loss: 0.0375252/252 - 0s - loss: 0.0306 - 274ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 997/1415 [====================>.........] - ETA: 0s - loss: 0.0336252/252 - 0s - loss: 0.0314 - 348ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 469/1415 [========>.....................] - ETA: 2s - loss: 0.0336252/252 - 0s - loss: 0.0348 - 346ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1056/1415 [=====================>........] - ETA: 0s - loss: 0.0336252/252 - 0s - loss: 0.0314 - 331ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1093/1415 [======================>.......] - ETA: 0s - loss: 0.0336252/252 - 0s - loss: 0.0306 - 236ms/epoch - 936us/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0306 - 359ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0336 - val_loss: 0.0333\n",
      "Epoch 13/28\n",
      " 626/1415 [============>.................] - ETA: 1s - loss: 0.0336252/252 - 0s - loss: 0.0313 - 282ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "  21/1415 [..............................] - ETA: 3s - loss: 0.0333252/252 - 0s - loss: 0.0347 - 294ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0322 - val_loss: 0.0321\n",
      "Epoch 13/28\n",
      "1275/1415 [==========================>...] - ETA: 0s - loss: 0.0335252/252 - 0s - loss: 0.0305 - 273ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0306 - 309ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1298/1415 [==========================>...] - ETA: 0s - loss: 0.0335252/252 - 0s - loss: 0.0314 - 369ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0346 - 190ms/epoch - 754us/step\n",
      "Epoch 24/28\n",
      " 190/1415 [===>..........................] - ETA: 2s - loss: 0.0332252/252 - 0s - loss: 0.0313 - 326ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0373 - val_loss: 0.0367\n",
      "Epoch 9/28\n",
      " 904/1415 [==================>...........] - ETA: 1s - loss: 0.0335252/252 - 0s - loss: 0.0305 - 352ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "  51/1415 [>.............................] - ETA: 2s - loss: 0.0367252/252 - 0s - loss: 0.0305 - 347ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "252/252 - 0s - loss: 0.0314 - 333ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "  78/1415 [>.............................] - ETA: 2s - loss: 0.0367252/252 - 0s - loss: 0.0346 - 344ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1015/1415 [====================>.........] - ETA: 0s - loss: 0.0335252/252 - 0s - loss: 0.0313 - 382ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 389/1415 [=======>......................] - ETA: 2s - loss: 0.0332252/252 - 0s - loss: 0.0345 - 290ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0335 - val_loss: 0.0332\n",
      "Epoch 13/28\n",
      " 469/1415 [========>.....................] - ETA: 1s - loss: 0.0319252/252 - 0s - loss: 0.0305 - 475ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "  20/1415 [..............................] - ETA: 8s - loss: 0.0332252/252 - 0s - loss: 0.0313 - 472ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "1138/1415 [=======================>......] - ETA: 0s - loss: 0.0335252/252 - 1s - loss: 0.0304 - 535ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      " 558/1415 [==========>...................] - ETA: 1s - loss: 0.0332252/252 - 0s - loss: 0.0312 - 500ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      " 633/1415 [============>.................] - ETA: 1s - loss: 0.0319252/252 - 1s - loss: 0.0345 - 560ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 673/1415 [=============>................] - ETA: 1s - loss: 0.0332252/252 - 1s - loss: 0.0305 - 637ms/epoch - 3ms/step\n",
      "Epoch 25/28\n",
      " 586/1415 [===========>..................] - ETA: 1s - loss: 0.0365252/252 - 1s - loss: 0.0313 - 703ms/epoch - 3ms/step\n",
      "Epoch 17/28\n",
      " 668/1415 [=============>................] - ETA: 1s - loss: 0.0319252/252 - 1s - loss: 0.0304 - 703ms/epoch - 3ms/step\n",
      "Epoch 25/28\n",
      " 643/1415 [============>.................] - ETA: 1s - loss: 0.0364252/252 - 1s - loss: 0.0312 - 598ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      " 749/1415 [==============>...............] - ETA: 1s - loss: 0.0364252/252 - 1s - loss: 0.0344 - 546ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      " 357/1415 [======>.......................] - ETA: 3s - loss: 0.0331252/252 - 0s - loss: 0.0304 - 289ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 844/1415 [================>.............] - ETA: 1s - loss: 0.0318252/252 - 0s - loss: 0.0304 - 394ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      " 874/1415 [=================>............] - ETA: 1s - loss: 0.0363252/252 - 0s - loss: 0.0313 - 415ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      " 438/1415 [========>.....................] - ETA: 2s - loss: 0.0331252/252 - 0s - loss: 0.0311 - 364ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 516/1415 [=========>....................] - ETA: 2s - loss: 0.0331252/252 - 0s - loss: 0.0303 - 309ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      " 992/1415 [====================>.........] - ETA: 1s - loss: 0.0331252/252 - 0s - loss: 0.0344 - 403ms/epoch - 2ms/step\n",
      "1052/1415 [=====================>........] - ETA: 0s - loss: 0.0363252/252 - 0s - loss: 0.0304 - 392ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0334 - val_loss: 0.0329\n",
      "Epoch 13/28\n",
      "1102/1415 [======================>.......] - ETA: 0s - loss: 0.0331252/252 - 0s - loss: 0.0312 - 498ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0311 - 446ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 640/1415 [============>.................] - ETA: 2s - loss: 0.033163/63 - 0s - loss: 0.0343 - 319ms/epoch - 5ms/step\n",
      " 686/1415 [=============>................] - ETA: 2s - loss: 0.0331252/252 - 0s - loss: 0.0303 - 461ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      " 199/1415 [===>..........................] - ETA: 2s - loss: 0.0331252/252 - 0s - loss: 0.0343 - 259ms/epoch - 1ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1, optimizer=sgd; total time= 1.8min\n",
      "252/252 - 0s - loss: 0.0312 - 381ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "252/252 - 0s - loss: 0.0311 - 381ms/epoch - 2ms/step\n",
      " 783/1415 [===============>..............] - ETA: 1s - loss: 0.0330Epoch 28/28\n",
      " 215/1415 [===>..........................] - ETA: 2s - loss: 0.0331252/252 - 1s - loss: 0.0304 - 627ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      " 355/1415 [======>.......................] - ETA: 2s - loss: 0.0330252/252 - 0s - loss: 0.0303 - 450ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0361 - val_loss: 0.0359\n",
      "Epoch 10/28\n",
      " 411/1415 [=======>......................] - ETA: 2s - loss: 0.0330252/252 - 0s - loss: 0.0311 - 360ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 436/1415 [========>.....................] - ETA: 1s - loss: 0.0330252/252 - 0s - loss: 0.0310 - 391ms/epoch - 2ms/step\n",
      "63/63 - 0s - loss: 0.0300 - 125ms/epoch - 2ms/step\n",
      "1003/1415 [====================>.........] - ETA: 1s - loss: 0.0330252/252 - 0s - loss: 0.0304 - 380ms/epoch - 2ms/step\n",
      " 570/1415 [===========>..................] - ETA: 1s - loss: 0.033063/63 - 0s - loss: 0.0309 - 343ms/epoch - 5ms/step\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0330 - val_loss: 0.0328\n",
      "Epoch 14/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.032763/63 - 0s - loss: 0.0302 - 345ms/epoch - 5ms/step\n",
      "252/252 - 1s - loss: 0.0311 - 505ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0317 - val_loss: 0.0314\n",
      "Epoch 14/28\n",
      "  51/1415 [>.............................] - ETA: 1s - loss: 0.0316252/252 - 1s - loss: 0.0302 - 525ms/epoch - 2ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1, optimizer=sgd; total time= 1.8min\n",
      " 119/1415 [=>............................] - ETA: 1s - loss: 0.0316252/252 - 0s - loss: 0.0310 - 257ms/epoch - 1ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1, optimizer=sgd; total time= 1.8min\n",
      " 158/1415 [==>...........................] - ETA: 2s - loss: 0.0315252/252 - 0s - loss: 0.0311 - 290ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 775/1415 [===============>..............] - ETA: 1s - loss: 0.0330252/252 - 0s - loss: 0.0303 - 362ms/epoch - 1ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1, optimizer=sgd; total time= 1.8min\n",
      " 916/1415 [==================>...........] - ETA: 1s - loss: 0.0329252/252 - 0s - loss: 0.0310 - 389ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0329 - val_loss: 0.0325\n",
      "Epoch 14/28\n",
      " 546/1415 [==========>...................] - ETA: 1s - loss: 0.0327252/252 - 0s - loss: 0.0310 - 391ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "1259/1415 [=========================>....] - ETA: 0s - loss: 0.0353252/252 - 0s - loss: 0.0310 - 374ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 369/1415 [======>.......................] - ETA: 2s - loss: 0.0326252/252 - 0s - loss: 0.0310 - 339ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0352 - val_loss: 0.0348\n",
      "Epoch 11/28\n",
      "1095/1415 [======================>.......] - ETA: 0s - loss: 0.0314252/252 - 0s - loss: 0.0309 - 250ms/epoch - 992us/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0328 - val_loss: 0.0324\n",
      "1135/1415 [=======================>......] - ETA: 0s - loss: 0.0313Epoch 14/28\n",
      " 110/1415 [=>............................] - ETA: 4s - loss: 0.0325252/252 - 0s - loss: 0.0309 - 448ms/epoch - 2ms/step\n",
      "1347/1415 [===========================>..] - ETA: 0s - loss: 0.032663/63 - 0s - loss: 0.0309 - 493ms/epoch - 8ms/step\n",
      " 425/1415 [========>.....................] - ETA: 2s - loss: 0.0325252/252 - 0s - loss: 0.0308 - 210ms/epoch - 835us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1, optimizer=sgd; total time= 1.9min\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0313 - val_loss: 0.0311\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0325 - val_loss: 0.0323\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0344 - val_loss: 0.0341\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0324 - val_loss: 0.0323\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0318 - val_loss: 0.0310\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0309 - val_loss: 0.0308\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.0319............] - ETA: 4s - loss: 0.030\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0337 - val_loss: 0.0329\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0320 - val_loss: 0.0318>..............] - ETA: 1s - loss: 0.030\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0308 - val_loss: 0.0307\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0306 - val_loss: 0.0305\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0318 - val_loss: 0.0316\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0320 - val_loss: 0.0317\n",
      "Epoch 14/28\n",
      " 472/1415 [=========>....................] - ETA: 2s - loss: 0.0316Epoch 1/28\n",
      " 762/1415 [===============>..............] - ETA: 0s - loss: 0.0316Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0317 - val_loss: 0.0314\n",
      "Epoch 17/28\n",
      " 739/1415 [==============>...............] - ETA: 1s - loss: 0.0315Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0305 - val_loss: 0.0303\n",
      "Epoch 17/28\n",
      "  22/1415 [..............................] - ETA: 3s - loss: 2.7740  Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0315 - val_loss: 0.0313\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0303 - val_loss: 0.0302\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 0.0314 - val_loss: 0.0313.............] - ETA: 0s - loss: 1.892 - ETA: 1s - loss: 0.03\n",
      "Epoch 18/28\n",
      "1359/1415 [===========================>..] - ETA: 0s - loss: 0.0302Epoch 1/28===========>..........] - ETA: 0s - loss: 0.031- ETA: 1s - loss: 0.031\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0313 - val_loss: 0.0311\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0311 - val_loss: 0.0310\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0302 - val_loss: 0.0301\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0300 - val_loss: 0.0299\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0312 - val_loss: 0.0309\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.6490 - val_loss: 1.2157\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0308 - val_loss: 0.0305\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.6813 - val_loss: 1.2315\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0310 - val_loss: 0.0309.......] - ETA: 0s - loss: 0.029\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.6807 - val_loss: 1.2303\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0300 - val_loss: 0.0297\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.6931 - val_loss: 1.2404\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0297 - val_loss: 0.0295\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0305 - val_loss: 0.03041.111\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.9349 - val_loss: 0.7016\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0309 - val_loss: 0.0308\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.9473 - val_loss: 0.7112\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.9463 - val_loss: 0.7105\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0308 - val_loss: 0.0307\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.6856 - val_loss: 1.2403\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.9539 - val_loss: 0.7159\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0297 - val_loss: 0.0296\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0302 - val_loss: 0.0301\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.5421 - val_loss: 0.4095\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0295 - val_loss: 0.0294\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.5497 - val_loss: 0.4154\n",
      " 210/1415 [===>..........................] - ETA: 1s - loss: 0.3939Epoch 4/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 0.0307 - val_loss: 0.0304TA: 2s - loss: 0.383\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.5491 - val_loss: 0.4150\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0300 - val_loss: 0.0298\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.9540 - val_loss: 0.7162\n",
      " 110/1415 [=>............................] - ETA: 1s - loss: 0.4067Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.5532 - val_loss: 0.4179\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0305 - val_loss: 0.0305\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0295 - val_loss: 0.0294\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.3188 - val_loss: 0.2432=====>.......] - ETA: 0s - loss: 0.029=>..] - ETA: 0s - loss: 0.328\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.3234 - val_loss: 0.2468\n",
      " 823/1415 [================>.............] - ETA: 1s - loss: 0.0305Epoch 5/28\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0293 - val_loss: 0.0292.................] - ETA: 6s - loss: 0.029==>..........] - ETA: 1s - loss: 0.030\n",
      "1190/1415 [========================>.....] - ETA: 0s - loss: 0.0298Epoch 22/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0297 - val_loss: 0.0297\n",
      "1194/1415 [========================>.....] - ETA: 0s - loss: 0.5751Epoch 21/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.3231 - val_loss: 0.2466\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.5535 - val_loss: 0.4182\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0304 - val_loss: 0.0304\n",
      "1224/1415 [========================>.....] - ETA: 0s - loss: 0.3358Epoch 22/28\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.3252 - val_loss: 0.2480\n",
      " 868/1415 [=================>............] - ETA: 2s - loss: 0.2124Epoch 5/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0303 - val_loss: 0.0300- ETA: 1s - loss: 0.029loss: 0.197\n",
      " 410/1415 [=======>......................] - ETA: 4s - loss: 0.0303Epoch 22/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1914 - val_loss: 0.1481\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0293 - val_loss: 0.0292\n",
      " 155/1415 [==>...........................] - ETA: 2s - loss: 0.1442Epoch 22/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0295 - val_loss: 0.0295\n",
      " 264/1415 [====>.........................] - ETA: 3s - loss: 0.1417Epoch 22/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.1943 - val_loss: 0.1504\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0291 - val_loss: 0.0291..........] - ETA: 5s - loss: 0.029\n",
      "1073/1415 [=====================>........] - ETA: 1s - loss: 0.2061Epoch 23/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.3256 - val_loss: 0.2484\n",
      " 160/1415 [==>...........................] - ETA: 2s - loss: 0.0290Epoch 5/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.1940 - val_loss: 0.1502\n",
      " 609/1415 [===========>..................] - ETA: 2s - loss: 0.1361Epoch 6/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1950 - val_loss: 0.1508 0.2\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0302 - val_loss: 0.0301\n",
      " 834/1415 [================>.............] - ETA: 2s - loss: 0.1314Epoch 23/28\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1184 - val_loss: 0.0935loss: 0.030.030\n",
      "1013/1415 [====================>.........] - ETA: 1s - loss: 0.1277Epoch 7/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0293 - val_loss: 0.0293TA: 0s - loss: 0.123\n",
      " 680/1415 [=============>................] - ETA: 3s - loss: 0.2206Epoch 23/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0301 - val_loss: 0.0301\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1202 - val_loss: 0.0949\n",
      " 445/1415 [========>.....................] - ETA: 5s - loss: 0.0301Epoch 7/28\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0291 - val_loss: 0.0290\n",
      "1134/1415 [=======================>......] - ETA: 1s - loss: 0.2045Epoch 23/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1955 - val_loss: 0.1513....] - ETA: 15s - loss: 0.029>.] - ETA: 0s - loss: 0.195\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1200 - val_loss: 0.0948\n",
      "Epoch 6/28\n",
      "1380/1415 [============================>.] - ETA: 0s - loss: 0.1210Epoch 7/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1204 - val_loss: 0.0950===========>....] - ETA: 0s - loss: 0.028\n",
      "1319/1415 [==========================>...] - ETA: 0s - loss: 0.0289Epoch 7/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0289 - val_loss: 0.0290\n",
      "1186/1415 [========================>.....] - ETA: 0s - loss: 0.0292Epoch 24/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0763 - val_loss: 0.0618\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0292 - val_loss: 0.0291\n",
      "Epoch 8/28\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0300 - val_loss: 0.0301\n",
      " 989/1415 [===================>..........] - ETA: 1s - loss: 0.0299Epoch 24/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0775 - val_loss: 0.0628ETA: 2s - loss: 0.085\n",
      " 256/1415 [====>.........................] - ETA: 3s - loss: 0.0599Epoch 8/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0299 - val_loss: 0.0298: 0s - loss: 0.125\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.1209 - val_loss: 0.0954\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0774 - val_loss: 0.0627\n",
      " 185/1415 [==>...........................] - ETA: 5s - loss: 0.0298Epoch 8/28\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0290 - val_loss: 0.0289\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0774 - val_loss: 0.0627\n",
      " 802/1415 [================>.............] - ETA: 2s - loss: 0.0299Epoch 24/28\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0290 - val_loss: 0.0288loss: 0.029\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0519 - val_loss: 0.0434\n",
      " 254/1415 [====>.........................] - ETA: 2s - loss: 0.0289Epoch 9/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0287 - val_loss: 0.0285s - loss: 0.029084\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0527 - val_loss: 0.0441\n",
      " 652/1415 [============>.................] - ETA: 2s - loss: 0.0289Epoch 25/28\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0299 - val_loss: 0.0297 1s - loss: 0.082\n",
      " 130/1415 [=>............................] - ETA: 2s - loss: 0.0435Epoch 25/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0778 - val_loss: 0.0631TA: 1s - loss: 0.029\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0525 - val_loss: 0.0438\n",
      "Epoch 8/28\n",
      "1369/1415 [============================>.] - ETA: 0s - loss: 0.0528Epoch 9/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0526 - val_loss: 0.0440\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0289 - val_loss: 0.0288\n",
      "1075/1415 [=====================>........] - ETA: 1s - loss: 0.0288Epoch 26/28\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0297 - val_loss: 0.0296\n",
      " 107/1415 [=>............................] - ETA: 4s - loss: 0.0433Epoch 25/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0375 - val_loss: 0.0324........] - ETA: 3s - loss: 0.029\n",
      " 242/1415 [====>.........................] - ETA: 4s - loss: 0.0427Epoch 10/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0288 - val_loss: 0.0287\n",
      " 997/1415 [====================>.........] - ETA: 1s - loss: 0.0286Epoch 25/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0381 - val_loss: 0.0329...........] - ETA: 5s - loss: 0.028\n",
      " 676/1415 [=============>................] - ETA: 2s - loss: 0.0287Epoch 10/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0286 - val_loss: 0.0285............] - ETA: 2s - loss: 0.030\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0297 - val_loss: 0.0294\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0287 - val_loss: 0.0287\n",
      "Epoch 26/28\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0528 - val_loss: 0.0441- loss: 0.029\n",
      " 208/1415 [===>..........................] - ETA: 1s - loss: 0.0286Epoch 9/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0378 - val_loss: 0.0327\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0380 - val_loss: 0.0329\n",
      " 315/1415 [=====>........................] - ETA: 4s - loss: 0.0285Epoch 10/28\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0289 - val_loss: 0.0258\n",
      "1207/1415 [========================>.....] - ETA: 0s - loss: 0.0295Epoch 11/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0295 - val_loss: 0.0294- ETA: 3s - loss: 0.025\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0294 - val_loss: 0.0262\n",
      "Epoch 26/28\n",
      " 638/1415 [============>.................] - ETA: 2s - loss: 0.0412Epoch 11/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0286 - val_loss: 0.0288.....] - ETA: 3s - loss: 0.029: 5s - loss: 0.029\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0287 - val_loss: 0.0285\n",
      " 917/1415 [==================>...........] - ETA: 2s - loss: 0.0296Epoch 28/28\n",
      " 384/1415 [=======>......................] - ETA: 3s - loss: 0.0294Epoch 26/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0293 - val_loss: 0.0261>.......] - ETA: 1s - loss: 0.028ETA: 1s - loss: 0.0\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0237 - val_loss: 0.0218\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0381 - val_loss: 0.0330\n",
      "1250/1415 [=========================>....] - ETA: 0s - loss: 0.0296Epoch 11/28\n",
      "Epoch 10/28\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0284 - val_loss: 0.0283\n",
      " 417/1415 [=======>......................] - ETA: 4s - loss: 0.0286Epoch 27/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0296 - val_loss: 0.0294\n",
      "1115/1415 [======================>.......] - ETA: 1s - loss: 0.0244Epoch 27/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0291 - val_loss: 0.0259- loss: 0.032\n",
      " 140/1415 [=>............................] - ETA: 2s - loss: 0.0284Epoch 11/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0240 - val_loss: 0.0221\n",
      " 103/1415 [=>............................] - ETA: 4s - loss: 0.0257Epoch 12/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0284 - val_loss: 0.02843s - loss: 0.021\n",
      "1392/1415 [============================>.] - ETA: 0s - loss: 0.0294Epoch 1/28 4s - loss: 0.028\n",
      " 637/1415 [============>.................] - ETA: 3s - loss: 0.0284252/252 - 0s - loss: 0.0284 - 241ms/epoch - 958us/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0294 - val_loss: 0.0294\n",
      "Epoch 2/28\n",
      " 500/1415 [=========>....................] - ETA: 2s - loss: 0.0215Epoch 27/28\n",
      "  73/1415 [>.............................] - ETA: 5s - loss: 0.0293252/252 - 0s - loss: 0.0284 - 353ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0240 - val_loss: 0.0220\n",
      "1168/1415 [=======================>......] - ETA: 0s - loss: 0.0299Epoch 12/28\n",
      "Epoch 3/28\n",
      " 984/1415 [===================>..........] - ETA: 1s - loss: 0.0244252/252 - 0s - loss: 0.0283 - 290ms/epoch - 1ms/step\n",
      " 123/1415 [=>............................] - ETA: 3s - loss: 0.0220Epoch 4/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0285 - val_loss: 0.0286\n",
      "Epoch 27/28\n",
      "1172/1415 [=======================>......] - ETA: 0s - loss: 0.0283252/252 - 0s - loss: 0.0283 - 292ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0294 - val_loss: 0.0262\n",
      "Epoch 5/28\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0204 - val_loss: 0.0192\n",
      "252/252 - 0s - loss: 0.0283 - 412ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "Epoch 6/28\n",
      "1403/1415 [============================>.] - ETA: 0s - loss: 0.0207252/252 - 0s - loss: 0.0283 - 360ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0283 - val_loss: 0.0282\n",
      "Epoch 7/28\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0237 - val_loss: 0.0218\n",
      " 407/1415 [=======>......................] - ETA: 3s - loss: 0.0284Epoch 12/28\n",
      " 679/1415 [=============>................] - ETA: 3s - loss: 0.0293252/252 - 0s - loss: 0.0283 - 301ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      " 520/1415 [==========>...................] - ETA: 2s - loss: 0.0254252/252 - 0s - loss: 0.0282 - 190ms/epoch - 754us/step\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0207 - val_loss: 0.0194\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0294 - val_loss: 0.0295\n",
      " 708/1415 [==============>...............] - ETA: 3s - loss: 0.0293Epoch 9/28\n",
      "Epoch 28/28\n",
      " 386/1415 [=======>......................] - ETA: 2s - loss: 0.0189Epoch 13/28\n",
      " 112/1415 [=>............................] - ETA: 3s - loss: 0.0293252/252 - 0s - loss: 0.0282 - 321ms/epoch - 1ms/step\n",
      " 993/1415 [====================>.........] - ETA: 1s - loss: 0.0210Epoch 10/28\n",
      " 327/1415 [=====>........................] - ETA: 4s - loss: 0.0282252/252 - 0s - loss: 0.0282 - 336ms/epoch - 1ms/step\n",
      " 533/1415 [==========>...................] - ETA: 2s - loss: 0.0213Epoch 11/28\n",
      " 493/1415 [=========>....................] - ETA: 2s - loss: 0.0191252/252 - 0s - loss: 0.0282 - 313ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 955/1415 [===================>..........] - ETA: 1s - loss: 0.0247252/252 - 0s - loss: 0.0282 - 163ms/epoch - 647us/step\n",
      " 656/1415 [============>.................] - ETA: 2s - loss: 0.0211Epoch 13/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0206 - val_loss: 0.0193- loss: 0.021\n",
      "252/252 - 0s - loss: 0.0282 - 402ms/epoch - 2ms/step\n",
      "1114/1415 [======================>.......] - ETA: 1s - loss: 0.0245Epoch 13/28\n",
      "Epoch 14/28\n",
      " 631/1415 [============>.................] - ETA: 3s - loss: 0.0293252/252 - 0s - loss: 0.0282 - 362ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0183 - val_loss: 0.0174\n",
      "Epoch 15/28\n",
      "Epoch 14/28\n",
      " 261/1415 [====>.........................] - ETA: 3s - loss: 0.0191252/252 - 0s - loss: 0.0282 - 285ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1091/1415 [======================>.......] - ETA: 0s - loss: 0.0187252/252 - 0s - loss: 0.0281 - 192ms/epoch - 763us/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0292 - val_loss: 0.0292\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0240 - val_loss: 0.0220\n",
      "1401/1415 [============================>.] - ETA: 0s - loss: 0.0284Epoch 28/28\n",
      "Epoch 12/28\n",
      "  48/1415 [>.............................] - ETA: 5s - loss: 0.0292252/252 - 0s - loss: 0.0281 - 295ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 115/1415 [=>............................] - ETA: 3s - loss: 0.0292252/252 - 0s - loss: 0.0281 - 275ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0204 - val_loss: 0.0192\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0284 - val_loss: 0.0285\n",
      " 978/1415 [===================>..........] - ETA: 1s - loss: 0.0293Epoch 19/28\n",
      "Epoch 28/28\n",
      "1097/1415 [======================>.......] - ETA: 1s - loss: 0.0282Epoch 13/28\n",
      "  67/1415 [>.............................] - ETA: 5s - loss: 0.0283252/252 - 0s - loss: 0.0281 - 332ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "  98/1415 [=>............................] - ETA: 4s - loss: 0.0283252/252 - 0s - loss: 0.0281 - 164ms/epoch - 651us/step\n",
      " 744/1415 [==============>...............] - ETA: 1s - loss: 0.0188Epoch 21/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0185 - val_loss: 0.0176\n",
      " 272/1415 [====>.........................] - ETA: 3s - loss: 0.0292Epoch 14/28\n",
      " 416/1415 [=======>......................] - ETA: 2s - loss: 0.0189252/252 - 0s - loss: 0.0280 - 317ms/epoch - 1ms/step\n",
      " 266/1415 [====>.........................] - ETA: 3s - loss: 0.0283Epoch 22/28\n",
      " 997/1415 [====================>.........] - ETA: 1s - loss: 0.0187252/252 - 0s - loss: 0.0280 - 325ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1041/1415 [=====================>........] - ETA: 1s - loss: 0.0187252/252 - 0s - loss: 0.0280 - 163ms/epoch - 648us/step\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0282 - val_loss: 0.0282\n",
      "Epoch 24/28\n",
      " 487/1415 [=========>....................] - ETA: 3s - loss: 0.0174252/252 - 1s - loss: 0.0280 - 509ms/epoch - 2ms/step16\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0293 - val_loss: 0.0292\n",
      " 634/1415 [============>.................] - ETA: 3s - loss: 0.0283252/252 - 0s - loss: 0.0280 - 319ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 723/1415 [==============>...............] - ETA: 2s - loss: 0.0283Epoch 1/28...] - ETA: 1s - loss: 0.017\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0184 - val_loss: 0.0175\n",
      " 787/1415 [===============>..............] - ETA: 2s - loss: 0.0283252/252 - 0s - loss: 0.0280 - 258ms/epoch - 1ms/step\n",
      " 922/1415 [==================>...........] - ETA: 1s - loss: 0.0291252/252 - 0s - loss: 0.0281 - 233ms/epoch - 925us/step\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0168 - val_loss: 0.0162\n",
      " 858/1415 [=================>............] - ETA: 1s - loss: 0.0173Epoch 27/28\n",
      "Epoch 14/28\n",
      "1125/1415 [======================>.......] - ETA: 0s - loss: 0.0209Epoch 15/28\n",
      "Epoch 2/28\n",
      "1277/1415 [==========================>...] - ETA: 0s - loss: 0.0184252/252 - 0s - loss: 0.0280 - 296ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 154/1415 [==>...........................] - ETA: 2s - loss: 0.0175Epoch 1/28\n",
      "252/252 - 0s - loss: 0.0280 - 208ms/epoch - 826us/step\n",
      "252/252 - 0s - loss: 0.0281 - 428ms/epoch - 2ms/step\n",
      " 226/1415 [===>..........................] - ETA: 2s - loss: 0.0175Epoch 3/28\n",
      "1333/1415 [===========================>..] - ETA: 0s - loss: 0.017163/63 - 0s - loss: 0.0279 - 112ms/epoch - 2ms/step\n",
      "1383/1415 [============================>.] - ETA: 0s - loss: 0.0170252/252 - 0s - loss: 0.0292 - 364ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      " 307/1415 [=====>........................] - ETA: 3s - loss: 0.0161252/252 - 0s - loss: 0.0279 - 158ms/epoch - 626us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1768a6130>; total time= 2.1min\n",
      "1209/1415 [========================>.....] - ETA: 0s - loss: 0.0283252/252 - 0s - loss: 0.0281 - 476ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0291 - 325ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0207 - val_loss: 0.0194\n",
      "1248/1415 [=========================>....] - ETA: 0s - loss: 0.0283Epoch 13/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0170 - val_loss: 0.0164\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0183 - val_loss: 0.0174\n",
      "Epoch 14/28\n",
      " 114/1415 [=>............................] - ETA: 1s - loss: 0.0174252/252 - 0s - loss: 0.0281 - 416ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0291 - val_loss: 0.0291\n",
      " 794/1415 [===============>..............] - ETA: 1s - loss: 0.0172252/252 - 0s - loss: 0.0291 - 440ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0283 - val_loss: 0.0283\n",
      " 358/1415 [======>.......................] - ETA: 1s - loss: 0.0163252/252 - 0s - loss: 0.0281 - 433ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      " 432/1415 [========>.....................] - ETA: 1s - loss: 0.0191252/252 - 0s - loss: 0.0291 - 423ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      " 657/1415 [============>.................] - ETA: 1s - loss: 0.0162Epoch 1/28\n",
      " 723/1415 [==============>...............] - ETA: 1s - loss: 0.0162252/252 - 0s - loss: 0.0290 - 343ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0280 - 398ms/epoch - 2ms/step\n",
      " 773/1415 [===============>..............] - ETA: 0s - loss: 0.0171Epoch 6/28\n",
      "Epoch 7/28\n",
      "1044/1415 [=====================>........] - ETA: 0s - loss: 0.0170252/252 - 0s - loss: 0.0291 - 497ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 974/1415 [===================>..........] - ETA: 0s - loss: 0.0187252/252 - 0s - loss: 0.0280 - 462ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1013/1415 [====================>.........] - ETA: 0s - loss: 0.0187252/252 - 1s - loss: 0.0286 - 511ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1186/1415 [========================>.....] - ETA: 0s - loss: 0.0169Epoch 1/28] - ETA: 0s - loss: 0.018\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0158 - val_loss: 0.0154\n",
      "Epoch 16/28\n",
      "1219/1415 [========================>.....] - ETA: 0s - loss: 0.0160252/252 - 0s - loss: 0.0280 - 364ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0169 - val_loss: 0.0163\n",
      "Epoch 15/28\n",
      "1260/1415 [=========================>....] - ETA: 0s - loss: 0.0160252/252 - 0s - loss: 0.0290 - 440ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "1323/1415 [===========================>..] - ETA: 0s - loss: 0.0160252/252 - 0s - loss: 0.0284 - 467ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "  98/1415 [=>............................] - ETA: 2s - loss: 0.0163252/252 - 0s - loss: 0.0282 - 351ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      " 224/1415 [===>..........................] - ETA: 1s - loss: 0.0162252/252 - 0s - loss: 0.0290 - 336ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0280 - 416ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 339/1415 [======>.......................] - ETA: 1s - loss: 0.0153252/252 - 0s - loss: 0.0283 - 365ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0282 - 344ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0168 - val_loss: 0.0162.........] - ETA: 1s - loss: 0.016\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0160 - val_loss: 0.0155\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0184 - val_loss: 0.0176\n",
      "Epoch 14/28\n",
      "  60/1415 [>.............................] - ETA: 1s - loss: 0.0175252/252 - 0s - loss: 0.0290 - 473ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "252/252 - 0s - loss: 0.0280 - 473ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      " 667/1415 [=============>................] - ETA: 1s - loss: 0.0152252/252 - 0s - loss: 0.0283 - 486ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 223/1415 [===>..........................] - ETA: 1s - loss: 0.0175252/252 - 1s - loss: 0.0282 - 562ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      " 407/1415 [=======>......................] - ETA: 1s - loss: 0.0161252/252 - 0s - loss: 0.0290 - 406ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      " 846/1415 [================>.............] - ETA: 0s - loss: 0.0152252/252 - 0s - loss: 0.0280 - 423ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      " 864/1415 [=================>............] - ETA: 0s - loss: 0.0152252/252 - 0s - loss: 0.0282 - 282ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 483/1415 [=========>....................] - ETA: 1s - loss: 0.0161252/252 - 0s - loss: 0.0283 - 404ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "1022/1415 [====================>.........] - ETA: 0s - loss: 0.0159252/252 - 1s - loss: 0.0290 - 519ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      " 616/1415 [============>.................] - ETA: 1s - loss: 0.0173252/252 - 0s - loss: 0.0280 - 490ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 687/1415 [=============>................] - ETA: 1s - loss: 0.0173252/252 - 1s - loss: 0.0282 - 541ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      " 740/1415 [==============>...............] - ETA: 1s - loss: 0.0172252/252 - 1s - loss: 0.0282 - 568ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      " 977/1415 [===================>..........] - ETA: 0s - loss: 0.0153252/252 - 0s - loss: 0.0280 - 433ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      " 974/1415 [===================>..........] - ETA: 0s - loss: 0.0171252/252 - 0s - loss: 0.0290 - 472ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1017/1415 [====================>.........] - ETA: 0s - loss: 0.0152252/252 - 0s - loss: 0.0281 - 427ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1387/1415 [============================>.] - ETA: 0s - loss: 0.0158252/252 - 0s - loss: 0.0282 - 392ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0150 - val_loss: 0.0147\n",
      "Epoch 17/28\n",
      "1213/1415 [========================>.....] - ETA: 0s - loss: 0.0152252/252 - 0s - loss: 0.0279 - 468ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1291/1415 [==========================>...] - ETA: 0s - loss: 0.0158252/252 - 0s - loss: 0.0290 - 442ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "  89/1415 [>.............................] - ETA: 1s - loss: 0.0146252/252 - 0s - loss: 0.0281 - 392ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1296/1415 [==========================>...] - ETA: 0s - loss: 0.0152252/252 - 0s - loss: 0.0282 - 427ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0158 - val_loss: 0.0153\n",
      "Epoch 16/28\n",
      " 276/1415 [====>.........................] - ETA: 2s - loss: 0.0146252/252 - 1s - loss: 0.0279 - 514ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      " 144/1415 [==>...........................] - ETA: 3s - loss: 0.0154252/252 - 1s - loss: 0.0289 - 594ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 406/1415 [=======>......................] - ETA: 1s - loss: 0.0146252/252 - 1s - loss: 0.0281 - 581ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      " 434/1415 [========>.....................] - ETA: 1s - loss: 0.0146252/252 - 1s - loss: 0.0282 - 585ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0169 - val_loss: 0.0163\n",
      "Epoch 15/28\n",
      " 521/1415 [==========>...................] - ETA: 1s - loss: 0.0146252/252 - 0s - loss: 0.0279 - 358ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0158 - val_loss: 0.0153\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0152 - val_loss: 0.0148\n",
      "Epoch 17/28\n",
      "  66/1415 [>.............................] - ETA: 2s - loss: 0.0154252/252 - 0s - loss: 0.0289 - 404ms/epoch - 2ms/step\n",
      "  68/1415 [>.............................] - ETA: 3s - loss: 0.0163Epoch 11/28\n",
      " 371/1415 [======>.......................] - ETA: 2s - loss: 0.0153252/252 - 0s - loss: 0.0281 - 409ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 415/1415 [=======>......................] - ETA: 2s - loss: 0.0153252/252 - 0s - loss: 0.0282 - 415ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      " 764/1415 [===============>..............] - ETA: 1s - loss: 0.0146252/252 - 0s - loss: 0.0279 - 401ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      " 332/1415 [======>.......................] - ETA: 1s - loss: 0.0163252/252 - 0s - loss: 0.0289 - 359ms/epoch - 1ms/step\n",
      " 255/1415 [====>.........................] - ETA: 1s - loss: 0.0148Epoch 12/28\n",
      " 868/1415 [=================>............] - ETA: 0s - loss: 0.0145252/252 - 0s - loss: 0.0281 - 421ms/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      " 414/1415 [=======>......................] - ETA: 1s - loss: 0.0162252/252 - 0s - loss: 0.0282 - 424ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      " 478/1415 [=========>....................] - ETA: 1s - loss: 0.0153252/252 - 0s - loss: 0.0279 - 387ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "1096/1415 [======================>.......] - ETA: 0s - loss: 0.0145252/252 - 0s - loss: 0.0289 - 426ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1140/1415 [=======================>......] - ETA: 0s - loss: 0.0145252/252 - 0s - loss: 0.0281 - 390ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      " 663/1415 [=============>................] - ETA: 1s - loss: 0.0152252/252 - 0s - loss: 0.0282 - 403ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      " 728/1415 [==============>...............] - ETA: 1s - loss: 0.0147252/252 - 0s - loss: 0.0279 - 448ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 809/1415 [================>.............] - ETA: 1s - loss: 0.0161Epoch 1/28\n",
      "1360/1415 [===========================>..] - ETA: 0s - loss: 0.0145252/252 - 0s - loss: 0.0289 - 449ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0281 - 412ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 940/1415 [==================>...........] - ETA: 0s - loss: 0.0152252/252 - 0s - loss: 0.0281 - 411ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "1013/1415 [====================>.........] - ETA: 0s - loss: 0.0151252/252 - 0s - loss: 0.0279 - 319ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1049/1415 [=====================>........] - ETA: 0s - loss: 0.0146252/252 - 0s - loss: 0.0288 - 450ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1346/1415 [===========================>..] - ETA: 0s - loss: 0.0150252/252 - 0s - loss: 0.0281 - 485ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0144 - val_loss: 0.0142\n",
      "Epoch 18/28\n",
      "1224/1415 [========================>.....] - ETA: 0s - loss: 0.0151252/252 - 1s - loss: 0.0282 - 524ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 397/1415 [=======>......................] - ETA: 1s - loss: 0.4732252/252 - 1s - loss: 0.0278 - 503ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "1300/1415 [==========================>...] - ETA: 0s - loss: 0.0146252/252 - 0s - loss: 0.0288 - 400ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "1343/1415 [===========================>..] - ETA: 0s - loss: 0.0159252/252 - 0s - loss: 0.0280 - 436ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 301/1415 [=====>........................] - ETA: 1s - loss: 0.0141252/252 - 0s - loss: 0.0281 - 450ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 17/28\n",
      "  19/1415 [..............................] - ETA: 4s - loss: 0.0148252/252 - 0s - loss: 0.0278 - 485ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0150 - val_loss: 0.0147\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0145 - val_loss: 0.0142\n",
      "Epoch 18/28\n",
      "  54/1415 [>.............................] - ETA: 1s - loss: 0.0146252/252 - 0s - loss: 0.0280 - 479ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      " 117/1415 [=>............................] - ETA: 1s - loss: 0.0146252/252 - 1s - loss: 0.0288 - 595ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "  43/1415 [..............................] - ETA: 8s - loss: 0.0142252/252 - 1s - loss: 0.0281 - 578ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0159 - val_loss: 0.0154\n",
      "Epoch 16/28\n",
      " 292/1415 [=====>........................] - ETA: 1s - loss: 0.0146252/252 - 1s - loss: 0.0278 - 602ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      " 797/1415 [===============>..............] - ETA: 1s - loss: 0.0140252/252 - 1s - loss: 0.0280 - 561ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      " 832/1415 [================>.............] - ETA: 0s - loss: 0.0140252/252 - 1s - loss: 0.0288 - 564ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      " 478/1415 [=========>....................] - ETA: 1s - loss: 0.0146252/252 - 0s - loss: 0.0281 - 449ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 533/1415 [==========>...................] - ETA: 1s - loss: 0.0146252/252 - 0s - loss: 0.0278 - 410ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      " 539/1415 [==========>...................] - ETA: 1s - loss: 0.0153252/252 - 0s - loss: 0.0281 - 359ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 517/1415 [=========>....................] - ETA: 1s - loss: 0.0142252/252 - 0s - loss: 0.0280 - 497ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "1104/1415 [======================>.......] - ETA: 0s - loss: 0.0140252/252 - 0s - loss: 0.0288 - 464ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      " 556/1415 [==========>...................] - ETA: 1s - loss: 0.0142252/252 - 0s - loss: 0.0278 - 387ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      " 820/1415 [================>.............] - ETA: 0s - loss: 0.0152252/252 - 0s - loss: 0.0280 - 445ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0280 - 459ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "1405/1415 [============================>.] - ETA: 0s - loss: 0.0140252/252 - 0s - loss: 0.0288 - 469ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "1076/1415 [=====================>........] - ETA: 0s - loss: 0.0144252/252 - 0s - loss: 0.0278 - 449ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1217/1415 [========================>.....] - ETA: 0s - loss: 0.0144252/252 - 0s - loss: 0.0280 - 371ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1186/1415 [========================>.....] - ETA: 0s - loss: 0.0145252/252 - 0s - loss: 0.0280 - 395ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "1297/1415 [==========================>...] - ETA: 0s - loss: 0.0144252/252 - 0s - loss: 0.0288 - 482ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0278 - 409ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0140 - val_loss: 0.013715\n",
      "Epoch 19/28\n",
      " 115/1415 [=>............................] - ETA: 2s - loss: 0.0137252/252 - 0s - loss: 0.0280 - 404ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      " 158/1415 [==>...........................] - ETA: 2s - loss: 0.0137252/252 - 0s - loss: 0.0280 - 416ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1286/1415 [==========================>...] - ETA: 0s - loss: 0.0140252/252 - 0s - loss: 0.0288 - 388ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 218/1415 [===>..........................] - ETA: 1s - loss: 0.0137252/252 - 0s - loss: 0.0277 - 418ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0144 - val_loss: 0.0141\n",
      "Epoch 18/28\n",
      " 332/1415 [======>.......................] - ETA: 1s - loss: 0.0137252/252 - 0s - loss: 0.0279 - 386ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 375/1415 [======>.......................] - ETA: 1s - loss: 0.0137252/252 - 0s - loss: 0.0280 - 369ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 419/1415 [=======>......................] - ETA: 1s - loss: 0.013663/63 - 0s - loss: 0.0277 - 302ms/epoch - 5ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0144 - val_loss: 0.0141\n",
      "Epoch 18/28\n",
      " 500/1415 [=========>....................] - ETA: 1s - loss: 0.0136252/252 - 0s - loss: 0.0288 - 420ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0151 - val_loss: 0.0147\n",
      " 573/1415 [===========>..................] - ETA: 1s - loss: 0.0136Epoch 17/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.1505 - val_loss: 0.0158\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0140 - val_loss: 0.0138\n",
      "Epoch 19/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0135252/252 - 0s - loss: 0.0279 - 451ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "  19/1415 [..............................] - ETA: 3s - loss: 0.0137252/252 - 0s - loss: 0.0277 - 367ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0280 - 464ms/epoch - 2ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x13a743130>; total time= 2.6min\n",
      " 188/1415 [==>...........................] - ETA: 1s - loss: 0.0137252/252 - 1s - loss: 0.0287 - 520ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      " 258/1415 [====>.........................] - ETA: 1s - loss: 0.0137252/252 - 0s - loss: 0.0279 - 360ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 840/1415 [================>.............] - ETA: 0s - loss: 0.0139252/252 - 1s - loss: 0.0287 - 518ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      " 608/1415 [===========>..................] - ETA: 1s - loss: 0.0146252/252 - 0s - loss: 0.0279 - 469ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "1122/1415 [======================>.......] - ETA: 0s - loss: 0.0139252/252 - 0s - loss: 0.0287 - 405ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "1095/1415 [======================>.......] - ETA: 0s - loss: 0.0140252/252 - 0s - loss: 0.0279 - 407ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "1152/1415 [=======================>......] - ETA: 0s - loss: 0.0145252/252 - 0s - loss: 0.0287 - 345ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0135 - val_loss: 0.0133\n",
      "Epoch 20/28\n",
      "1251/1415 [=========================>....] - ETA: 0s - loss: 0.0145252/252 - 0s - loss: 0.0279 - 359ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1409/1415 [============================>.] - ETA: 0s - loss: 0.0145252/252 - 0s - loss: 0.0287 - 349ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0139 - val_loss: 0.0136\n",
      "Epoch 19/28\n",
      " 362/1415 [======>.......................] - ETA: 1s - loss: 0.0133252/252 - 0s - loss: 0.0279 - 365ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0139 - val_loss: 0.0137\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0128 - val_loss: 0.0113\n",
      "Epoch 3/28\n",
      " 546/1415 [==========>...................] - ETA: 1s - loss: 0.0133252/252 - 0s - loss: 0.0287 - 340ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0144 - val_loss: 0.0142\n",
      "Epoch 18/28\n",
      "   1/1415 [..............................] - ETA: 0s - loss: 0.0139252/252 - 0s - loss: 0.0279 - 311ms/epoch - 1ms/step\n",
      " 181/1415 [==>...........................] - ETA: 1s - loss: 0.014263/63 - 0s - loss: 0.0285 - 192ms/epoch - 3ms/step\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0136 - val_loss: 0.0134\n",
      "Epoch 20/28\n",
      " 379/1415 [=======>......................] - ETA: 1s - loss: 0.010463/63 - 0s - loss: 0.0278 - 183ms/epoch - 3ms/step\n",
      " 630/1415 [============>.................] - ETA: 0s - loss: 0.0136252/252 - 0s - loss: 0.0285 - 260ms/epoch - 1ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16dc83130>; total time= 2.6min\n",
      " 485/1415 [=========>....................] - ETA: 0s - loss: 0.0141252/252 - 0s - loss: 0.0278 - 250ms/epoch - 992us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16a103130>; total time= 2.6min\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0132 - val_loss: 0.0130=============>.] - ETA: 0s - loss: 0.013\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0091 - val_loss: 0.0075\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0135 - val_loss: 0.0133\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0139 - val_loss: 0.0137\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0128 - val_loss: 0.0127: 0s - loss: 0.013\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0071 - val_loss: 0.0061\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0135 - val_loss: 0.0133\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0129 - val_loss: 0.0127\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0126 - val_loss: 0.0124......] - ETA: 0s - loss: 0.012TA: 0s - loss: 0.01\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 6/28\n",
      " 174/1415 [==>...........................] - ETA: 1s - loss: 0.005263/63 - 0s - loss: 0.0281 - 109ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0128 - val_loss: 0.0127\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0127 - val_loss: 0.0126\n",
      "Epoch 22/28\n",
      " 379/1415 [=======>......................] - ETA: 1s - loss: 0.0124252/252 - 0s - loss: 0.0282 - 263ms/epoch - 1ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1547c3130>; total time= 2.8min\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0126 - val_loss: 0.0124\n",
      "Epoch 23/28\n",
      " 925/1415 [==================>...........] - ETA: 0s - loss: 0.0052Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0123 - val_loss: 0.0121\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0125 - val_loss: 0.0124\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0125 - val_loss: 0.0123\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0128 - val_loss: 0.0127\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0123 - val_loss: 0.0122.229\n",
      "Epoch 24/28\n",
      "1365/1415 [===========================>..] - ETA: 0s - loss: 0.0046Epoch 1/28==========================>..] - ETA: 0s - loss: 0.004\n",
      " 945/1415 [===================>..........] - ETA: 0s - loss: 0.0122Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0120 - val_loss: 0.0119..................] - ETA: 2s - loss: 1.047\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0046 - val_loss: 0.0043\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0123 - val_loss: 0.0121\n",
      "Epoch 8/28\n",
      "Epoch 24/28\n",
      " 157/1415 [==>...........................] - ETA: 2s - loss: 1.0381Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0122 - val_loss: 0.0120\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0125 - val_loss: 0.0124\n",
      "Epoch 24/28\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.1527 - val_loss: 0.0168\n",
      " 495/1415 [=========>....................] - ETA: 3s - loss: 0.4001Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0121 - val_loss: 0.0119] - ETA: 2s - loss: 0.012\n",
      " 129/1415 [=>............................] - ETA: 3s - loss: 0.0162Epoch 25/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0042 - val_loss: 0.0039.] - ETA: 1s - loss: 0.012s - loss: 0.012\n",
      "1186/1415 [========================>.....] - ETA: 0s - loss: 0.1809Epoch 9/28\n",
      " 229/1415 [===>..........................] - ETA: 1s - loss: 0.0039Epoch 1/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0120 - val_loss: 0.0119\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 25/28\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0122 - val_loss: 0.0121\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 24/28\n",
      " 252/1415 [====>.........................] - ETA: 1s - loss: 0.0117Epoch 25/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      " 916/1415 [==================>...........] - ETA: 1s - loss: 0.0039Epoch 26/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.1545 - val_loss: 0.0159\n",
      " 183/1415 [==>...........................] - ETA: 2s - loss: 0.0117Epoch 2/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.1541 - val_loss: 0.0144\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0132 - val_loss: 0.0117ETA: 0s - loss: 0.013 loss: 0.015\n",
      " 799/1415 [===============>..............] - ETA: 2s - loss: 0.2613Epoch 3/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      " 939/1415 [==================>...........] - ETA: 2s - loss: 0.2258Epoch 26/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0116 - val_loss: 0.0115\n",
      "1191/1415 [========================>.....] - ETA: 0s - loss: 0.0117Epoch 27/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0120 - val_loss: 0.0119.........] - ETA: 2s - loss: 0.011\n",
      " 939/1415 [==================>...........] - ETA: 1s - loss: 0.0116Epoch 25/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      "1167/1415 [=======================>......] - ETA: 0s - loss: 0.0116Epoch 26/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0116 - val_loss: 0.0115............] - ETA: 4s - loss: 0.010] - ETA: 3s - loss: 0.011\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.1561 - val_loss: 0.0158=>.......] - ETA: 1s - loss: 0.003 - ETA: 0s - loss: 0.011\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "1257/1415 [=========================>....] - ETA: 0s - loss: 0.0119Epoch 2/28\n",
      "1004/1415 [====================>.........] - ETA: 1s - loss: 0.0118Epoch 11/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0116 - val_loss: 0.0115..............] - ETA: 5s - loss: 0.015\n",
      "1116/1415 [======================>.......] - ETA: 0s - loss: 0.0115Epoch 27/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0124 - val_loss: 0.0104\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0114 - val_loss: 0.0113\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0116 - val_loss: 0.0090\n",
      " 204/1415 [===>..........................] - ETA: 5s - loss: 0.0150Epoch 3/28\n",
      "1285/1415 [==========================>...] - ETA: 0s - loss: 0.0097Epoch 28/28\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0095 - val_loss: 0.0080\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0118 - val_loss: 0.0116\n",
      "  97/1415 [=>............................] - ETA: 3s - loss: 0.0090Epoch 4/28\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0115 - val_loss: 0.0114ETA: 2s - loss: 0.011\n",
      " 230/1415 [===>..........................] - ETA: 4s - loss: 0.0096Epoch 27/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0114 - val_loss: 0.0113\n",
      " 321/1415 [=====>........................] - ETA: 5s - loss: 0.0096Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0034 - val_loss: 0.0033 - ETA: 1s - loss: 0.003s - loss: 0.0\n",
      " 735/1415 [==============>...............] - ETA: 2s - loss: 0.0116Epoch 12/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0114 - val_loss: 0.0113.] - ETA: 3s - loss: 0.009 - loss: 0.011\n",
      " 849/1415 [=================>............] - ETA: 3s - loss: 0.0088Epoch 28/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0112 - val_loss: 0.0111\n",
      "1103/1415 [======================>.......] - ETA: 1s - loss: 0.0089Epoch 1/28\n",
      "1229/1415 [=========================>....] - ETA: 1s - loss: 0.0088252/252 - 0s - loss: 0.0111 - 292ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0123 - val_loss: 0.0111\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 3/28\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 27/28\n",
      "Epoch 2/28\n",
      "1321/1415 [===========================>..] - ETA: 0s - loss: 0.0083252/252 - 0s - loss: 0.0111 - 342ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "  68/1415 [>.............................] - ETA: 5s - loss: 0.0098252/252 - 0s - loss: 0.0111 - 101ms/epoch - 400us/step\n",
      "1248/1415 [=========================>....] - ETA: 0s - loss: 0.0076Epoch 4/28\n",
      " 328/1415 [=====>........................] - ETA: 3s - loss: 0.0112252/252 - 0s - loss: 0.0110 - 360ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0083 - val_loss: 0.0081\n",
      "252/252 - 0s - loss: 0.0110 - 136ms/epoch - 539us/step\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0112 - val_loss: 0.0111\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0076 - val_loss: 0.0070\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0087 - val_loss: 0.0075\n",
      "1308/1415 [==========================>...] - ETA: 0s - loss: 0.0032Epoch 5/28\n",
      "Epoch 4/28\n",
      "Epoch 6/28\n",
      "Epoch 4/28\n",
      " 120/1415 [=>............................] - ETA: 3s - loss: 0.0067252/252 - 0s - loss: 0.0110 - 383ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0032 - val_loss: 0.0029\n",
      " 155/1415 [==>...........................] - ETA: 2s - loss: 0.0067Epoch 13/28\n",
      "252/252 - 0s - loss: 0.0109 - 111ms/epoch - 441us/step\n",
      " 648/1415 [============>.................] - ETA: 1s - loss: 0.0112Epoch 8/28\n",
      " 194/1415 [===>..........................] - ETA: 4s - loss: 0.0067Epoch 1/28\n",
      " 515/1415 [=========>....................] - ETA: 4s - loss: 0.0096252/252 - 0s - loss: 0.0111 - 362ms/epoch - 1ms/step\n",
      "1399/1415 [============================>.] - ETA: 0s - loss: 0.0112252/252 - 0s - loss: 0.0109 - 428ms/epoch - 2ms/step\n",
      " 194/1415 [===>..........................] - ETA: 2s - loss: 0.0030Epoch 9/28\n",
      "Epoch 2/28\n",
      " 372/1415 [======>.......................] - ETA: 4s - loss: 0.0072252/252 - 0s - loss: 0.0111 - 404ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0112 - val_loss: 0.0111\n",
      " 998/1415 [====================>.........] - ETA: 1s - loss: 0.0111252/252 - 0s - loss: 0.0109 - 393ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "Epoch 10/28\n",
      " 568/1415 [===========>..................] - ETA: 3s - loss: 0.0069252/252 - 0s - loss: 0.0109 - 321ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1057/1415 [=====================>........] - ETA: 1s - loss: 0.0114252/252 - 0s - loss: 0.0110 - 355ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0108 - 127ms/epoch - 503us/step\n",
      "Epoch 12/28\n",
      "1225/1415 [========================>.....] - ETA: 0s - loss: 0.0111252/252 - 0s - loss: 0.0110 - 122ms/epoch - 485us/step\n",
      "1114/1415 [======================>.......] - ETA: 1s - loss: 0.0114252/252 - 0s - loss: 0.0108 - 108ms/epoch - 427us/step\n",
      "Epoch 5/28\n",
      "Epoch 13/28\n",
      "1192/1415 [========================>.....] - ETA: 0s - loss: 0.0114Epoch 1/28\n",
      " 778/1415 [===============>..............] - ETA: 2s - loss: 0.0068252/252 - 0s - loss: 0.0110 - 404ms/epoch - 2ms/step\n",
      "1027/1415 [====================>.........] - ETA: 1s - loss: 0.0091252/252 - 0s - loss: 0.0108 - 395ms/epoch - 2ms/step\n",
      "1413/1415 [============================>.] - ETA: 0s - loss: 0.0111252/252 - 0s - loss: 0.0111 - 267ms/epoch - 1ms/step\n",
      " 768/1415 [===============>..............] - ETA: 2s - loss: 0.0067Epoch 14/28\n",
      "Epoch 6/28\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0111 - val_loss: 0.0110ETA: 1s - loss: 0.007\n",
      "252/252 - 0s - loss: 0.0107 - 385ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1002/1415 [====================>.........] - ETA: 1s - loss: 0.0067252/252 - 0s - loss: 0.0109 - 453ms/epoch - 2ms/step\n",
      "252/252 - 0s - loss: 0.0107 - 150ms/epoch - 596us/step\n",
      "1309/1415 [==========================>...] - ETA: 0s - loss: 0.0030252/252 - 0s - loss: 0.0111 - 461ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0114 - val_loss: 0.0112\n",
      "1012/1415 [====================>.........] - ETA: 1s - loss: 0.0068Epoch 16/28\n",
      "Epoch 7/28\n",
      "Epoch 3/28\n",
      "Epoch 28/28\n",
      " 151/1415 [==>...........................] - ETA: 2s - loss: 0.0112252/252 - 0s - loss: 0.0110 - 354ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0109 - 360ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1172/1415 [=======================>......] - ETA: 0s - loss: 0.0066252/252 - 0s - loss: 0.0107 - 413ms/epoch - 2ms/step\n",
      "1204/1415 [========================>.....] - ETA: 0s - loss: 0.0066Epoch 17/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 14/28\n",
      "1259/1415 [=========================>....] - ETA: 0s - loss: 0.0072Epoch 1/28\n",
      "  45/1415 [..............................] - ETA: 1s - loss: 0.0029252/252 - 0s - loss: 0.0109 - 210ms/epoch - 833us/step\n",
      "1302/1415 [==========================>...] - ETA: 0s - loss: 0.0072252/252 - 0s - loss: 0.0107 - 181ms/epoch - 718us/step\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0088 - val_loss: 0.0085\n",
      "252/252 - 0s - loss: 0.0110 - 184ms/epoch - 730us/step\n",
      "Epoch 18/28\n",
      "Epoch 9/28\n",
      "Epoch 5/28\n",
      "Epoch 4/28\n",
      "   1/1415 [..............................] - ETA: 4s - loss: 0.0085252/252 - 0s - loss: 0.0110 - 126ms/epoch - 502us/step\n",
      "Epoch 2/28\n",
      "1380/1415 [============================>.] - ETA: 0s - loss: 0.0066252/252 - 0s - loss: 0.0109 - 455ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 125/1415 [=>............................] - ETA: 4s - loss: 0.0079252/252 - 0s - loss: 0.0110 - 457ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      " 528/1415 [==========>...................] - ETA: 1s - loss: 0.0112252/252 - 1s - loss: 0.0106 - 500ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "1414/1415 [============================>.] - ETA: 0s - loss: 0.0066252/252 - 0s - loss: 0.0108 - 188ms/epoch - 746us/step\n",
      " 570/1415 [===========>..................] - ETA: 1s - loss: 0.0112252/252 - 1s - loss: 0.0110 - 535ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0071 - val_loss: 0.0064\n",
      "252/252 - 0s - loss: 0.0106 - 151ms/epoch - 601us/step\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0066 - val_loss: 0.0058\n",
      "252/252 - 0s - loss: 0.0109 - 195ms/epoch - 775us/step\n",
      "Epoch 20/28\n",
      "Epoch 3/28\n",
      "Epoch 11/28\n",
      "Epoch 5/28\n",
      "Epoch 5/28\n",
      "   1/1415 [..............................] - ETA: 22s - loss: 0.0057Epoch 7/28\n",
      " 344/1415 [======>.......................] - ETA: 3s - loss: 0.0078252/252 - 0s - loss: 0.0109 - 376ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 369/1415 [======>.......................] - ETA: 3s - loss: 0.0077252/252 - 0s - loss: 0.0108 - 413ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0066 - val_loss: 0.0059\n",
      "Epoch 12/28\n",
      "Epoch 6/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0063252/252 - 0s - loss: 0.0106 - 421ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      " 693/1415 [=============>................] - ETA: 1s - loss: 0.0029252/252 - 0s - loss: 0.0109 - 454ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      " 919/1415 [==================>...........] - ETA: 1s - loss: 0.0112252/252 - 0s - loss: 0.0106 - 155ms/epoch - 615us/step\n",
      "252/252 - 0s - loss: 0.0109 - 209ms/epoch - 831us/step\n",
      "252/252 - 0s - loss: 0.0108 - 187ms/epoch - 741us/step\n",
      " 227/1415 [===>..........................] - ETA: 2s - loss: 0.0066252/252 - 0s - loss: 0.0109 - 132ms/epoch - 524us/step\n",
      "Epoch 5/28\n",
      "Epoch 22/28\n",
      "Epoch 9/28\n",
      "Epoch 13/28\n",
      " 361/1415 [======>.......................] - ETA: 2s - loss: 0.0059252/252 - 0s - loss: 0.0105 - 338ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 391/1415 [=======>......................] - ETA: 2s - loss: 0.0059252/252 - 0s - loss: 0.0109 - 375ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0107 - 381ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0109 - 397ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "1221/1415 [========================>.....] - ETA: 0s - loss: 0.0112252/252 - 0s - loss: 0.0105 - 188ms/epoch - 748us/step\n",
      "Epoch 24/28\n",
      " 247/1415 [====>.........................] - ETA: 3s - loss: 0.0061252/252 - 0s - loss: 0.0108 - 162ms/epoch - 643us/step\n",
      "1298/1415 [==========================>...] - ETA: 0s - loss: 0.0112252/252 - 0s - loss: 0.0109 - 149ms/epoch - 591us/step\n",
      "252/252 - 0s - loss: 0.0105 - 104ms/epoch - 411us/step\n",
      " 418/1415 [=======>......................] - ETA: 2s - loss: 0.0064252/252 - 0s - loss: 0.0107 - 156ms/epoch - 617us/step\n",
      " 424/1415 [=======>......................] - ETA: 2s - loss: 0.0059Epoch 25/28\n",
      "Epoch 7/28\n",
      "Epoch 15/28\n",
      "1030/1415 [====================>.........] - ETA: 0s - loss: 0.0029Epoch 11/28\n",
      " 353/1415 [======>.......................] - ETA: 3s - loss: 0.0060252/252 - 0s - loss: 0.0108 - 304ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 416/1415 [=======>......................] - ETA: 3s - loss: 0.0060252/252 - 0s - loss: 0.0108 - 412ms/epoch - 2ms/step002\n",
      "Epoch 8/28\n",
      " 443/1415 [========>.....................] - ETA: 2s - loss: 0.0061252/252 - 0s - loss: 0.0105 - 437ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0107 - 458ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0108 - 276ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 815/1415 [================>.............] - ETA: 1s - loss: 0.0074252/252 - 0s - loss: 0.0108 - 185ms/epoch - 733us/step\n",
      "252/252 - 0s - loss: 0.0107 - 143ms/epoch - 566us/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0112 - val_loss: 0.0111\n",
      "252/252 - 0s - loss: 0.0104 - 165ms/epoch - 656us/step\n",
      "Epoch 17/28\n",
      "Epoch 9/28\n",
      "Epoch 27/28\n",
      " 816/1415 [================>.............] - ETA: 1s - loss: 0.0074252/252 - 0s - loss: 0.0108 - 135ms/epoch - 536us/step\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 15/28\n",
      "  77/1415 [>.............................] - ETA: 1s - loss: 0.0028252/252 - 0s - loss: 0.0108 - 400ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 106/1415 [=>............................] - ETA: 1s - loss: 0.0028252/252 - 0s - loss: 0.0107 - 406ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0106 - 458ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "1080/1415 [=====================>........] - ETA: 0s - loss: 0.0073252/252 - 0s - loss: 0.0104 - 463ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      " 132/1415 [=>............................] - ETA: 1s - loss: 0.0028252/252 - 0s - loss: 0.0107 - 211ms/epoch - 836us/step\n",
      " 756/1415 [===============>..............] - ETA: 1s - loss: 0.0059Epoch 11/28\n",
      " 173/1415 [==>...........................] - ETA: 1s - loss: 0.0028252/252 - 0s - loss: 0.0107 - 176ms/epoch - 699us/step\n",
      "252/252 - 0s - loss: 0.0104 - 161ms/epoch - 639us/step\n",
      "252/252 - 0s - loss: 0.0106 - 171ms/epoch - 678us/step\n",
      "1111/1415 [======================>.......] - ETA: 0s - loss: 0.0073Epoch 16/28\n",
      "Epoch 19/28\n",
      "1031/1415 [====================>.........] - ETA: 1s - loss: 0.006163/63 - 0s - loss: 0.0103 - 196ms/epoch - 3ms/step\n",
      " 918/1415 [==================>...........] - ETA: 1s - loss: 0.0059Epoch 1/28\n",
      "1140/1415 [=======================>......] - ETA: 0s - loss: 0.0060252/252 - 0s - loss: 0.0106 - 417ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "252/252 - 0s - loss: 0.0107 - 423ms/epoch - 2ms/step\n",
      " 980/1415 [===================>..........] - ETA: 1s - loss: 0.0059Epoch 17/28\n",
      "252/252 - 0s - loss: 0.0107 - 495ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1007/1415 [====================>.........] - ETA: 1s - loss: 0.0059252/252 - 0s - loss: 0.0103 - 326ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0110 - 239ms/epoch - 948us/step\n",
      "Epoch 2/28\n",
      "1192/1415 [========================>.....] - ETA: 0s - loss: 0.0060252/252 - 0s - loss: 0.0106 - 202ms/epoch - 801us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=sgd; total time= 2.3min252/252 - 0s - loss: 0.0106 - 197ms/epoch - 783us/step\n",
      "\n",
      "252/252 - 0s - loss: 0.0107 - 194ms/epoch - 770us/step\n",
      "1196/1415 [========================>.....] - ETA: 0s - loss: 0.0057Epoch 21/28\n",
      "Epoch 18/28\n",
      "Epoch 13/28\n",
      "1258/1415 [=========================>....] - ETA: 0s - loss: 0.0057252/252 - 0s - loss: 0.0110 - 229ms/epoch - 908us/step\n",
      "Epoch 3/28\n",
      "1333/1415 [===========================>..] - ETA: 0s - loss: 0.0056252/252 - 0s - loss: 0.0105 - 400ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "1367/1415 [===========================>..] - ETA: 0s - loss: 0.0056252/252 - 0s - loss: 0.0106 - 466ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "1399/1415 [============================>.] - ETA: 0s - loss: 0.0056252/252 - 0s - loss: 0.0107 - 498ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1299/1415 [==========================>...] - ETA: 0s - loss: 0.0058252/252 - 0s - loss: 0.0110 - 454ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 5/28\n",
      "1363/1415 [===========================>..] - ETA: 0s - loss: 0.0058252/252 - 0s - loss: 0.0105 - 289ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1036/1415 [====================>.........] - ETA: 0s - loss: 0.0028252/252 - 0s - loss: 0.0106 - 316ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1401/1415 [============================>.] - ETA: 0s - loss: 0.0058252/252 - 0s - loss: 0.0106 - 308ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 126/1415 [=>............................] - ETA: 2s - loss: 0.0062252/252 - 0s - loss: 0.0109 - 366ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0056 - val_loss: 0.0089\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 6/28\n",
      "  31/1415 [..............................] - ETA: 2s - loss: 0.0054252/252 - 0s - loss: 0.0105 - 434ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "  62/1415 [>.............................] - ETA: 2s - loss: 0.0057252/252 - 0s - loss: 0.0106 - 362ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 7/28\n",
      "  94/1415 [>.............................] - ETA: 2s - loss: 0.0061252/252 - 0s - loss: 0.0106 - 384ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      " 356/1415 [======>.......................] - ETA: 1s - loss: 0.0063252/252 - 0s - loss: 0.0109 - 320ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "  92/1415 [>.............................] - ETA: 2s - loss: 0.0054252/252 - 0s - loss: 0.0105 - 226ms/epoch - 896us/step\n",
      "Epoch 22/28\n",
      " 146/1415 [==>...........................] - ETA: 2s - loss: 0.0053252/252 - 0s - loss: 0.0105 - 339ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 16/28\n",
      " 165/1415 [==>...........................] - ETA: 2s - loss: 0.0053252/252 - 0s - loss: 0.0106 - 341ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 567/1415 [===========>..................] - ETA: 1s - loss: 0.0063252/252 - 0s - loss: 0.0109 - 412ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      " 274/1415 [====>.........................] - ETA: 2s - loss: 0.0052252/252 - 0s - loss: 0.0105 - 368ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 370/1415 [======>.......................] - ETA: 2s - loss: 0.0052252/252 - 0s - loss: 0.0104 - 407ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0106 - 346ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 470/1415 [========>.....................] - ETA: 1s - loss: 0.0026252/252 - 0s - loss: 0.0108 - 369ms/epoch - 1ms/step\n",
      " 482/1415 [=========>....................] - ETA: 1s - loss: 0.0052Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0105 - 350ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 514/1415 [=========>....................] - ETA: 1s - loss: 0.0052252/252 - 0s - loss: 0.0104 - 282ms/epoch - 1ms/step\n",
      " 649/1415 [============>.................] - ETA: 1s - loss: 0.0053Epoch 27/28\n",
      " 578/1415 [===========>..................] - ETA: 1s - loss: 0.0026252/252 - 0s - loss: 0.0105 - 344ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 776/1415 [===============>..............] - ETA: 1s - loss: 0.0053252/252 - 0s - loss: 0.0108 - 339ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 666/1415 [=============>................] - ETA: 1s - loss: 0.0052252/252 - 0s - loss: 0.0105 - 361ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 700/1415 [=============>................] - ETA: 1s - loss: 0.0053252/252 - 0s - loss: 0.0105 - 284ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 827/1415 [================>.............] - ETA: 0s - loss: 0.0026252/252 - 0s - loss: 0.0104 - 367ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 861/1415 [=================>............] - ETA: 1s - loss: 0.0055252/252 - 0s - loss: 0.0104 - 236ms/epoch - 935us/step\n",
      " 888/1415 [=================>............] - ETA: 0s - loss: 0.0055252/252 - 0s - loss: 0.0108 - 376ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 907/1415 [==================>...........] - ETA: 0s - loss: 0.0055252/252 - 0s - loss: 0.0105 - 332ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0104 - 396ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "1361/1415 [===========================>..] - ETA: 0s - loss: 0.006163/63 - 0s - loss: 0.0104 - 252ms/epoch - 4ms/step\n",
      "252/252 - 0s - loss: 0.0104 - 287ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "252/252 - 0s - loss: 0.0108 - 353ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1326/1415 [===========================>..] - ETA: 0s - loss: 0.0026252/252 - 0s - loss: 0.0105 - 338ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1113/1415 [======================>.......] - ETA: 0s - loss: 0.0054252/252 - 0s - loss: 0.0103 - 138ms/epoch - 548us/step\n",
      "1158/1415 [=======================>......] - ETA: 0s - loss: 0.0054[CV] END activation=relu, kernel_regularizer=l2, optimizer=sgd; total time= 2.4min\n",
      "1123/1415 [======================>.......] - ETA: 0s - loss: 0.0054252/252 - 0s - loss: 0.0107 - 221ms/epoch - 877us/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0104 - 249ms/epoch - 987us/step\n",
      "Epoch 28/28\n",
      "252/252 - 0s - loss: 0.0104 - 207ms/epoch - 821us/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0026 - val_loss: 0.0024\n",
      "Epoch 17/28\n",
      "1409/1415 [============================>.] - ETA: 0s - loss: 0.0051252/252 - 0s - loss: 0.0104 - 451ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "252/252 - 1s - loss: 0.0107 - 512ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1389/1415 [============================>.] - ETA: 0s - loss: 0.0053252/252 - 1s - loss: 0.0104 - 505ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 6/28\n",
      " 307/1415 [=====>........................] - ETA: 1s - loss: 0.002563/63 - 0s - loss: 0.0102 - 162ms/epoch - 3ms/step\n",
      " 351/1415 [======>.......................] - ETA: 1s - loss: 0.0025252/252 - 0s - loss: 0.0104 - 405ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      " 141/1415 [=>............................] - ETA: 1s - loss: 0.0064252/252 - 0s - loss: 0.0107 - 404ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0053 - val_loss: 0.0057\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0053 - val_loss: 0.0066\n",
      "Epoch 8/28\n",
      "  90/1415 [>.............................] - ETA: 2s - loss: 0.0049252/252 - 0s - loss: 0.0103 - 252ms/epoch - 999us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=sgd; total time= 2.4min\n",
      " 303/1415 [=====>........................] - ETA: 1s - loss: 0.0059252/252 - 0s - loss: 0.0104 - 344ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 149/1415 [==>...........................] - ETA: 2s - loss: 0.0048252/252 - 0s - loss: 0.0107 - 345ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 343/1415 [======>.......................] - ETA: 2s - loss: 0.0052252/252 - 0s - loss: 0.0103 - 430ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 333/1415 [======>.......................] - ETA: 2s - loss: 0.0047252/252 - 0s - loss: 0.0106 - 404ms/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      " 480/1415 [=========>....................] - ETA: 1s - loss: 0.0049252/252 - 0s - loss: 0.0103 - 338ms/epoch - 1ms/step\n",
      " 677/1415 [=============>................] - ETA: 1s - loss: 0.0056Epoch 28/28\n",
      "252/252 - 0s - loss: 0.0106 - 328ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 625/1415 [============>.................] - ETA: 1s - loss: 0.0049252/252 - 0s - loss: 0.0103 - 233ms/epoch - 923us/step\n",
      " 644/1415 [============>.................] - ETA: 1s - loss: 0.0048252/252 - 0s - loss: 0.0106 - 296ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 704/1415 [=============>................] - ETA: 1s - loss: 0.004663/63 - 0s - loss: 0.0101 - 142ms/epoch - 2ms/step\n",
      " 828/1415 [================>.............] - ETA: 1s - loss: 0.0048252/252 - 0s - loss: 0.0105 - 336ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 18/28\n",
      " 896/1415 [=================>............] - ETA: 1s - loss: 0.0048252/252 - 0s - loss: 0.0102 - 432ms/epoch - 2ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=sgd; total time= 2.4min\n",
      "1102/1415 [======================>.......] - ETA: 0s - loss: 0.0056252/252 - 0s - loss: 0.0105 - 341ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1283/1415 [==========================>...] - ETA: 0s - loss: 0.0056252/252 - 0s - loss: 0.0105 - 304ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1311/1415 [==========================>...] - ETA: 0s - loss: 0.0047252/252 - 0s - loss: 0.0105 - 375ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 763/1415 [===============>..............] - ETA: 0s - loss: 0.0024252/252 - 0s - loss: 0.0104 - 241ms/epoch - 958us/steps: 0.002\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0049 - val_loss: 0.0045\n",
      "Epoch 9/28\n",
      " 988/1415 [===================>..........] - ETA: 0s - loss: 0.0024252/252 - 0s - loss: 0.0104 - 307ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1145/1415 [=======================>......] - ETA: 0s - loss: 0.0024252/252 - 0s - loss: 0.0104 - 263ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 415/1415 [=======>......................] - ETA: 1s - loss: 0.0043252/252 - 0s - loss: 0.0104 - 370ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 19/28\n",
      " 567/1415 [===========>..................] - ETA: 1s - loss: 0.0043252/252 - 0s - loss: 0.0103 - 336ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      " 770/1415 [===============>..............] - ETA: 1s - loss: 0.0044252/252 - 0s - loss: 0.0103 - 338ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 939/1415 [==================>...........] - ETA: 0s - loss: 0.0043252/252 - 0s - loss: 0.0103 - 294ms/epoch - 1ms/step\n",
      "1116/1415 [======================>.......] - ETA: 0s - loss: 0.004263/63 - 0s - loss: 0.0102 - 250ms/epoch - 4ms/step\n",
      "1302/1415 [==========================>...] - ETA: 0s - loss: 0.0042252/252 - 0s - loss: 0.0102 - 258ms/epoch - 1ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=sgd; total time= 2.4min\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0042 - val_loss: 0.0039\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 20/28\n",
      "1321/1415 [===========================>..] - ETA: 0s - loss: 0.0040Epoch 1/28 ETA: 1s - loss: 0.\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0041 - val_loss: 0.0038- loss: 0.00\n",
      "Epoch 11/28\n",
      "1146/1415 [=======================>......] - ETA: 0s - loss: 0.0021Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "1119/1415 [======================>.......] - ETA: 0s - loss: 0.0037Epoch 22/28\n",
      "1233/1415 [=========================>....] - ETA: 0s - loss: 0.0037Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0044 - val_loss: 0.0061...................] - ETA: 2s - loss: 16.978\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0037 - val_loss: 0.0033\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 11/28\n",
      "  97/1415 [=>............................] - ETA: 2s - loss: 0.0042Epoch 11/28\n",
      "Epoch 12/28\n",
      " 854/1415 [=================>............] - ETA: 1s - loss: 0.0021Epoch 1/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 7.1234 - val_loss: 0.6697\n",
      " 285/1415 [=====>........................] - ETA: 4s - loss: 0.0036Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 23/28\n",
      " 793/1415 [===============>..............] - ETA: 2s - loss: 0.0041Epoch 1/28=============>.] - ETA: 0s - loss: 7.430\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 7.2011 - val_loss: 0.6991\n",
      " 946/1415 [===================>..........] - ETA: 1s - loss: 0.0036Epoch 2/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 7.2311 - val_loss: 0.7175 - ETA: 1s - loss: 0.004\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0035 - val_loss: 0.0032\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0040 - val_loss: 0.0037\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0035 - val_loss: 0.0032\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.2595 - val_loss: 0.1469\n",
      " 357/1415 [======>.......................] - ETA: 3s - loss: 0.4808Epoch 3/28\n",
      "Epoch 11/28\n",
      "Epoch 12/28\n",
      "1018/1415 [====================>.........] - ETA: 1s - loss: 9.4375Epoch 12/28\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 7.1845 - val_loss: 0.6839.........] - ETA: 4s - loss: 0.003\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0021 - val_loss: 0.0019\n",
      " 965/1415 [===================>..........] - ETA: 1s - loss: 0.3188Epoch 2/28\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.2677 - val_loss: 0.1430.........] - ETA: 6s - loss: 0.003- ETA: 5s - loss: 0.003\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 7.1979 - val_loss: 0.6479.......] - ETA: 3s - loss: 0.137\n",
      " 753/1415 [==============>...............] - ETA: 2s - loss: 0.0020Epoch 2/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.2663 - val_loss: 0.1418\n",
      " 999/1415 [====================>.........] - ETA: 1s - loss: 0.3076Epoch 3/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1255 - val_loss: 0.1105.........] - ETA: 2s - loss: 0.003 ETA: 2s - loss: 0.003\n",
      " 561/1415 [==========>...................] - ETA: 2s - loss: 0.3500Epoch 4/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.2663 - val_loss: 0.1546\n",
      " 738/1415 [==============>...............] - ETA: 2s - loss: 0.3101Epoch 3/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0040 - val_loss: 0.0035\n",
      "Epoch 12/28\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0036 - val_loss: 0.0032\n",
      " 757/1415 [===============>..............] - ETA: 2s - loss: 0.1299Epoch 14/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0032 - val_loss: 0.0030\n",
      " 395/1415 [=======>......................] - ETA: 4s - loss: 0.1453Epoch 13/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.1208 - val_loss: 0.1040...........] - ETA: 4s - loss: 0.003\n",
      " 197/1415 [===>..........................] - ETA: 6s - loss: 0.0035Epoch 4/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.2293 - val_loss: 0.1198\n",
      " 227/1415 [===>..........................] - ETA: 5s - loss: 0.0030Epoch 3/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.1219 - val_loss: 0.1071..............] - ETA: 4s - loss: 0.120..] - ETA: 5s - loss: 0.003\n",
      " 694/1415 [=============>................] - ETA: 3s - loss: 0.0019Epoch 4/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1026 - val_loss: 0.0956\n",
      " 546/1415 [==========>...................] - ETA: 4s - loss: 0.0032Epoch 5/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1276 - val_loss: 0.11010.127\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0019 - val_loss: 0.0018............] - ETA: 3s - loss: 0.003.] - ETA: 2s - loss: 0.00\n",
      " 801/1415 [===============>..............] - ETA: 2s - loss: 0.1011Epoch 26/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0963 - val_loss: 0.0908=======>....] - ETA: 1s - loss: 0.003\n",
      "1271/1415 [=========================>....] - ETA: 0s - loss: 0.0032Epoch 5/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.1019 - val_loss: 0.0895\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0031 - val_loss: 0.0028\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0031 - val_loss: 0.0034\n",
      " 328/1415 [=====>........................] - ETA: 3s - loss: 0.0902Epoch 14/28\n",
      "Epoch 4/28\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 15/28\n",
      "1379/1415 [============================>.] - ETA: 0s - loss: 0.0981Epoch 13/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0980 - val_loss: 0.09092s - loss: 0.002\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0897 - val_loss: 0.0841\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1016 - val_loss: 0.0955\n",
      " 216/1415 [===>..........................] - ETA: 5s - loss: 0.0030Epoch 5/28\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0019 - val_loss: 0.0018.] - ETA: 4s - loss: 0.081\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0877 - val_loss: 0.0844\n",
      "1102/1415 [======================>.......] - ETA: 1s - loss: 0.0876Epoch 6/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0842 - val_loss: 0.0784\n",
      "1154/1415 [=======================>......] - ETA: 1s - loss: 0.0792Epoch 5/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0866 - val_loss: 0.0820..........] - ETA: 4s - loss: 0.083\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0914 - val_loss: 0.0873\n",
      " 178/1415 [==>...........................] - ETA: 2s - loss: 0.0782Epoch 6/28\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0783 - val_loss: 0.0730\n",
      "1322/1415 [===========================>..] - ETA: 0s - loss: 0.0032Epoch 7/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0031 - val_loss: 0.00290.001\n",
      " 683/1415 [=============>................] - ETA: 3s - loss: 0.0834Epoch 16/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0035 - val_loss: 0.0031\n",
      "1399/1415 [============================>.] - ETA: 0s - loss: 0.0030Epoch 14/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0030 - val_loss: 0.0027\n",
      " 830/1415 [================>.............] - ETA: 2s - loss: 0.0831Epoch 15/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0030 - val_loss: 0.0027\n",
      " 115/1415 [=>............................] - ETA: 4s - loss: 0.0032Epoch 15/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      " 803/1415 [================>.............] - ETA: 2s - loss: 0.0759Epoch 28/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0820 - val_loss: 0.0791 ETA: 7s - loss: 0.003oss: 0.00\n",
      " 533/1415 [==========>...................] - ETA: 5s - loss: 0.0031Epoch 7/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0740 - val_loss: 0.0698TA: 2s - loss: 0.079\n",
      " 624/1415 [============>.................] - ETA: 5s - loss: 0.0031Epoch 6/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0844 - val_loss: 0.0819\n",
      " 698/1415 [=============>................] - ETA: 4s - loss: 0.0028Epoch 7/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0699 - val_loss: 0.0675] - ETA: 4s - loss: 0.0817\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0785 - val_loss: 0.0745\n",
      " 753/1415 [==============>...............] - ETA: 4s - loss: 0.0029Epoch 7/28\n",
      " 131/1415 [=>............................] - ETA: 2s - loss: 0.0818Epoch 8/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0018 - val_loss: 0.0017.] - ETA: 2s - loss: 0.002s - loss: 0.\n",
      "1327/1415 [===========================>..] - ETA: 0s - loss: 0.0685Epoch 1/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0030 - val_loss: 0.0027\n",
      "1415/1415 [==============================] - 9s 7ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0766 - val_loss: 0.0735\n",
      "Epoch 16/28\n",
      "Epoch 17/28\n",
      " 975/1415 [===================>..........] - ETA: 1s - loss: 0.0726Epoch 15/28\n",
      "Epoch 8/28\n",
      "  18/1415 [..............................] - ETA: 8s - loss: 0.0027 252/252 - 0s - loss: 0.0020 - 220ms/epoch - 874us/step\n",
      "Epoch 2/28\n",
      " 142/1415 [==>...........................] - ETA: 2s - loss: 0.0030252/252 - 0s - loss: 0.0017 - 284ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0684 - val_loss: 0.0666\n",
      "Epoch 3/28\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 16/28\n",
      " 131/1415 [=>............................] - ETA: 3s - loss: 0.0027252/252 - 1s - loss: 0.0021 - 693ms/epoch - 3ms/step\n",
      "Epoch 4/28\n",
      " 347/1415 [======>.......................] - ETA: 3s - loss: 0.0733252/252 - 0s - loss: 0.0019 - 167ms/epoch - 663us/step\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0798 - val_loss: 0.0777\n",
      " 281/1415 [====>.........................] - ETA: 4s - loss: 0.0029Epoch 5/28\n",
      "Epoch 8/28\n",
      " 288/1415 [=====>........................] - ETA: 4s - loss: 0.0026252/252 - 0s - loss: 0.0017 - 455ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0662 - val_loss: 0.0650\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0717 - val_loss: 0.0695\n",
      "Epoch 6/28\n",
      " 190/1415 [===>..........................] - ETA: 2s - loss: 0.0775Epoch 8/28\n",
      "Epoch 9/28\n",
      " 134/1415 [=>............................] - ETA: 2s - loss: 0.0646252/252 - 0s - loss: 0.0020 - 300ms/epoch - 1ms/step\n",
      " 727/1415 [==============>...............] - ETA: 2s - loss: 0.0723Epoch 7/28\n",
      " 684/1415 [=============>................] - ETA: 3s - loss: 0.0028252/252 - 1s - loss: 0.0017 - 773ms/epoch - 3ms/step\n",
      " 337/1415 [======>.......................] - ETA: 3s - loss: 0.0688Epoch 8/28\n",
      " 818/1415 [================>.............] - ETA: 2s - loss: 0.0029252/252 - 0s - loss: 0.0018 - 362ms/epoch - 1ms/step\n",
      " 493/1415 [=========>....................] - ETA: 3s - loss: 0.0644Epoch 9/28\n",
      " 651/1415 [============>.................] - ETA: 2s - loss: 0.0643252/252 - 0s - loss: 0.0025 - 342ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 812/1415 [================>.............] - ETA: 2s - loss: 0.0027252/252 - 0s - loss: 0.0017 - 156ms/epoch - 621us/step\n",
      " 710/1415 [==============>...............] - ETA: 2s - loss: 0.0642Epoch 11/28\n",
      "1124/1415 [======================>.......] - ETA: 1s - loss: 0.0033252/252 - 0s - loss: 0.0017 - 432ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1020/1415 [====================>.........] - ETA: 2s - loss: 0.0027252/252 - 0s - loss: 0.0017 - 379ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0706 - val_loss: 0.0669\n",
      "252/252 - 0s - loss: 0.0021 - 159ms/epoch - 632us/step\n",
      "1050/1415 [=====================>........] - ETA: 1s - loss: 0.0640Epoch 14/28\n",
      "Epoch 9/28\n",
      "1141/1415 [=======================>......] - ETA: 1s - loss: 0.0027252/252 - 0s - loss: 0.0017 - 334ms/epoch - 1ms/step\n",
      "1216/1415 [========================>.....] - ETA: 1s - loss: 0.0030Epoch 15/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0654 - val_loss: 0.0638\n",
      "1325/1415 [===========================>..] - ETA: 0s - loss: 0.0032Epoch 8/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0027 - val_loss: 0.0024\n",
      "252/252 - 0s - loss: 0.0021 - 356ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0759 - val_loss: 0.0741\n",
      "1302/1415 [==========================>...] - ETA: 0s - loss: 0.0027Epoch 9/28\n",
      "Epoch 16/28\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 16/28\n",
      " 124/1415 [=>............................] - ETA: 3s - loss: 0.0025252/252 - 0s - loss: 0.0021 - 321ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 197/1415 [===>..........................] - ETA: 2s - loss: 0.0031252/252 - 0s - loss: 0.0017 - 241ms/epoch - 956us/step\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0679 - val_loss: 0.0665\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0029 - val_loss: 0.0027\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0027 - val_loss: 0.0032\n",
      " 178/1415 [==>...........................] - ETA: 2s - loss: 0.0025Epoch 18/28\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0638 - val_loss: 0.0627\n",
      "Epoch 18/28\n",
      "Epoch 9/28\n",
      "Epoch 10/28\n",
      "  90/1415 [>.............................] - ETA: 5s - loss: 0.0627252/252 - 0s - loss: 0.0018 - 409ms/epoch - 2ms/step03\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0016 - 158ms/epoch - 625us/step\n",
      "  86/1415 [>.............................] - ETA: 5s - loss: 0.0666Epoch 20/28\n",
      " 916/1415 [==================>...........] - ETA: 1s - loss: 0.0649252/252 - 1s - loss: 0.0017 - 543ms/epoch - 2ms/step\n",
      " 296/1415 [=====>........................] - ETA: 3s - loss: 0.0625Epoch 21/28\n",
      " 663/1415 [=============>................] - ETA: 2s - loss: 0.0030252/252 - 0s - loss: 0.0016 - 342ms/epoch - 1ms/step\n",
      " 770/1415 [===============>..............] - ETA: 2s - loss: 0.0733Epoch 22/28\n",
      "1267/1415 [=========================>....] - ETA: 0s - loss: 0.0642252/252 - 0s - loss: 0.0025 - 325ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 607/1415 [===========>..................] - ETA: 3s - loss: 0.0661252/252 - 0s - loss: 0.0016 - 155ms/epoch - 616us/step\n",
      "Epoch 24/28\n",
      " 579/1415 [===========>..................] - ETA: 4s - loss: 0.0027252/252 - 0s - loss: 0.0016 - 345ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0640 - val_loss: 0.0611\n",
      "Epoch 25/28\n",
      "Epoch 10/28\n",
      " 903/1415 [==================>...........] - ETA: 2s - loss: 0.0026252/252 - 0s - loss: 0.0032 - 446ms/epoch - 2ms/step\n",
      " 892/1415 [=================>............] - ETA: 2s - loss: 0.0659Epoch 26/28\n",
      "1106/1415 [======================>.......] - ETA: 1s - loss: 0.0658252/252 - 0s - loss: 0.0016 - 389ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1167/1415 [=======================>......] - ETA: 1s - loss: 0.0657252/252 - 0s - loss: 0.0017 - 166ms/epoch - 660us/step\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0726 - val_loss: 0.0709\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0629 - val_loss: 0.0616\n",
      "Epoch 10/28\n",
      "Epoch 9/28\n",
      "Epoch 28/28\n",
      "1284/1415 [==========================>...] - ETA: 0s - loss: 0.0618252/252 - 0s - loss: 0.0016 - 333ms/epoch - 1ms/step\n",
      "1016/1415 [====================>.........] - ETA: 2s - loss: 0.002563/63 - 0s - loss: 0.0017 - 76ms/epoch - 1ms/step\n",
      "1011/1415 [====================>.........] - ETA: 2s - loss: 0.0027252/252 - 0s - loss: 0.0017 - 115ms/epoch - 455us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1768a67c0>; total time= 2.3min\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0031 - val_loss: 0.0027\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0618 - val_loss: 0.0608\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0655 - val_loss: 0.0643\n",
      "Epoch 17/28\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0026 - val_loss: 0.0030\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0026 - val_loss: 0.0024TA: 2s - loss: 0.061\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0027 - val_loss: 0.0030\n",
      "1164/1415 [=======================>......] - ETA: 0s - loss: 0.0604Epoch 19/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0602 - val_loss: 0.0589...............] - ETA: 6s - loss: 0.003\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0608 - val_loss: 0.0597\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0694 - val_loss: 0.0675\n",
      "Epoch 10/28\n",
      "1012/1415 [====================>.........] - ETA: 1s - loss: 0.0602Epoch 11/28\n",
      " 501/1415 [=========>....................] - ETA: 3s - loss: 0.0595Epoch 1/28\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0600 - val_loss: 0.0591\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0635 - val_loss: 0.0624\n",
      "Epoch 12/28\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0025 - val_loss: 0.0037.........] - ETA: 2s - loss: 0.002.......] - ETA: 1s - loss: 0.00\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0031 - val_loss: 0.0026\n",
      "1068/1415 [=====================>........] - ETA: 1s - loss: 0.0592Epoch 18/28\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0582 - val_loss: 0.0572 loss: 0.003\n",
      "1387/1415 [============================>.] - ETA: 0s - loss: 0.0024Epoch 12/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0659 - val_loss: 0.0636\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0590 - val_loss: 0.0580\n",
      "Epoch 12/28\n",
      " 931/1415 [==================>...........] - ETA: 1s - loss: 0.0587Epoch 19/28\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 9s 7ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0616 - val_loss: 0.0606..............] - ETA: 7s - loss: 0.002..] - ETA: 7s - loss: 0.002\n",
      " 512/1415 [=========>....................] - ETA: 3s - loss: 0.0577Epoch 12/28\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0584 - val_loss: 0.0577\n",
      " 374/1415 [======>.......................] - ETA: 5s - loss: 0.0029Epoch 13/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0565 - val_loss: 0.0554: 0.415 [===========>..................] - ETA: 3s - loss: 0.057\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0616 - val_loss: 0.0591\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 1.0716 - val_loss: 0.0970\n",
      " 877/1415 [=================>............] - ETA: 3s - loss: 0.0024Epoch 13/28\n",
      "Epoch 2/28\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0573 - val_loss: 0.0565\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "1113/1415 [======================>.......] - ETA: 1s - loss: 0.0024Epoch 19/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "1157/1415 [=======================>......] - ETA: 1s - loss: 0.0027Epoch 20/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0600 - val_loss: 0.0591\n",
      "1354/1415 [===========================>..] - ETA: 0s - loss: 0.0027Epoch 13/28\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 9s 7ms/step - loss: 0.0027 - val_loss: 0.0024\n",
      " 416/1415 [=======>......................] - ETA: 6s - loss: 0.0030Epoch 21/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0570 - val_loss: 0.0564\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0842 - val_loss: 0.0716....] - ETA: 9s - loss: 0.00A: 5s - loss: 0.002\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0571 - val_loss: 0.0551\n",
      "Epoch 3/28\n",
      " 785/1415 [===============>..............] - ETA: 4s - loss: 0.0028Epoch 14/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0548 - val_loss: 0.0538=====>..] - ETA: 0s - loss: 0.055\n",
      " 916/1415 [==================>...........] - ETA: 3s - loss: 0.0029Epoch 14/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0559 - val_loss: 0.0552\n",
      " 301/1415 [=====>........................] - ETA: 3s - loss: 0.0551Epoch 13/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0557 - val_loss: 0.0550s: 0.\n",
      " 821/1415 [================>.............] - ETA: 2s - loss: 0.0536Epoch 15/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0584 - val_loss: 0.0575\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0028 - val_loss: 0.0025\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      " 174/1415 [==>...........................] - ETA: 2s - loss: 0.0551Epoch 20/28\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0022 - val_loss: 0.0021s - loss: 0.002: 0.002\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0546 - val_loss: 0.0539\n",
      " 245/1415 [====>.........................] - ETA: 5s - loss: 0.0022Epoch 21/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0533 - val_loss: 0.0523\n",
      "Epoch 15/28\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0617 - val_loss: 0.0565\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      " 343/1415 [======>.......................] - ETA: 5s - loss: 0.0022Epoch 4/28\n",
      " 653/1415 [============>.................] - ETA: 3s - loss: 0.0574Epoch 22/28\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0545 - val_loss: 0.0538...........] - ETA: 3s - loss: 0.002\n",
      " 785/1415 [===============>..............] - ETA: 2s - loss: 0.0573Epoch 14/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0570 - val_loss: 0.0561.............] - ETA: 4s - loss: 0.053..] - ETA: 5s - loss: 0.0\n",
      " 709/1415 [==============>...............] - ETA: 4s - loss: 0.0027Epoch 15/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0545 - val_loss: 0.0541\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0517 - val_loss: 0.0508: 0.\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0534 - val_loss: 0.0528\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0528 - val_loss: 0.0485\n",
      "1406/1415 [============================>.] - ETA: 0s - loss: 0.0533Epoch 22/28\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0027 - val_loss: 0.0024s - loss: 0.051\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0533 - val_loss: 0.0527\n",
      "1342/1415 [===========================>..] - ETA: 0s - loss: 0.0022Epoch 21/28\n",
      " 111/1415 [=>............................] - ETA: 4s - loss: 0.0022Epoch 15/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      " 222/1415 [===>..........................] - ETA: 5s - loss: 0.0021Epoch 23/28\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0534 - val_loss: 0.0529...........] - ETA: 3s - loss: 0.052\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0556 - val_loss: 0.0547\n",
      " 766/1415 [===============>..............] - ETA: 2s - loss: 0.0478Epoch 17/28\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0524 - val_loss: 0.0519.............] - ETA: 7s - loss: 0.002..] - ETA: 6s - loss: 0.002\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0500 - val_loss: 0.0487\n",
      " 737/1415 [==============>...............] - ETA: 4s - loss: 0.0027Epoch 17/28\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0523 - val_loss: 0.0517===========>.....] - ETA: 1s - loss: 0.047.....] - ETA: 4s - loss: 0.002\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0470 - val_loss: 0.0455\n",
      " 888/1415 [=================>............] - ETA: 3s - loss: 0.0022Epoch 6/28\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0022 - val_loss: 0.0020loss: 0.045\n",
      " 414/1415 [=======>......................] - ETA: 4s - loss: 0.0516Epoch 23/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0524 - val_loss: 0.0519s - loss: 0.002\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0542 - val_loss: 0.0533\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0026 - val_loss: 0.0024\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 22/28\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 17/28\n",
      "1210/1415 [========================>.....] - ETA: 0s - loss: 0.0478Epoch 24/28\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0476 - val_loss: 0.0463\n",
      " 192/1415 [===>..........................] - ETA: 6s - loss: 0.0020Epoch 18/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0515 - val_loss: 0.0509\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0513 - val_loss: 0.0507\n",
      "1252/1415 [=========================>....] - ETA: 0s - loss: 0.0444Epoch 17/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0443 - val_loss: 0.0429oss: 0.0\n",
      " 311/1415 [=====>........................] - ETA: 3s - loss: 0.0507Epoch 7/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0515 - val_loss: 0.0509s - loss: 0.0020\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0524 - val_loss: 0.0511\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "1184/1415 [========================>.....] - ETA: 1s - loss: 0.0462Epoch 18/28\n",
      "Epoch 19/28\n",
      " 881/1415 [=================>............] - ETA: 2s - loss: 0.0505Epoch 24/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0506 - val_loss: 0.0501\n",
      "1198/1415 [========================>.....] - ETA: 1s - loss: 0.0021Epoch 19/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0461 - val_loss: 0.0454TA: 0s - loss: 0.002\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      " 323/1415 [=====>........................] - ETA: 6s - loss: 0.0021Epoch 23/28\n",
      " 510/1415 [=========>....................] - ETA: 3s - loss: 0.0507Epoch 25/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0021 - val_loss: 0.0019s: 0.050\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0503 - val_loss: 0.0499\n",
      " 677/1415 [=============>................] - ETA: 2s - loss: 0.0507Epoch 24/28\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0423 - val_loss: 0.0413 7s - loss: 0.0020.002\n",
      " 468/1415 [========>.....................] - ETA: 3s - loss: 0.0497Epoch 8/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0506 - val_loss: 0.0502 3s - loss: 0.002\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0500 - val_loss: 0.0486\n",
      "1178/1415 [=======================>......] - ETA: 1s - loss: 0.0499Epoch 20/28\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0498 - val_loss: 0.0493TA: 3s - loss: 0.048\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0452 - val_loss: 0.0447\n",
      "Epoch 20/28\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0495 - val_loss: 0.0490- ETA: 0s - loss: 0.002\n",
      "1031/1415 [====================>.........] - ETA: 2s - loss: 0.0022Epoch 19/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0021 - val_loss: 0.0019\n",
      "1222/1415 [========================>.....] - ETA: 1s - loss: 0.0025Epoch 25/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0407 - val_loss: 0.0400...] - ETA: 0s - loss: 0.002A: 6s - loss: 0.0\n",
      "1415/1415 [==============================] - 11s 7ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 9/28\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 11s 7ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      " 982/1415 [===================>..........] - ETA: 1s - loss: 0.0446Epoch 26/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0498 - val_loss: 0.0494\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0483 - val_loss: 0.0480\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 21/28\n",
      " 448/1415 [========>.....................] - ETA: 5s - loss: 0.0019Epoch 20/28\n",
      " 131/1415 [=>............................] - ETA: 3s - loss: 0.0025Epoch 25/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0445 - val_loss: 0.0440\n",
      "1170/1415 [=======================>......] - ETA: 1s - loss: 0.0488Epoch 21/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0490 - val_loss: 0.0486loss: 0.001\n",
      "1309/1415 [==========================>...] - ETA: 0s - loss: 0.0487Epoch 21/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0487 - val_loss: 0.0482\n",
      " 556/1415 [==========>...................] - ETA: 5s - loss: 0.0023Epoch 20/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0491 - val_loss: 0.0486 - loss: 0.0020\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0475 - val_loss: 0.0470\n",
      "  97/1415 [=>............................] - ETA: 5s - loss: 0.0486Epoch 21/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0394 - val_loss: 0.0387\n",
      "1415/1415 [==============================] - 11s 7ms/step - loss: 0.0020 - val_loss: 0.0018\n",
      "1181/1415 [========================>.....] - ETA: 1s - loss: 0.0438Epoch 26/28\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "1370/1415 [============================>.] - ETA: 0s - loss: 0.0020Epoch 27/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0438 - val_loss: 0.0433\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0026 - val_loss: 0.0022\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0020 - val_loss: 0.0018\n",
      "1301/1415 [==========================>...] - ETA: 0s - loss: 0.0480Epoch 25/28\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0483 - val_loss: 0.0479\n",
      " 159/1415 [==>...........................] - ETA: 2s - loss: 0.0020Epoch 22/28\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0479 - val_loss: 0.0475\n",
      " 564/1415 [==========>...................] - ETA: 4s - loss: 0.0386Epoch 21/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0484 - val_loss: 0.0479: 6s - loss: 0.002 0.04\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0468 - val_loss: 0.0463\n",
      " 570/1415 [===========>..................] - ETA: 5s - loss: 0.0021Epoch 23/28\n",
      " 892/1415 [=================>............] - ETA: 3s - loss: 0.0019Epoch 22/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0383 - val_loss: 0.0378\n",
      "1338/1415 [===========================>..] - ETA: 0s - loss: 0.0477Epoch 11/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0476 - val_loss: 0.0472\n",
      "1411/1415 [============================>.] - ETA: 0s - loss: 0.0431Epoch 23/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0431 - val_loss: 0.0427\n",
      "1332/1415 [===========================>..] - ETA: 0s - loss: 0.0473Epoch 23/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0472 - val_loss: 0.0468\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      " 121/1415 [=>............................] - ETA: 3s - loss: 0.0430Epoch 27/28\n",
      " 965/1415 [===================>..........] - ETA: 1s - loss: 0.0463Epoch 22/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0022 - val_loss: 0.0020.] - ETA: 1s - loss: 0.002\n",
      " 553/1415 [==========>...................] - ETA: 2s - loss: 0.0473Epoch 28/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      " 712/1415 [==============>...............] - ETA: 2s - loss: 0.0472Epoch 27/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0477 - val_loss: 0.0473\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0462 - val_loss: 0.0460\n",
      "1415/1415 [==============================] - 11s 7ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      " 905/1415 [==================>...........] - ETA: 2s - loss: 0.0376Epoch 24/28\n",
      "Epoch 23/28\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0470 - val_loss: 0.0466.........] - ETA: 6s - loss: 0.001] - ETA: 6s - loss: 0.0\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0374 - val_loss: 0.0370\n",
      " 713/1415 [==============>...............] - ETA: 4s - loss: 0.0020Epoch 12/28\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0426 - val_loss: 0.04220.04\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0466 - val_loss: 0.0462\n",
      " 936/1415 [==================>...........] - ETA: 2s - loss: 0.0471Epoch 23/28\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0455 - val_loss: 0.0453s: 0.1415 [==========>...................] - ETA: 4s - loss: 0.042\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0470 - val_loss: 0.0466\n",
      "1140/1415 [=======================>......] - ETA: 1s - loss: 0.0025Epoch 24/28\n",
      "Epoch 28/28\n",
      " 832/1415 [================>.............] - ETA: 2s - loss: 0.0368Epoch 25/28\n",
      " 770/1415 [===============>..............] - ETA: 2s - loss: 0.0461Epoch 1/28\n",
      " 273/1415 [====>.........................] - ETA: 4s - loss: 0.0453252/252 - 0s - loss: 0.0026 - 475ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 28/28\n",
      "252/252 - 0s - loss: 0.0025 - 166ms/epoch - 658us/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0464 - val_loss: 0.0460\n",
      "252/252 - 0s - loss: 0.0020 - 418ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      "Epoch 25/28\n",
      "Epoch 27/28\n",
      " 637/1415 [============>.................] - ETA: 3s - loss: 0.0452252/252 - 0s - loss: 0.0021 - 453ms/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0366 - val_loss: 0.0361\n",
      " 654/1415 [============>.................] - ETA: 3s - loss: 0.0452252/252 - 0s - loss: 0.0029 - 163ms/epoch - 645us/step\n",
      "Epoch 13/28\n",
      " 474/1415 [=========>....................] - ETA: 5s - loss: 0.0018Epoch 6/28\n",
      " 815/1415 [================>.............] - ETA: 2s - loss: 0.0452252/252 - 0s - loss: 0.0019 - 389ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0459 - val_loss: 0.0456\n",
      " 135/1415 [=>............................] - ETA: 2s - loss: 0.0362Epoch 24/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0420 - val_loss: 0.0416\n",
      " 806/1415 [================>.............] - ETA: 2s - loss: 0.0465Epoch 7/28\n",
      "Epoch 25/28\n",
      " 165/1415 [==>...........................] - ETA: 2s - loss: 0.0417252/252 - 0s - loss: 0.0019 - 395ms/epoch - 2ms/step\n",
      " 509/1415 [=========>....................] - ETA: 4s - loss: 0.0018Epoch 8/28\n",
      " 335/1415 [======>.......................] - ETA: 3s - loss: 0.0417252/252 - 1s - loss: 0.0019 - 501ms/epoch - 2ms/step\n",
      " 409/1415 [=======>......................] - ETA: 4s - loss: 0.0361Epoch 9/28\n",
      " 516/1415 [=========>....................] - ETA: 3s - loss: 0.0455252/252 - 0s - loss: 0.0020 - 383ms/epoch - 2ms/step 0.002\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0024 - 161ms/epoch - 639us/step\n",
      " 524/1415 [==========>...................] - ETA: 3s - loss: 0.0455Epoch 11/28\n",
      " 652/1415 [============>.................] - ETA: 4s - loss: 0.0024252/252 - 1s - loss: 0.0020 - 642ms/epoch - 3ms/step\n",
      "1024/1415 [====================>.........] - ETA: 2s - loss: 0.0019Epoch 12/28\n",
      " 754/1415 [==============>...............] - ETA: 3s - loss: 0.0416252/252 - 0s - loss: 0.0023 - 419ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "252/252 - 0s - loss: 0.0020 - 165ms/epoch - 656us/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0019 - 163ms/epoch - 647us/step\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0450 - val_loss: 0.0447\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0464 - val_loss: 0.0460\n",
      "Epoch 25/28\n",
      "Epoch 26/28\n",
      "1046/1415 [=====================>........] - ETA: 1s - loss: 0.0454252/252 - 0s - loss: 0.0023 - 367ms/epoch - 1ms/step\n",
      "1101/1415 [======================>.......] - ETA: 1s - loss: 0.0360Epoch 16/28\n",
      " 303/1415 [=====>........................] - ETA: 4s - loss: 0.0460252/252 - 0s - loss: 0.0021 - 482ms/epoch - 2ms/step\n",
      "1229/1415 [=========================>....] - ETA: 1s - loss: 0.0459Epoch 17/28\n",
      "1415/1415 [==============================] - 9s 7ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "1402/1415 [============================>.] - ETA: 0s - loss: 0.0458252/252 - 0s - loss: 0.0019 - 408ms/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0458 - val_loss: 0.0454\n",
      "1412/1415 [============================>.] - ETA: 0s - loss: 0.0359252/252 - 0s - loss: 0.0019 - 165ms/epoch - 655us/step\n",
      "1362/1415 [===========================>..] - ETA: 0s - loss: 0.0019Epoch 19/28\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0359 - val_loss: 0.0356\n",
      "Epoch 14/28\n",
      "  98/1415 [=>............................] - ETA: 2s - loss: 0.0356252/252 - 0s - loss: 0.0040 - 437ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 795/1415 [===============>..............] - ETA: 2s - loss: 0.0446252/252 - 0s - loss: 0.0020 - 471ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0453 - val_loss: 0.0450\n",
      "252/252 - 0s - loss: 0.0021 - 197ms/epoch - 782us/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0415 - val_loss: 0.0413\n",
      "252/252 - 0s - loss: 0.0018 - 238ms/epoch - 945us/step\n",
      "1415/1415 [==============================] - 9s 7ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 21/28\n",
      "Epoch 25/28\n",
      " 822/1415 [================>.............] - ETA: 2s - loss: 0.0446Epoch 26/28\n",
      "Epoch 3/28\n",
      "1033/1415 [====================>.........] - ETA: 1s - loss: 0.0445252/252 - 0s - loss: 0.0019 - 446ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0018 - 471ms/epoch - 2ms/step\n",
      "1082/1415 [=====================>........] - ETA: 1s - loss: 0.0445Epoch 4/28\n",
      "1415/1415 [==============================] - 9s 7ms/step - loss: 0.0025 - val_loss: 0.0020\n",
      "252/252 - 0s - loss: 0.0019 - 166ms/epoch - 658us/step\n",
      "Epoch 28/28\n",
      "Epoch 23/28\n",
      "   1/1415 [..............................] - ETA: 6s - loss: 0.0020Epoch 1/28\n",
      " 815/1415 [================>.............] - ETA: 1s - loss: 0.0454252/252 - 0s - loss: 0.0021 - 434ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "1306/1415 [==========================>...] - ETA: 0s - loss: 0.0445252/252 - 1s - loss: 0.0020 - 775ms/epoch - 3ms/step\n",
      "252/252 - 0s - loss: 0.0020 - 171ms/epoch - 680us/step\n",
      " 490/1415 [=========>....................] - ETA: 2s - loss: 0.0449252/252 - 0s - loss: 0.0019 - 479ms/epoch - 2ms/step\n",
      " 445/1415 [========>.....................] - ETA: 2s - loss: 0.0412Epoch 25/28\n",
      "Epoch 2/28\n",
      "Epoch 5/28\n",
      " 910/1415 [==================>...........] - ETA: 1s - loss: 0.0354252/252 - 1s - loss: 0.0020 - 566ms/epoch - 2ms/step\n",
      " 768/1415 [===============>..............] - ETA: 1s - loss: 0.0449252/252 - 1s - loss: 0.0023 - 576ms/epoch - 2ms/step\n",
      "252/252 - 1s - loss: 0.0017 - 580ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0445 - val_loss: 0.0441\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0458 - val_loss: 0.0454\n",
      "1062/1415 [=====================>........] - ETA: 0s - loss: 0.0453Epoch 26/28\n",
      " 323/1415 [=====>........................] - ETA: 4s - loss: 0.0021Epoch 3/28\n",
      "Epoch 26/28\n",
      "Epoch 6/28\n",
      "Epoch 27/28\n",
      " 940/1415 [==================>...........] - ETA: 1s - loss: 0.0449252/252 - 0s - loss: 0.0019 - 451ms/epoch - 2ms/step\n",
      "1161/1415 [=======================>......] - ETA: 0s - loss: 0.0354252/252 - 0s - loss: 0.0022 - 459ms/epoch - 2ms/step\n",
      "252/252 - 0s - loss: 0.0017 - 463ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      " 191/1415 [===>..........................] - ETA: 2s - loss: 0.0442Epoch 27/28\n",
      "1002/1415 [====================>.........] - ETA: 1s - loss: 0.0449Epoch 7/28\n",
      " 669/1415 [=============>................] - ETA: 3s - loss: 0.0021252/252 - 0s - loss: 0.0022 - 452ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0453 - val_loss: 0.0448\n",
      "Epoch 28/28\n",
      "252/252 - 1s - loss: 0.0019 - 780ms/epoch - 3ms/step\n",
      "252/252 - 0s - loss: 0.0030 - 418ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "Epoch 5/28\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0353 - val_loss: 0.0352\n",
      "Epoch 15/28\n",
      "1302/1415 [==========================>...] - ETA: 0s - loss: 0.0410252/252 - 0s - loss: 0.0018 - 445ms/epoch - 2ms/step\n",
      " 706/1415 [=============>................] - ETA: 1s - loss: 0.0453252/252 - 0s - loss: 0.0018 - 447ms/epoch - 2ms/step\n",
      "252/252 - 0s - loss: 0.0018 - 455ms/epoch - 2ms/step\n",
      "63/63 - 0s - loss: 0.0018 - 79ms/epoch - 1ms/step\n",
      " 628/1415 [============>.................] - ETA: 2s - loss: 0.0441Epoch 6/28\n",
      "Epoch 9/28\n",
      " 973/1415 [===================>..........] - ETA: 1s - loss: 0.0453252/252 - 1s - loss: 0.0018 - 503ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0448 - val_loss: 0.0445\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0410 - val_loss: 0.0407\n",
      "252/252 - 0s - loss: 0.0017 - 457ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0018 - 464ms/epoch - 2ms/step\n",
      "1094/1415 [======================>.......] - ETA: 1s - loss: 0.0024Epoch 10/28\n",
      "Epoch 27/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.0449[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x13a7437c0>; total time= 4.0min\n",
      "1256/1415 [=========================>....] - ETA: 0s - loss: 0.0024252/252 - 0s - loss: 0.0019 - 411ms/epoch - 2ms/step\n",
      " 735/1415 [==============>...............] - ETA: 1s - loss: 0.0349252/252 - 0s - loss: 0.0024 - 421ms/epoch - 2ms/step\n",
      "1059/1415 [=====================>........] - ETA: 1s - loss: 0.0441Epoch 8/28\n",
      " 189/1415 [===>..........................] - ETA: 2s - loss: 0.0446Epoch 11/28\n",
      "1206/1415 [========================>.....] - ETA: 0s - loss: 0.0440252/252 - 0s - loss: 0.0017 - 433ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "1269/1415 [=========================>....] - ETA: 0s - loss: 0.0440252/252 - 0s - loss: 0.0017 - 165ms/epoch - 656us/step\n",
      " 386/1415 [=======>......................] - ETA: 3s - loss: 0.0445252/252 - 1s - loss: 0.0017 - 848ms/epoch - 3ms/step\n",
      " 941/1415 [==================>...........] - ETA: 1s - loss: 0.0349Epoch 10/28\n",
      "Epoch 12/28\n",
      "1148/1415 [=======================>......] - ETA: 0s - loss: 0.0448252/252 - 0s - loss: 0.0022 - 492ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1177/1415 [=======================>......] - ETA: 0s - loss: 0.0447252/252 - 1s - loss: 0.0021 - 529ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0451 - val_loss: 0.0447\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0440 - val_loss: 0.0436\n",
      "252/252 - 0s - loss: 0.0018 - 195ms/epoch - 774us/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0023 - val_loss: 0.0020\n",
      "1218/1415 [========================>.....] - ETA: 0s - loss: 0.0447Epoch 28/28\n",
      "Epoch 27/28\n",
      "Epoch 14/28\n",
      "1335/1415 [===========================>..] - ETA: 0s - loss: 0.0348Epoch 11/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0348 - val_loss: 0.0345\n",
      "Epoch 16/28\n",
      " 305/1415 [=====>........................] - ETA: 1s - loss: 0.0437252/252 - 0s - loss: 0.0024 - 413ms/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1091/1415 [======================>.......] - ETA: 0s - loss: 0.0406252/252 - 0s - loss: 0.0018 - 468ms/epoch - 2ms/step\n",
      " 332/1415 [======>.......................] - ETA: 1s - loss: 0.0437Epoch 15/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0447 - val_loss: 0.0443\n",
      "252/252 - 0s - loss: 0.0017 - 211ms/epoch - 838us/step\n",
      "Epoch 28/28\n",
      "Epoch 13/28\n",
      " 237/1415 [====>.........................] - ETA: 3s - loss: 0.0448Epoch 1/28\n",
      "1344/1415 [===========================>..] - ETA: 0s - loss: 0.0406252/252 - 1s - loss: 0.0039 - 777ms/epoch - 3ms/step\n",
      "Epoch 16/28\n",
      "1277/1415 [==========================>...] - ETA: 0s - loss: 0.0443252/252 - 0s - loss: 0.0020 - 488ms/epoch - 2ms/step\n",
      " 458/1415 [========>.....................] - ETA: 2s - loss: 0.0449252/252 - 0s - loss: 0.0019 - 196ms/epoch - 777us/step\n",
      "1393/1415 [============================>.] - ETA: 0s - loss: 0.0406Epoch 17/28\n",
      "252/252 - 0s - loss: 0.0075 - 498ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0406 - val_loss: 0.0402 0.034\n",
      "Epoch 28/28\n",
      "   5/1415 [..............................] - ETA: 18s - loss: 0.0400252/252 - 1s - loss: 0.0019 - 501ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 494/1415 [=========>....................] - ETA: 2s - loss: 0.0444252/252 - 1s - loss: 0.0022 - 566ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "252/252 - 1s - loss: 0.0017 - 574ms/epoch - 2ms/step\n",
      " 544/1415 [==========>...................] - ETA: 2s - loss: 0.0443Epoch 18/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0443 - val_loss: 0.0439\n",
      "252/252 - 0s - loss: 0.0021 - 223ms/epoch - 885us/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0020 - 195ms/epoch - 774us/step\n",
      "Epoch 27/28\n",
      "Epoch 4/28\n",
      " 727/1415 [==============>...............] - ETA: 1s - loss: 0.0443252/252 - 1s - loss: 0.0017 - 621ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "1003/1415 [====================>.........] - ETA: 1s - loss: 0.0447252/252 - 0s - loss: 0.0016 - 463ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "252/252 - 0s - loss: 0.0020 - 463ms/epoch - 2ms/step\n",
      " 751/1415 [==============>...............] - ETA: 1s - loss: 0.0443252/252 - 0s - loss: 0.0017 - 255ms/epoch - 1ms/step\n",
      "1051/1415 [=====================>........] - ETA: 0s - loss: 0.0446252/252 - 0s - loss: 0.0018 - 262ms/epoch - 1ms/step\n",
      "1088/1415 [======================>.......] - ETA: 0s - loss: 0.0436Epoch 5/28\n",
      "Epoch 18/28\n",
      "Epoch 20/28\n",
      " 511/1415 [=========>....................] - ETA: 2s - loss: 0.0439252/252 - 1s - loss: 0.0023 - 910ms/epoch - 4ms/step\n",
      "252/252 - 1s - loss: 0.0022 - 613ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "Epoch 21/28\n",
      "252/252 - 1s - loss: 0.0020 - 620ms/epoch - 2ms/step\n",
      " 997/1415 [====================>.........] - ETA: 1s - loss: 0.0443Epoch 19/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0343 - val_loss: 0.0341s - loss: 0.044\n",
      "Epoch 17/28\n",
      " 811/1415 [================>.............] - ETA: 1s - loss: 0.0439252/252 - 1s - loss: 0.0022 - 511ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1269/1415 [=========================>....] - ETA: 0s - loss: 0.0442252/252 - 1s - loss: 0.0018 - 527ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0445 - val_loss: 0.0442\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0435 - val_loss: 0.0433\n",
      " 864/1415 [=================>............] - ETA: 1s - loss: 0.0402252/252 - 0s - loss: 0.0017 - 225ms/epoch - 895us/step\n",
      " 845/1415 [================>.............] - ETA: 1s - loss: 0.0439252/252 - 1s - loss: 0.0019 - 556ms/epoch - 2ms/step\n",
      "252/252 - 0s - loss: 0.0022 - 234ms/epoch - 931us/step\n",
      "  78/1415 [>.............................] - ETA: 1s - loss: 0.0340Epoch 28/28\n",
      "Epoch 20/28\n",
      "Epoch 8/28\n",
      "Epoch 23/28\n",
      " 228/1415 [===>..........................] - ETA: 2s - loss: 0.0432252/252 - 0s - loss: 0.0018 - 425ms/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0022 - 427ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      " 253/1415 [====>.........................] - ETA: 2s - loss: 0.0432252/252 - 1s - loss: 0.0019 - 500ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "1248/1415 [=========================>....] - ETA: 0s - loss: 0.0402252/252 - 0s - loss: 0.0035 - 287ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0442 - val_loss: 0.0438\n",
      "1292/1415 [==========================>...] - ETA: 0s - loss: 0.0402252/252 - 0s - loss: 0.0018 - 295ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0024 - 245ms/epoch - 973us/step\n",
      "Epoch 22/28\n",
      "Epoch 25/28\n",
      " 366/1415 [======>.......................] - ETA: 2s - loss: 0.0433Epoch 1/28\n",
      " 458/1415 [========>.....................] - ETA: 2s - loss: 0.0432252/252 - 0s - loss: 0.0025 - 323ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 800/1415 [===============>..............] - ETA: 1s - loss: 0.0340252/252 - 0s - loss: 0.0017 - 419ms/epoch - 2ms/step\n",
      " 616/1415 [============>.................] - ETA: 1s - loss: 0.0432Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0021 - 442ms/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 730/1415 [==============>...............] - ETA: 1s - loss: 0.0432252/252 - 0s - loss: 0.0440 - 425ms/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 752/1415 [==============>...............] - ETA: 1s - loss: 0.0432252/252 - 0s - loss: 0.0020 - 376ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1412/1415 [===============>..............] - ETA: 1s - loss: 0.0435/1415 [==============================] - 4s 3ms/step - loss: 0.0401 - val_loss: 0.0399\n",
      " 863/1415 [=================>............] - ETA: 1s - loss: 0.0432252/252 - 0s - loss: 0.0439 - 255ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0438 - val_loss: 0.0434\n",
      "Epoch 28/28\n",
      "  27/1415 [..............................] - ETA: 2s - loss: 0.0434252/252 - 0s - loss: 0.0018 - 421ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "252/252 - 0s - loss: 0.0017 - 446ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 139/1415 [=>............................] - ETA: 3s - loss: 0.0434252/252 - 1s - loss: 0.0034 - 582ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 194/1415 [===>..........................] - ETA: 2s - loss: 0.0434252/252 - 1s - loss: 0.0438 - 541ms/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      " 231/1415 [===>..........................] - ETA: 2s - loss: 0.0434Epoch 1/28\n",
      "252/252 - 1s - loss: 0.0016 - 506ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      " 266/1415 [====>.........................] - ETA: 2s - loss: 0.0435252/252 - 1s - loss: 0.0017 - 523ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      " 369/1415 [======>.......................] - ETA: 2s - loss: 0.0434252/252 - 0s - loss: 0.0439 - 201ms/epoch - 799us/step\n",
      "Epoch 2/28\n",
      "252/252 - 0s - loss: 0.0020 - 383ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0339 - val_loss: 0.0337\n",
      "Epoch 18/28\n",
      " 444/1415 [========>.....................] - ETA: 1s - loss: 0.0434252/252 - 0s - loss: 0.0436 - 322ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 655/1415 [============>.................] - ETA: 1s - loss: 0.0434252/252 - 0s - loss: 0.0025 - 465ms/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0051 - 484ms/epoch - 2ms/step\n",
      " 229/1415 [===>..........................] - ETA: 1s - loss: 0.0336252/252 - 0s - loss: 0.0438 - 394ms/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "Epoch 1/28\n",
      " 265/1415 [====>.........................] - ETA: 1s - loss: 0.0337252/252 - 0s - loss: 0.0434 - 399ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "63/63 - 0s - loss: 0.0017 - 143ms/epoch - 2ms/step\n",
      " 309/1415 [=====>........................] - ETA: 1s - loss: 0.0337252/252 - 1s - loss: 0.0028 - 542ms/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0431 - val_loss: 0.0430\n",
      " 438/1415 [========>.....................] - ETA: 1s - loss: 0.0336252/252 - 0s - loss: 0.0018 - 168ms/epoch - 668us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16dc837c0>; total time= 4.2min\n",
      "252/252 - 0s - loss: 0.0399 - 302ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      " 970/1415 [===================>..........] - ETA: 0s - loss: 0.0434252/252 - 0s - loss: 0.0437 - 341ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0017 - 476ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1045/1415 [=====================>........] - ETA: 0s - loss: 0.0434252/252 - 0s - loss: 0.0433 - 367ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0026 - 301ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 673/1415 [=============>................] - ETA: 1s - loss: 0.0336252/252 - 0s - loss: 0.0398 - 317ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      " 719/1415 [==============>...............] - ETA: 0s - loss: 0.0336252/252 - 0s - loss: 0.0016 - 332ms/epoch - 1ms/step\n",
      " 754/1415 [==============>...............] - ETA: 0s - loss: 0.0336Epoch 28/28\n",
      "1215/1415 [========================>.....] - ETA: 0s - loss: 0.0433252/252 - 0s - loss: 0.0436 - 373ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1242/1415 [=========================>....] - ETA: 0s - loss: 0.0433252/252 - 0s - loss: 0.0430 - 335ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1302/1415 [==========================>...] - ETA: 0s - loss: 0.0433252/252 - 0s - loss: 0.0020 - 400ms/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "1369/1415 [============================>.] - ETA: 0s - loss: 0.0433252/252 - 0s - loss: 0.0435 - 235ms/epoch - 933us/step\n",
      "Epoch 6/28\n",
      "1406/1415 [============================>.] - ETA: 0s - loss: 0.0433252/252 - 0s - loss: 0.0397 - 362ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1023/1415 [====================>.........] - ETA: 0s - loss: 0.0336252/252 - 0s - loss: 0.0016 - 356ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0428 - 267ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1097/1415 [======================>.......] - ETA: 0s - loss: 0.0336252/252 - 0s - loss: 0.0020 - 317ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1143/1415 [=======================>......] - ETA: 0s - loss: 0.0336252/252 - 0s - loss: 0.0434 - 258ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "63/63 - 0s - loss: 0.0016 - 172ms/epoch - 3ms/step\n",
      "1191/1415 [========================>.....] - ETA: 0s - loss: 0.0336252/252 - 0s - loss: 0.0397 - 248ms/epoch - 985us/step\n",
      "Epoch 5/28\n",
      "1231/1415 [=========================>....] - ETA: 0s - loss: 0.0335252/252 - 0s - loss: 0.0426 - 260ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0433 - val_loss: 0.0431\n",
      "1353/1415 [===========================>..] - ETA: 0s - loss: 0.0335252/252 - 0s - loss: 0.0023 - 287ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0396 - 228ms/epoch - 906us/step\n",
      "Epoch 6/28\n",
      "1396/1415 [============================>.] - ETA: 0s - loss: 0.0335252/252 - 0s - loss: 0.0433 - 297ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0016 - 333ms/epoch - 1ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16a1037c0>; total time= 4.2min\n",
      "Epoch 1/28\n",
      "252/252 - 0s - loss: 0.0423 - 412ms/epoch - 2ms/step\n",
      "Epoch 1/28\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0335 - val_loss: 0.0333\n",
      "Epoch 19/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0333252/252 - 0s - loss: 0.0432 - 398ms/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0028 - 462ms/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "252/252 - 0s - loss: 0.0395 - 462ms/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "   1/1415 [..............................] - ETA: 6:48 - loss: 20.1316252/252 - 0s - loss: 0.0429 - 300ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "  20/1415 [..............................] - ETA: 3s - loss: 17.9606  252/252 - 0s - loss: 0.0420 - 350ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "  76/1415 [>.............................] - ETA: 4s - loss: 13.0352252/252 - 0s - loss: 0.0395 - 340ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0023 - 363ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0431 - 388ms/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 352/1415 [======>.......................] - ETA: 1s - loss: 0.0333252/252 - 0s - loss: 0.0429 - 345ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "Epoch 1/28\n",
      " 156/1415 [==>...........................] - ETA: 3s - loss: 8.2879252/252 - 0s - loss: 0.0416 - 362ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 561/1415 [==========>...................] - ETA: 1s - loss: 0.0333252/252 - 0s - loss: 0.0394 - 334ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0430 - 324ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 313/1415 [=====>........................] - ETA: 2s - loss: 4.3765252/252 - 0s - loss: 0.0037 - 352ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 649/1415 [============>.................] - ETA: 0s - loss: 0.0333252/252 - 0s - loss: 0.0429 - 322ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "252/252 - 0s - loss: 0.0429 - 338ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 695/1415 [=============>................] - ETA: 0s - loss: 0.0332252/252 - 0s - loss: 0.0411 - 324ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 834/1415 [================>.............] - ETA: 0s - loss: 0.0332252/252 - 0s - loss: 0.0393 - 306ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 498/1415 [=========>....................] - ETA: 1s - loss: 2.8346252/252 - 0s - loss: 0.0429 - 330ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 877/1415 [=================>............] - ETA: 0s - loss: 0.0332252/252 - 0s - loss: 0.0408 - 187ms/epoch - 741us/step\n",
      "Epoch 15/28\n",
      " 923/1415 [==================>...........] - ETA: 0s - loss: 0.0332252/252 - 0s - loss: 0.0428 - 304ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "252/252 - 0s - loss: 0.0428 - 316ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "252/252 - 0s - loss: 0.0020 - 371ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1089/1415 [======================>.......] - ETA: 0s - loss: 0.0332252/252 - 0s - loss: 0.0407 - 289ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1132/1415 [=======================>......] - ETA: 0s - loss: 0.0332252/252 - 0s - loss: 0.0393 - 368ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1178/1415 [=======================>......] - ETA: 0s - loss: 0.0332252/252 - 0s - loss: 0.0427 - 340ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1224/1415 [========================>.....] - ETA: 0s - loss: 0.0332252/252 - 0s - loss: 0.0428 - 346ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0428 - 416ms/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1274/1415 [==========================>...] - ETA: 0s - loss: 0.0332252/252 - 0s - loss: 0.0020 - 390ms/epoch - 2ms/step\n",
      "Epoch 24/28\n",
      "1323/1415 [===========================>..] - ETA: 0s - loss: 0.0332252/252 - 0s - loss: 0.0406 - 246ms/epoch - 978us/step\n",
      "Epoch 17/28\n",
      "1410/1415 [============================>.] - ETA: 0s - loss: 0.0332252/252 - 0s - loss: 0.0392 - 298ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 884/1415 [=================>............] - ETA: 0s - loss: 1.6622252/252 - 0s - loss: 0.0426 - 271ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 916/1415 [==================>...........] - ETA: 0s - loss: 1.6084252/252 - 0s - loss: 0.0426 - 308ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0427 - 309ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 944/1415 [===================>..........] - ETA: 0s - loss: 1.5643252/252 - 0s - loss: 0.0405 - 222ms/epoch - 882us/step\n",
      "Epoch 18/28\n",
      "1004/1415 [====================>.........] - ETA: 0s - loss: 1.4780252/252 - 0s - loss: 0.0022 - 403ms/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "1038/1415 [=====================>........] - ETA: 0s - loss: 1.4334252/252 - 0s - loss: 0.0391 - 318ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0332 - val_loss: 0.0330\n",
      "Epoch 20/28\n",
      "1087/1415 [======================>.......] - ETA: 0s - loss: 1.3739252/252 - 0s - loss: 0.0425 - 433ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "1098/1415 [======================>.......] - ETA: 0s - loss: 1.3613252/252 - 0s - loss: 0.0426 - 462ms/epoch - 2ms/step\n",
      "252/252 - 0s - loss: 0.0425 - 497ms/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "Epoch 15/28\n",
      "1139/1415 [=======================>......] - ETA: 0s - loss: 1.3162252/252 - 0s - loss: 0.0404 - 471ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      " 252/1415 [====>.........................] - ETA: 1s - loss: 0.0330252/252 - 0s - loss: 0.0024 - 496ms/epoch - 2ms/step 0.033\n",
      "Epoch 26/28\n",
      "1245/1415 [=========================>....] - ETA: 0s - loss: 1.2132252/252 - 0s - loss: 0.0403 - 201ms/epoch - 797us/step\n",
      "Epoch 20/28\n",
      "1278/1415 [==========================>...] - ETA: 0s - loss: 1.1846252/252 - 1s - loss: 0.0391 - 511ms/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0425 - 326ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1308/1415 [==========================>...] - ETA: 0s - loss: 1.1597252/252 - 0s - loss: 0.0424 - 346ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0425 - 348ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 459/1415 [========>.....................] - ETA: 1s - loss: 0.0330252/252 - 0s - loss: 0.0390 - 242ms/epoch - 962us/step\n",
      "Epoch 15/28\n",
      "1406/1415 [============================>.] - ETA: 0s - loss: 1.0859252/252 - 0s - loss: 0.0423 - 260ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0021 - 376ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "252/252 - 0s - loss: 0.0402 - 319ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 527/1415 [==========>...................] - ETA: 1s - loss: 0.0329252/252 - 0s - loss: 0.0424 - 322ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 647/1415 [============>.................] - ETA: 1s - loss: 0.0329252/252 - 0s - loss: 0.0389 - 270ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0402 - 234ms/epoch - 927us/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0425 - 477ms/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      " 737/1415 [==============>...............] - ETA: 1s - loss: 0.0329252/252 - 0s - loss: 0.0423 - 342ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0027 - 343ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 815/1415 [================>.............] - ETA: 0s - loss: 0.0329252/252 - 0s - loss: 0.0422 - 331ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "252/252 - 0s - loss: 0.0389 - 212ms/epoch - 841us/step\n",
      "Epoch 17/28\n",
      "252/252 - 0s - loss: 0.0401 - 227ms/epoch - 901us/step\n",
      "Epoch 23/28\n",
      " 910/1415 [==================>...........] - ETA: 0s - loss: 0.0329252/252 - 0s - loss: 0.0424 - 308ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 951/1415 [===================>..........] - ETA: 0s - loss: 0.0329252/252 - 0s - loss: 0.0423 - 275ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 991/1415 [====================>.........] - ETA: 0s - loss: 0.0329252/252 - 0s - loss: 0.0020 - 320ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0400 - 196ms/epoch - 778us/step\n",
      "Epoch 24/28\n",
      "1036/1415 [====================>.........] - ETA: 0s - loss: 0.0329252/252 - 0s - loss: 0.0420 - 272ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0388 - 269ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1172/1415 [=======================>......] - ETA: 0s - loss: 0.0329252/252 - 0s - loss: 0.0423 - 278ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1217/1415 [========================>.....] - ETA: 0s - loss: 0.0329252/252 - 0s - loss: 0.0422 - 298ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "63/63 - 0s - loss: 0.0019 - 244ms/epoch - 4ms/step\n",
      "1263/1415 [=========================>....] - ETA: 0s - loss: 0.0329252/252 - 0s - loss: 0.0400 - 271ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1303/1415 [==========================>...] - ETA: 0s - loss: 0.0329252/252 - 0s - loss: 0.0387 - 270ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0419 - 292ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1392/1415 [============================>.] - ETA: 0s - loss: 0.0328252/252 - 0s - loss: 0.0422 - 284ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0399 - 223ms/epoch - 885us/step\n",
      "Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0422 - 274ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0019 - 231ms/epoch - 915us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1547c37c0>; total time= 4.2min\n",
      "252/252 - 0s - loss: 0.0387 - 235ms/epoch - 932us/step\n",
      "Epoch 20/28\n",
      "252/252 - 0s - loss: 0.0417 - 265ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0328 - val_loss: 0.0327\n",
      "Epoch 21/28\n",
      "  44/1415 [..............................] - ETA: 1s - loss: 0.0327252/252 - 0s - loss: 0.0398 - 206ms/epoch - 816us/step\n",
      "Epoch 27/28\n",
      "  97/1415 [=>............................] - ETA: 1s - loss: 0.0327252/252 - 0s - loss: 0.0422 - 256ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0421 - 245ms/epoch - 971us/step\n",
      "Epoch 15/28\n",
      " 144/1415 [==>...........................] - ETA: 1s - loss: 0.0327252/252 - 0s - loss: 0.0386 - 217ms/epoch - 860us/step\n",
      "Epoch 21/28\n",
      " 235/1415 [===>..........................] - ETA: 1s - loss: 0.0326252/252 - 0s - loss: 0.0416 - 283ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 260/1415 [====>.........................] - ETA: 1s - loss: 0.0327252/252 - 0s - loss: 0.0398 - 278ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 312/1415 [=====>........................] - ETA: 1s - loss: 0.0327252/252 - 0s - loss: 0.0421 - 359ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "252/252 - 0s - loss: 0.0421 - 349ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0386 - 349ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 399/1415 [=======>......................] - ETA: 1s - loss: 0.0327252/252 - 0s - loss: 0.0397 - 225ms/epoch - 891us/step\n",
      "252/252 - 0s - loss: 0.0414 - 333ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 513/1415 [=========>....................] - ETA: 1s - loss: 0.0326252/252 - 0s - loss: 0.0385 - 221ms/epoch - 877us/step\n",
      "252/252 - 0s - loss: 0.0420 - 257ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0421 - 249ms/epoch - 989us/step\n",
      "Epoch 17/28\n",
      " 550/1415 [==========>...................] - ETA: 1s - loss: 0.032663/63 - 0s - loss: 0.0390 - 203ms/epoch - 3ms/step\n",
      " 592/1415 [===========>..................] - ETA: 1s - loss: 0.0326252/252 - 0s - loss: 0.0412 - 224ms/epoch - 891us/step\n",
      "Epoch 24/28\n",
      " 667/1415 [=============>................] - ETA: 1s - loss: 0.0326252/252 - 0s - loss: 0.0385 - 209ms/epoch - 831us/step\n",
      "Epoch 24/28\n",
      "252/252 - 0s - loss: 0.0393 - 102ms/epoch - 406us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1_l2, optimizer=sgd; total time= 3.6min\n",
      " 710/1415 [==============>...............] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0420 - 255ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0419 - 260ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 775/1415 [===============>..............] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0409 - 310ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 818/1415 [================>.............] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0384 - 282ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 899/1415 [==================>...........] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0419 - 291ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0422 - 281ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 972/1415 [===================>..........] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0407 - 246ms/epoch - 978us/step\n",
      "Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0383 - 172ms/epoch - 683us/step\n",
      "Epoch 26/28\n",
      "1004/1415 [====================>.........] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0418 - 190ms/epoch - 756us/step\n",
      "Epoch 17/28\n",
      "1038/1415 [=====================>........] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0418 - 216ms/epoch - 859us/step\n",
      "Epoch 20/28\n",
      "1090/1415 [======================>.......] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0383 - 230ms/epoch - 914us/step\n",
      "Epoch 27/28\n",
      "1123/1415 [======================>.......] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0402 - 241ms/epoch - 956us/step\n",
      "Epoch 27/28\n",
      "1206/1415 [========================>.....] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0418 - 309ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1237/1415 [=========================>....] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0418 - 312ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0382 - 189ms/epoch - 749us/step\n",
      "Epoch 28/28\n",
      "1345/1415 [===========================>..] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0399 - 445ms/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "1388/1415 [============================>.] - ETA: 0s - loss: 0.0326252/252 - 0s - loss: 0.0382 - 342ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0417 - 404ms/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0416 - 391ms/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0395 - 223ms/epoch - 884us/step\n",
      "63/63 - 0s - loss: 0.0380 - 126ms/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 7s 4ms/step - loss: 1.0800 - val_loss: 0.0983\n",
      "Epoch 2/28\n",
      "  53/1415 [>.............................] - ETA: 1s - loss: 0.0982252/252 - 0s - loss: 0.0419 - 203ms/epoch - 805us/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0326 - val_loss: 0.0324\n",
      "Epoch 22/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0323252/252 - 0s - loss: 0.0416 - 240ms/epoch - 953us/step\n",
      "Epoch 20/28\n",
      "  93/1415 [>.............................] - ETA: 1s - loss: 0.0982252/252 - 0s - loss: 0.0379 - 113ms/epoch - 449us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1_l2, optimizer=sgd; total time= 3.5min\n",
      " 131/1415 [=>............................] - ETA: 1s - loss: 0.097563/63 - 0s - loss: 0.0386 - 163ms/epoch - 3ms/step\n",
      " 241/1415 [====>.........................] - ETA: 1s - loss: 0.0959252/252 - 0s - loss: 0.0416 - 335ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "252/252 - 0s - loss: 0.0416 - 349ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 329/1415 [=====>........................] - ETA: 1s - loss: 0.0947252/252 - 0s - loss: 0.0390 - 349ms/epoch - 1ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1_l2, optimizer=sgd; total time= 3.5min\n",
      " 402/1415 [=======>......................] - ETA: 1s - loss: 0.0938252/252 - 0s - loss: 0.0431 - 231ms/epoch - 917us/step\n",
      "Epoch 25/28\n",
      " 377/1415 [======>.......................] - ETA: 1s - loss: 0.0324252/252 - 0s - loss: 0.0415 - 205ms/epoch - 814us/step\n",
      "Epoch 22/28\n",
      " 552/1415 [==========>...................] - ETA: 1s - loss: 0.0920252/252 - 0s - loss: 0.0414 - 174ms/epoch - 692us/step\n",
      "Epoch 26/28\n",
      " 592/1415 [===========>..................] - ETA: 1s - loss: 0.0916252/252 - 0s - loss: 0.0414 - 235ms/epoch - 931us/step\n",
      "Epoch 23/28\n",
      " 760/1415 [===============>..............] - ETA: 0s - loss: 0.0899252/252 - 0s - loss: 0.0416 - 245ms/epoch - 972us/step\n",
      "Epoch 27/28\n",
      "252/252 - 0s - loss: 0.0414 - 217ms/epoch - 863us/step\n",
      "Epoch 24/28\n",
      " 922/1415 [==================>...........] - ETA: 0s - loss: 0.0883252/252 - 0s - loss: 0.0415 - 194ms/epoch - 769us/step\n",
      "Epoch 28/28\n",
      "252/252 - 0s - loss: 0.0413 - 175ms/epoch - 696us/step\n",
      "Epoch 25/28\n",
      "1048/1415 [=====================>........] - ETA: 0s - loss: 0.0871252/252 - 0s - loss: 0.0414 - 183ms/epoch - 726us/step\n",
      "252/252 - 0s - loss: 0.0412 - 178ms/epoch - 707us/step\n",
      "Epoch 26/28\n",
      "1156/1415 [=======================>......] - ETA: 0s - loss: 0.086163/63 - 0s - loss: 0.0408 - 137ms/epoch - 2ms/step\n",
      "1193/1415 [========================>.....] - ETA: 0s - loss: 0.0857252/252 - 0s - loss: 0.0412 - 187ms/epoch - 740us/step\n",
      "Epoch 27/28\n",
      "1285/1415 [==========================>...] - ETA: 0s - loss: 0.0323252/252 - 0s - loss: 0.0410 - 187ms/epoch - 742us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1_l2, optimizer=sgd; total time= 3.5min\n",
      "1326/1415 [===========================>..] - ETA: 0s - loss: 0.0323252/252 - 0s - loss: 0.0411 - 190ms/epoch - 756us/step\n",
      "Epoch 28/28\n",
      "1376/1415 [============================>.] - ETA: 0s - loss: 0.0323252/252 - 0s - loss: 0.0410 - 143ms/epoch - 566us/step\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0837 - val_loss: 0.0701\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0323 - val_loss: 0.0321\n",
      "Epoch 23/28\n",
      "  54/1415 [>.............................] - ETA: 1s - loss: 0.070063/63 - 0s - loss: 0.0410 - 101ms/epoch - 2ms/step\n",
      " 201/1415 [===>..........................] - ETA: 1s - loss: 0.0687252/252 - 0s - loss: 0.0409 - 172ms/epoch - 682us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1_l2, optimizer=sgd; total time= 3.5min\n",
      " 746/1415 [==============>...............] - ETA: 0s - loss: 0.0643Epoch 1/28\n",
      "1269/1415 [=========================>....] - ETA: 0s - loss: 0.03207 Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0320 - val_loss: 0.0319\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0609 - val_loss: 0.0546\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0318 - val_loss: 0.0317\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0507 - val_loss: 0.0480\n",
      "Epoch 5/28\n",
      " 182/1415 [==>...........................] - ETA: 1s - loss: 0.0481Epoch 1/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 1.0712 - val_loss: 0.0932\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0316 - val_loss: 0.0314\n",
      "1372/1415 [============================>.] - ETA: 0s - loss: 0.0467Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0467 - val_loss: 0.0452\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 1.0654 - val_loss: 0.1042\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0796 - val_loss: 0.0669\n",
      "1165/1415 [=======================>......] - ETA: 0s - loss: 0.0442Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0314 - val_loss: 0.0313\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0440 - val_loss: 0.0428\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0831 - val_loss: 0.0640\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 1.0613 - val_loss: 0.0954\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0592 - val_loss: 0.0536\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0312 - val_loss: 0.0311\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0420 - val_loss: 0.0410\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0568 - val_loss: 0.0517\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0837 - val_loss: 0.0745\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0310 - val_loss: 0.0309\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0507 - val_loss: 0.0486\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0404 - val_loss: 0.0396\n",
      "Epoch 9/28\n",
      "  47/1415 [..............................] - ETA: 1s - loss: 0.0396Epoch 1/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0499 - val_loss: 0.0476\n",
      " 659/1415 [============>.................] - ETA: 0s - loss: 0.0480Epoch 5/28\n",
      " 257/1415 [====>.........................] - ETA: 1s - loss: 0.0397252/252 - 0s - loss: 0.0309 - 238ms/epoch - 944us/step\n",
      "Epoch 2/28\n",
      " 426/1415 [========>.....................] - ETA: 1s - loss: 0.0396252/252 - 0s - loss: 0.0309 - 252ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      " 630/1415 [============>.................] - ETA: 1s - loss: 0.0394252/252 - 0s - loss: 0.0309 - 256ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0684 - val_loss: 0.0631\n",
      "Epoch 4/28\n",
      "  48/1415 [>.............................] - ETA: 1s - loss: 0.0630252/252 - 0s - loss: 0.0308 - 236ms/epoch - 938us/step\n",
      "Epoch 5/28\n",
      " 854/1415 [=================>............] - ETA: 0s - loss: 0.0469252/252 - 0s - loss: 0.0308 - 324ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0472 - val_loss: 0.0455\n",
      "Epoch 6/28\n",
      " 515/1415 [=========>....................] - ETA: 1s - loss: 0.0615252/252 - 0s - loss: 0.0308 - 254ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 690/1415 [=============>................] - ETA: 0s - loss: 0.0610252/252 - 0s - loss: 0.0308 - 230ms/epoch - 913us/step\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0391 - val_loss: 0.0385\n",
      "Epoch 10/28\n",
      "  45/1415 [..............................] - ETA: 1s - loss: 0.0386252/252 - 0s - loss: 0.0308 - 244ms/epoch - 970us/step\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0464 - val_loss: 0.0450\n",
      "Epoch 6/28\n",
      " 223/1415 [===>..........................] - ETA: 2s - loss: 0.0384252/252 - 0s - loss: 0.0307 - 358ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 260/1415 [====>.........................] - ETA: 1s - loss: 0.0449252/252 - 0s - loss: 0.0307 - 232ms/epoch - 919us/step\n",
      "Epoch 11/28\n",
      " 484/1415 [=========>....................] - ETA: 1s - loss: 0.0446252/252 - 0s - loss: 0.0307 - 249ms/epoch - 988us/step\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0586 - val_loss: 0.0531\n",
      "Epoch 5/28\n",
      " 121/1415 [=>............................] - ETA: 1s - loss: 0.0525252/252 - 0s - loss: 0.0307 - 262ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 328/1415 [=====>........................] - ETA: 1s - loss: 0.0518252/252 - 0s - loss: 0.0307 - 248ms/epoch - 984us/step\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0447 - val_loss: 0.0437\n",
      "Epoch 7/28\n",
      " 124/1415 [=>............................] - ETA: 1s - loss: 0.0433252/252 - 0s - loss: 0.0306 - 256ms/epoch - 1ms/steploss: 0.051\n",
      "Epoch 15/28\n",
      " 364/1415 [======>.......................] - ETA: 1s - loss: 0.0434252/252 - 0s - loss: 0.0306 - 272ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0380 - val_loss: 0.0373\n",
      "Epoch 11/28\n",
      "  85/1415 [>.............................] - ETA: 1s - loss: 0.0375252/252 - 0s - loss: 0.0306 - 283ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0438 - val_loss: 0.0427\n",
      "Epoch 7/28\n",
      " 729/1415 [==============>...............] - ETA: 0s - loss: 0.0431252/252 - 0s - loss: 0.0306 - 227ms/epoch - 899us/step\n",
      "Epoch 18/28\n",
      " 339/1415 [======>.......................] - ETA: 1s - loss: 0.0425252/252 - 0s - loss: 0.0305 - 240ms/epoch - 953us/step\n",
      "Epoch 19/28\n",
      " 538/1415 [==========>...................] - ETA: 1s - loss: 0.0424252/252 - 0s - loss: 0.0305 - 257ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0499 - val_loss: 0.0478\n",
      "1152/1415 [=======================>......] - ETA: 0s - loss: 0.0428Epoch 6/28\n",
      " 751/1415 [==============>...............] - ETA: 0s - loss: 0.0422252/252 - 0s - loss: 0.0305 - 294ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1095/1415 [======================>.......] - ETA: 0s - loss: 0.0372252/252 - 0s - loss: 0.0305 - 237ms/epoch - 942us/step\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0427 - val_loss: 0.0418\n",
      "Epoch 8/28\n",
      "1305/1415 [==========================>...] - ETA: 0s - loss: 0.0372252/252 - 0s - loss: 0.0305 - 259ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      " 339/1415 [======>.......................] - ETA: 0s - loss: 0.047: 1s - loss: 0.0417252/252 - 0s - loss: 0.0305 - 253ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0371 - val_loss: 0.0365\n",
      "Epoch 12/28\n",
      " 503/1415 [=========>....................] - ETA: 1s - loss: 0.0416252/252 - 0s - loss: 0.0304 - 233ms/epoch - 925us/step\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0419 - val_loss: 0.0408\n",
      "Epoch 8/28\n",
      " 720/1415 [==============>...............] - ETA: 0s - loss: 0.0415252/252 - 0s - loss: 0.0304 - 233ms/epoch - 926us/step\n",
      "Epoch 26/28\n",
      " 369/1415 [======>.......................] - ETA: 1s - loss: 0.0409252/252 - 0s - loss: 0.0304 - 413ms/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 538/1415 [==========>...................] - ETA: 1s - loss: 0.0408252/252 - 0s - loss: 0.0304 - 230ms/epoch - 912us/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0465 - val_loss: 0.0450\n",
      "Epoch 7/28\n",
      " 173/1415 [==>...........................] - ETA: 1s - loss: 0.0451252/252 - 0s - loss: 0.0303 - 247ms/epoch - 980us/step\n",
      " 879/1415 [=================>............] - ETA: 0s - loss: 0.040663/63 - 0s - loss: 0.0303 - 121ms/epoch - 2ms/step\n",
      " 994/1415 [====================>.........] - ETA: 0s - loss: 0.0405252/252 - 0s - loss: 0.0303 - 123ms/epoch - 486us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1768a6130>; total time= 2.6min\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0411 - val_loss: 0.0404\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0363 - val_loss: 0.0359\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0403 - val_loss: 0.0394\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0441 - val_loss: 0.0431\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0398 - val_loss: 0.0391\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0356 - val_loss: 0.0351\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0391 - val_loss: 0.0383\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0422 - val_loss: 0.04148\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0387 - val_loss: 0.0380\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0350 - val_loss: 0.0348\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0380 - val_loss: 0.0376\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0406 - val_loss: 0.0399\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0377 - val_loss: 0.0370oss: 0.034\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0345 - val_loss: 0.0342\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0371 - val_loss: 0.0366\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0394 - val_loss: 0.0388\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0369 - val_loss: 0.0365\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0340 - val_loss: 0.0337TA: 0s - loss: 0.038\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0363 - val_loss: 0.0359\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0384 - val_loss: 0.0379\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0362 - val_loss: 0.0359..] - ETA: 0s - loss: 0.033\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0336 - val_loss: 0.0333\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0356 - val_loss: 0.0353\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0375 - val_loss: 0.0372\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0355 - val_loss: 0.0352\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0332 - val_loss: 0.0328==========>.] - ETA: 0s - loss: 0.035\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0350 - val_loss: 0.0347\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0368 - val_loss: 0.0363\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0350 - val_loss: 0.0346\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0328 - val_loss: 0.0327\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0345 - val_loss: 0.0342\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0361 - val_loss: 0.0358 1s - loss: 0.034\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0345 - val_loss: 0.0342\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0325 - val_loss: 0.0323\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0340 - val_loss: 0.0336\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0356 - val_loss: 0.0353\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0340 - val_loss: 0.0338\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0322 - val_loss: 0.0321\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0336 - val_loss: 0.0334\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0351 - val_loss: 0.0349........] - ETA: 0s - loss: 0.033\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0336 - val_loss: 0.0334\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0319 - val_loss: 0.0316\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0332 - val_loss: 0.0332\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0346 - val_loss: 0.0344\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0328 - val_loss: 0.0326 loss: 0.032\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0317 - val_loss: 0.0315\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0325Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0342 - val_loss: 0.0339\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0329 - val_loss: 0.0329\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0325 - val_loss: 0.0322\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0314 - val_loss: 0.0313\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0339 - val_loss: 0.0338\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0326 - val_loss: 0.0325\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0322 - val_loss: 0.0319\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0312 - val_loss: 0.0311\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0335 - val_loss: 0.0333\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0323 - val_loss: 0.03211\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0320 - val_loss: 0.0318\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0310 - val_loss: 0.0309\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0331 - val_loss: 0.0325s: 0.032\n",
      "1316/1415 [==========================>...] - ETA: 0s - loss: 0.0321Epoch 22/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0321 - val_loss: 0.0318\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0317 - val_loss: 0.0317\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0308 - val_loss: 0.0308\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0320 - val_loss: 0.0318\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0318 - val_loss: 0.0317: 0s - loss: 0.031\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0315 - val_loss: 0.0313\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0307 - val_loss: 0.0305\n",
      "1323/1415 [===========================>..] - ETA: 0s - loss: 0.0317Epoch 1/28\n",
      "1270/1415 [=========================>....] - ETA: 0s - loss: 0.0316252/252 - 0s - loss: 0.0306 - 246ms/epoch - 978us/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0317 - val_loss: 0.0316\n",
      "Epoch 24/28\n",
      "1143/1415 [=======================>......] - ETA: 0s - loss: 0.0313252/252 - 0s - loss: 0.0305 - 227ms/epoch - 900us/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0316 - val_loss: 0.0315\n",
      "Epoch 26/28\n",
      "1389/1415 [============================>.] - ETA: 0s - loss: 0.0313252/252 - 0s - loss: 0.0305 - 240ms/epoch - 954us/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0312 - val_loss: 0.0311\n",
      "Epoch 26/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0313252/252 - 0s - loss: 0.0305 - 273ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      " 239/1415 [====>.........................] - ETA: 1s - loss: 0.0311252/252 - 0s - loss: 0.0305 - 247ms/epoch - 979us/step\n",
      "Epoch 6/28\n",
      " 440/1415 [========>.....................] - ETA: 1s - loss: 0.0311252/252 - 0s - loss: 0.0304 - 221ms/epoch - 877us/step\n",
      "Epoch 7/28\n",
      " 694/1415 [=============>................] - ETA: 0s - loss: 0.0311252/252 - 0s - loss: 0.0304 - 230ms/epoch - 911us/step\n",
      "Epoch 8/28\n",
      "1245/1415 [=========================>....] - ETA: 0s - loss: 0.0314252/252 - 0s - loss: 0.0304 - 343ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0315 - val_loss: 0.0312\n",
      "Epoch 25/28\n",
      "1138/1415 [=======================>......] - ETA: 0s - loss: 0.0311252/252 - 0s - loss: 0.0304 - 230ms/epoch - 913us/step\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0314 - val_loss: 0.0313\n",
      "Epoch 27/28\n",
      "  99/1415 [=>............................] - ETA: 1s - loss: 0.0312252/252 - 0s - loss: 0.0303 - 224ms/epoch - 890us/step\n",
      "Epoch 11/28\n",
      " 616/1415 [============>.................] - ETA: 0s - loss: 0.0313252/252 - 0s - loss: 0.0303 - 262ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0311 - val_loss: 0.0309\n",
      "Epoch 27/28\n",
      " 851/1415 [=================>............] - ETA: 0s - loss: 0.0313252/252 - 0s - loss: 0.0304 - 251ms/epoch - 998us/step\n",
      "Epoch 13/28\n",
      " 425/1415 [========>.....................] - ETA: 1s - loss: 0.0309252/252 - 0s - loss: 0.0303 - 230ms/epoch - 913us/step\n",
      "Epoch 14/28\n",
      " 662/1415 [=============>................] - ETA: 0s - loss: 0.0309252/252 - 0s - loss: 0.0303 - 244ms/epoch - 969us/step\n",
      "Epoch 15/28\n",
      " 861/1415 [=================>............] - ETA: 0s - loss: 0.0309252/252 - 0s - loss: 0.0302 - 236ms/epoch - 935us/step\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0312 - val_loss: 0.0310\n",
      "Epoch 26/28\n",
      "  97/1415 [=>............................] - ETA: 1s - loss: 0.0311252/252 - 0s - loss: 0.0302 - 256ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0312 - val_loss: 0.0311\n",
      "Epoch 28/28\n",
      "1339/1415 [===========================>..] - ETA: 0s - loss: 0.0309252/252 - 0s - loss: 0.0302 - 220ms/epoch - 875us/step\n",
      "Epoch 18/28\n",
      " 299/1415 [=====>........................] - ETA: 1s - loss: 0.0311252/252 - 0s - loss: 0.0302 - 217ms/epoch - 861us/step\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0309 - val_loss: 0.0307\n",
      "Epoch 28/28\n",
      " 531/1415 [==========>...................] - ETA: 0s - loss: 0.0311252/252 - 0s - loss: 0.0302 - 248ms/epoch - 984us/step\n",
      "Epoch 20/28\n",
      " 759/1415 [===============>..............] - ETA: 0s - loss: 0.0310252/252 - 0s - loss: 0.0301 - 243ms/epoch - 963us/step\n",
      "Epoch 21/28\n",
      " 994/1415 [====================>.........] - ETA: 0s - loss: 0.0310252/252 - 0s - loss: 0.0301 - 242ms/epoch - 961us/step\n",
      "Epoch 22/28\n",
      "1211/1415 [========================>.....] - ETA: 0s - loss: 0.0310252/252 - 0s - loss: 0.0301 - 261ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0310 - val_loss: 0.0308\n",
      "Epoch 27/28\n",
      "1409/1415 [============================>.] - ETA: 0s - loss: 0.0310252/252 - 0s - loss: 0.0301 - 341ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0310 - val_loss: 0.0309\n",
      " 270/1415 [====>.........................] - ETA: 1s - loss: 0.0309252/252 - 0s - loss: 0.0301 - 244ms/epoch - 969us/step\n",
      "Epoch 25/28\n",
      " 495/1415 [=========>....................] - ETA: 0s - loss: 0.0309252/252 - 0s - loss: 0.0300 - 202ms/epoch - 800us/step\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0307 - val_loss: 0.0305\n",
      " 726/1415 [==============>...............] - ETA: 0s - loss: 0.0309252/252 - 0s - loss: 0.0300 - 194ms/epoch - 768us/step\n",
      "Epoch 27/28\n",
      " 912/1415 [==================>...........] - ETA: 0s - loss: 0.0309Epoch 1/28\n",
      " 930/1415 [==================>...........] - ETA: 0s - loss: 0.0309252/252 - 0s - loss: 0.0300 - 221ms/epoch - 879us/step\n",
      "Epoch 28/28\n",
      "1090/1415 [======================>.......] - ETA: 0s - loss: 0.0309252/252 - 0s - loss: 0.0309 - 241ms/epoch - 957us/step\n",
      "Epoch 2/28\n",
      "1145/1415 [=======================>......] - ETA: 0s - loss: 0.0308252/252 - 0s - loss: 0.0300 - 210ms/epoch - 833us/step\n",
      "1199/1415 [========================>.....] - ETA: 0s - loss: 0.0308Epoch 1/28\n",
      "1303/1415 [==========================>...] - ETA: 0s - loss: 0.030863/63 - 0s - loss: 0.0300 - 119ms/epoch - 2ms/step\n",
      "1354/1415 [===========================>..] - ETA: 0s - loss: 0.0308252/252 - 0s - loss: 0.0309 - 213ms/epoch - 847us/step\n",
      "Epoch 3/28\n",
      "1405/1415 [============================>.] - ETA: 0s - loss: 0.0308252/252 - 0s - loss: 0.0300 - 125ms/epoch - 495us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x13a743130>; total time= 1.2min\n",
      "252/252 - 0s - loss: 0.0306 - 236ms/epoch - 938us/step\n",
      "Epoch 2/28\n",
      "252/252 - 0s - loss: 0.0309 - 206ms/epoch - 818us/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0308 - val_loss: 0.0307\n",
      "Epoch 28/28\n",
      " 115/1415 [=>............................] - ETA: 1s - loss: 0.0308252/252 - 0s - loss: 0.0306 - 196ms/epoch - 780us/step\n",
      "Epoch 3/28\n",
      " 171/1415 [==>...........................] - ETA: 1s - loss: 0.0308252/252 - 0s - loss: 0.0308 - 199ms/epoch - 789us/step\n",
      "Epoch 5/28\n",
      " 338/1415 [======>.......................] - ETA: 0s - loss: 0.0307252/252 - 0s - loss: 0.0305 - 204ms/epoch - 810us/step\n",
      "Epoch 4/28\n",
      " 393/1415 [=======>......................] - ETA: 0s - loss: 0.0307252/252 - 0s - loss: 0.0308 - 205ms/epoch - 815us/step\n",
      "Epoch 6/28\n",
      " 557/1415 [==========>...................] - ETA: 0s - loss: 0.0307252/252 - 0s - loss: 0.0305 - 203ms/epoch - 805us/step\n",
      "Epoch 5/28\n",
      " 613/1415 [===========>..................] - ETA: 0s - loss: 0.0307252/252 - 0s - loss: 0.0308 - 203ms/epoch - 807us/step\n",
      "Epoch 7/28\n",
      " 782/1415 [===============>..............] - ETA: 0s - loss: 0.0307252/252 - 0s - loss: 0.0305 - 196ms/epoch - 780us/step\n",
      "Epoch 6/28\n",
      " 838/1415 [================>.............] - ETA: 0s - loss: 0.0307252/252 - 0s - loss: 0.0307 - 200ms/epoch - 793us/step\n",
      "Epoch 8/28\n",
      " 951/1415 [===================>..........] - ETA: 0s - loss: 0.0307252/252 - 0s - loss: 0.0305 - 198ms/epoch - 784us/step\n",
      "Epoch 7/28\n",
      "1061/1415 [=====================>........] - ETA: 0s - loss: 0.0307252/252 - 0s - loss: 0.0307 - 202ms/epoch - 800us/step\n",
      "Epoch 9/28\n",
      "1174/1415 [=======================>......] - ETA: 0s - loss: 0.0307252/252 - 0s - loss: 0.0304 - 201ms/epoch - 796us/step\n",
      "Epoch 8/28\n",
      "1281/1415 [==========================>...] - ETA: 0s - loss: 0.0307252/252 - 0s - loss: 0.0307 - 205ms/epoch - 814us/step\n",
      "Epoch 10/28\n",
      "1393/1415 [============================>.] - ETA: 0s - loss: 0.0307252/252 - 0s - loss: 0.0304 - 202ms/epoch - 801us/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0307 - 194ms/epoch - 768us/step\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0307 - val_loss: 0.0307\n",
      "252/252 - 0s - loss: 0.0304 - 189ms/epoch - 751us/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0307 - 185ms/epoch - 734us/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0304 - 182ms/epoch - 723us/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0306 - 193ms/epoch - 768us/step\n",
      "Epoch 13/28\n",
      "252/252 - 0s - loss: 0.0304 - 191ms/epoch - 757us/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0306 - 185ms/epoch - 735us/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0303 - 185ms/epoch - 734us/step\n",
      "Epoch 13/28\n",
      "Epoch 1/28\n",
      "252/252 - 0s - loss: 0.0306 - 225ms/epoch - 891us/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0303 - 257ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0306 - 272ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "252/252 - 0s - loss: 0.0306 - 234ms/epoch - 930us/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0303 - 216ms/epoch - 857us/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0305 - 215ms/epoch - 852us/step\n",
      "Epoch 3/28\n",
      "252/252 - 0s - loss: 0.0305 - 220ms/epoch - 874us/step\n",
      "Epoch 17/28\n",
      "252/252 - 0s - loss: 0.0303 - 214ms/epoch - 851us/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0305 - 215ms/epoch - 852us/step\n",
      "Epoch 4/28\n",
      "252/252 - 0s - loss: 0.0305 - 211ms/epoch - 838us/step\n",
      "Epoch 18/28\n",
      "252/252 - 0s - loss: 0.0303 - 331ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "252/252 - 0s - loss: 0.0305 - 339ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "252/252 - 0s - loss: 0.0305 - 344ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0303 - 226ms/epoch - 896us/step\n",
      "Epoch 18/28\n",
      "252/252 - 0s - loss: 0.0304 - 218ms/epoch - 866us/step\n",
      "Epoch 6/28\n",
      "252/252 - 0s - loss: 0.0305 - 213ms/epoch - 845us/step\n",
      "Epoch 20/28\n",
      "252/252 - 0s - loss: 0.0302 - 205ms/epoch - 813us/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0304 - 203ms/epoch - 806us/step\n",
      "Epoch 7/28\n",
      "252/252 - 0s - loss: 0.0305 - 200ms/epoch - 792us/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0302 - 216ms/epoch - 855us/step\n",
      "Epoch 20/28\n",
      "252/252 - 0s - loss: 0.0304 - 217ms/epoch - 859us/step\n",
      "Epoch 8/28\n",
      "252/252 - 0s - loss: 0.0304 - 213ms/epoch - 846us/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0302 - 229ms/epoch - 908us/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0303 - 230ms/epoch - 914us/step\n",
      "Epoch 9/28\n",
      "252/252 - 0s - loss: 0.0304 - 226ms/epoch - 898us/step\n",
      "Epoch 23/28\n",
      "252/252 - 0s - loss: 0.0302 - 210ms/epoch - 834us/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0303 - 205ms/epoch - 814us/step\n",
      "Epoch 10/28\n",
      "252/252 - 0s - loss: 0.0304 - 203ms/epoch - 804us/step\n",
      "Epoch 24/28\n",
      "252/252 - 0s - loss: 0.0301 - 206ms/epoch - 816us/step\n",
      "Epoch 23/28\n",
      "252/252 - 0s - loss: 0.0303 - 211ms/epoch - 836us/step\n",
      "Epoch 11/28\n",
      "252/252 - 0s - loss: 0.0304 - 208ms/epoch - 827us/step\n",
      "Epoch 25/28\n",
      "252/252 - 0s - loss: 0.0301 - 242ms/epoch - 959us/step\n",
      "Epoch 24/28\n",
      "252/252 - 0s - loss: 0.0303 - 240ms/epoch - 954us/step\n",
      "Epoch 12/28\n",
      "252/252 - 0s - loss: 0.0304 - 242ms/epoch - 962us/step\n",
      "Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0301 - 203ms/epoch - 804us/step\n",
      "Epoch 25/28\n",
      "252/252 - 0s - loss: 0.0303 - 206ms/epoch - 817us/step\n",
      "Epoch 13/28\n",
      "252/252 - 0s - loss: 0.0303 - 210ms/epoch - 834us/step\n",
      "Epoch 27/28\n",
      "252/252 - 0s - loss: 0.0301 - 210ms/epoch - 832us/step\n",
      "Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0303 - 208ms/epoch - 827us/step\n",
      "Epoch 14/28\n",
      "252/252 - 0s - loss: 0.0303 - 206ms/epoch - 816us/step\n",
      "Epoch 28/28\n",
      "252/252 - 0s - loss: 0.0301 - 208ms/epoch - 827us/step\n",
      "Epoch 27/28\n",
      "252/252 - 0s - loss: 0.0302 - 215ms/epoch - 852us/step\n",
      "Epoch 15/28\n",
      "252/252 - 0s - loss: 0.0303 - 221ms/epoch - 875us/step\n",
      "252/252 - 0s - loss: 0.0300 - 210ms/epoch - 834us/step\n",
      "Epoch 28/28\n",
      "63/63 - 0s - loss: 0.0303 - 107ms/epoch - 2ms/step\n",
      "252/252 - 0s - loss: 0.0302 - 210ms/epoch - 835us/step\n",
      "Epoch 16/28\n",
      "252/252 - 0s - loss: 0.0304 - 110ms/epoch - 437us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16dc83130>; total time= 1.2min\n",
      "252/252 - 0s - loss: 0.0300 - 195ms/epoch - 775us/step\n",
      "252/252 - 0s - loss: 0.0302 - 191ms/epoch - 759us/step\n",
      "Epoch 17/28\n",
      "63/63 - 0s - loss: 0.0299 - 93ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0302 - 170ms/epoch - 676us/step\n",
      "Epoch 18/28\n",
      "252/252 - 0s - loss: 0.0299 - 100ms/epoch - 398us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16a103130>; total time= 1.2min\n",
      "252/252 - 0s - loss: 0.0302 - 164ms/epoch - 650us/step\n",
      "Epoch 19/28\n",
      "252/252 - 0s - loss: 0.0301 - 167ms/epoch - 661us/step\n",
      "Epoch 20/28\n",
      "252/252 - 0s - loss: 0.0301 - 282ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "252/252 - 0s - loss: 0.0301 - 164ms/epoch - 650us/step\n",
      "Epoch 22/28\n",
      "252/252 - 0s - loss: 0.0301 - 164ms/epoch - 652us/step\n",
      "Epoch 23/28\n",
      "252/252 - 0s - loss: 0.0300 - 161ms/epoch - 638us/step\n",
      "Epoch 24/28\n",
      "252/252 - 0s - loss: 0.0301 - 164ms/epoch - 652us/step\n",
      "Epoch 25/28\n",
      "252/252 - 0s - loss: 0.0300 - 161ms/epoch - 639us/step\n",
      "Epoch 26/28\n",
      "252/252 - 0s - loss: 0.0300 - 165ms/epoch - 655us/step\n",
      "Epoch 27/28\n",
      "252/252 - 0s - loss: 0.0300 - 161ms/epoch - 639us/step\n",
      "Epoch 28/28\n",
      "252/252 - 0s - loss: 0.0301 - 164ms/epoch - 650us/step\n",
      "63/63 - 0s - loss: 0.0300 - 85ms/epoch - 1ms/step\n",
      "252/252 - 0s - loss: 0.0300 - 217ms/epoch - 860us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1547c3130>; total time= 1.1min\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 4\n",
      "n_resources: 30174\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/28\n",
      " 322/1415 [=====>........................] - ETA: 3s - loss: 2.1897Epoch 1/28\n",
      " 350/1415 [======>.......................] - ETA: 3s - loss: 2.1646Epoch 1/28\n",
      "Epoch 1/28\n",
      " 365/1415 [======>.......................] - ETA: 3s - loss: 2.1521Epoch 1/28\n",
      " 380/1415 [=======>......................] - ETA: 3s - loss: 2.1402Epoch 1/28\n",
      "Epoch 1/28\n",
      " 389/1415 [=======>......................] - ETA: 3s - loss: 2.1332Epoch 1/28\n",
      "Epoch 1/28\n",
      "   1/1415 [..............................] - ETA: 7:12 - loss: 2.8794Epoch 1/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 1.6819 - val_loss: 1.2261TA: 0s - loss: 1.655\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 1.6344 - val_loss: 1.2125\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 1.6715 - val_loss: 1.2311\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 1.6725 - val_loss: 1.2370\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 1.6508 - val_loss: 1.2244\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 1.6884 - val_loss: 1.2342\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 1.6390 - val_loss: 1.2163\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 1.6708 - val_loss: 1.2331\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 1.6508 - val_loss: 1.2244\n",
      "  83/1415 [>.............................] - ETA: 1s - loss: 1.1970Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9428 - val_loss: 0.7075..] - ETA: 1s - loss: 1.1911s - loss: 1.136\n",
      " 606/1415 [===========>..................] - ETA: 1s - loss: 1.0818Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9468 - val_loss: 0.7107] - ETA: 0s - loss: 1.025\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9388 - val_loss: 0.7019\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.9512 - val_loss: 0.7139\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.9295 - val_loss: 0.6948\n",
      "  91/1415 [>.............................] - ETA: 2s - loss: 0.6986Epoch 3/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 1.6349 - val_loss: 1.2145\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.5465 - val_loss: 0.4128\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.9490 - val_loss: 0.7122\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.9324 - val_loss: 0.6969\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.9389 - val_loss: 0.7020\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.9483 - val_loss: 0.7118\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.3212 - val_loss: 0.2449..........] - ETA: 1s - loss: 0.610 - ETA: 0s - loss: 0.593\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.5406 - val_loss: 0.4066\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.5491 - val_loss: 0.4148\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.5349 - val_loss: 0.4021\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.5515 - val_loss: 0.4165\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.9309 - val_loss: 0.6956\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.5364 - val_loss: 0.4032ss: 0.217\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.5502 - val_loss: 0.4155\n",
      "Epoch 4/28\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.5405 - val_loss: 0.4063\n",
      " 548/1415 [==========>...................] - ETA: 2s - loss: 0.3766Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.5499 - val_loss: 0.4154\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.1925 - val_loss: 0.1489\n",
      " 492/1415 [=========>....................] - ETA: 5s - loss: 0.6332Epoch 6/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.3240 - val_loss: 0.2471\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.3228 - val_loss: 0.2461\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.3151 - val_loss: 0.2390\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.3115 - val_loss: 0.2361\n",
      "Epoch 5/28\n",
      " 757/1415 [===============>..............] - ETA: 3s - loss: 0.3532Epoch 5/28\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1188 - val_loss: 0.0937...........] - ETA: 2s - loss: 0.241 - ETA: 2s - loss: 0.\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.5352 - val_loss: 0.4020\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.3123 - val_loss: 0.2367\n",
      "1380/1415 [============================>.] - ETA: 0s - loss: 0.3252Epoch 7/28\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.3233 - val_loss: 0.2465\n",
      "Epoch 5/28\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.3148 - val_loss: 0.2386===>..........] - ETA: 1s - loss: 0.201\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.3233 - val_loss: 0.2466\n",
      " 169/1415 [==>...........................] - ETA: 2s - loss: 0.2297Epoch 5/28\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1871 - val_loss: 0.1438\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.1935 - val_loss: 0.1496\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.1944 - val_loss: 0.1504\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.1847 - val_loss: 0.1418\n",
      "Epoch 6/28\n",
      " 119/1415 [=>............................] - ETA: 3s - loss: 0.1411Epoch 6/28\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0764 - val_loss: 0.0618\n",
      " 328/1415 [=====>........................] - ETA: 3s - loss: 0.1424Epoch 8/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.1851 - val_loss: 0.14214s - loss: 0.131.19\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.1938 - val_loss: 0.1498\n",
      " 896/1415 [=================>............] - ETA: 2s - loss: 0.1239Epoch 6/28\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.3112 - val_loss: 0.2356\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.1940 - val_loss: 0.1501\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.1866 - val_loss: 0.1432\n",
      " 254/1415 [====>.........................] - ETA: 1s - loss: 0.1437Epoch 6/28\n",
      "Epoch 6/28\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.1143 - val_loss: 0.0895\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.1202 - val_loss: 0.0948\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0517 - val_loss: 0.0432\n",
      "Epoch 7/28\n",
      "Epoch 7/28\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.1194 - val_loss: 0.0941\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.1126 - val_loss: 0.0881\n",
      "Epoch 7/28\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.1196 - val_loss: 0.0943====>........] - ETA: 1s - loss: 0.119\n",
      "1141/1415 [=======================>......] - ETA: 1s - loss: 0.1186Epoch 7/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0373 - val_loss: 0.0322\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.1137 - val_loss: 0.0889\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.1841 - val_loss: 0.1411\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.1128 - val_loss: 0.0882\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.1198 - val_loss: 0.0945\n",
      "  69/1415 [>.............................] - ETA: 0s - loss: 0.0321Epoch 7/28\n",
      "Epoch 6/28\n",
      "Epoch 7/28\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0727 - val_loss: 0.058539.085\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0773 - val_loss: 0.0626\n",
      " 545/1415 [==========>...................] - ETA: 3s - loss: 0.0307Epoch 8/28\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0715 - val_loss: 0.0575 4s - loss: 0.083\n",
      " 199/1415 [===>..........................] - ETA: 1s - loss: 0.0610Epoch 8/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0767 - val_loss: 0.0621\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0287 - val_loss: 0.0256 4s - loss: 0.059 0.077\n",
      " 924/1415 [==================>...........] - ETA: 2s - loss: 0.0825Epoch 11/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0769 - val_loss: 0.0622\n",
      "1093/1415 [======================>.......] - ETA: 1s - loss: 0.0806Epoch 8/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0715 - val_loss: 0.0574\n",
      " 989/1415 [===================>..........] - ETA: 2s - loss: 0.0552Epoch 8/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.1118 - val_loss: 0.0873\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0721 - val_loss: 0.0578\n",
      "1160/1415 [=======================>......] - ETA: 1s - loss: 0.0504Epoch 7/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0771 - val_loss: 0.0625\n",
      "Epoch 8/28\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0520 - val_loss: 0.0434ETA: 1s - loss: 0.023\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0525 - val_loss: 0.0439\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0489 - val_loss: 0.0406\n",
      " 688/1415 [=============>................] - ETA: 3s - loss: 0.0569Epoch 9/28\n",
      "Epoch 9/28\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0235 - val_loss: 0.0216.] - ETA: 7s - loss: 0.043\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0479 - val_loss: 0.0398\n",
      " 167/1415 [==>...........................] - ETA: 2s - loss: 0.0431Epoch 12/28\n",
      " 862/1415 [=================>............] - ETA: 2s - loss: 0.0556Epoch 9/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0520 - val_loss: 0.0434..] - ETA: 4s - loss: 0.0413s - loss: 0.03\n",
      " 819/1415 [================>.............] - ETA: 2s - loss: 0.0402Epoch 9/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0479 - val_loss: 0.0398.] - ETA: 3s - loss: 0.036\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0482 - val_loss: 0.0400\n",
      "1238/1415 [=========================>....] - ETA: 0s - loss: 0.0204Epoch 9/28\n",
      " 887/1415 [=================>............] - ETA: 2s - loss: 0.0362Epoch 9/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0707 - val_loss: 0.0566\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0202 - val_loss: 0.0191\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0523 - val_loss: 0.0437\n",
      " 380/1415 [=======>......................] - ETA: 3s - loss: 0.0416Epoch 13/28\n",
      "Epoch 9/28\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0351 - val_loss: 0.0303\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0379 - val_loss: 0.03284\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0344 - val_loss: 0.0296\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0374 - val_loss: 0.0324\n",
      " 577/1415 [===========>..................] - ETA: 3s - loss: 0.0376Epoch 10/28\n",
      "Epoch 10/28\n",
      " 330/1415 [=====>........................] - ETA: 4s - loss: 0.0422Epoch 10/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0181 - val_loss: 0.0173s: 0.1415 [===========>..................] - ETA: 4s - loss: 0.030\n",
      "1387/1415 [============================>.] - ETA: 0s - loss: 0.0376Epoch 14/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0375 - val_loss: 0.0324\n",
      " 846/1415 [================>.............] - ETA: 2s - loss: 0.0301Epoch 10/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0345 - val_loss: 0.0297===>............] - ETA: 3s - loss: 0.027\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0343 - val_loss: 0.0295\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0471 - val_loss: 0.0390\n",
      "Epoch 10/28\n",
      "Epoch 10/28\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0378 - val_loss: 0.0327\n",
      " 817/1415 [================>.............] - ETA: 1s - loss: 0.0169Epoch 10/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0271 - val_loss: 0.0242\n",
      " 304/1415 [=====>........................] - ETA: 4s - loss: 0.0289Epoch 11/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0288 - val_loss: 0.0257\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0292 - val_loss: 0.0260\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0265 - val_loss: 0.0236\n",
      "Epoch 11/28\n",
      "Epoch 11/28\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0167 - val_loss: 0.0161\n",
      " 222/1415 [===>..........................] - ETA: 1s - loss: 0.0254Epoch 15/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0288 - val_loss: 0.0257.......] - ETA: 1s - loss: 0.022\n",
      " 927/1415 [==================>...........] - ETA: 1s - loss: 0.0158Epoch 11/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0291 - val_loss: 0.0260\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0264 - val_loss: 0.0235\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0265 - val_loss: 0.0237\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0336 - val_loss: 0.0288\n",
      "1260/1415 [=========================>....] - ETA: 0s - loss: 0.0225Epoch 11/28\n",
      "Epoch 11/28\n",
      "1015/1415 [====================>.........] - ETA: 2s - loss: 0.0223Epoch 11/28\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0157 - val_loss: 0.0152\n",
      "1292/1415 [==========================>...] - ETA: 0s - loss: 0.0220Epoch 16/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0224 - val_loss: 0.0206\n",
      "1240/1415 [=========================>....] - ETA: 0s - loss: 0.0238Epoch 12/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0218 - val_loss: 0.0201\n",
      "1375/1415 [============================>.] - ETA: 0s - loss: 0.0239Epoch 12/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0238 - val_loss: 0.0219.............] - ETA: 4s - loss: 0.025\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0236 - val_loss: 0.0217\n",
      " 616/1415 [============>.................] - ETA: 1s - loss: 0.0151Epoch 12/28\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0149 - val_loss: 0.0146oss: 0.02\n",
      "1249/1415 [=========================>....] - ETA: 0s - loss: 0.0260Epoch 17/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0236 - val_loss: 0.0217\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0218 - val_loss: 0.0201\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0257 - val_loss: 0.0230\n",
      " 852/1415 [=================>............] - ETA: 2s - loss: 0.0208Epoch 12/28\n",
      "1222/1415 [========================>.....] - ETA: 0s - loss: 0.0197Epoch 11/28\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0238 - val_loss: 0.0219\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0217 - val_loss: 0.0200\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0195 - val_loss: 0.0184\n",
      "Epoch 12/28\n",
      "1042/1415 [=====================>........] - ETA: 1s - loss: 0.0208Epoch 12/28\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0190 - val_loss: 0.0179- ETA: 0s - loss: 0.020\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0205 - val_loss: 0.0192\n",
      " 169/1415 [==>...........................] - ETA: 2s - loss: 0.0179Epoch 13/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0203 - val_loss: 0.0191\n",
      " 446/1415 [========>.....................] - ETA: 4s - loss: 0.0196Epoch 13/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0143 - val_loss: 0.0140\n",
      " 826/1415 [================>.............] - ETA: 2s - loss: 0.0208Epoch 18/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0205 - val_loss: 0.0193....] - ETA: 4s - loss: 0.018: 0s - loss: 0.02\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0211 - val_loss: 0.0194\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0178 - val_loss: 0.0170\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0203 - val_loss: 0.0191\n",
      " 863/1415 [=================>............] - ETA: 2s - loss: 0.0185Epoch 14/28\n",
      "Epoch 12/28\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0190 - val_loss: 0.0179\n",
      "Epoch 13/28\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0189 - val_loss: 0.0178=====>.........] - ETA: 1s - loss: 0.018\n",
      " 150/1415 [==>...........................] - ETA: 2s - loss: 0.0192Epoch 13/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0138 - val_loss: 0.0136\n",
      " 446/1415 [========>.....................] - ETA: 2s - loss: 0.0189Epoch 19/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0166\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0183 - val_loss: 0.0175\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0182 - val_loss: 0.0174\n",
      " 817/1415 [================>.............] - ETA: 2s - loss: 0.0186Epoch 14/28\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0166 - val_loss: 0.0161s - loss: 0.018\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0182 - val_loss: 0.0173\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0184 - val_loss: 0.0173\n",
      " 713/1415 [==============>...............] - ETA: 3s - loss: 0.0171Epoch 15/28\n",
      "1245/1415 [=========================>....] - ETA: 0s - loss: 0.0173Epoch 13/28\n",
      "Epoch 20/28\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0184 - val_loss: 0.0175ETA: 0s - loss: 0.017\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0173 - val_loss: 0.0166\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0172 - val_loss: 0.0165\n",
      "1062/1415 [=====================>........] - ETA: 1s - loss: 0.0163Epoch 14/28\n",
      "Epoch 14/28\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0162 - val_loss: 0.0157...] - ETA: 1s - loss: 0.0161s - loss: 0.016\n",
      " 341/1415 [======>.......................] - ETA: 3s - loss: 0.0174Epoch 15/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0168 - val_loss: 0.0162.] - ETA: 4s - loss: 0.015\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0168 - val_loss: 0.0162\n",
      " 694/1415 [=============>................] - ETA: 3s - loss: 0.0163Epoch 15/28\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0168 - val_loss: 0.0162\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0167 - val_loss: 0.0160\n",
      "1323/1415 [===========================>..] - ETA: 0s - loss: 0.0169Epoch 14/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0158 - val_loss: 0.0154\n",
      "Epoch 15/28\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0169 - val_loss: 0.0162\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0162 - val_loss: 0.0156\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0161 - val_loss: 0.0157\n",
      " 825/1415 [================>.............] - ETA: 2s - loss: 0.0160Epoch 15/28\n",
      "Epoch 15/28\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0127 - val_loss: 0.0126\n",
      "1252/1415 [=========================>....] - ETA: 0s - loss: 0.0158Epoch 22/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0155 - val_loss: 0.0151\n",
      " 340/1415 [======>.......................] - ETA: 3s - loss: 0.0155Epoch 16/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0157 - val_loss: 0.0153\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0158 - val_loss: 0.0153\n",
      "Epoch 16/28\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0157 - val_loss: 0.0152- loss: 0.01515\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0158 - val_loss: 0.0153\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0153 - val_loss: 0.0149\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0125 - val_loss: 0.0123\n",
      " 667/1415 [=============>................] - ETA: 3s - loss: 0.0152Epoch 15/28\n",
      "Epoch 16/28\n",
      "Epoch 23/28\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0158 - val_loss: 0.0154\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0154 - val_loss: 0.0150\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0153 - val_loss: 0.0149\n",
      " 861/1415 [=================>............] - ETA: 2s - loss: 0.0151Epoch 16/28\n",
      "Epoch 16/28\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0149 - val_loss: 0.0146\n",
      " 527/1415 [==========>...................] - ETA: 3s - loss: 0.0151Epoch 17/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "1330/1415 [===========================>..] - ETA: 0s - loss: 0.0122Epoch 17/28\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0122 - val_loss: 0.0120\n",
      " 907/1415 [==================>...........] - ETA: 2s - loss: 0.0151Epoch 24/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0149 - val_loss: 0.0146=====>.] - ETA: 0s - loss: 0.014\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0149 - val_loss: 0.0146\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0149 - val_loss: 0.0146\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "1409/1415 [============================>.] - ETA: 0s - loss: 0.0150Epoch 16/28\n",
      "Epoch 17/28\n",
      " 730/1415 [==============>...............] - ETA: 3s - loss: 0.0145Epoch 17/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0150 - val_loss: 0.0147\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0148 - val_loss: 0.0145\n",
      "1069/1415 [=====================>........] - ETA: 1s - loss: 0.0120Epoch 17/28\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0120 - val_loss: 0.0118 loss: 0.014\n",
      " 361/1415 [======>.......................] - ETA: 3s - loss: 0.0146Epoch 25/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0145 - val_loss: 0.0143\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0144 - val_loss: 0.0141s - loss: 0.014\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0144 - val_loss: 0.0141\n",
      " 267/1415 [====>.........................] - ETA: 1s - loss: 0.0118Epoch 18/28\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0145 - val_loss: 0.0143 5s - loss: 0.014 0.014\n",
      "1211/1415 [========================>.....] - ETA: 1s - loss: 0.0145Epoch 19/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0117 - val_loss: 0.01160s - loss: 0.014\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0145 - val_loss: 0.0142\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0144 - val_loss: 0.0141\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0144 - val_loss: 0.0141\n",
      "Epoch 26/28\n",
      "Epoch 17/28\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0145 - val_loss: 0.0143\n",
      "Epoch 18/28\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0144 - val_loss: 0.0141\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0139 - val_loss: 0.0136......] - ETA: 1s - loss: 0.013 ETA: 1s - loss: 0.011\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0142 - val_loss: 0.0140\n",
      " 485/1415 [=========>....................] - ETA: 4s - loss: 0.0144Epoch 19/28\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0139 - val_loss: 0.0137............] - ETA: 4s - loss: 0.013\n",
      " 167/1415 [==>...........................] - ETA: 1s - loss: 0.0141Epoch 19/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0115 - val_loss: 0.0114\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0143 - val_loss: 0.0140\n",
      " 474/1415 [=========>....................] - ETA: 4s - loss: 0.0135Epoch 27/28\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0142 - val_loss: 0.0139\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0139 - val_loss: 0.0137\n",
      " 514/1415 [=========>....................] - ETA: 2s - loss: 0.0114Epoch 19/28\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0141 - val_loss: 0.0138\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0141 - val_loss: 0.0139\n",
      " 245/1415 [====>.........................] - ETA: 1s - loss: 0.0136Epoch 19/28\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0139 - val_loss: 0.0136s - loss: 0.011\n",
      "1136/1415 [=======================>......] - ETA: 1s - loss: 0.0135Epoch 19/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0140 - val_loss: 0.0138\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0113 - val_loss: 0.0112\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      " 491/1415 [=========>....................] - ETA: 4s - loss: 0.0138Epoch 28/28\n",
      " 331/1415 [======>.......................] - ETA: 4s - loss: 0.0136Epoch 20/28\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0135 - val_loss: 0.0133\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0140 - val_loss: 0.0139\n",
      " 510/1415 [=========>....................] - ETA: 3s - loss: 0.0132Epoch 21/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0135 - val_loss: 0.0133 - ETA: 1s - loss: 0.013\n",
      " 688/1415 [=============>................] - ETA: 3s - loss: 0.0138Epoch 20/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0139 - val_loss: 0.0137\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0138 - val_loss: 0.0137\n",
      "1380/1415 [============================>.] - ETA: 0s - loss: 0.0138Epoch 20/28\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0138 - val_loss: 0.0136\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0135 - val_loss: 0.0133\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0111 - val_loss: 0.0110\n",
      " 243/1415 [====>.........................] - ETA: 1s - loss: 0.0138Epoch 20/28\n",
      "Epoch 20/28\n",
      "1228/1415 [=========================>....] - ETA: 0s - loss: 0.0132Epoch 1/28..........] - ETA: 3s - loss: 0.013\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0137 - val_loss: 0.0135\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      " 907/1415 [==================>...........] - ETA: 2s - loss: 0.0139Epoch 21/28\n",
      "Epoch 21/28\n",
      " 603/1415 [===========>..................] - ETA: 3s - loss: 0.0132755/755 - 1s - loss: 0.0110 - 636ms/epoch - 843us/step\n",
      "Epoch 2/28\n",
      " 903/1415 [==================>...........] - ETA: 1s - loss: 0.0138755/755 - 0s - loss: 0.0109 - 297ms/epoch - 393us/step\n",
      "Epoch 3/28\n",
      "1275/1415 [==========================>...] - ETA: 0s - loss: 0.0132755/755 - 1s - loss: 0.0108 - 524ms/epoch - 694us/stepss: 0.013\n",
      "Epoch 4/28\n",
      "755/755 - 0s - loss: 0.0107 - 301ms/epoch - 399us/step\n",
      "1290/1415 [==========================>...] - ETA: 0s - loss: 0.0132Epoch 5/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      " 912/1415 [==================>...........] - ETA: 2s - loss: 0.0136Epoch 21/28\n",
      " 182/1415 [==>...........................] - ETA: 1s - loss: 0.0130755/755 - 1s - loss: 0.0107 - 785ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0131 - val_loss: 0.0130\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0138 - val_loss: 0.0136\n",
      "Epoch 6/28\n",
      " 185/1415 [==>...........................] - ETA: 2s - loss: 0.0130Epoch 22/28\n",
      " 935/1415 [==================>...........] - ETA: 2s - loss: 0.0137Epoch 21/28\n",
      "1249/1415 [=========================>....] - ETA: 0s - loss: 0.0131755/755 - 1s - loss: 0.0106 - 889ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 365/1415 [======>.......................] - ETA: 3s - loss: 0.0130755/755 - 0s - loss: 0.0105 - 298ms/epoch - 395us/step\n",
      " 754/1415 [==============>...............] - ETA: 2s - loss: 0.0135Epoch 8/28\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0137 - val_loss: 0.0136\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0136 - val_loss: 0.013312\n",
      " 567/1415 [===========>..................] - ETA: 3s - loss: 0.0129755/755 - 1s - loss: 0.0104 - 531ms/epoch - 703us/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 9/28\n",
      "Epoch 20/28\n",
      "Epoch 21/28\n",
      " 483/1415 [=========>....................] - ETA: 4s - loss: 0.0129755/755 - 0s - loss: 0.0104 - 462ms/epoch - 612us/step\n",
      "1112/1415 [======================>.......] - ETA: 1s - loss: 0.0128Epoch 10/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0136 - val_loss: 0.0134\n",
      " 357/1415 [======>.......................] - ETA: 3s - loss: 0.0135Epoch 21/28\n",
      " 369/1415 [======>.......................] - ETA: 3s - loss: 0.0134755/755 - 1s - loss: 0.0103 - 1s/epoch - 1ms/step\n",
      " 372/1415 [======>.......................] - ETA: 3s - loss: 0.0128Epoch 11/28\n",
      " 545/1415 [==========>...................] - ETA: 3s - loss: 0.0134755/755 - 1s - loss: 0.0102 - 965ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0127 - val_loss: 0.0126\n",
      " 707/1415 [=============>................] - ETA: 3s - loss: 0.0135Epoch 12/28\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0135 - val_loss: 0.0133\n",
      " 349/1415 [======>.......................] - ETA: 3s - loss: 0.0135Epoch 22/28\n",
      " 165/1415 [==>...........................] - ETA: 1s - loss: 0.0132755/755 - 0s - loss: 0.0102 - 465ms/epoch - 616us/step\n",
      " 715/1415 [==============>...............] - ETA: 3s - loss: 0.0134Epoch 13/28\n",
      " 343/1415 [======>.......................] - ETA: 3s - loss: 0.0125755/755 - 1s - loss: 0.0101 - 998ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0128 - val_loss: 0.0126\n",
      " 758/1415 [===============>..............] - ETA: 2s - loss: 0.0134Epoch 14/28\n",
      "1400/1415 [============================>.] - ETA: 0s - loss: 0.0136Epoch 22/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0136 - val_loss: 0.0134\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0128 - val_loss: 0.0127\n",
      " 175/1415 [==>...........................] - ETA: 3s - loss: 0.0127755/755 - 1s - loss: 0.0100 - 623ms/epoch - 825us/step\n",
      "Epoch 22/28\n",
      "Epoch 23/28\n",
      "Epoch 15/28\n",
      " 777/1415 [===============>..............] - ETA: 2s - loss: 0.0133755/755 - 0s - loss: 0.0100 - 479ms/epoch - 635us/step\n",
      " 326/1415 [=====>........................] - ETA: 4s - loss: 0.0127Epoch 16/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0135 - val_loss: 0.0133\n",
      " 810/1415 [================>.............] - ETA: 2s - loss: 0.0133Epoch 22/28\n",
      "1009/1415 [====================>.........] - ETA: 1s - loss: 0.0133755/755 - 1s - loss: 0.0099 - 984ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0128 - val_loss: 0.0126\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      " 853/1415 [=================>............] - ETA: 2s - loss: 0.0125Epoch 22/28\n",
      " 393/1415 [=======>......................] - ETA: 3s - loss: 0.0126Epoch 21/28\n",
      " 570/1415 [===========>..................] - ETA: 3s - loss: 0.0126755/755 - 1s - loss: 0.0098 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 201/1415 [===>..........................] - ETA: 1s - loss: 0.0126755/755 - 0s - loss: 0.0098 - 301ms/epoch - 398us/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 19/28\n",
      " 721/1415 [==============>...............] - ETA: 3s - loss: 0.0126Epoch 22/28\n",
      "1228/1415 [=========================>....] - ETA: 0s - loss: 0.0125755/755 - 1s - loss: 0.0097 - 584ms/epoch - 773us/step\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0133 - val_loss: 0.0131\n",
      " 893/1415 [=================>............] - ETA: 2s - loss: 0.0126Epoch 20/28\n",
      " 726/1415 [==============>...............] - ETA: 3s - loss: 0.0135Epoch 23/28\n",
      "1107/1415 [======================>.......] - ETA: 1s - loss: 0.0126755/755 - 0s - loss: 0.0097 - 475ms/epoch - 630us/step\n",
      " 435/1415 [========>.....................] - ETA: 2s - loss: 0.0133Epoch 21/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0124 - val_loss: 0.0123\n",
      "Epoch 23/28\n",
      " 751/1415 [==============>...............] - ETA: 2s - loss: 0.0132755/755 - 1s - loss: 0.0096 - 509ms/epoch - 674us/step\n",
      " 952/1415 [===================>..........] - ETA: 2s - loss: 0.0134Epoch 22/28\n",
      " 926/1415 [==================>...........] - ETA: 2s - loss: 0.0132755/755 - 1s - loss: 0.0096 - 606ms/epoch - 803us/step13\n",
      "Epoch 23/28\n",
      "755/755 - 0s - loss: 0.0095 - 299ms/epoch - 396us/step\n",
      "1379/1415 [============================>.] - ETA: 0s - loss: 0.0125Epoch 24/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0125 - val_loss: 0.0124\n",
      "1275/1415 [==========================>...] - ETA: 0s - loss: 0.0134Epoch 23/28\n",
      " 774/1415 [===============>..............] - ETA: 2s - loss: 0.0132755/755 - 1s - loss: 0.0095 - 622ms/epoch - 824us/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0125 - val_loss: 0.0124\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "1131/1415 [======================>.......] - ETA: 1s - loss: 0.0132Epoch 25/28\n",
      "1078/1415 [=====================>........] - ETA: 1s - loss: 0.0125Epoch 24/28\n",
      "1028/1415 [====================>.........] - ETA: 1s - loss: 0.0132Epoch 23/28\n",
      "1249/1415 [=========================>....] - ETA: 0s - loss: 0.0125755/755 - 1s - loss: 0.0094 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0133 - val_loss: 0.0134\n",
      "755/755 - 0s - loss: 0.0094 - 388ms/epoch - 514us/step\n",
      " 199/1415 [===>..........................] - ETA: 1s - loss: 0.0133Epoch 27/28\n",
      "1288/1415 [==========================>...] - ETA: 0s - loss: 0.0125Epoch 23/28\n",
      " 656/1415 [============>.................] - ETA: 2s - loss: 0.0123755/755 - 0s - loss: 0.0093 - 475ms/epoch - 629us/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0125 - val_loss: 0.0123\n",
      " 932/1415 [==================>...........] - ETA: 2s - loss: 0.0122Epoch 28/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "1233/1415 [=========================>....] - ETA: 0s - loss: 0.0132Epoch 23/28\n",
      "Epoch 22/28\n",
      " 542/1415 [==========>...................] - ETA: 3s - loss: 0.0124755/755 - 1s - loss: 0.0093 - 593ms/epoch - 786us/step\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "189/189 - 0s - loss: 0.0093 - 113ms/epoch - 600us/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      " 150/1415 [==>...........................] - ETA: 3s - loss: 0.0130Epoch 24/28\n",
      "Epoch 23/28\n",
      " 699/1415 [=============>................] - ETA: 3s - loss: 0.0123755/755 - 0s - loss: 0.0092 - 409ms/epoch - 542us/step\n",
      " 363/1415 [======>.......................] - ETA: 3s - loss: 0.0123[CV] END activation=relu, kernel_regularizer=l2, optimizer=sgd; total time= 3.0min\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0122 - val_loss: 0.0120\n",
      " 726/1415 [==============>...............] - ETA: 3s - loss: 0.0132Epoch 24/28\n",
      "1415/1415 [==============================] - 6s 5ms/step - loss: 0.0123 - val_loss: 0.0121TA: 3s - loss: 0.012\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0133 - val_loss: 0.0131\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0123 - val_loss: 0.0121\n",
      "Epoch 25/28\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "1230/1415 [=========================>....] - ETA: 0s - loss: 0.0130Epoch 24/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0130 - val_loss: 0.0128.........] - ETA: 1s - loss: 0.012\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0122 - val_loss: 0.0121\n",
      "Epoch 23/28\n",
      "Epoch 25/28\n",
      "Epoch 24/28\n",
      " 366/1415 [======>.......................] - ETA: 3s - loss: 0.0130Epoch 1/28=========>...................] - ETA: 3s - loss: 0.013\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0119 - val_loss: 0.0118 - ETA: 4s - loss: 0.012\n",
      " 513/1415 [=========>....................] - ETA: 4s - loss: 0.0120Epoch 25/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0120 - val_loss: 0.0119\n",
      " 686/1415 [=============>................] - ETA: 3s - loss: 0.0120Epoch 25/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0120 - val_loss: 0.0119...] - ETA: 1s - loss: 0.0123s - loss: 0.012\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 25/28\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0130 - val_loss: 0.0128......] - ETA: 2s - loss: 0.012\n",
      "1204/1415 [========================>.....] - ETA: 1s - loss: 0.0120Epoch 25/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0128 - val_loss: 0.0127\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0129 - val_loss: 0.0127\n",
      " 690/1415 [=============>................] - ETA: 3s - loss: 0.0119Epoch 26/28\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0120 - val_loss: 0.0118011\n",
      "1073/1415 [=====================>........] - ETA: 1s - loss: 0.0118Epoch 25/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0129 - val_loss: 0.0127\n",
      " 166/1415 [==>...........................] - ETA: 1s - loss: 0.0118Epoch 25/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0117 - val_loss: 0.0116 0.012\n",
      " 720/1415 [==============>...............] - ETA: 3s - loss: 0.0129Epoch 26/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0118 - val_loss: 0.0117..............] - ETA: 4s - loss: 0.011\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0130 - val_loss: 0.0128\n",
      "1185/1415 [========================>.....] - ETA: 1s - loss: 0.0127Epoch 27/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.2273 - val_loss: 0.0183oss: 0.011\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0129 - val_loss: 0.0127\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0127 - val_loss: 0.0125\n",
      "1288/1415 [==========================>...] - ETA: 0s - loss: 0.0117Epoch 27/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0127 - val_loss: 0.0126\n",
      "1144/1415 [=======================>......] - ETA: 1s - loss: 0.0127Epoch 25/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      " 300/1415 [=====>........................] - ETA: 7s - loss: 0.0176Epoch 26/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0127 - val_loss: 0.0126\n",
      " 480/1415 [=========>....................] - ETA: 4s - loss: 0.0128Epoch 26/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0115 - val_loss: 0.0114\n",
      " 708/1415 [==============>...............] - ETA: 3s - loss: 0.0126Epoch 27/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0116 - val_loss: 0.0115 1s - loss: 0.011\n",
      " 479/1415 [=========>....................] - ETA: 4s - loss: 0.0126Epoch 27/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0116 - val_loss: 0.0115\n",
      " 819/1415 [================>.............] - ETA: 3s - loss: 0.0126Epoch 27/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0128 - val_loss: 0.0126\n",
      "1168/1415 [=======================>......] - ETA: 1s - loss: 0.0126Epoch 28/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0127 - val_loss: 0.0125\n",
      " 136/1415 [=>............................] - ETA: 2s - loss: 0.0115Epoch 27/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0126 - val_loss: 0.0124- ETA: 1s - loss: 0.016\n",
      "1352/1415 [===========================>..] - ETA: 0s - loss: 0.0115Epoch 28/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0115 - val_loss: 0.0114\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0160 - val_loss: 0.0149\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0126 - val_loss: 0.0124\n",
      " 827/1415 [================>.............] - ETA: 3s - loss: 0.0114Epoch 26/28\n",
      "Epoch 3/28\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0126 - val_loss: 0.0124\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0113 - val_loss: 0.0112: 0.0\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0114 - val_loss: 0.0113\n",
      " 648/1415 [============>.................] - ETA: 4s - loss: 0.0125Epoch 28/28\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0114 - val_loss: 0.0113 loss: 0.011\n",
      "1217/1415 [========================>.....] - ETA: 1s - loss: 0.0126Epoch 28/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0124 - val_loss: 0.0123\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0127 - val_loss: 0.0125\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0126 - val_loss: 0.0124\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0125 - val_loss: 0.0123....] - ETA: 1s - loss: 0.012\n",
      "1164/1415 [=======================>......] - ETA: 1s - loss: 0.0134Epoch 27/28\n",
      " 556/1415 [==========>...................] - ETA: 2s - loss: 0.0125Epoch 1/28\n",
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0132 - val_loss: 0.0117\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0113 - val_loss: 0.0112\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0125 - val_loss: 0.0123\n",
      " 208/1415 [===>..........................] - ETA: 2s - loss: 0.0124Epoch 4/28\n",
      "Epoch 28/28\n",
      "Epoch 28/28\n",
      " 814/1415 [================>.............] - ETA: 2s - loss: 0.0113755/755 - 1s - loss: 0.0125 - 666ms/epoch - 883us/step\n",
      " 817/1415 [================>.............] - ETA: 1s - loss: 0.0125755/755 - 1s - loss: 0.0125 - 671ms/epoch - 889us/step\n",
      "Epoch 2/28\n",
      "Epoch 2/28\n",
      " 414/1415 [=======>......................] - ETA: 3s - loss: 0.0112755/755 - 1s - loss: 0.0125 - 585ms/epoch - 775us/step\n",
      "Epoch 3/28\n",
      " 395/1415 [=======>......................] - ETA: 3s - loss: 0.0125755/755 - 1s - loss: 0.0124 - 618ms/epoch - 819us/step\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0111 - val_loss: 0.0110\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0112 - val_loss: 0.0111\n",
      "755/755 - 0s - loss: 0.0124 - 306ms/epoch - 406us/step\n",
      "Epoch 3/28\n",
      " 458/1415 [========>.....................] - ETA: 2s - loss: 0.0112Epoch 4/28\n",
      "1301/1415 [==========================>...] - ETA: 0s - loss: 0.0112755/755 - 1s - loss: 0.0123 - 612ms/epoch - 811us/step\n",
      "755/755 - 1s - loss: 0.0124 - 612ms/epoch - 810us/step\n",
      "Epoch 4/28\n",
      "Epoch 5/28\n",
      " 726/1415 [==============>...............] - ETA: 2s - loss: 0.0112755/755 - 0s - loss: 0.0123 - 348ms/epoch - 460us/step\n",
      " 745/1415 [==============>...............] - ETA: 2s - loss: 0.0121755/755 - 0s - loss: 0.0123 - 352ms/epoch - 466us/step\n",
      " 880/1415 [=================>............] - ETA: 2s - loss: 0.0124Epoch 6/28\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0112 - val_loss: 0.0111\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0125 - val_loss: 0.0123\n",
      "1112/1415 [======================>.......] - ETA: 1s - loss: 0.0112755/755 - 1s - loss: 0.0122 - 720ms/epoch - 954us/step\n",
      "Epoch 7/28\n",
      "755/755 - 1s - loss: 0.0122 - 727ms/epoch - 964us/step\n",
      "1180/1415 [========================>.....] - ETA: 0s - loss: 0.0112Epoch 6/28\n",
      "1110/1415 [======================>.......] - ETA: 1s - loss: 0.0119755/755 - 0s - loss: 0.0122 - 390ms/epoch - 517us/step\n",
      "Epoch 8/28\n",
      "1411/1415 [============================>.] - ETA: 0s - loss: 0.0119Epoch 1/28\n",
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0123 - val_loss: 0.0122\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0111 - val_loss: 0.0110\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0123 - val_loss: 0.0122\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0119 - val_loss: 0.0102\n",
      "Epoch 5/28\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.0128755/755 - 1s - loss: 0.0122 - 1s/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "  45/1415 [..............................] - ETA: 1s - loss: 0.0113755/755 - 1s - loss: 0.0121 - 926ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 525/1415 [==========>...................] - ETA: 1s - loss: 0.0122755/755 - 1s - loss: 0.0111 - 1s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 561/1415 [==========>...................] - ETA: 1s - loss: 0.0122755/755 - 1s - loss: 0.0110 - 1s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 667/1415 [=============>................] - ETA: 1s - loss: 0.0109755/755 - 2s - loss: 0.0121 - 2s/epoch - 2ms/step\n",
      " 669/1415 [=============>................] - ETA: 1s - loss: 0.0109Epoch 8/28\n",
      " 768/1415 [===============>..............] - ETA: 1s - loss: 0.0109755/755 - 2s - loss: 0.0121 - 2s/epoch - 3ms/step\n",
      "Epoch 10/28\n",
      " 966/1415 [===================>..........] - ETA: 1s - loss: 0.0109Epoch 1/28\n",
      "1050/1415 [=====================>........] - ETA: 0s - loss: 0.0109755/755 - 1s - loss: 0.0109 - 1s/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "1194/1415 [========================>.....] - ETA: 0s - loss: 0.0122Epoch 1/28\n",
      "1081/1415 [=====================>........] - ETA: 0s - loss: 0.0109755/755 - 2s - loss: 0.0110 - 2s/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "1345/1415 [===========================>..] - ETA: 0s - loss: 0.0108755/755 - 1s - loss: 0.0121 - 1s/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - ETA: 0s - loss: 0.0108755/755 - 1s - loss: 0.0120 - 1s/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0122 - val_loss: 0.0121\n",
      "  98/1415 [=>............................] - ETA: 2s - loss: 0.0102755/755 - 1s - loss: 0.0108 - 1s/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      " 139/1415 [=>............................] - ETA: 4s - loss: 0.0101755/755 - 2s - loss: 0.0123 - 2s/epoch - 2ms/step\n",
      " 140/1415 [=>............................] - ETA: 5s - loss: 0.0101Epoch 2/28\n",
      "755/755 - 2s - loss: 0.0109 - 2s/epoch - 2ms/step\n",
      "Epoch 4/28\n",
      " 188/1415 [==>...........................] - ETA: 4s - loss: 0.0102755/755 - 2s - loss: 0.0111 - 2s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 528/1415 [==========>...................] - ETA: 2s - loss: 0.0101755/755 - 2s - loss: 0.0120 - 2s/epoch - 3ms/step\n",
      "Epoch 10/28\n",
      " 612/1415 [===========>..................] - ETA: 2s - loss: 0.0100755/755 - 2s - loss: 0.0120 - 2s/epoch - 3ms/step\n",
      "Epoch 12/28\n",
      " 824/1415 [================>.............] - ETA: 1s - loss: 0.0102755/755 - 2s - loss: 0.0108 - 2s/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      " 914/1415 [==================>...........] - ETA: 1s - loss: 0.0101755/755 - 1s - loss: 0.0108 - 1s/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "755/755 - 1s - loss: 0.0123 - 1s/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      " 982/1415 [===================>..........] - ETA: 0s - loss: 0.0102755/755 - 1s - loss: 0.0110 - 1s/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "1207/1415 [========================>.....] - ETA: 0s - loss: 0.0101755/755 - 1s - loss: 0.0119 - 892ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "1255/1415 [=========================>....] - ETA: 0s - loss: 0.0101755/755 - 1s - loss: 0.0119 - 750ms/epoch - 993us/step\n",
      "Epoch 13/28\n",
      "1342/1415 [===========================>..] - ETA: 0s - loss: 0.0101Epoch 1/28\n",
      "1371/1415 [============================>.] - ETA: 0s - loss: 0.0100Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0101 - val_loss: 0.0087\n",
      "Epoch 7/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0085755/755 - 1s - loss: 0.0107 - 886ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "755/755 - 1s - loss: 0.0122 - 902ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "  95/1415 [=>............................] - ETA: 1s - loss: 0.0095755/755 - 1s - loss: 0.0107 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 145/1415 [==>...........................] - ETA: 1s - loss: 0.0095755/755 - 1s - loss: 0.0109 - 909ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 362/1415 [======>.......................] - ETA: 1s - loss: 0.0098755/755 - 1s - loss: 0.0119 - 933ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 474/1415 [=========>....................] - ETA: 1s - loss: 0.0099755/755 - 1s - loss: 0.0119 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 509/1415 [=========>....................] - ETA: 1s - loss: 0.0098755/755 - 1s - loss: 0.0110 - 1s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 587/1415 [===========>..................] - ETA: 1s - loss: 0.0098755/755 - 1s - loss: 0.0122 - 1s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 807/1415 [================>.............] - ETA: 0s - loss: 0.0097755/755 - 1s - loss: 0.0107 - 1s/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      " 850/1415 [=================>............] - ETA: 0s - loss: 0.0097755/755 - 1s - loss: 0.0121 - 1s/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      " 942/1415 [==================>...........] - ETA: 0s - loss: 0.0096755/755 - 1s - loss: 0.0106 - 1s/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "755/755 - 1s - loss: 0.0108 - 1s/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "1149/1415 [=======================>......] - ETA: 0s - loss: 0.0095755/755 - 1s - loss: 0.0118 - 1s/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1279/1415 [==========================>...] - ETA: 0s - loss: 0.0095755/755 - 1s - loss: 0.0118 - 1s/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "755/755 - 1s - loss: 0.0109 - 971ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1341/1415 [===========================>..] - ETA: 0s - loss: 0.0094755/755 - 1s - loss: 0.0122 - 992ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - ETA: 0s - loss: 0.0094755/755 - 1s - loss: 0.0106 - 954ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0094 - val_loss: 0.0119\n",
      "Epoch 8/28\n",
      "  87/1415 [>.............................] - ETA: 1s - loss: 0.0096755/755 - 1s - loss: 0.0121 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 135/1415 [=>............................] - ETA: 1s - loss: 0.0094755/755 - 1s - loss: 0.0107 - 999ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "755/755 - 1s - loss: 0.0105 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      " 308/1415 [=====>........................] - ETA: 1s - loss: 0.0090755/755 - 1s - loss: 0.0118 - 937ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 474/1415 [=========>....................] - ETA: 1s - loss: 0.0090755/755 - 1s - loss: 0.0108 - 945ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 510/1415 [=========>....................] - ETA: 1s - loss: 0.0090755/755 - 1s - loss: 0.0118 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 616/1415 [============>.................] - ETA: 0s - loss: 0.0091755/755 - 1s - loss: 0.0121 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 740/1415 [==============>...............] - ETA: 0s - loss: 0.0092755/755 - 1s - loss: 0.0105 - 972ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 922/1415 [==================>...........] - ETA: 0s - loss: 0.0092755/755 - 1s - loss: 0.0104 - 1s/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      " 954/1415 [===================>..........] - ETA: 0s - loss: 0.0092755/755 - 1s - loss: 0.0120 - 1s/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      " 992/1415 [====================>.........] - ETA: 0s - loss: 0.0092755/755 - 1s - loss: 0.0106 - 1s/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1010/1415 [====================>.........] - ETA: 0s - loss: 0.0091Epoch 1/28\n",
      "1153/1415 [=======================>......] - ETA: 0s - loss: 0.0091755/755 - 1s - loss: 0.0118 - 1s/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      "1189/1415 [========================>.....] - ETA: 0s - loss: 0.0091755/755 - 1s - loss: 0.0108 - 1s/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "1304/1415 [==========================>...] - ETA: 0s - loss: 0.0092755/755 - 1s - loss: 0.0117 - 1s/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "755/755 - 1s - loss: 0.0121 - 1s/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "1383/1415 [============================>.] - ETA: 0s - loss: 0.0092755/755 - 1s - loss: 0.0104 - 1s/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 9/28\n",
      "  78/1415 [>.............................] - ETA: 1s - loss: 0.0090755/755 - 1s - loss: 0.0104 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 143/1415 [==>...........................] - ETA: 3s - loss: 0.0089755/755 - 1s - loss: 0.0120 - 1s/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      " 165/1415 [==>...........................] - ETA: 3s - loss: 0.0089755/755 - 1s - loss: 0.0121 - 1s/epoch - 2ms/step\n",
      "Epoch 2/28\n",
      " 183/1415 [==>...........................] - ETA: 3s - loss: 0.0088755/755 - 1s - loss: 0.0105 - 1s/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      " 327/1415 [=====>........................] - ETA: 2s - loss: 0.0086755/755 - 1s - loss: 0.0117 - 1s/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      " 367/1415 [======>.......................] - ETA: 2s - loss: 0.0086755/755 - 1s - loss: 0.0107 - 1s/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      " 407/1415 [=======>......................] - ETA: 1s - loss: 0.0086755/755 - 1s - loss: 0.0120 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 450/1415 [========>.....................] - ETA: 1s - loss: 0.0086755/755 - 1s - loss: 0.0117 - 1s/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      " 607/1415 [===========>..................] - ETA: 1s - loss: 0.0087755/755 - 1s - loss: 0.0103 - 1s/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      " 878/1415 [=================>............] - ETA: 0s - loss: 0.0087755/755 - 1s - loss: 0.0121 - 984ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      " 914/1415 [==================>...........] - ETA: 0s - loss: 0.0087755/755 - 1s - loss: 0.0103 - 1s/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "755/755 - 1s - loss: 0.0119 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 951/1415 [===================>..........] - ETA: 0s - loss: 0.0087755/755 - 1s - loss: 0.0105 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1154/1415 [=======================>......] - ETA: 0s - loss: 0.0087755/755 - 1s - loss: 0.0106 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "755/755 - 1s - loss: 0.0117 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1196/1415 [========================>.....] - ETA: 0s - loss: 0.0087755/755 - 1s - loss: 0.0119 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1277/1415 [==========================>...] - ETA: 0s - loss: 0.0087755/755 - 1s - loss: 0.0116 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1392/1415 [============================>.] - ETA: 0s - loss: 0.0087755/755 - 1s - loss: 0.0103 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0087 - val_loss: 0.0075\n",
      "Epoch 10/28\n",
      "   1/1415 [..............................] - ETA: 3s - loss: 0.0076755/755 - 1s - loss: 0.0120 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "755/755 - 1s - loss: 0.0102 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "  25/1415 [..............................] - ETA: 2s - loss: 0.0100755/755 - 1s - loss: 0.0119 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "  53/1415 [>.............................] - ETA: 2s - loss: 0.0105755/755 - 1s - loss: 0.0104 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 223/1415 [===>..........................] - ETA: 1s - loss: 0.0093755/755 - 1s - loss: 0.0119 - 925ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "755/755 - 1s - loss: 0.0116 - 970ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "755/755 - 1s - loss: 0.0105 - 1s/epoch - 1ms/step\n",
      " 267/1415 [====>.........................] - ETA: 1s - loss: 0.0094Epoch 8/28\n",
      " 362/1415 [======>.......................] - ETA: 1s - loss: 0.0092755/755 - 1s - loss: 0.0116 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 493/1415 [=========>....................] - ETA: 1s - loss: 0.0089755/755 - 1s - loss: 0.0102 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 734/1415 [==============>...............] - ETA: 0s - loss: 0.0086755/755 - 1s - loss: 0.0120 - 1s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "755/755 - 1s - loss: 0.0103 - 979ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 774/1415 [===============>..............] - ETA: 0s - loss: 0.0085755/755 - 1s - loss: 0.0101 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "755/755 - 1s - loss: 0.0118 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 987/1415 [===================>..........] - ETA: 0s - loss: 0.0084755/755 - 1s - loss: 0.0118 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1070/1415 [=====================>........] - ETA: 0s - loss: 0.0084755/755 - 1s - loss: 0.0116 - 1s/epoch - 2ms/step\n",
      "Epoch 19/28\n",
      "1111/1415 [======================>.......] - ETA: 0s - loss: 0.0084755/755 - 1s - loss: 0.0104 - 1s/epoch - 2ms/step\n",
      "Epoch 9/28\n",
      "1217/1415 [========================>.....] - ETA: 0s - loss: 0.0085755/755 - 1s - loss: 0.0115 - 1s/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      "1290/1415 [==========================>...] - ETA: 0s - loss: 0.0085755/755 - 1s - loss: 0.0101 - 1s/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1409/1415 [============================>.] - ETA: 0s - loss: 0.0084755/755 - 1s - loss: 0.0102 - 1s/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "755/755 - 1s - loss: 0.0118 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0084 - val_loss: 0.0072\n",
      "Epoch 11/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0067755/755 - 1s - loss: 0.0119 - 1s/epoch - 2ms/step\n",
      "Epoch 6/28\n",
      "755/755 - 1s - loss: 0.0101 - 1s/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      " 171/1415 [==>...........................] - ETA: 1s - loss: 0.0086755/755 - 1s - loss: 0.0118 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 194/1415 [===>..........................] - ETA: 1s - loss: 0.0089755/755 - 1s - loss: 0.0115 - 1s/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      " 273/1415 [====>.........................] - ETA: 1s - loss: 0.0086755/755 - 1s - loss: 0.0104 - 1s/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      " 358/1415 [======>.......................] - ETA: 1s - loss: 0.0084755/755 - 1s - loss: 0.0115 - 962ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 444/1415 [========>.....................] - ETA: 1s - loss: 0.0084755/755 - 1s - loss: 0.0101 - 983ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 663/1415 [=============>................] - ETA: 0s - loss: 0.0082755/755 - 1s - loss: 0.0102 - 936ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 743/1415 [==============>...............] - ETA: 0s - loss: 0.0082755/755 - 1s - loss: 0.0119 - 1s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 777/1415 [===============>..............] - ETA: 0s - loss: 0.0082755/755 - 1s - loss: 0.0117 - 1s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "755/755 - 1s - loss: 0.0100 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 905/1415 [==================>...........] - ETA: 0s - loss: 0.0081755/755 - 1s - loss: 0.0117 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 983/1415 [===================>..........] - ETA: 0s - loss: 0.0082755/755 - 1s - loss: 0.0115 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1135/1415 [=======================>......] - ETA: 0s - loss: 0.0083755/755 - 1s - loss: 0.0103 - 1s/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "1168/1415 [=======================>......] - ETA: 0s - loss: 0.0083755/755 - 1s - loss: 0.0114 - 1s/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "1308/1415 [==========================>...] - ETA: 0s - loss: 0.0082755/755 - 1s - loss: 0.0100 - 1s/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "1412/1415 [============================>.] - ETA: 0s - loss: 0.0082755/755 - 1s - loss: 0.0100 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "755/755 - 1s - loss: 0.0101 - 1s/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "755/755 - 1s - loss: 0.0118 - 1s/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      "755/755 - 1s - loss: 0.0117 - 1s/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0082 - val_loss: 0.0071\n",
      "Epoch 12/28\n",
      "  30/1415 [..............................] - ETA: 2s - loss: 0.0072755/755 - 1s - loss: 0.0117 - 1s/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "  96/1415 [=>............................] - ETA: 2s - loss: 0.0074755/755 - 1s - loss: 0.0114 - 1s/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 222/1415 [===>..........................] - ETA: 1s - loss: 0.0077755/755 - 1s - loss: 0.0102 - 1s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 263/1415 [====>.........................] - ETA: 1s - loss: 0.0080755/755 - 1s - loss: 0.0114 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 430/1415 [========>.....................] - ETA: 1s - loss: 0.0081755/755 - 1s - loss: 0.0100 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 541/1415 [==========>...................] - ETA: 1s - loss: 0.0082755/755 - 1s - loss: 0.0099 - 984ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 642/1415 [============>.................] - ETA: 1s - loss: 0.0082755/755 - 1s - loss: 0.0100 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 667/1415 [=============>................] - ETA: 1s - loss: 0.0082755/755 - 1s - loss: 0.0118 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 699/1415 [=============>................] - ETA: 1s - loss: 0.0081755/755 - 1s - loss: 0.0116 - 1s/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 834/1415 [================>.............] - ETA: 0s - loss: 0.0080755/755 - 1s - loss: 0.0116 - 1s/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 872/1415 [=================>............] - ETA: 0s - loss: 0.0080755/755 - 1s - loss: 0.0114 - 1s/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "1028/1415 [====================>.........] - ETA: 0s - loss: 0.0079755/755 - 1s - loss: 0.0114 - 1s/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "1045/1415 [=====================>........] - ETA: 0s - loss: 0.0079755/755 - 1s - loss: 0.0102 - 1s/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1194/1415 [========================>.....] - ETA: 0s - loss: 0.0079755/755 - 1s - loss: 0.0099 - 1s/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "1315/1415 [==========================>...] - ETA: 0s - loss: 0.0079755/755 - 1s - loss: 0.0100 - 1s/epoch - 2ms/step\n",
      "Epoch 16/28\n",
      "1345/1415 [===========================>..] - ETA: 0s - loss: 0.0079755/755 - 1s - loss: 0.0098 - 1s/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "1395/1415 [============================>.] - ETA: 0s - loss: 0.0079755/755 - 1s - loss: 0.0117 - 1s/epoch - 2ms/step\n",
      "Epoch 10/28\n",
      "755/755 - 1s - loss: 0.0116 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "755/755 - 1s - loss: 0.0116 - 955ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "755/755 - 1s - loss: 0.0114 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0079 - val_loss: 0.0072\n",
      "Epoch 13/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0069755/755 - 1s - loss: 0.0113 - 935ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "755/755 - 1s - loss: 0.0101 - 915ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 155/1415 [==>...........................] - ETA: 1s - loss: 0.0082755/755 - 1s - loss: 0.0098 - 904ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 303/1415 [=====>........................] - ETA: 1s - loss: 0.0078755/755 - 1s - loss: 0.0099 - 939ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 344/1415 [======>.......................] - ETA: 1s - loss: 0.0078755/755 - 1s - loss: 0.0098 - 921ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 394/1415 [=======>......................] - ETA: 1s - loss: 0.0077755/755 - 1s - loss: 0.0117 - 927ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 478/1415 [=========>....................] - ETA: 1s - loss: 0.0078755/755 - 1s - loss: 0.0116 - 932ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 501/1415 [=========>....................] - ETA: 1s - loss: 0.0078755/755 - 1s - loss: 0.0115 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      " 565/1415 [==========>...................] - ETA: 1s - loss: 0.0077755/755 - 1s - loss: 0.0113 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 753/1415 [==============>...............] - ETA: 0s - loss: 0.0076755/755 - 1s - loss: 0.0113 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "755/755 - 1s - loss: 0.0100 - 1s/epoch - 2ms/step\n",
      "Epoch 15/28\n",
      " 864/1415 [=================>............] - ETA: 0s - loss: 0.0076755/755 - 1s - loss: 0.0098 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1086/1415 [======================>.......] - ETA: 0s - loss: 0.0076755/755 - 1s - loss: 0.0098 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1121/1415 [======================>.......] - ETA: 0s - loss: 0.0076755/755 - 1s - loss: 0.0097 - 1s/epoch - 2ms/step\n",
      "Epoch 20/28\n",
      "1191/1415 [========================>.....] - ETA: 0s - loss: 0.0076755/755 - 1s - loss: 0.0116 - 1s/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1235/1415 [=========================>....] - ETA: 0s - loss: 0.0075755/755 - 1s - loss: 0.0115 - 1s/epoch - 2ms/step\n",
      "Epoch 18/28\n",
      "1275/1415 [==========================>...] - ETA: 0s - loss: 0.0075755/755 - 1s - loss: 0.0113 - 907ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "755/755 - 1s - loss: 0.0115 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1406/1415 [============================>.] - ETA: 0s - loss: 0.0076755/755 - 1s - loss: 0.0112 - 960ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "755/755 - 1s - loss: 0.0100 - 1s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0076 - val_loss: 0.0083\n",
      "Epoch 14/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0085755/755 - 1s - loss: 0.0097 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 162/1415 [==>...........................] - ETA: 1s - loss: 0.0088755/755 - 1s - loss: 0.0098 - 981ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "755/755 - 1s - loss: 0.0097 - 944ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 242/1415 [====>.........................] - ETA: 1s - loss: 0.0082755/755 - 1s - loss: 0.0116 - 914ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 323/1415 [=====>........................] - ETA: 1s - loss: 0.0079755/755 - 1s - loss: 0.0114 - 907ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "755/755 - 1s - loss: 0.0115 - 988ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 438/1415 [========>.....................] - ETA: 1s - loss: 0.0077755/755 - 1s - loss: 0.0113 - 1s/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 585/1415 [===========>..................] - ETA: 1s - loss: 0.0077755/755 - 1s - loss: 0.0112 - 1s/epoch - 2ms/step\n",
      " 636/1415 [============>.................] - ETA: 1s - loss: 0.0076755/755 - 1s - loss: 0.0099 - 1s/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      " 743/1415 [==============>...............] - ETA: 1s - loss: 0.0076755/755 - 1s - loss: 0.0097 - 991ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 775/1415 [===============>..............] - ETA: 1s - loss: 0.0076755/755 - 1s - loss: 0.0097 - 1s/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 839/1415 [================>.............] - ETA: 0s - loss: 0.0076755/755 - 1s - loss: 0.0096 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "189/189 - 0s - loss: 0.0107 - 375ms/epoch - 2ms/step\n",
      " 865/1415 [=================>............] - ETA: 0s - loss: 0.0076755/755 - 1s - loss: 0.0115 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 933/1415 [==================>...........] - ETA: 0s - loss: 0.0075755/755 - 1s - loss: 0.0114 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "755/755 - 1s - loss: 0.0114 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 974/1415 [===================>..........] - ETA: 0s - loss: 0.0075755/755 - 1s - loss: 0.0112 - 911ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1334/1415 [===========================>..] - ETA: 0s - loss: 0.0074755/755 - 1s - loss: 0.0098 - 1s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1414/1415 [============================>.] - ETA: 0s - loss: 0.0074755/755 - 1s - loss: 0.0096 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "755/755 - 1s - loss: 0.0097 - 1s/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      "755/755 - 1s - loss: 0.0106 - 1s/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=sgd; total time= 4.2min\n",
      "755/755 - 1s - loss: 0.0115 - 1s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "755/755 - 1s - loss: 0.0095 - 1s/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "755/755 - 1s - loss: 0.0114 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 15/28\n",
      "755/755 - 1s - loss: 0.0114 - 1s/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      " 108/1415 [=>............................] - ETA: 1s - loss: 0.0074755/755 - 1s - loss: 0.0112 - 1s/epoch - 2ms/step\n",
      " 382/1415 [=======>......................] - ETA: 1s - loss: 0.0076755/755 - 1s - loss: 0.0098 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 425/1415 [========>.....................] - ETA: 1s - loss: 0.0076189/189 - 0s - loss: 0.0106 - 339ms/epoch - 2ms/step\n",
      " 506/1415 [=========>....................] - ETA: 1s - loss: 0.0075755/755 - 1s - loss: 0.0096 - 898ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 588/1415 [===========>..................] - ETA: 0s - loss: 0.0075755/755 - 1s - loss: 0.0096 - 985ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 631/1415 [============>.................] - ETA: 0s - loss: 0.0075755/755 - 1s - loss: 0.0114 - 903ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 670/1415 [=============>................] - ETA: 0s - loss: 0.0074755/755 - 1s - loss: 0.0095 - 912ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 719/1415 [==============>...............] - ETA: 0s - loss: 0.0074755/755 - 1s - loss: 0.0113 - 892ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 757/1415 [===============>..............] - ETA: 0s - loss: 0.0075755/755 - 1s - loss: 0.0113 - 938ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1051/1415 [=====================>........] - ETA: 10s - loss: 0.0073755/755 - 30s - loss: 0.0106 - 30s/epoch - 39ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=sgd; total time= 4.3min\n",
      "1124/1415 [======================>.......] - ETA: 7s - loss: 0.0073755/755 - 30s - loss: 0.0097 - 30s/epoch - 40ms/step\n",
      "Epoch 20/28\n",
      "1310/1415 [==========================>...] - ETA: 2s - loss: 0.0074755/755 - 30s - loss: 0.0096 - 30s/epoch - 40ms/step\n",
      "Epoch 23/28\n",
      "755/755 - 30s - loss: 0.0095 - 30s/epoch - 40ms/step\n",
      "Epoch 25/28\n",
      "1389/1415 [============================>.] - ETA: 0s - loss: 0.0074755/755 - 30s - loss: 0.0094 - 30s/epoch - 40ms/step\n",
      "Epoch 25/28\n",
      "755/755 - 30s - loss: 0.0114 - 30s/epoch - 40ms/step\n",
      "Epoch 17/28\n",
      "755/755 - 30s - loss: 0.0113 - 30s/epoch - 40ms/step\n",
      "Epoch 21/28\n",
      "755/755 - 30s - loss: 0.0113 - 30s/epoch - 40ms/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 31s 22ms/step - loss: 0.0074 - val_loss: 0.0064\n",
      "Epoch 16/28\n",
      " 197/1415 [===>..........................] - ETA: 1s - loss: 0.0070755/755 - 1s - loss: 0.0097 - 849ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 326/1415 [=====>........................] - ETA: 1s - loss: 0.0071755/755 - 1s - loss: 0.0095 - 797ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 370/1415 [======>.......................] - ETA: 1s - loss: 0.0071755/755 - 1s - loss: 0.0094 - 835ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 478/1415 [=========>....................] - ETA: 1s - loss: 0.0069755/755 - 1s - loss: 0.0094 - 858ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 516/1415 [=========>....................] - ETA: 1s - loss: 0.0069755/755 - 1s - loss: 0.0114 - 889ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "755/755 - 1s - loss: 0.0112 - 895ms/epoch - 1ms/step\n",
      " 551/1415 [==========>...................] - ETA: 1s - loss: 0.0070Epoch 22/28\n",
      " 637/1415 [============>.................] - ETA: 1s - loss: 0.0074755/755 - 1s - loss: 0.0113 - 909ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 901/1415 [==================>...........] - ETA: 0s - loss: 0.0072755/755 - 1s - loss: 0.0096 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 938/1415 [==================>...........] - ETA: 0s - loss: 0.0072755/755 - 1s - loss: 0.0094 - 885ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      " 965/1415 [===================>..........] - ETA: 0s - loss: 0.0072755/755 - 1s - loss: 0.0095 - 983ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1143/1415 [=======================>......] - ETA: 0s - loss: 0.0071755/755 - 1s - loss: 0.0112 - 946ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1186/1415 [========================>.....] - ETA: 0s - loss: 0.0071755/755 - 1s - loss: 0.0113 - 1s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "755/755 - 1s - loss: 0.0093 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1229/1415 [=========================>....] - ETA: 0s - loss: 0.0071755/755 - 1s - loss: 0.0112 - 963ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0070 - val_loss: 0.0060\n",
      "Epoch 17/28\n",
      "   1/1415 [..............................] - ETA: 1s - loss: 0.0059755/755 - 1s - loss: 0.0096 - 911ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "  48/1415 [>.............................] - ETA: 1s - loss: 0.0069755/755 - 1s - loss: 0.0094 - 843ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "755/755 - 1s - loss: 0.0093 - 905ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 248/1415 [====>.........................] - ETA: 1s - loss: 0.0068755/755 - 1s - loss: 0.0111 - 879ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 291/1415 [=====>........................] - ETA: 1s - loss: 0.0068755/755 - 1s - loss: 0.0093 - 897ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "755/755 - 1s - loss: 0.0112 - 830ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 315/1415 [=====>........................] - ETA: 1s - loss: 0.0069755/755 - 1s - loss: 0.0113 - 1s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 613/1415 [===========>..................] - ETA: 1s - loss: 0.0071755/755 - 1s - loss: 0.0093 - 916ms/epoch - 1ms/step\n",
      " 650/1415 [============>.................] - ETA: 1s - loss: 0.0070755/755 - 1s - loss: 0.0095 - 1s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 687/1415 [=============>................] - ETA: 1s - loss: 0.0070755/755 - 1s - loss: 0.0094 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      " 821/1415 [================>.............] - ETA: 0s - loss: 0.0069189/189 - 0s - loss: 0.0092 - 357ms/epoch - 2ms/step\n",
      " 927/1415 [==================>...........] - ETA: 0s - loss: 0.0070755/755 - 1s - loss: 0.0111 - 1s/epoch - 2ms/step\n",
      "Epoch 25/28\n",
      "755/755 - 1s - loss: 0.0092 - 1s/epoch - 1ms/step\n",
      "755/755 - 1s - loss: 0.0111 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      " 960/1415 [===================>..........] - ETA: 0s - loss: 0.0069755/755 - 1s - loss: 0.0112 - 997ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1210/1415 [========================>.....] - ETA: 0s - loss: 0.0069755/755 - 1s - loss: 0.0094 - 853ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1244/1415 [=========================>....] - ETA: 0s - loss: 0.0069189/189 - 0s - loss: 0.0092 - 404ms/epoch - 2ms/step\n",
      "1366/1415 [===========================>..] - ETA: 0s - loss: 0.0069755/755 - 1s - loss: 0.0093 - 1s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1403/1415 [============================>.] - ETA: 0s - loss: 0.0069755/755 - 1s - loss: 0.0093 - 1s/epoch - 1ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=sgd; total time= 4.8min\n",
      "755/755 - 1s - loss: 0.0111 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 18/28\n",
      "  38/1415 [..............................] - ETA: 4s - loss: 0.0067755/755 - 1s - loss: 0.0111 - 1s/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "  54/1415 [>.............................] - ETA: 4s - loss: 0.0065755/755 - 1s - loss: 0.0112 - 1s/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 213/1415 [===>..........................] - ETA: 3s - loss: 0.0066755/755 - 1s - loss: 0.0092 - 1s/epoch - 2ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=sgd; total time= 4.9min\n",
      " 244/1415 [====>.........................] - ETA: 2s - loss: 0.0066755/755 - 1s - loss: 0.0094 - 1s/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      " 434/1415 [========>.....................] - ETA: 2s - loss: 0.0066755/755 - 2s - loss: 0.0093 - 2s/epoch - 2ms/step\n",
      " 583/1415 [===========>..................] - ETA: 2s - loss: 0.0067755/755 - 2s - loss: 0.0110 - 2s/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      " 612/1415 [===========>..................] - ETA: 2s - loss: 0.0067755/755 - 2s - loss: 0.0112 - 2s/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      " 629/1415 [============>.................] - ETA: 2s - loss: 0.0067755/755 - 2s - loss: 0.0111 - 2s/epoch - 2ms/step\n",
      " 675/1415 [=============>................] - ETA: 2s - loss: 0.0067189/189 - 1s - loss: 0.0092 - 742ms/epoch - 4ms/step\n",
      " 839/1415 [================>.............] - ETA: 1s - loss: 0.0067755/755 - 2s - loss: 0.0093 - 2s/epoch - 3ms/step\n",
      "Epoch 27/28\n",
      " 935/1415 [==================>...........] - ETA: 1s - loss: 0.0067189/189 - 1s - loss: 0.0107 - 1s/epoch - 6ms/step\n",
      "1277/1415 [==========================>...] - ETA: 0s - loss: 0.0067755/755 - 3s - loss: 0.0110 - 3s/epoch - 4ms/step\n",
      "Epoch 28/28\n",
      "1316/1415 [==========================>...] - ETA: 0s - loss: 0.0067755/755 - 3s - loss: 0.0111 - 3s/epoch - 4ms/step\n",
      "Epoch 24/28\n",
      "1394/1415 [============================>.] - ETA: 0s - loss: 0.0066755/755 - 3s - loss: 0.0092 - 3s/epoch - 4ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=sgd; total time= 4.9min\n",
      "755/755 - 2s - loss: 0.0093 - 2s/epoch - 3ms/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0066 - val_loss: 0.0061\n",
      "Epoch 19/28\n",
      "  46/1415 [..............................] - ETA: 1s - loss: 0.0073755/755 - 2s - loss: 0.0105 - 2s/epoch - 3ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=sgd; total time= 4.9min\n",
      " 267/1415 [====>.........................] - ETA: 2s - loss: 0.0067755/755 - 1s - loss: 0.0109 - 1s/epoch - 1ms/step\n",
      " 279/1415 [====>.........................] - ETA: 2s - loss: 0.0067755/755 - 1s - loss: 0.0111 - 1s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      " 530/1415 [==========>...................] - ETA: 1s - loss: 0.0067189/189 - 1s - loss: 0.0104 - 620ms/epoch - 3ms/step\n",
      " 537/1415 [==========>...................] - ETA: 1s - loss: 0.0066755/755 - 1s - loss: 0.0092 - 1s/epoch - 2ms/step\n",
      " 722/1415 [==============>...............] - ETA: 1s - loss: 0.0068189/189 - 0s - loss: 0.0092 - 366ms/epoch - 2ms/step\n",
      " 882/1415 [=================>............] - ETA: 1s - loss: 0.0067755/755 - 2s - loss: 0.0111 - 2s/epoch - 2ms/step\n",
      "Epoch 26/28\n",
      "1013/1415 [====================>.........] - ETA: 0s - loss: 0.0068Epoch 1/28\n",
      "1046/1415 [=====================>........] - ETA: 0s - loss: 0.0068755/755 - 1s - loss: 0.0105 - 1s/epoch - 2ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=sgd; total time= 5.0min\n",
      "1355/1415 [===========================>..] - ETA: 0s - loss: 0.0067755/755 - 2s - loss: 0.0092 - 2s/epoch - 2ms/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=sgd; total time= 5.0min\n",
      " 364/1415 [======>.......................] - ETA: 2s - loss: 0.7022Epoch 1/28\n",
      " 381/1415 [=======>......................] - ETA: 2s - loss: 0.6756755/755 - 1s - loss: 0.0110 - 1s/epoch - 2ms/step\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0067 - val_loss: 0.0089\n",
      "Epoch 20/28\n",
      " 518/1415 [=========>....................] - ETA: 1s - loss: 0.5336755/755 - 1s - loss: 0.0110 - 1s/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "1230/1415 [=========================>....] - ETA: 0s - loss: 0.2401755/755 - 2s - loss: 0.0110 - 2s/epoch - 2ms/step\n",
      "1407/1415 [============================>.] - ETA: 0s - loss: 0.0064189/189 - 0s - loss: 0.0103 - 404ms/epoch - 2ms/step\n",
      "1412/1415 [============================>.] - ETA: 0s - loss: 0.2116Epoch 1/28\n",
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 21/28\n",
      " 279/1415 [====>.........................] - ETA: 1s - loss: 0.8069755/755 - 1s - loss: 0.0104 - 1s/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=sgd; total time= 5.1min\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.2038 - val_loss: 0.0184\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.2112 - val_loss: 0.0171\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0055\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0156 - val_loss: 0.0142\n",
      "Epoch 3/28\n",
      " 502/1415 [=========>....................] - ETA: 3s - loss: 0.0137Epoch 1/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1736 - val_loss: 0.0155\n",
      "Epoch 2/28\n",
      " 649/1415 [============>.................] - ETA: 2s - loss: 0.0136Epoch 1/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.1849 - val_loss: 0.0159\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0161 - val_loss: 0.0132\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 0.0063 - val_loss: 0.0071................] - ETA: 2s - loss: 0.673\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0130 - val_loss: 0.0120\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0147 - val_loss: 0.0122\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0151 - val_loss: 0.0127\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0134 - val_loss: 0.0118\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0116 - val_loss: 0.0105\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0062 - val_loss: 0.0067\n",
      "Epoch 24/28\n",
      "1194/1415 [========================>.....] - ETA: 0s - loss: 0.0126Epoch 1/28\n",
      " 193/1415 [===>..........................] - ETA: 1s - loss: 0.8653Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0125 - val_loss: 0.0119\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0130 - val_loss: 0.0112\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0107 - val_loss: 0.0144\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 2s 2ms/step - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 25/28\n",
      " 334/1415 [======>.......................] - ETA: 1s - loss: 0.0059Epoch 1/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.1508 - val_loss: 0.0153\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.1532 - val_loss: 0.0160\n",
      "1374/1415 [============================>.] - ETA: 0s - loss: 0.0113Epoch 2/28\n",
      " 374/1415 [======>.......................] - ETA: 1s - loss: 0.0059Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0099 - val_loss: 0.0089\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0059 - val_loss: 0.0070\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0104 - val_loss: 0.0091\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0101 - val_loss: 0.0091\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0096 - val_loss: 0.0082\n",
      " 864/1415 [=================>............] - ETA: 1s - loss: 0.0099Epoch 8/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0060 - val_loss: 0.0055\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0123 - val_loss: 0.0096\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0127 - val_loss: 0.0100\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.1488 - val_loss: 0.0157\n",
      " 385/1415 [=======>......................] - ETA: 2s - loss: 0.0060Epoch 2/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0099 - val_loss: 0.0093\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.1539 - val_loss: 0.0156\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.1493 - val_loss: 0.0170\n",
      "Epoch 2/28\n",
      "Epoch 2/28\n",
      "Epoch 3/28\n",
      "   1/1415 [..............................] - ETA: 39s - loss: 0.0156Epoch 7/28\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 910s 644ms/step - loss: 0.0099 - val_loss: 0.0093\n",
      " 547/1415 [==========>...................] - ETA: 24:20 - loss: 0.0059Epoch 7/28\n",
      "1415/1415 [==============================] - 921s 651ms/step - loss: 0.0097 - val_loss: 0.0115\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 921s 651ms/step - loss: 0.0089 - val_loss: 0.0079\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 921s 651ms/step - loss: 0.0059 - val_loss: 0.0055\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 15s 11ms/step - loss: 0.0093 - val_loss: 0.0086\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0090 - val_loss: 0.0080\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0087 - val_loss: 0.0075\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0058 - val_loss: 0.0056\n",
      "1203/1415 [========================>.....] - ETA: 0s - loss: 0.0086Epoch 1/28\n",
      "1187/1415 [========================>.....] - ETA: 3s - loss: 0.0128755/755 - 0s - loss: 0.0073 - 496ms/epoch - 657us/step\n",
      "Epoch 2/28\n",
      " 498/1415 [=========>....................] - ETA: 14s - loss: 0.0084755/755 - 1s - loss: 0.0062 - 858ms/epoch - 1ms/step\n",
      "1366/1415 [===========================>..] - ETA: 0s - loss: 0.0090Epoch 3/28\n",
      "1415/1415 [==============================] - 27s 19ms/step - loss: 0.0126 - val_loss: 0.0103\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0086 - val_loss: 0.0089\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0090 - val_loss: 0.0078\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 39s 28ms/step - loss: 0.0117 - val_loss: 0.0089\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 39s 28ms/step - loss: 0.0088 - val_loss: 0.0074\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 39s 28ms/step - loss: 0.0088 - val_loss: 0.0090\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0087 - val_loss: 0.0083\n",
      "Epoch 10/28\n",
      "1266/1415 [=========================>....] - ETA: 2s - loss: 0.0081755/755 - 13s - loss: 0.0065 - 13s/epoch - 17ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 40s 28ms/step - loss: 0.0125 - val_loss: 0.0099\n",
      " 547/1415 [==========>...................] - ETA: 2s - loss: 0.0085Epoch 3/28\n",
      "1415/1415 [==============================] - 22s 16ms/step - loss: 0.0081 - val_loss: 0.0073\n",
      "Epoch 11/28\n",
      " 806/1415 [================>.............] - ETA: 1s - loss: 0.0083755/755 - 2s - loss: 0.0061 - 2s/epoch - 2ms/step\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0086 - val_loss: 0.0078\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0083 - val_loss: 0.0096\n",
      "Epoch 10/28\n",
      " 671/1415 [=============>................] - ETA: 4s - loss: 0.0074755/755 - 2s - loss: 0.0068 - 2s/epoch - 3ms/step\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0083 - val_loss: 0.0074\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0079 - val_loss: 0.0069\n",
      "Epoch 12/28\n",
      "1063/1415 [=====================>........] - ETA: 0s - loss: 0.0081755/755 - 1s - loss: 0.0059 - 1s/epoch - 2ms/step\n",
      "Epoch 7/28\n",
      "1071/1415 [=====================>........] - ETA: 4:51 - loss: 0.0070755/755 - 901s - loss: 0.0059 - 901s/epoch - 1s/step\n",
      "Epoch 8/28\n",
      "1134/1415 [=======================>......] - ETA: 3:44 - loss: 0.0084755/755 - 0s - loss: 0.0060 - 499ms/epoch - 661us/step\n",
      "1415/1415 [==============================] - 908s 642ms/step - loss: 0.0090 - val_loss: 0.0076\n",
      "1403/1415 [============================>.] - ETA: 7s - loss: 0.0080 Epoch 4/28\n",
      "Epoch 9/28\n",
      "1088/1415 [======================>.......] - ETA: 4:34 - loss: 0.0092755/755 - 1s - loss: 0.0063 - 851ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 911s 644ms/step - loss: 0.0080 - val_loss: 0.0071\n",
      "1415/1415 [==============================] - 911s 644ms/step - loss: 0.0082 - val_loss: 0.0071\n",
      "1154/1415 [=======================>......] - ETA: 3:26 - loss: 0.0070Epoch 10/28\n",
      "Epoch 11/28\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 913s 646ms/step - loss: 0.0081 - val_loss: 0.0071\n",
      "Epoch 12/28\n",
      " 670/1415 [=============>................] - ETA: 1s - loss: 0.0081755/755 - 2s - loss: 0.0061 - 2s/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 918s 649ms/step - loss: 0.0082 - val_loss: 0.0070\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 913s 646ms/step - loss: 0.0078 - val_loss: 0.0074\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 918s 649ms/step - loss: 0.0071 - val_loss: 0.0063\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 918s 649ms/step - loss: 0.0070 - val_loss: 0.0094\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 918s 649ms/step - loss: 0.0090 - val_loss: 0.0081\n",
      "Epoch 4/28\n",
      " 493/1415 [=========>....................] - ETA: 1s - loss: 0.0073755/755 - 1s - loss: 0.0065 - 1s/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0080 - val_loss: 0.0070\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0079 - val_loss: 0.0069\n",
      "Epoch 12/28\n",
      " 431/1415 [========>.....................] - ETA: 2s - loss: 0.0074755/755 - 2s - loss: 0.0060 - 2s/epoch - 3ms/step\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0077 - val_loss: 0.0072\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0073 - val_loss: 0.0064\n",
      "Epoch 14/28\n",
      "1082/1415 [=====================>........] - ETA: 1s - loss: 0.0075755/755 - 2s - loss: 0.0058 - 2s/epoch - 3ms/step\n",
      "1098/1415 [======================>.......] - ETA: 1s - loss: 0.0075Epoch 14/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0072 - val_loss: 0.0064\n",
      "755/755 - 7s - loss: 0.0065 - 7s/epoch - 10ms/step\n",
      " 457/1415 [========>.....................] - ETA: 53s - loss: 0.0078Epoch 5/28\n",
      "   1/1415 [..............................] - ETA: 14s - loss: 0.0067Epoch 15/28\n",
      "1415/1415 [==============================] - 29s 20ms/step - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 29s 20ms/step - loss: 0.0075 - val_loss: 0.0082\n",
      "Epoch 13/28\n",
      "1237/1415 [=========================>....] - ETA: 4s - loss: 0.0067755/755 - 2s - loss: 0.0057 - 2s/epoch - 3ms/step\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 29s 20ms/step - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 29s 20ms/step - loss: 0.0073 - val_loss: 0.0070ETA: 3s - loss: 0.006\n",
      "Epoch 15/28\n",
      "1414/1415 [============================>.] - ETA: 0s - loss: 0.0061755/755 - 1s - loss: 0.0059 - 1s/epoch - 2ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 34s 24ms/step - loss: 0.0066 - val_loss: 0.0058\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0075 - val_loss: 0.0124\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0074 - val_loss: 0.0191\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 35s 25ms/step - loss: 0.0061 - val_loss: 0.0053\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 35s 25ms/step - loss: 0.0060 - val_loss: 0.0052\n",
      "Epoch 6/28\n",
      " 129/1415 [=>............................] - ETA: 2s - loss: 0.0077755/755 - 2s - loss: 0.0054 - 2s/epoch - 2ms/step\n",
      " 237/1415 [====>.........................] - ETA: 8s - loss: 0.0060Epoch 18/28\n",
      "1415/1415 [==============================] - 35s 25ms/step - loss: 0.0072 - val_loss: 0.0065\n",
      "Epoch 5/28\n",
      " 296/1415 [=====>........................] - ETA: 2s - loss: 0.0076755/755 - 1s - loss: 0.0055 - 768ms/epoch - 1ms/step\n",
      "1003/1415 [====================>.........] - ETA: 2s - loss: 0.0059Epoch 19/28\n",
      " 190/1415 [===>..........................] - ETA: 1:01 - loss: 0.0053755/755 - 1s - loss: 0.0059 - 937ms/epoch - 1ms/step\n",
      "1061/1415 [=====================>........] - ETA: 4s - loss: 0.0059Epoch 20/28\n",
      "1415/1415 [==============================] - 12s 9ms/step - loss: 0.0072 - val_loss: 0.0066\n",
      " 408/1415 [=======>......................] - ETA: 27s - loss: 0.0059Epoch 15/28\n",
      " 576/1415 [===========>..................] - ETA: 17s - loss: 0.0071755/755 - 1s - loss: 0.0056 - 743ms/epoch - 984us/step\n",
      "1175/1415 [=======================>......] - ETA: 3s - loss: 0.0059Epoch 21/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0072 - val_loss: 0.0062\n",
      "Epoch 16/28\n",
      " 474/1415 [=========>....................] - ETA: 2s - loss: 0.0069755/755 - 2s - loss: 0.0060 - 2s/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 22s 16ms/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0071 - val_loss: 0.0114\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0075 - val_loss: 0.0063\n",
      "Epoch 15/28\n",
      " 333/1415 [======>.......................] - ETA: 2s - loss: 0.0071755/755 - 2s - loss: 0.0058 - 2s/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0069 - val_loss: 0.0062\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0071 - val_loss: 0.0061\n",
      "Epoch 17/28\n",
      "1203/1415 [========================>.....] - ETA: 3s - loss: 0.0054755/755 - 2s - loss: 0.0057 - 2s/epoch - 3ms/step\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 24s 17ms/step - loss: 0.0058 - val_loss: 0.0051\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0072 - val_loss: 0.0062\n",
      "755/755 - 3s - loss: 0.0053 - 3s/epoch - 4ms/step\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "1099/1415 [======================>.......] - ETA: 1s - loss: 0.0068Epoch 16/28\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0070 - val_loss: 0.0062\n",
      "Epoch 25/28\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 29s 20ms/step - loss: 0.0062 - val_loss: 0.0055\n",
      "755/755 - 1s - loss: 0.0051 - 736ms/epoch - 975us/step\n",
      "1415/1415 [==============================] - 29s 21ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      " 217/1415 [===>..........................] - ETA: 2s - loss: 0.0070Epoch 26/28\n",
      " 276/1415 [====>.........................] - ETA: 27s - loss: 0.0053Epoch 6/28\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 22s 16ms/step - loss: 0.0068 - val_loss: 0.0059\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 22s 16ms/step - loss: 0.0069 - val_loss: 0.0061\n",
      "Epoch 18/28\n",
      " 341/1415 [======>.......................] - ETA: 3s - loss: 0.0063755/755 - 2s - loss: 0.0058 - 2s/epoch - 3ms/step\n",
      "Epoch 27/28\n",
      " 962/1415 [===================>..........] - ETA: 1s - loss: 0.0065755/755 - 1s - loss: 0.0051 - 1s/epoch - 2ms/step\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0070 - val_loss: 0.0061\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0069 - val_loss: 0.0074\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 28s 20ms/step - loss: 0.0052 - val_loss: 0.0058\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0065 - val_loss: 0.0055\n",
      "Epoch 18/28\n",
      " 582/1415 [===========>..................] - ETA: 3s - loss: 0.0067755/755 - 2s - loss: 0.0050 - 2s/epoch - 3ms/step\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0066 - val_loss: 0.0090\n",
      "Epoch 19/28\n",
      " 971/1415 [===================>..........] - ETA: 2s - loss: 0.0054189/189 - 0s - loss: 0.0045 - 384ms/epoch - 2ms/step\n",
      "1168/1415 [=======================>......] - ETA: 3s - loss: 0.0049755/755 - 1s - loss: 0.0045 - 804ms/epoch - 1ms/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x175ae0100>; total time=34.8min\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0071 - val_loss: 0.0081\n",
      "1275/1415 [==========================>...] - ETA: 0s - loss: 0.0054Epoch 18/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0067 - val_loss: 0.0057\n",
      "1415/1415 [==============================] - 25s 18ms/step - loss: 0.0052 - val_loss: 0.0046\n",
      "1415/1415 [==============================] - 19s 14ms/step - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 18/28\n",
      "Epoch 8/28\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 15s 11ms/step - loss: 0.0047 - val_loss: 0.0050\n",
      "1415/1415 [==============================] - 15s 11ms/step - loss: 0.0053 - val_loss: 0.0048\n",
      "Epoch 7/28\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 22s 15ms/step - loss: 0.0062 - val_loss: 0.0059\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 22s 15ms/step - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0066 - val_loss: 0.0057\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 15s 10ms/step - loss: 0.0067 - val_loss: 0.0093\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 25s 18ms/step - loss: 0.0046 - val_loss: 0.0049\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0061 - val_loss: 0.0080\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0067 - val_loss: 0.0061\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0067 - val_loss: 0.0063TA: 1s - loss: 0.006ss: 0.\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 18s 13ms/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 19s 13ms/step - loss: 0.0045 - val_loss: 0.0076\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0066 - val_loss: 0.0062\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "1130/1415 [======================>.......] - ETA: 0s - loss: 0.0060Epoch 8/28\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0045 - val_loss: 0.0042\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0064 - val_loss: 0.0055: 3s - loss: 0.006\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0060 - val_loss: 0.0056\n",
      "Epoch 21/28\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 929s 657ms/step - loss: 0.0064 - val_loss: 0.00561:02:14 - losTA: 1:27 - loss: 0.006\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 932s 659ms/step - loss: 0.0043 - val_loss: 0.0038\n",
      "1072/1415 [=====================>........] - ETA: 4:57 - loss: 0.0043Epoch 9/28\n",
      "1415/1415 [==============================] - 930s 658ms/step - loss: 0.0064 - val_loss: 0.0054\n",
      "1148/1415 [=======================>......] - ETA: 3:34 - loss: 0.0062Epoch 21/28\n",
      "1415/1415 [==============================] - 934s 660ms/step - loss: 0.0044 - val_loss: 0.0042\n",
      "1415/1415 [==============================] - 926s 655ms/step - loss: 0.0057 - val_loss: 0.0082\n",
      "1415/1415 [==============================] - 934s 660ms/step - loss: 0.0042 - val_loss: 0.0039\n",
      "1415/1415 [==============================] - 926s 655ms/step - loss: 0.0063 - val_loss: 0.0055\n",
      "Epoch 22/28\n",
      " 369/1415 [======>.......................] - ETA: 7s - loss: 0.0066Epoch 10/28\n",
      "Epoch 23/28\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 935s 661ms/step - loss: 0.0043 - val_loss: 0.0040\n",
      " 503/1415 [=========>....................] - ETA: 8s - loss: 0.0041Epoch 9/28\n",
      "1415/1415 [==============================] - 936s 662ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      " 620/1415 [============>.................] - ETA: 6s - loss: 0.0067Epoch 10/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0063 - val_loss: 0.0055\n",
      " 467/1415 [========>.....................] - ETA: 12s - loss: 0.0039Epoch 22/28\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0065 - val_loss: 0.0054\n",
      "1166/1415 [=======================>......] - ETA: 1s - loss: 0.0062Epoch 22/28\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0056 - val_loss: 0.0048.....] - ETA: 1s - loss: 0.006...........] - ETA: 9s - loss: 0.003ss: 0.004\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0061 - val_loss: 0.0053\n",
      " 817/1415 [================>.............] - ETA: 7s - loss: 0.0038Epoch 23/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 24/28\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      " 908/1415 [==================>...........] - ETA: 4s - loss: 0.0061Epoch 11/28\n",
      "1415/1415 [==============================] - 19s 13ms/step - loss: 0.0040 - val_loss: 0.0038.......] - ETA: 3s - loss: 0.003\n",
      " 468/1415 [========>.....................] - ETA: 10s - loss: 0.0038Epoch 10/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0038 - val_loss: 0.0041\n",
      "1208/1415 [========================>.....] - ETA: 1s - loss: 0.0063Epoch 11/28\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0063 - val_loss: 0.0053\n",
      " 163/1415 [==>...........................] - ETA: 11s - loss: 0.0039Epoch 23/28\n",
      "1415/1415 [==============================] - 432s 305ms/step - loss: 0.0062 - val_loss: 0.0069............] - ETA: 5:45:27 - loss: 0.0\n",
      "1045/1415 [=====================>........] - ETA: 2:31 - loss: 0.0061Epoch 23/28\n",
      "1415/1415 [==============================] - 435s 307ms/step - loss: 0.0056 - val_loss: 0.00503:15 - loss: 0.[=========>....................] - ETA: 12:27 - loss: 0.00\n",
      "1415/1415 [==============================] - 435s 307ms/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 24/28\n",
      " 509/1415 [=========>....................] - ETA: 12:44 - loss: 0.0038Epoch 25/28\n",
      "1415/1415 [==============================] - 440s 311ms/step - loss: 0.0037 - val_loss: 0.0037ETA: 8s - loss: 0.00==========>.............] - ETA: 5:24 - loss: 0.004- loss: 0.00\n",
      " 734/1415 [==============>...............] - ETA: 4s - loss: 0.0063Epoch 11/28\n",
      "1415/1415 [==============================] - 12s 9ms/step - loss: 0.0061 - val_loss: 0.0055>.......] - ETA: 2:06 - loss: 0.004\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 434s 307ms/step - loss: 0.0062 - val_loss: 0.0052\n",
      " 145/1415 [==>...........................] - ETA: 3s - loss: 0.0059Epoch 24/28\n",
      "1415/1415 [==============================] - 440s 311ms/step - loss: 0.0037 - val_loss: 0.0034ETA: 11s - loss: 0.003\n",
      "1415/1415 [==============================] - 439s 311ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "1415/1415 [==============================] - 439s 311ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "1336/1415 [===========================>..] - ETA: 0s - loss: 0.0054Epoch 11/28\n",
      "Epoch 12/28\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0060 - val_loss: 0.0052\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0054 - val_loss: 0.0049...] - ETA: 4s - loss: 0.004\n",
      " 111/1415 [=>............................] - ETA: 4s - loss: 0.0033Epoch 25/28\n",
      "1415/1415 [==============================] - 439s 311ms/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 9s 7ms/step - loss: 0.0060 - val_loss: 0.0072\n",
      " 614/1415 [============>.................] - ETA: 7s - loss: 0.0033Epoch 25/28\n",
      "1415/1415 [==============================] - 9s 7ms/step - loss: 0.0058 - val_loss: 0.0051..............] - ETA: 7s - loss: 0.003\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 9s 7ms/step - loss: 0.0061 - val_loss: 0.0054\n",
      "1294/1415 [==========================>...] - ETA: 0s - loss: 0.0052Epoch 27/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0051 - val_loss: 0.0048 0.005\n",
      "1129/1415 [======================>.......] - ETA: 2s - loss: 0.0036Epoch 26/28\n",
      "1415/1415 [==============================] - 15s 11ms/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0036 - val_loss: 0.0033: 3s - loss: 0.006 5s - loss: 0.00\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0035 - val_loss: 0.0032\n",
      "1410/1415 [============================>.] - ETA: 0s - loss: 0.0034Epoch 12/28\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 15s 11ms/step - loss: 0.0037 - val_loss: 0.0033\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 12/28\n",
      "1318/1415 [==========================>...] - ETA: 0s - loss: 0.0060Epoch 13/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0059 - val_loss: 0.0050\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0057 - val_loss: 0.0048\n",
      "1200/1415 [========================>.....] - ETA: 1s - loss: 0.0059Epoch 26/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0059 - val_loss: 0.0052 0.0030.006\n",
      " 610/1415 [===========>..................] - ETA: 6s - loss: 0.0035Epoch 28/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0051 - val_loss: 0.0047\n",
      " 572/1415 [===========>..................] - ETA: 7s - loss: 0.0033Epoch 27/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0033 - val_loss: 0.0030 ETA: 2s - loss: 0.003\n",
      "1057/1415 [=====================>........] - ETA: 3s - loss: 0.0034Epoch 13/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0059 - val_loss: 0.0055\n",
      "1095/1415 [======================>.......] - ETA: 3s - loss: 0.0032Epoch 27/28\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0033 - val_loss: 0.0031=>..] - ETA: 0s - loss: 0.003\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 27/28\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0035 - val_loss: 0.0031\n",
      "1238/1415 [=========================>....] - ETA: 1s - loss: 0.0051Epoch 14/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0032 - val_loss: 0.0030........] - ETA: 3s - loss: 0.003\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0034 - val_loss: 0.0031\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0058 - val_loss: 0.0066\n",
      "Epoch 13/28\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0050 - val_loss: 0.0049\n",
      " 145/1415 [==>...........................] - ETA: 3s - loss: 0.0031Epoch 28/28\n",
      " 432/1415 [========>.....................] - ETA: 6s - loss: 0.0031Epoch 1/28\n",
      "755/755 - 0s - loss: 0.0064 - 482ms/epoch - 638us/step\n",
      "Epoch 2/28\n",
      " 578/1415 [===========>..................] - ETA: 7s - loss: 0.0032755/755 - 1s - loss: 0.0072 - 702ms/epoch - 929us/step\n",
      "1093/1415 [======================>.......] - ETA: 1s - loss: 0.0057Epoch 3/28\n",
      " 733/1415 [==============>...............] - ETA: 3s - loss: 0.0051755/755 - 1s - loss: 0.0059 - 674ms/epoch - 892us/step\n",
      "Epoch 4/28\n",
      "755/755 - 0s - loss: 0.0064 - 473ms/epoch - 627us/step\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      " 605/1415 [===========>..................] - ETA: 6s - loss: 0.0035755/755 - 1s - loss: 0.0060 - 664ms/epoch - 880us/step\n",
      "Epoch 6/28\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 28/28\n",
      " 929/1415 [==================>...........] - ETA: 4s - loss: 0.0033755/755 - 1s - loss: 0.0062 - 805ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 802/1415 [================>.............] - ETA: 5s - loss: 0.0035TA: 5s - loss: 0.0032755/755 - 1s - loss: 0.0055 - 631ms/epoch - 836us/step\n",
      "Epoch 8/28\n",
      "1335/1415 [===========================>..] - ETA: 0s - loss: 0.0051755/755 - 1s - loss: 0.0059 - 516ms/epoch - 683us/step\n",
      "Epoch 9/28\n",
      "1061/1415 [=====================>........] - ETA: 3s - loss: 0.0031755/755 - 1s - loss: 0.0061 - 529ms/epoch - 700us/step\n",
      "Epoch 10/28\n",
      "1381/1415 [============================>.] - ETA: 0s - loss: 0.0032755/755 - 1s - loss: 0.0056 - 633ms/epoch - 839us/step6\n",
      "Epoch 11/28\n",
      "755/755 - 0s - loss: 0.0056 - 471ms/epoch - 623us/step\n",
      "1193/1415 [========================>.....] - ETA: 1s - loss: 0.0032Epoch 12/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0051 - val_loss: 0.0045\n",
      "1415/1415 [==============================] - 15s 10ms/step - loss: 0.0032 - val_loss: 0.0030: 3s - loss: 0.006\n",
      "1090/1415 [======================>.......] - ETA: 2s - loss: 0.0033755/755 - 1s - loss: 0.0059 - 861ms/epoch - 1ms/step\n",
      "1325/1415 [===========================>..] - ETA: 0s - loss: 0.0032Epoch 13/28\n",
      "Epoch 14/28\n",
      " 919/1415 [==================>...........] - ETA: 2s - loss: 0.0054755/755 - 1s - loss: 0.0056 - 775ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0032 - val_loss: 0.0029\n",
      "1259/1415 [=========================>....] - ETA: 1s - loss: 0.0033Epoch 1/28\n",
      "1415/1415 [==============================] - 12s 9ms/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 14/28\n",
      "Epoch 14/28\n",
      "Epoch 15/28\n",
      "1159/1415 [=======================>......] - ETA: 1s - loss: 0.0053755/755 - 1s - loss: 0.0059 - 1s/epoch - 1ms/step\n",
      "1264/1415 [=========================>....] - ETA: 0s - loss: 0.0058755/755 - 1s - loss: 0.0060 - 782ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 12s 9ms/step - loss: 0.0031 - val_loss: 0.0033\n",
      "1415/1415 [==============================] - 12s 9ms/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 15/28\n",
      "Epoch 15/28\n",
      "Epoch 2/28\n",
      "Epoch 14/28\n",
      "1400/1415 [============================>.] - ETA: 0s - loss: 0.0053755/755 - 1s - loss: 0.0064 - 794ms/epoch - 1ms/steposs: 0.003\n",
      "Epoch 3/28\n",
      "755/755 - 1s - loss: 0.0052 - 797ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0058 - val_loss: 0.0056\n",
      "1415/1415 [==============================] - 7s 5ms/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 16/28\n",
      " 524/1415 [==========>...................] - ETA: 6s - loss: 0.0030755/755 - 1s - loss: 0.0049 - 609ms/epoch - 806us/step\n",
      "Epoch 4/28\n",
      " 288/1415 [=====>........................] - ETA: 7s - loss: 0.0030755/755 - 1s - loss: 0.0059 - 1s/epoch - 2ms/step\n",
      "755/755 - 1s - loss: 0.0057 - 762ms/epoch - 1ms/step\n",
      " 489/1415 [=========>....................] - ETA: 5s - loss: 0.0031Epoch 17/28\n",
      "Epoch 5/28\n",
      " 481/1415 [=========>....................] - ETA: 5s - loss: 0.0030Epoch 1/28\n",
      "Epoch 1/28\n",
      "755/755 - 1s - loss: 0.0051 - 860ms/epoch - 1ms/step\n",
      " 680/1415 [=============>................] - ETA: 4s - loss: 0.0032Epoch 18/28\n",
      " 511/1415 [=========>....................] - ETA: 5s - loss: 0.0030755/755 - 1s - loss: 0.0048 - 878ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 934/1415 [==================>...........] - ETA: 3s - loss: 0.0030755/755 - 1s - loss: 0.0072 - 698ms/epoch - 925us/step\n",
      "Epoch 2/28\n",
      " 618/1415 [============>.................] - ETA: 5s - loss: 0.0030755/755 - 1s - loss: 0.0063 - 751ms/epoch - 995us/step\n",
      "Epoch 2/28\n",
      " 659/1415 [============>.................] - ETA: 4s - loss: 0.0030755/755 - 1s - loss: 0.0059 - 967ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1007/1415 [====================>.........] - ETA: 2s - loss: 0.0030755/755 - 1s - loss: 0.0054 - 809ms/epoch - 1ms/step\n",
      " 688/1415 [=============>................] - ETA: 4s - loss: 0.0030755/755 - 1s - loss: 0.0059 - 760ms/epoch - 1ms/step\n",
      " 850/1415 [=================>............] - ETA: 3s - loss: 0.0032755/755 - 1s - loss: 0.0059 - 1s/epoch - 2ms/step\n",
      "Epoch 3/28\n",
      "Epoch 3/28\n",
      "Epoch 19/28\n",
      "1030/1415 [====================>.........] - ETA: 2s - loss: 0.0032755/755 - 1s - loss: 0.0053 - 1s/epoch - 1ms/step\n",
      " 886/1415 [=================>............] - ETA: 3s - loss: 0.0031755/755 - 1s - loss: 0.0058 - 1s/epoch - 1ms/step\n",
      "1077/1415 [=====================>........] - ETA: 2s - loss: 0.0032755/755 - 1s - loss: 0.0057 - 1s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "755/755 - 1s - loss: 0.0047 - 1s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "Epoch 4/28\n",
      "Epoch 20/28\n",
      "1114/1415 [======================>.......] - ETA: 1s - loss: 0.0030755/755 - 1s - loss: 0.0052 - 925ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "1110/1415 [======================>.......] - ETA: 1s - loss: 0.0031755/755 - 1s - loss: 0.0060 - 963ms/epoch - 1ms/step\n",
      "1132/1415 [=======================>......] - ETA: 1s - loss: 0.0030755/755 - 1s - loss: 0.0053 - 960ms/epoch - 1ms/step\n",
      "755/755 - 1s - loss: 0.0055 - 966ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0030 - val_loss: 0.0027\n",
      "1281/1415 [==========================>...] - ETA: 0s - loss: 0.0031Epoch 5/28\n",
      "Epoch 21/28\n",
      "Epoch 5/28\n",
      "Epoch 15/28\n",
      "1367/1415 [===========================>..] - ETA: 0s - loss: 0.0031755/755 - 1s - loss: 0.0055 - 907ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 193/1415 [===>..........................] - ETA: 4s - loss: 0.0028755/755 - 1s - loss: 0.0057 - 1s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "755/755 - 1s - loss: 0.0055 - 1s/epoch - 1ms/step\n",
      "755/755 - 1s - loss: 0.0051 - 1s/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0031 - val_loss: 0.0028\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0031 - val_loss: 0.0028\n",
      "Epoch 6/28\n",
      "Epoch 15/28\n",
      "Epoch 16/28\n",
      "Epoch 22/28\n",
      " 341/1415 [======>.......................] - ETA: 7s - loss: 0.0029755/755 - 2s - loss: 0.0051 - 2s/epoch - 2ms/step\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0031 - val_loss: 0.0027\n",
      " 254/1415 [====>.........................] - ETA: 12s - loss: 0.0030Epoch 16/28\n",
      " 529/1415 [==========>...................] - ETA: 7s - loss: 0.0028755/755 - 3s - loss: 0.0055 - 3s/epoch - 5ms/step\n",
      "Epoch 7/28\n",
      " 374/1415 [======>.......................] - ETA: 9s - loss: 0.0030 755/755 - 4s - loss: 0.0049 - 4s/epoch - 5ms/step1s - loss: 0.00\n",
      "Epoch 23/28\n",
      " 217/1415 [===>..........................] - ETA: 6s - loss: 0.0030755/755 - 4s - loss: 0.0064 - 4s/epoch - 5ms/step\n",
      " 399/1415 [=======>......................] - ETA: 9s - loss: 0.0029 Epoch 7/28\n",
      " 617/1415 [============>.................] - ETA: 6s - loss: 0.0028755/755 - 3s - loss: 0.0049 - 3s/epoch - 3ms/step\n",
      " 400/1415 [=======>......................] - ETA: 9s - loss: 0.0029Epoch 12/28\n",
      " 516/1415 [=========>....................] - ETA: 7s - loss: 0.0029755/755 - 1s - loss: 0.0053 - 1s/epoch - 2ms/step\n",
      "Epoch 8/28\n",
      " 351/1415 [======>.......................] - ETA: 6s - loss: 0.0029755/755 - 1s - loss: 0.0050 - 1s/epoch - 2ms/step\n",
      "755/755 - 1s - loss: 0.0050 - 1s/epoch - 2ms/step\n",
      " 527/1415 [==========>...................] - ETA: 7s - loss: 0.0030755/755 - 1s - loss: 0.0051 - 1s/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "Epoch 8/28\n",
      "Epoch 24/28\n",
      " 889/1415 [=================>............] - ETA: 4s - loss: 0.0028755/755 - 1s - loss: 0.0048 - 1s/epoch - 1ms/step\n",
      " 452/1415 [========>.....................] - ETA: 6s - loss: 0.0028755/755 - 1s - loss: 0.0062 - 1s/epoch - 1ms/step\n",
      " 687/1415 [=============>................] - ETA: 6s - loss: 0.0030755/755 - 1s - loss: 0.0055 - 1s/epoch - 1ms/step\n",
      " 684/1415 [=============>................] - ETA: 6s - loss: 0.0029755/755 - 1s - loss: 0.0052 - 1s/epoch - 1ms/step\n",
      " 517/1415 [=========>....................] - ETA: 5s - loss: 0.0030Epoch 9/28\n",
      "Epoch 14/28\n",
      "Epoch 9/28\n",
      "Epoch 25/28\n",
      " 845/1415 [================>.............] - ETA: 4s - loss: 0.0029755/755 - 1s - loss: 0.0053 - 1s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1057/1415 [=====================>........] - ETA: 2s - loss: 0.0029755/755 - 1s - loss: 0.0050 - 1s/epoch - 1ms/step\n",
      "755/755 - 1s - loss: 0.0053 - 1s/epoch - 1ms/step\n",
      " 573/1415 [===========>..................] - ETA: 5s - loss: 0.0031755/755 - 1s - loss: 0.0053 - 1s/epoch - 1ms/step\n",
      " 876/1415 [=================>............] - ETA: 4s - loss: 0.0029Epoch 26/28\n",
      "Epoch 15/28\n",
      "Epoch 10/28\n",
      "1012/1415 [====================>.........] - ETA: 3s - loss: 0.0029755/755 - 1s - loss: 0.0052 - 973ms/epoch - 1ms/step2\n",
      "Epoch 11/28\n",
      "755/755 - 1s - loss: 0.0047 - 1s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "1211/1415 [========================>.....] - ETA: 1s - loss: 0.0029755/755 - 1s - loss: 0.0053 - 1s/epoch - 1ms/step\n",
      " 981/1415 [===================>..........] - ETA: 3s - loss: 0.0029755/755 - 1s - loss: 0.0050 - 1s/epoch - 1ms/step\n",
      " 816/1415 [================>.............] - ETA: 4s - loss: 0.0030Epoch 16/28\n",
      "Epoch 11/28\n",
      "1214/1415 [========================>.....] - ETA: 1s - loss: 0.0030755/755 - 1s - loss: 0.0054 - 1s/epoch - 1ms/step\n",
      "1000/1415 [====================>.........] - ETA: 2s - loss: 0.0030Epoch 12/28\n",
      "1233/1415 [=========================>....] - ETA: 1s - loss: 0.0030755/755 - 1s - loss: 0.0051 - 992ms/epoch - 1ms/step\n",
      "755/755 - 1s - loss: 0.0051 - 1s/epoch - 1ms/step\n",
      " 901/1415 [==================>...........] - ETA: 3s - loss: 0.0030Epoch 12/28\n",
      "Epoch 28/28\n",
      "755/755 - 1s - loss: 0.0048 - 984ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 16/28\n",
      "   1/1415 [..............................] - ETA: 8s - loss: 0.0027755/755 - 1s - loss: 0.0047 - 1s/epoch - 1ms/step\n",
      "755/755 - 2s - loss: 0.0053 - 2s/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0029 - val_loss: 0.0027\n",
      "1241/1415 [=========================>....] - ETA: 1s - loss: 0.0030755/755 - 1s - loss: 0.0053 - 1s/epoch - 2ms/step\n",
      "755/755 - 1s - loss: 0.0052 - 1s/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0029 - val_loss: 0.0026\n",
      "  32/1415 [..............................] - ETA: 2s - loss: 0.0028Epoch 13/28\n",
      "Epoch 16/28\n",
      "Epoch 18/28\n",
      "Epoch 17/28\n",
      " 100/1415 [=>............................] - ETA: 7s - loss: 0.0028189/189 - 0s - loss: 0.0043 - 228ms/epoch - 1ms/step\n",
      "1398/1415 [============================>.] - ETA: 0s - loss: 0.0030755/755 - 1s - loss: 0.0043 - 567ms/epoch - 751us/step\n",
      "1296/1415 [==========================>...] - ETA: 0s - loss: 0.0029755/755 - 1s - loss: 0.0047 - 1s/epoch - 1ms/stepoptimizer=<keras.optimizers.legacy.adam.Adam object at 0x13a0544c0>; total time=58.4mi\n",
      " 176/1415 [==>...........................] - ETA: 4s - loss: 0.0027\n",
      "755/755 - 1s - loss: 0.0050 - 1s/epoch - 1ms/step\n",
      " 174/1415 [==>...........................] - ETA: 4s - loss: 0.0028Epoch 14/28\n",
      "755/755 - 1s - loss: 0.0050 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0031 - val_loss: 0.0036......] - ETA: 4s - loss: 0.002\n",
      "Epoch 16/28\n",
      " 491/1415 [=========>....................] - ETA: 3s - loss: 0.0031755/755 - 1s - loss: 0.0054 - 941ms/epoch - 1ms/step\n",
      " 488/1415 [=========>....................] - ETA: 3s - loss: 0.0028755/755 - 1s - loss: 0.0053 - 964ms/epoch - 1ms/step\n",
      " 538/1415 [==========>...................] - ETA: 3s - loss: 0.0028755/755 - 1s - loss: 0.0047 - 957ms/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0029 - val_loss: 0.0027\n",
      "  90/1415 [>.............................] - ETA: 2s - loss: 0.0030Epoch 15/28\n",
      "Epoch 15/28\n",
      "Epoch 20/28\n",
      "Epoch 17/28\n",
      " 692/1415 [=============>................] - ETA: 3s - loss: 0.0029755/755 - 1s - loss: 0.0051 - 1s/epoch - 2ms/step\n",
      " 645/1415 [============>.................] - ETA: 3s - loss: 0.0030755/755 - 1s - loss: 0.0053 - 1s/epoch - 2ms/step\n",
      " 142/1415 [==>...........................] - ETA: 5s - loss: 0.0028755/755 - 1s - loss: 0.0047 - 1s/epoch - 2ms/step\n",
      "Epoch 21/28\n",
      "Epoch 16/28\n",
      "Epoch 16/28\n",
      " 825/1415 [================>.............] - ETA: 2s - loss: 0.0029755/755 - 1s - loss: 0.0051 - 1s/epoch - 1ms/step\n",
      " 881/1415 [=================>............] - ETA: 2s - loss: 0.0028755/755 - 1s - loss: 0.0048 - 1s/epoch - 1ms/step\n",
      " 289/1415 [=====>........................] - ETA: 6s - loss: 0.0028Epoch 17/28\n",
      "755/755 - 1s - loss: 0.0048 - 1s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "Epoch 17/28\n",
      " 489/1415 [=========>....................] - ETA: 5s - loss: 0.0028755/755 - 1s - loss: 0.0048 - 1s/epoch - 1ms/steps - loss: 0.002\n",
      "Epoch 18/28\n",
      "755/755 - 1s - loss: 0.0047 - 1s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1048/1415 [=====================>........] - ETA: 1s - loss: 0.0028755/755 - 1s - loss: 0.0052 - 1s/epoch - 1ms/step\n",
      "1016/1415 [====================>.........] - ETA: 2s - loss: 0.0029Epoch 18/28\n",
      " 731/1415 [==============>...............] - ETA: 3s - loss: 0.0028755/755 - 1s - loss: 0.0048 - 982ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 779/1415 [===============>..............] - ETA: 3s - loss: 0.0031755/755 - 1s - loss: 0.0053 - 1s/epoch - 1ms/step\n",
      "755/755 - 1s - loss: 0.0045 - 1s/epoch - 2ms/step\n",
      "1265/1415 [=========================>....] - ETA: 0s - loss: 0.0029Epoch 24/28\n",
      "Epoch 19/28\n",
      "1414/1415 [============================>.] - ETA: 0s - loss: 0.0028755/755 - 1s - loss: 0.0048 - 897ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0028 - val_loss: 0.0025\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0029 - val_loss: 0.0026\n",
      "755/755 - 1s - loss: 0.0055 - 1s/epoch - 1ms/step\n",
      "755/755 - 1s - loss: 0.0056 - 1s/epoch - 2ms/step\n",
      " 975/1415 [===================>..........] - ETA: 2s - loss: 0.0027Epoch 25/28\n",
      "Epoch 20/28\n",
      "Epoch 18/28\n",
      "Epoch 17/28\n",
      "Epoch 17/28\n",
      "  97/1415 [=>............................] - ETA: 3s - loss: 0.0028755/755 - 1s - loss: 0.0050 - 926ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 167/1415 [==>...........................] - ETA: 4s - loss: 0.0028755/755 - 1s - loss: 0.0046 - 1s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1192/1415 [========================>.....] - ETA: 1s - loss: 0.0028755/755 - 1s - loss: 0.0043 - 1s/epoch - 1ms/step\n",
      " 195/1415 [===>..........................] - ETA: 4s - loss: 0.0027755/755 - 1s - loss: 0.0049 - 915ms/epoch - 1ms/step\n",
      "1183/1415 [========================>.....] - ETA: 1s - loss: 0.0030Epoch 22/28\n",
      "Epoch 26/28\n",
      " 300/1415 [=====>........................] - ETA: 6s - loss: 0.0025755/755 - 1s - loss: 0.0049 - 1s/epoch - 2ms/step\n",
      "Epoch 22/28\n",
      " 337/1415 [======>.......................] - ETA: 6s - loss: 0.0027755/755 - 1s - loss: 0.0044 - 1s/epoch - 2ms/step\n",
      "755/755 - 1s - loss: 0.0052 - 1s/epoch - 2ms/step\n",
      " 298/1415 [=====>........................] - ETA: 6s - loss: 0.0028Epoch 27/28\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 18/28\n",
      "755/755 - 1s - loss: 0.0054 - 1s/epoch - 1ms/step\n",
      " 679/1415 [=============>................] - ETA: 3s - loss: 0.0028755/755 - 1s - loss: 0.0046 - 1s/epoch - 1ms/step\n",
      "  33/1415 [..............................] - ETA: 2s - loss: 0.0027Epoch 23/28\n",
      "755/755 - 1s - loss: 0.0050 - 1s/epoch - 1ms/step\n",
      " 628/1415 [============>.................] - ETA: 4s - loss: 0.0026Epoch 24/28\n",
      "Epoch 28/28\n",
      " 713/1415 [==============>...............] - ETA: 4s - loss: 0.0027755/755 - 1s - loss: 0.0045 - 1s/epoch - 1ms/step\n",
      " 133/1415 [=>............................] - ETA: 10s - loss: 0.0027755/755 - 1s - loss: 0.0051 - 1s/epoch - 2ms/step\n",
      " 736/1415 [==============>...............] - ETA: 3s - loss: 0.0026755/755 - 1s - loss: 0.0047 - 1s/epoch - 1ms/step\n",
      " 744/1415 [==============>...............] - ETA: 3s - loss: 0.0028Epoch 24/28\n",
      "Epoch 25/28\n",
      " 218/1415 [===>..........................] - ETA: 9s - loss: 0.0029 189/189 - 0s - loss: 0.0039 - 227ms/epoch - 1ms/step\n",
      " 921/1415 [==================>...........] - ETA: 2s - loss: 0.0027755/755 - 1s - loss: 0.0040 - 599ms/epoch - 793us/step\n",
      "755/755 - 1s - loss: 0.0048 - 971ms/epoch - 1ms/step\n",
      "1030/1415 [====================>.........] - ETA: 2s - loss: 0.0028[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x148c944c0>; total time=58.1mi\n",
      "Epoch 25/28\n",
      "755/755 - 1s - loss: 0.0046 - 973ms/epoch - 1ms/step\n",
      " 339/1415 [======>.......................] - ETA: 7s - loss: 0.0029Epoch 26/28\n",
      " 577/1415 [===========>..................] - ETA: 4s - loss: 0.0028755/755 - 1s - loss: 0.0048 - 1s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 566/1415 [===========>..................] - ETA: 4s - loss: 0.0027755/755 - 1s - loss: 0.0049 - 837ms/epoch - 1ms/step\n",
      "1223/1415 [========================>.....] - ETA: 1s - loss: 0.0026Epoch 27/28\n",
      " 706/1415 [=============>................] - ETA: 4s - loss: 0.0030755/755 - 1s - loss: 0.0048 - 676ms/epoch - 895us/step\n",
      "Epoch 27/28\n",
      " 833/1415 [================>.............] - ETA: 3s - loss: 0.0027755/755 - 1s - loss: 0.0045 - 957ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      " 917/1415 [==================>...........] - ETA: 2s - loss: 0.0029755/755 - 1s - loss: 0.0050 - 732ms/epoch - 969us/step\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0026 - val_loss: 0.0028\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 28/28\n",
      "Epoch 19/28\n",
      "Epoch 18/28\n",
      "Epoch 18/28\n",
      "1075/1415 [=====================>........] - ETA: 1s - loss: 0.0026755/755 - 1s - loss: 0.0046 - 876ms/epoch - 1ms/step\n",
      "1079/1415 [=====================>........] - ETA: 1s - loss: 0.0029189/189 - 0s - loss: 0.0040 - 130ms/epoch - 689us/step\n",
      " 250/1415 [====>.........................] - ETA: 2s - loss: 0.0025755/755 - 1s - loss: 0.0048 - 871ms/epoch - 1ms/step\n",
      " 331/1415 [======>.......................] - ETA: 4s - loss: 0.0025189/189 - 0s - loss: 0.0054 - 182ms/epoch - 962us/step\n",
      "1301/1415 [==========================>...] - ETA: 0s - loss: 0.0026755/755 - 0s - loss: 0.0054 - 350ms/epoch - 463us/step\n",
      " 444/1415 [========>.....................] - ETA: 3s - loss: 0.0025755/755 - 1s - loss: 0.0038 - 502ms/epoch - 665us/step\n",
      " 508/1415 [=========>....................] - ETA: 3s - loss: 0.0027[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x13954f340>; total time=58.0min\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x148e3d8e0>; total time=58.1min\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0028 - val_loss: 0.0025\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0027 - val_loss: 0.0024\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 20/28\n",
      "Epoch 19/28\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0028 - val_loss: 0.0025\n",
      " 788/1415 [===============>..............] - ETA: 1s - loss: 0.0027Epoch 20/28\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      " 877/1415 [=================>............] - ETA: 1s - loss: 0.0025Epoch 21/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0024 - val_loss: 0.0023002\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0024 - val_loss: 0.0022...............] - ETA: 2s - loss: 0.002\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      " 743/1415 [==============>...............] - ETA: 2s - loss: 0.0024Epoch 22/28\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0027 - val_loss: 0.0043\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 4s 3ms/ - loss: 0.0026step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0026 - val_loss: 0.0023.......] - ETA: 3s - loss: 0.002TA: 2s - loss: 0.002\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      " 864/1415 [=================>............] - ETA: 1s - loss: 0.0024Epoch 23/28\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0022 - val_loss: 0.0026===>......] - ETA: 0s - loss: 0.002\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0.......] - ETA: 0s - loss: 0.002025 - val_loss: 0.0022\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0022 - val_loss: 0.0021: 0s - loss: 0.002\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0023 - val_loss: 0.0023 - ETA: 2s - loss: 0.002- loss: 0.002\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "1323/1415 [===========================>..] - ETA: 0s - loss: 0.0021Epoch 25/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0023 - val_loss: 0.0196\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0023 - val_loss: 0.00280\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0020 - val_loss: 0.00190.002\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      " 933/1415 [==================>...........] - ETA: 0s - loss: 0.0022Epoch 26/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      " 648/1415 [============>.................] - ETA: 1s - loss: 0.0020Epoch 26/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "1022/1415 [====================>.........] - ETA: 1s - loss: 0.0023Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0020 - val_loss: 0.0018..........] - ETA: 2s - loss: 0.002\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 28/28\n",
      "1415/1415 [=====================.........] - ETA: 1s - loss: 0.0029=========] - 4s 3ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0023 - val_loss: 0.0019\n",
      "Epoch 28/28\n",
      "1126/1415 [======================>.......] - ETA: 0s - loss: 0.0019Epoch 1/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0020 - val_loss: 0.0018\n",
      "1191/1415 [========================>.....] - ETA: 0s - loss: 0.0021755/755 - 1s - loss: 0.0025 - 881ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "1247/1415 [=========================>....] - ETA: 0s - loss: 0.0021755/755 - 1s - loss: 0.0025 - 506ms/epoch - 670us/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0020 - val_loss: 0.0018\n",
      "1334/1415 [===========================>..] - ETA: 0s - loss: 0.0020755/755 - 1s - loss: 0.0026 - 779ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 1/28\n",
      "755/755 - 1s - loss: 0.0022 - 516ms/epoch - 683us/step\n",
      "Epoch 5/28\n",
      "Epoch 1/28\n",
      "755/755 - 1s - loss: 0.0020 - 784ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "755/755 - 1s - loss: 0.0022 - 774ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "Epoch 1/28\n",
      "755/755 - 1s - loss: 0.0022 - 681ms/epoch - 902us/step\n",
      "Epoch 2/28\n",
      "755/755 - 1s - loss: 0.0020 - 703ms/epoch - 931us/step\n",
      "Epoch 3/28\n",
      "755/755 - 1s - loss: 0.0020 - 696ms/epoch - 922us/step\n",
      "Epoch 7/28\n",
      "Epoch 1/28\n",
      "755/755 - 1s - loss: 0.0022 - 936ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "755/755 - 1s - loss: 0.0022 - 956ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "755/755 - 1s - loss: 0.0021 - 965ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "755/755 - 1s - loss: 0.0022 - 973ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "755/755 - 1s - loss: 0.0024 - 814ms/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "755/755 - 1s - loss: 0.0021 - 979ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "755/755 - 1s - loss: 0.0020 - 998ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "755/755 - 1s - loss: 0.0022 - 972ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "755/755 - 1s - loss: 0.0021 - 998ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "755/755 - 1s - loss: 0.0022 - 969ms/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "755/755 - 1s - loss: 0.0023 - 785ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "755/755 - 1s - loss: 0.0020 - 949ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "755/755 - 1s - loss: 0.0019 - 993ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "755/755 - 1s - loss: 0.0022 - 952ms/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "755/755 - 1s - loss: 0.0020 - 973ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "755/755 - 1s - loss: 0.0019 - 981ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "755/755 - 1s - loss: 0.0020 - 855ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "755/755 - 1s - loss: 0.0021 - 832ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "755/755 - 1s - loss: 0.0021 - 808ms/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "755/755 - 1s - loss: 0.0022 - 799ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "755/755 - 1s - loss: 0.0021 - 963ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "755/755 - 1s - loss: 0.0020 - 969ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "755/755 - 1s - loss: 0.0019 - 970ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "755/755 - 1s - loss: 0.0021 - 954ms/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "755/755 - 1s - loss: 0.0021 - 965ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "755/755 - 1s - loss: 0.0030 - 795ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "755/755 - 1s - loss: 0.0018 - 961ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "755/755 - 1s - loss: 0.0019 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "755/755 - 1s - loss: 0.0023 - 992ms/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "755/755 - 1s - loss: 0.0019 - 988ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "755/755 - 1s - loss: 0.0018 - 986ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "755/755 - 1s - loss: 0.0021 - 963ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "755/755 - 1s - loss: 0.0019 - 989ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "755/755 - 1s - loss: 0.0022 - 985ms/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "755/755 - 1s - loss: 0.0022 - 1s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "755/755 - 1s - loss: 0.0019 - 1s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "755/755 - 1s - loss: 0.0020 - 842ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "755/755 - 1s - loss: 0.0022 - 815ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "755/755 - 1s - loss: 0.0020 - 824ms/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "755/755 - 1s - loss: 0.0021 - 791ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "755/755 - 1s - loss: 0.0022 - 904ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "755/755 - 1s - loss: 0.0018 - 847ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "755/755 - 1s - loss: 0.0017 - 876ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "755/755 - 1s - loss: 0.0025 - 889ms/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "755/755 - 1s - loss: 0.0018 - 884ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "755/755 - 1s - loss: 0.0020 - 915ms/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "755/755 - 1s - loss: 0.0020 - 961ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "755/755 - 1s - loss: 0.0022 - 990ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "755/755 - 1s - loss: 0.0020 - 1s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "755/755 - 1s - loss: 0.0021 - 1s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "755/755 - 1s - loss: 0.0018 - 848ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "755/755 - 1s - loss: 0.0017 - 804ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "755/755 - 1s - loss: 0.0017 - 797ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "755/755 - 1s - loss: 0.0022 - 778ms/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "755/755 - 1s - loss: 0.0018 - 774ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "755/755 - 1s - loss: 0.0024 - 781ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "755/755 - 1s - loss: 0.0020 - 823ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "755/755 - 1s - loss: 0.0018 - 816ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "755/755 - 1s - loss: 0.0020 - 785ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "755/755 - 1s - loss: 0.0019 - 845ms/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "755/755 - 1s - loss: 0.0018 - 906ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "755/755 - 1s - loss: 0.0019 - 886ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "755/755 - 1s - loss: 0.0017 - 874ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "755/755 - 1s - loss: 0.0020 - 919ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "755/755 - 1s - loss: 0.0019 - 860ms/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "755/755 - 1s - loss: 0.0019 - 735ms/epoch - 973us/step\n",
      "Epoch 15/28\n",
      "755/755 - 1s - loss: 0.0018 - 789ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "755/755 - 1s - loss: 0.0019 - 777ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "755/755 - 1s - loss: 0.0025 - 773ms/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "755/755 - 1s - loss: 0.0020 - 819ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "755/755 - 1s - loss: 0.0019 - 770ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "755/755 - 1s - loss: 0.0019 - 774ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "755/755 - 1s - loss: 0.0018 - 774ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "755/755 - 1s - loss: 0.0018 - 787ms/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "755/755 - 1s - loss: 0.0018 - 802ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "755/755 - 1s - loss: 0.0018 - 787ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "755/755 - 1s - loss: 0.0021 - 819ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "755/755 - 1s - loss: 0.0020 - 808ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "755/755 - 1s - loss: 0.0022 - 798ms/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "755/755 - 1s - loss: 0.0021 - 803ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "755/755 - 1s - loss: 0.0019 - 780ms/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "755/755 - 1s - loss: 0.0020 - 743ms/epoch - 984us/step\n",
      "Epoch 19/28\n",
      "755/755 - 1s - loss: 0.0017 - 750ms/epoch - 993us/step\n",
      "Epoch 24/28\n",
      "755/755 - 1s - loss: 0.0020 - 753ms/epoch - 998us/step\n",
      "Epoch 18/28\n",
      "755/755 - 1s - loss: 0.0017 - 771ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "755/755 - 1s - loss: 0.0021 - 755ms/epoch - 1000us/step\n",
      "Epoch 19/28\n",
      "755/755 - 1s - loss: 0.0017 - 797ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "755/755 - 1s - loss: 0.0019 - 782ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "755/755 - 1s - loss: 0.0018 - 783ms/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "755/755 - 1s - loss: 0.0018 - 788ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "755/755 - 1s - loss: 0.0017 - 790ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "755/755 - 1s - loss: 0.0017 - 784ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "755/755 - 1s - loss: 0.0017 - 784ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "755/755 - 1s - loss: 0.0019 - 760ms/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "755/755 - 1s - loss: 0.0019 - 776ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "755/755 - 1s - loss: 0.0022 - 784ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "755/755 - 1s - loss: 0.0019 - 787ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "755/755 - 1s - loss: 0.0022 - 816ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "755/755 - 1s - loss: 0.0020 - 813ms/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "755/755 - 1s - loss: 0.0020 - 852ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "755/755 - 1s - loss: 0.0017 - 852ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "755/755 - 1s - loss: 0.0017 - 843ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "755/755 - 1s - loss: 0.0016 - 812ms/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "755/755 - 1s - loss: 0.0022 - 792ms/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "755/755 - 1s - loss: 0.0017 - 794ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "755/755 - 1s - loss: 0.0019 - 784ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "755/755 - 1s - loss: 0.0018 - 832ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "755/755 - 1s - loss: 0.0019 - 801ms/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "755/755 - 1s - loss: 0.0018 - 821ms/epoch - 1ms/step\n",
      "755/755 - 1s - loss: 0.0017 - 800ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "755/755 - 1s - loss: 0.0018 - 808ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "189/189 - 0s - loss: 0.0016 - 167ms/epoch - 885us/step\n",
      "755/755 - 1s - loss: 0.0017 - 864ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "755/755 - 1s - loss: 0.0016 - 537ms/epoch - 712us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16e7b1100>; total time=59.2min\n",
      "755/755 - 1s - loss: 0.0019 - 869ms/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "755/755 - 1s - loss: 0.0018 - 932ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "755/755 - 1s - loss: 0.0021 - 912ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "755/755 - 1s - loss: 0.0019 - 837ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "755/755 - 1s - loss: 0.0022 - 801ms/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "755/755 - 1s - loss: 0.0016 - 803ms/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "755/755 - 1s - loss: 0.0019 - 785ms/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "755/755 - 1s - loss: 0.0018 - 689ms/epoch - 912us/step\n",
      "Epoch 27/28\n",
      "755/755 - 1s - loss: 0.0019 - 678ms/epoch - 898us/step\n",
      "Epoch 26/28\n",
      "755/755 - 1s - loss: 0.0023 - 709ms/epoch - 938us/step\n",
      "Epoch 28/28\n",
      "755/755 - 1s - loss: 0.0017 - 698ms/epoch - 924us/step\n",
      "Epoch 27/28\n",
      "755/755 - 1s - loss: 0.0018 - 688ms/epoch - 911us/step\n",
      "Epoch 28/28\n",
      "755/755 - 1s - loss: 0.0018 - 685ms/epoch - 908us/step\n",
      "Epoch 27/28\n",
      "755/755 - 1s - loss: 0.0017 - 696ms/epoch - 921us/step\n",
      "755/755 - 1s - loss: 0.0018 - 680ms/epoch - 901us/step\n",
      "Epoch 28/28\n",
      "189/189 - 0s - loss: 0.0015 - 151ms/epoch - 801us/step\n",
      "755/755 - 1s - loss: 0.0016 - 679ms/epoch - 900us/step\n",
      "755/755 - 1s - loss: 0.0018 - 681ms/epoch - 902us/step\n",
      "Epoch 28/28\n",
      "189/189 - 0s - loss: 0.0026 - 153ms/epoch - 811us/step\n",
      "755/755 - 0s - loss: 0.0015 - 348ms/epoch - 461us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x13e28f340>; total time=59.2min\n",
      "755/755 - 1s - loss: 0.0018 - 652ms/epoch - 863us/step\n",
      "189/189 - 0s - loss: 0.0016 - 135ms/epoch - 716us/step\n",
      "755/755 - 0s - loss: 0.0026 - 320ms/epoch - 424us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x157c55100>; total time=59.2min\n",
      "755/755 - 1s - loss: 0.0021 - 561ms/epoch - 743us/step\n",
      "755/755 - 0s - loss: 0.0016 - 252ms/epoch - 333us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16e6e1100>; total time=59.1min\n",
      "189/189 - 0s - loss: 0.0016 - 115ms/epoch - 606us/step\n",
      "755/755 - 0s - loss: 0.0016 - 205ms/epoch - 272us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x13d9d5100>; total time=59.2min\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 90522\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/28\n",
      "   1/1415 [..............................] - ETA: 5:23 - loss: 2.6652Epoch 1/28\n",
      "  12/1415 [..............................] - ETA: 10s - loss: 2.2018 Epoch 1/28\n",
      "  56/1415 [>.............................] - ETA: 6s - loss: 1.7916Epoch 1/28\n",
      "  46/1415 [..............................] - ETA: 3s - loss: 1.8851Epoch 1/28\n",
      "1336/1415 [===========================>..] - ETA: 0s - loss: 0.1651Epoch 1/28\n",
      "1391/1415 [============================>.] - ETA: 0s - loss: 0.1593Epoch 1/28\n",
      "1413/1415 [============================>.] - ETA: 0s - loss: 0.1571Epoch 1/28\n",
      "   4/1415 [..............................] - ETA: 23s - loss: 2.3669  Epoch 1/28\n",
      " 103/1415 [=>............................] - ETA: 7s - loss: 1.5433Epoch 1/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.1893 - val_loss: 0.0180\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.1841 - val_loss: 0.0159\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.1869 - val_loss: 0.0155\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.1617 - val_loss: 0.0172\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.1570 - val_loss: 0.0171......] - ETA: 2s - loss: 0.016\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0151 - val_loss: 0.0130TA: 2s - loss: 0.224\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0128 - val_loss: 0.0098\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0148 - val_loss: 0.0123\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0150 - val_loss: 0.0125\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0131 - val_loss: 0.0101\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0131 - val_loss: 0.0112\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0124 - val_loss: 0.0112\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0090 - val_loss: 0.0078\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0090 - val_loss: 0.0075\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0130 - val_loss: 0.0144\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.2143 - val_loss: 0.0174\n",
      "1415/1415 [==============================] - 10s 6ms/step - loss: 0.2014 - val_loss: 0.0159\n",
      " 668/1415 [=============>................] - ETA: 1s - loss: 0.0117Epoch 2/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.1522 - val_loss: 0.0160\n",
      "1415/1415 [==============================] - 10s 6ms/step - loss: 0.1506 - val_loss: 0.0159\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.1509 - val_loss: 0.0143\n",
      "Epoch 2/28\n",
      "Epoch 2/28\n",
      "Epoch 2/28\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0118 - val_loss: 0.0103\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0112 - val_loss: 0.0103\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0074 - val_loss: 0.0063\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0072 - val_loss: 0.0062\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0104 - val_loss: 0.0093\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0108 - val_loss: 0.0098\n",
      "1252/1415 [=========================>....] - ETA: 0s - loss: 0.0109Epoch 6/28\n",
      "1372/1415 [============================>.] - ETA: 0s - loss: 0.0065Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0065 - val_loss: 0.0066\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0062 - val_loss: 0.0059\n",
      "1413/1415 [============================>.] - ETA: 0s - loss: 0.0108Epoch 6/28\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0108 - val_loss: 0.0092\n",
      " 144/1415 [==>...........................] - ETA: 2s - loss: 0.0064Epoch 6/28\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0149 - val_loss: 0.0126\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0126 - val_loss: 0.0098\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0119 - val_loss: 0.0094\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0124 - val_loss: 0.0096\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0161 - val_loss: 0.0147\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0100 - val_loss: 0.0088\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0095 - val_loss: 0.0097\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0057 - val_loss: 0.0059\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0099 - val_loss: 0.0086\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0094 - val_loss: 0.0087\n",
      " 893/1415 [=================>............] - ETA: 2s - loss: 0.0090Epoch 8/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0091 - val_loss: 0.0082\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0051 - val_loss: 0.0044\n",
      " 153/1415 [==>...........................] - ETA: 2s - loss: 0.0100Epoch 8/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0094 - val_loss: 0.0089\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0124 - val_loss: 0.0112\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0090 - val_loss: 0.0079\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0086 - val_loss: 0.0072\n",
      "1122/1415 [======================>.......] - ETA: 0s - loss: 0.0091Epoch 4/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0086 - val_loss: 0.0074\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0135 - val_loss: 0.0118\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0092 - val_loss: 0.0082\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0086 - val_loss: 0.0095\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0090 - val_loss: 0.0085\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0087 - val_loss: 0.0105\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0043 - val_loss: 0.0058\n",
      " 801/1415 [===============>..............] - ETA: 11s - loss: 0.0077Epoch 10/28\n",
      "   1/1415 [..............................] - ETA: 29s - loss: 0.0055Epoch 10/28\n",
      "1415/1415 [==============================] - 37s 26ms/step - loss: 0.0085 - val_loss: 0.0072\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 37s 26ms/step - loss: 0.0045 - val_loss: 0.0062\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 38s 27ms/step - loss: 0.0088 - val_loss: 0.0089\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0041 - val_loss: 0.0102>....] - ETA: 5s - loss: 0.011 ETA: 1s - loss: 0.00\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0082 - val_loss: 0.0074\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 41s 29ms/step - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 41s 29ms/step - loss: 0.0073 - val_loss: 0.0063\n",
      " 168/1415 [==>...........................] - ETA: 2s - loss: 0.0081Epoch 5/28\n",
      "1415/1415 [==============================] - 41s 29ms/step - loss: 0.0120 - val_loss: 0.0102\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0080 - val_loss: 0.0072\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0045 - val_loss: 0.0038\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 42s 29ms/step - loss: 0.0068 - val_loss: 0.0060\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 42s 30ms/step - loss: 0.0072 - val_loss: 0.0120\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0083 - val_loss: 0.0075\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 20s 14ms/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 20s 14ms/step - loss: 0.0083 - val_loss: 0.0104\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 20s 14ms/step - loss: 0.0078 - val_loss: 0.0079\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 19s 14ms/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 19s 14ms/step - loss: 0.0080 - val_loss: 0.0140\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0037 - val_loss: 0.0059\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0079 - val_loss: 0.0068\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0038 - val_loss: 0.0059\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0104 - val_loss: 0.0091\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0063 - val_loss: 0.0054\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0077 - val_loss: 0.0082\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0059 - val_loss: 0.0052\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 24s 17ms/step - loss: 0.0063 - val_loss: 0.0057\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0076 - val_loss: 0.0081\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0074 - val_loss: 0.0067\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0075 - val_loss: 0.0088\n",
      " 505/1415 [=========>....................] - ETA: 19s - loss: 0.0105Epoch 14/28\n",
      "1415/1415 [==============================] - 24s 17ms/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 25s 17ms/step - loss: 0.0075 - val_loss: 0.0092\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 25s 17ms/step - loss: 0.0070 - val_loss: 0.0061\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 25s 18ms/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0075 - val_loss: 0.0066\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 29s 20ms/step - loss: 0.0099 - val_loss: 0.0097\n",
      "1415/1415 [==============================] - 28s 20ms/step - loss: 0.0055 - val_loss: 0.0078\n",
      "Epoch 7/28\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 40s 28ms/step - loss: 0.0102 - val_loss: 0.0092\n",
      "1415/1415 [==============================] - 39s 28ms/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 7/28\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 18s 13ms/step - loss: 0.0032 - val_loss: 0.0029\n",
      " 166/1415 [==>...........................] - ETA: 1:57 - loss: 0.0096Epoch 16/28\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0073 - val_loss: 0.0095\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 22s 15ms/step - loss: 0.0069 - val_loss: 0.0076\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 47s 34ms/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0073 - val_loss: 0.0067\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0071 - val_loss: 0.0083\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0068 - val_loss: 0.0057\n",
      " 650/1415 [============>.................] - ETA: 1s - loss: 0.0032Epoch 17/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0033 - val_loss: 0.0028\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0071 - val_loss: 0.0089\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 26s 19ms/step - loss: 0.0091 - val_loss: 0.0114\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0047 - val_loss: 0.0044\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0070 - val_loss: 0.0065\n",
      "1415/1415 [==============================] - 26s 19ms/step - loss: 0.0051 - val_loss: 0.0045\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0097 - val_loss: 0.0105\n",
      " 606/1415 [===========>..................] - ETA: 1s - loss: 0.0068Epoch 18/28\n",
      "Epoch 8/28\n",
      "Epoch 8/28\n",
      "Epoch 8/28\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 28s 20ms/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 28s 19ms/step - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 27s 19ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 32s 23ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 27s 19ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 18s 13ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0069 - val_loss: 0.0062\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0067 - val_loss: 0.0083\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0061 - val_loss: 0.0056\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 31s 22ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      " 806/1415 [================>.............] - ETA: 7s - loss: 0.0068Epoch 9/28\n",
      "1415/1415 [==============================] - 31s 22ms/step - loss: 0.0043 - val_loss: 0.0039\n",
      "1415/1415 [==============================] - 31s 22ms/step - loss: 0.0090 - val_loss: 0.0080\n",
      "1415/1415 [==============================] - 31s 22ms/step - loss: 0.0087 - val_loss: 0.0076\n",
      "Epoch 9/28\n",
      "Epoch 9/28\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 24s 17ms/step - loss: 0.0031 - val_loss: 0.0027\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 25s 17ms/step - loss: 0.0064 - val_loss: 0.0058\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 24s 17ms/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 25s 17ms/step - loss: 0.0067 - val_loss: 0.0057s: 0.00\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 29s 21ms/step - loss: 0.0048 - val_loss: 0.0041\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 16s 12ms/step - loss: 0.0060 - val_loss: 0.0051\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0066 - val_loss: 0.0055\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0027 - val_loss: 0.0024\n",
      " 591/1415 [===========>..................] - ETA: 1s - loss: 0.0055Epoch 22/28\n",
      "1415/1415 [==============================] - 15s 11ms/step - loss: 0.0065 - val_loss: 0.0056\n",
      " 945/1415 [===================>..........] - ETA: 8s - loss: 0.0043Epoch 22/28\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 20s 14ms/step - loss: 0.0029 - val_loss: 0.0055\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 25s 17ms/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 8s 6ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 20s 14ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 25s 18ms/step - loss: 0.0088 - val_loss: 0.0078\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 25s 18ms/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 25s 18ms/step - loss: 0.0084 - val_loss: 0.0087\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0058 - val_loss: 0.0048\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0025 - val_loss: 0.0028 - ETA: 3s - loss: 0.004\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0061 - val_loss: 0.0063\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.0061 - val_loss: 0.0059\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 25s 18ms/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 24s 17ms/step - loss: 0.0055 - val_loss: 0.0057>.........................] - ETA: 5s - loss: 0.005TA: 2s - loss: 0.00: 0.008\n",
      " 487/1415 [=========>....................] - ETA: 43s - loss: 0.0044Epoch 24/28\n",
      "1415/1415 [==============================] - 27s 19ms/step - loss: 0.0029 - val_loss: 0.0024\n",
      "1201/1415 [========================>.....] - ETA: 4s - loss: 0.0060Epoch 24/28\n",
      "1415/1415 [==============================] - 28s 20ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 29s 21ms/step - loss: 0.0059 - val_loss: 0.0063] - ETA: 20s - loss: 0.004\n",
      "1415/1415 [==============================] - 32s 23ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "1415/1415 [==============================] - 32s 23ms/step - loss: 0.0085 - val_loss: 0.0074\n",
      "1415/1415 [==============================] - 29s 21ms/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 24/28\n",
      "Epoch 11/28\n",
      "Epoch 11/28\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 33s 24ms/step - loss: 0.0038 - val_loss: 0.0036=>.] - ETA: 0s - loss: 0.003\n",
      "1415/1415 [==============================] - ETA: 0s - loss: 0.0080Epoch 11/28\n",
      "1415/1415 [==============================] - 34s 24ms/step - loss: 0.0080 - val_loss: 0.00710.004\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 37s 26ms/step - loss: 0.0043 - val_loss: 0.003813s - loss: 0.00========>.] - ETA: 0s - loss: 0.00\n",
      " 835/1415 [================>.............] - ETA: 4s - loss: 0.0063Epoch 11/28\n",
      "1415/1415 [==============================] - 15s 10ms/step - loss: 0.0052 - val_loss: 0.0046...........] - ETA: 11s - loss: 0.007\n",
      "1081/1415 [=====================>........] - ETA: 2s - loss: 0.0061Epoch 25/28\n",
      "1415/1415 [==============================] - 15s 10ms/step - loss: 0.0026 - val_loss: 0.0025 0.00.] - ETA: 8s - loss: 0.008\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      " 734/1415 [==============>...............] - ETA: 8s - loss: 0.0079Epoch 25/28\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0060 - val_loss: 0.0053\n",
      " 468/1415 [========>.....................] - ETA: 5s - loss: 0.0051Epoch 26/28\n",
      "1415/1415 [==============================] - 15s 10ms/step - loss: 0.0058 - val_loss: 0.00522s - loss: 0.00\n",
      " 643/1415 [============>.................] - ETA: 5s - loss: 0.0052Epoch 25/28\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0082 - val_loss: 0.0072\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0037 - val_loss: 0.0040\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0036 - val_loss: 0.0032\n",
      "1245/1415 [=========================>....] - ETA: 1s - loss: 0.0052Epoch 12/28\n",
      "Epoch 12/28\n",
      " 804/1415 [================>.............] - ETA: 7s - loss: 0.0029Epoch 12/28\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0079 - val_loss: 0.0069\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0052 - val_loss: 0.0045\n",
      "  94/1415 [>.............................] - ETA: 3s - loss: 0.0033Epoch 26/28\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 16s 11ms/step - loss: 0.0023 - val_loss: 0.00210.00................] - ETA: 12s - loss: 0.00815 [======>.......................] - ETA: 13s - loss: 0.0\n",
      " 291/1415 [=====>........................] - ETA: 16s - loss: 0.0074Epoch 27/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0027 - val_loss: 0.0024===========>.] - ETA: 0s - loss: 0.005\n",
      "1415/1415 [==============================] - 16s 11ms/step - loss: 0.0059 - val_loss: 0.0050\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      " 510/1415 [=========>....................] - ETA: 12s - loss: 0.0077Epoch 26/28\n",
      "Epoch 27/28\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 16s 11ms/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0052 - val_loss: 0.0044: 11s - loss: 0.0- ETA: 9s - loss: 0.==============>..........] - ETA: 5s - loss: 0.002\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 24s 17ms/step - loss: 0.0078 - val_loss: 0.0087\n",
      "1415/1415 [==============================] - 24s 17ms/step - loss: 0.0034 - val_loss: 0.0031\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0077 - val_loss: 0.0066\n",
      "1415/1415 [==============================] - 24s 17ms/step - loss: 0.0036 - val_loss: 0.0034\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 16s 11ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 16s 11ms/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 27/28\n",
      "Epoch 13/28\n",
      "Epoch 13/28\n",
      "Epoch 28/28\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 17s 12ms/step - loss: 0.0053 - val_loss: 0.006100\n",
      " 136/1415 [=>............................] - ETA: 14s - loss: 0.0036Epoch 27/28\n",
      "1415/1415 [==============================] - 25s 18ms/step - loss: 0.0036 - val_loss: 0.0033- loss: 0.........] - ETA: 11s - loss: 0.\n",
      " 502/1415 [=========>....................] - ETA: 14s - loss: 0.0038Epoch 13/28\n",
      "1415/1415 [==============================] - 15s 10ms/step - loss: 0.0050 - val_loss: 0.0043..............] - ETA: 13s - loss: 0.00\n",
      " 549/1415 [==========>...................] - ETA: 14s - loss: 0.0077Epoch 28/28\n",
      "1415/1415 [==============================] - 16s 11ms/step - loss: 0.0053 - val_loss: 0.0068..] - ETA: 13s - loss: 208/1415 [========================>.....] - ETA: 2s - loss: 0.005\n",
      "1415/1415 [==============================] - 16s 11ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "1103/1415 [======================>.......] - ETA: 4s - loss: 0.0073Epoch 28/28\n",
      "1415/1415 [==============================] - 16s 11ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "1415/1415 [==============================] - 15s 10ms/step - loss: 0.0053 - val_loss: 0.0046\n",
      "Epoch 28/28\n",
      " 569/1415 [===========>..................] - ETA: 6s - loss: 0.0024Epoch 1/28 ETA: 5s - loss: 0.005- loss: 0.0\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0035 - val_loss: 0.0031\n",
      "1415/1415 [==============================] - 21s 15ms/step - loss: 0.0073 - val_loss: 0.0065\n",
      "1377/1415 [============================>.] - ETA: 0s - loss: 0.0077Epoch 14/28\n",
      "Epoch 14/28\n",
      "  21/1415 [..............................] - ETA: 3s - loss: 0.0032Epoch 1/28\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0033 - val_loss: 0.0030.] - ETA: 6s - loss: 0.005\n",
      "1415/1415 [==============================] - 23s 16ms/step - loss: 0.0076 - val_loss: 0.0069\n",
      "1415/1415 [==============================] - 12s 9ms/step - loss: 0.0050 - val_loss: 0.0047\n",
      " 610/1415 [===========>..................] - ETA: 5s - loss: 0.0050Epoch 14/28\n",
      " 765/1415 [===============>..............] - ETA: 4s - loss: 0.0024Epoch 14/28\n",
      " 976/1415 [===================>..........] - ETA: 3s - loss: 0.00282264/2264 - 3s - loss: 0.0071 - 3s/epoch - 1ms/step\n",
      " 232/1415 [===>..........................] - ETA: 9s - loss: 0.00782264/2264 - 2s - loss: 0.0024 - 2s/epoch - 1ms/step\n",
      " 803/1415 [================>.............] - ETA: 4s - loss: 0.0052Epoch 2/28\n",
      "Epoch 2/28\n",
      " 408/1415 [=======>......................] - ETA: 11s - loss: 0.00752264/2264 - 3s - loss: 0.0022 - 3s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "2264/2264 - 3s - loss: 0.0056 - 3s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0026 - val_loss: 0.0021.....] - ETA: 10s - loss: 0.00\n",
      "1415/1415 [==============================] - 19s 14ms/step - loss: 0.0037 - val_loss: 0.0031\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0054 - val_loss: 0.0048\n",
      " 527/1415 [==========>...................] - ETA: 10s - loss: 0.0073Epoch 1/28\n",
      "2264/2264 - 2s - loss: 0.0055 - 2s/epoch - 1ms/step\n",
      " 713/1415 [==============>...............] - ETA: 7s - loss: 0.0035Epoch 4/28\n",
      " 667/1415 [=============>................] - ETA: 8s - loss: 0.00732264/2264 - 2s - loss: 0.0021 - 2s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 764/1415 [===============>..............] - ETA: 7s - loss: 0.00742264/2264 - 2s - loss: 0.0055 - 2s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      " 516/1415 [=========>....................] - ETA: 5s - loss: 0.00312264/2264 - 3s - loss: 0.0021 - 3s/epoch - 1ms/step\n",
      " 883/1415 [=================>............] - ETA: 5s - loss: 0.0073Epoch 5/28\n",
      " 878/1415 [=================>............] - ETA: 5s - loss: 0.00342264/2264 - 3s - loss: 0.0051 - 3s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "1034/1415 [====================>.........] - ETA: 3s - loss: 0.00732264/2264 - 2s - loss: 0.0054 - 2s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "1070/1415 [=====================>........] - ETA: 3s - loss: 0.0073Epoch 1/28\n",
      "1186/1415 [========================>.....] - ETA: 2s - loss: 0.0033Epoch 1/28\n",
      "1370/1415 [============================>.] - ETA: 0s - loss: 0.00332264/2264 - 3s - loss: 0.0052 - 3s/epoch - 1ms/step\n",
      " 837/1415 [================>.............] - ETA: 4s - loss: 0.0032Epoch 6/28\n",
      "1324/1415 [===========================>..] - ETA: 0s - loss: 0.00722264/2264 - 3s - loss: 0.0021 - 3s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0033 - val_loss: 0.0035\n",
      "1415/1415 [==============================] - 15s 10ms/step - loss: 0.0072 - val_loss: 0.0063\n",
      "Epoch 15/28\n",
      "1406/1415 [============================>.] - ETA: 0s - loss: 0.0074Epoch 15/28\n",
      "  82/1415 [>.............................] - ETA: 4s - loss: 0.00712264/2264 - 3s - loss: 0.0052 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 141/1415 [=>............................] - ETA: 4s - loss: 0.00302264/2264 - 3s - loss: 0.0027 - 3s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 15/28\n",
      "1284/1415 [==========================>...] - ETA: 0s - loss: 0.00322264/2264 - 3s - loss: 0.0059 - 3s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0074 - val_loss: 0.0080\n",
      "1312/1415 [==========================>...] - ETA: 0s - loss: 0.0032Epoch 15/28\n",
      " 278/1415 [====>.........................] - ETA: 5s - loss: 0.00282264/2264 - 3s - loss: 0.0020 - 3s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      " 446/1415 [========>.....................] - ETA: 4s - loss: 0.00712264/2264 - 3s - loss: 0.0049 - 3s/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 15/28\n",
      "Epoch 7/28\n",
      " 169/1415 [==>...........................] - ETA: 4s - loss: 0.00382264/2264 - 3s - loss: 0.0050 - 3s/epoch - 1ms/step\n",
      "2264/2264 - 3s - loss: 0.0025 - 3s/epoch - 1ms/step\n",
      " 616/1415 [============>.................] - ETA: 4s - loss: 0.00702264/2264 - 3s - loss: 0.0054 - 3s/epoch - 1ms/step\n",
      " 345/1415 [======>.......................] - ETA: 6s - loss: 0.0076Epoch 5/28\n",
      "Epoch 3/28\n",
      "Epoch 3/28\n",
      " 615/1415 [============>.................] - ETA: 5s - loss: 0.00302264/2264 - 3s - loss: 0.0019 - 3s/epoch - 1ms/step0.003\n",
      "Epoch 8/28\n",
      "2264/2264 - 3s - loss: 0.0050 - 3s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1080/1415 [=====================>........] - ETA: 2s - loss: 0.00322264/2264 - 3s - loss: 0.0050 - 3s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 831/1415 [===================>..........] - ETA: 3s - loss: 0.002==>.............] - ETA: 4s - loss: 0.00752264/2264 - 3s - loss: 0.0025 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      " 839/1415 [================>.............] - ETA: 4s - loss: 0.00752264/2264 - 3s - loss: 0.0052 - 3s/epoch - 1ms/step\n",
      " 654/1415 [============>.................] - ETA: 5s - loss: 0.0033Epoch 4/28\n",
      "1133/1415 [=======================>......] - ETA: 1s - loss: 0.00322264/2264 - 3s - loss: 0.0019 - 3s/epoch - 1ms/step\n",
      " 869/1415 [=================>............] - ETA: 3s - loss: 0.0075Epoch 9/28\n",
      "1304/1415 [==========================>...] - ETA: 0s - loss: 0.00692264/2264 - 3s - loss: 0.0047 - 3s/epoch - 1ms/steps: 0.006\n",
      "Epoch 9/28\n",
      "1350/1415 [===========================>..] - ETA: 0s - loss: 0.00292264/2264 - 3s - loss: 0.0049 - 3s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "2264/2264 - 3s - loss: 0.0055 - 3s/epoch - 1ms/step\n",
      "2264/2264 - 3s - loss: 0.0024 - 3s/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0069 - val_loss: 0.0062\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 5/28\n",
      "Epoch 16/28\n",
      "Epoch 5/28\n",
      "Epoch 16/28\n",
      "  24/1415 [..............................] - ETA: 3s - loss: 0.00722264/2264 - 3s - loss: 0.0019 - 3s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      " 231/1415 [===>..........................] - ETA: 3s - loss: 0.00762264/2264 - 3s - loss: 0.0051 - 3s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0073 - val_loss: 0.0067\n",
      "   1/1415 [..............................] - ETA: 2s - loss: 0.0028Epoch 16/28\n",
      " 422/1415 [=======>......................] - ETA: 4s - loss: 0.00752264/2264 - 3s - loss: 0.0055 - 3s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0034 - val_loss: 0.0029\n",
      " 162/1415 [==>...........................] - ETA: 5s - loss: 0.0028Epoch 16/28\n",
      " 541/1415 [==========>...................] - ETA: 5s - loss: 0.00322264/2264 - 3s - loss: 0.0022 - 3s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      " 571/1415 [===========>..................] - ETA: 4s - loss: 0.00322264/2264 - 3s - loss: 0.0049 - 3s/epoch - 1ms/step\n",
      "2264/2264 - 3s - loss: 0.0018 - 3s/epoch - 1ms/step\n",
      "2264/2264 - 3s - loss: 0.0047 - 3s/epoch - 1ms/step\n",
      " 338/1415 [======>.......................] - ETA: 6s - loss: 0.0075Epoch 6/28\n",
      "Epoch 11/28\n",
      " 604/1415 [===========>..................] - ETA: 4s - loss: 0.0032Epoch 11/28\n",
      " 765/1415 [===============>..............] - ETA: 4s - loss: 0.00712264/2264 - 3s - loss: 0.0045 - 3s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      " 373/1415 [======>.......................] - ETA: 8s - loss: 0.00292264/2264 - 2s - loss: 0.0023 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1086/1415 [======================>.......] - ETA: 2s - loss: 0.00302264/2264 - 3s - loss: 0.0018 - 3s/epoch - 1ms/step\n",
      "1096/1415 [======================>.......] - ETA: 2s - loss: 0.0071Epoch 12/28\n",
      "2264/2264 - 3s - loss: 0.0046 - 3s/epoch - 1ms/step\n",
      " 839/1415 [================>.............] - ETA: 3s - loss: 0.0030Epoch 12/28\n",
      " 577/1415 [===========>..................] - ETA: 6s - loss: 0.00292264/2264 - 3s - loss: 0.0050 - 3s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "1015/1415 [====================>.........] - ETA: 2s - loss: 0.00292264/2264 - 3s - loss: 0.0046 - 3s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "1282/1415 [==========================>...] - ETA: 0s - loss: 0.00302264/2264 - 3s - loss: 0.0021 - 3s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "1237/1415 [=========================>....] - ETA: 1s - loss: 0.00292264/2264 - 3s - loss: 0.0019 - 3s/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      "1175/1415 [=======================>......] - ETA: 1s - loss: 0.00722264/2264 - 3s - loss: 0.0051 - 3s/epoch - 2ms/step\n",
      "1252/1415 [=========================>....] - ETA: 1s - loss: 0.00292264/2264 - 3s - loss: 0.0046 - 3s/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 11s 7ms/step - loss: 0.0030 - val_loss: 0.0028\n",
      "1415/1415 [==============================] - 11s 7ms/step - loss: 0.0073 - val_loss: 0.0069\n",
      "Epoch 8/28\n",
      "Epoch 17/28\n",
      "Epoch 17/28\n",
      "Epoch 13/28\n",
      "1109/1415 [======================>.......] - ETA: 2s - loss: 0.00312264/2264 - 3s - loss: 0.0047 - 3s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      " 195/1415 [===>..........................] - ETA: 10s - loss: 0.00272264/2264 - 4s - loss: 0.0020 - 4s/epoch - 2ms/step\n",
      " 194/1415 [===>..........................] - ETA: 11s - loss: 0.0068Epoch 9/28\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0030 - val_loss: 0.0026\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0071 - val_loss: 0.0062\n",
      "Epoch 17/28\n",
      "Epoch 17/28\n",
      " 393/1415 [=======>......................] - ETA: 8s - loss: 0.00342264/2264 - 4s - loss: 0.0016 - 4s/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      " 106/1415 [=>............................] - ETA: 7s - loss: 0.00272264/2264 - 4s - loss: 0.0046 - 4s/epoch - 2ms/step\n",
      "Epoch 14/28\n",
      "2264/2264 - 4s - loss: 0.0048 - 4s/epoch - 2ms/step\n",
      "1407/1415 [============================>.] - ETA: 0s - loss: 0.0031Epoch 9/28\n",
      " 246/1415 [====>.........................] - ETA: 9s - loss: 0.00762264/2264 - 4s - loss: 0.0045 - 4s/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      " 263/1415 [====>.........................] - ETA: 9s - loss: 0.00752264/2264 - 3s - loss: 0.0021 - 3s/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0031 - val_loss: 0.0028\n",
      "Epoch 17/28\n",
      "Epoch 10/28\n",
      " 101/1415 [=>............................] - ETA: 7s - loss: 0.00282264/2264 - 3s - loss: 0.0018 - 3s/epoch - 1ms/step\n",
      " 415/1415 [=======>......................] - ETA: 8s - loss: 0.0027Epoch 15/28\n",
      " 778/1415 [===============>..............] - ETA: 6s - loss: 0.00312264/2264 - 4s - loss: 0.0045 - 4s/epoch - 2ms/step\n",
      " 498/1415 [=========>....................] - ETA: 8s - loss: 0.0027Epoch 15/28\n",
      " 783/1415 [===============>..............] - ETA: 6s - loss: 0.00302264/2264 - 4s - loss: 0.0049 - 4s/epoch - 2ms/step\n",
      " 452/1415 [========>.....................] - ETA: 10s - loss: 0.0072Epoch 10/28\n",
      " 580/1415 [===========>..................] - ETA: 8s - loss: 0.00272264/2264 - 4s - loss: 0.0077 - 4s/epoch - 2ms/step\n",
      "Epoch 13/28\n",
      " 363/1415 [======>.......................] - ETA: 9s - loss: 0.00282264/2264 - 4s - loss: 0.0020 - 4s/epoch - 2ms/step\n",
      " 569/1415 [===========>..................] - ETA: 8s - loss: 0.0071Epoch 11/28\n",
      "1106/1415 [======================>.......] - ETA: 2s - loss: 0.00312264/2264 - 3s - loss: 0.0017 - 3s/epoch - 2ms/step0.003\n",
      "Epoch 16/28\n",
      "2264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      " 853/1415 [=================>............] - ETA: 5s - loss: 0.00692264/2264 - 3s - loss: 0.0044 - 3s/epoch - 2ms/step\n",
      "2264/2264 - 4s - loss: 0.0048 - 4s/epoch - 2ms/step\n",
      " 898/1415 [==================>...........] - ETA: 5s - loss: 0.0027Epoch 14/28\n",
      "Epoch 11/28\n",
      "1046/1415 [=====================>........] - ETA: 3s - loss: 0.00272264/2264 - 4s - loss: 0.0019 - 4s/epoch - 2ms/step\n",
      "Epoch 12/28\n",
      " 784/1415 [===============>..............] - ETA: 5s - loss: 0.00292264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "1070/1415 [=====================>........] - ETA: 3s - loss: 0.00272264/2264 - 3s - loss: 0.0043 - 3s/epoch - 1ms/step\n",
      "1365/1415 [===========================>..] - ETA: 0s - loss: 0.0066Epoch 17/28\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 14s 10ms/step - loss: 0.0030 - val_loss: 0.0026\n",
      "1415/1415 [==============================] - 15s 10ms/step - loss: 0.0066 - val_loss: 0.0058\n",
      "Epoch 18/28\n",
      "Epoch 18/28\n",
      " 116/1415 [=>............................] - ETA: 5s - loss: 0.00272264/2264 - 3s - loss: 0.0046 - 3s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      " 106/1415 [=>............................] - ETA: 6s - loss: 0.00632264/2264 - 3s - loss: 0.0046 - 3s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1256/1415 [=========================>....] - ETA: 1s - loss: 0.00292264/2264 - 3s - loss: 0.0020 - 3s/epoch - 1ms/step\n",
      "1281/1415 [==========================>...] - ETA: 1s - loss: 0.0029Epoch 13/28\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0069 - val_loss: 0.0061\n",
      "Epoch 18/28\n",
      "Epoch 18/28\n",
      " 431/1415 [========>.....................] - ETA: 4s - loss: 0.00692264/2264 - 3s - loss: 0.0018 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 127/1415 [=>............................] - ETA: 6s - loss: 0.00272264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 18/28\n",
      " 576/1415 [===========>..................] - ETA: 4s - loss: 0.00682264/2264 - 3s - loss: 0.0047 - 3s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      " 175/1415 [==>...........................] - ETA: 4s - loss: 0.00292264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      " 356/1415 [======>.......................] - ETA: 5s - loss: 0.0073Epoch 16/28\n",
      " 672/1415 [=============>................] - ETA: 4s - loss: 0.00672264/2264 - 3s - loss: 0.0020 - 3s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 729/1415 [==============>...............] - ETA: 4s - loss: 0.00672264/2264 - 3s - loss: 0.0041 - 3s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      " 747/1415 [==============>...............] - ETA: 4s - loss: 0.00672264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      " 400/1415 [=======>......................] - ETA: 8s - loss: 0.0031Epoch 19/28\n",
      " 649/1415 [============>.................] - ETA: 5s - loss: 0.00712264/2264 - 3s - loss: 0.0053 - 3s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      " 561/1415 [==========>...................] - ETA: 6s - loss: 0.00282264/2264 - 3s - loss: 0.0045 - 3s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 822/1415 [================>.............] - ETA: 4s - loss: 0.00702264/2264 - 3s - loss: 0.0018 - 3s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "1093/1415 [======================>.......] - ETA: 2s - loss: 0.00672264/2264 - 3s - loss: 0.0048 - 3s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "1222/1415 [========================>.....] - ETA: 1s - loss: 0.00662264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      " 884/1415 [=================>............] - ETA: 4s - loss: 0.0028Epoch 20/28\n",
      "1277/1415 [==========================>...] - ETA: 0s - loss: 0.00662264/2264 - 3s - loss: 0.0046 - 3s/epoch - 1ms/step\n",
      " 972/1415 [===================>..........] - ETA: 3s - loss: 0.0069Epoch 15/28\n",
      "1271/1415 [=========================>....] - ETA: 1s - loss: 0.00292264/2264 - 3s - loss: 0.0047 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "2264/2264 - 3s - loss: 0.0041 - 3s/epoch - 1ms/step\n",
      "1102/1415 [======================>.......] - ETA: 2s - loss: 0.00272264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 10s 7ms/step - loss: 0.0066 - val_loss: 0.0057\n",
      "Epoch 16/28\n",
      "Epoch 21/28\n",
      "Epoch 19/28\n",
      "1231/1415 [=========================>....] - ETA: 1s - loss: 0.00272264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0028 - val_loss: 0.0025\n",
      "Epoch 19/28\n",
      "  59/1415 [>.............................] - ETA: 4s - loss: 0.00262264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "1238/1415 [=========================>....] - ETA: 1s - loss: 0.00282264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0068 - val_loss: 0.0074\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0027 - val_loss: 0.0024\n",
      "Epoch 19/28\n",
      "Epoch 19/28\n",
      "Epoch 19/28\n",
      "  89/1415 [>.............................] - ETA: 4s - loss: 0.00642264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "  99/1415 [=>............................] - ETA: 4s - loss: 0.00632264/2264 - 3s - loss: 0.0018 - 3s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      " 292/1415 [=====>........................] - ETA: 7s - loss: 0.00252264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 367/1415 [======>.......................] - ETA: 5s - loss: 0.00632264/2264 - 3s - loss: 0.0047 - 3s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0028 - val_loss: 0.0037\n",
      " 395/1415 [=======>......................] - ETA: 5s - loss: 0.0026Epoch 19/28\n",
      " 517/1415 [=========>....................] - ETA: 5s - loss: 0.00262264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 679/1415 [=============>................] - ETA: 5s - loss: 0.00262264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 690/1415 [=============>................] - ETA: 4s - loss: 0.00262264/2264 - 3s - loss: 0.0041 - 3s/epoch - 1ms/step\n",
      " 896/1415 [=================>............] - ETA: 3s - loss: 0.00652264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "Epoch 23/28\n",
      " 754/1415 [==============>...............] - ETA: 4s - loss: 0.00262264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      " 851/1415 [=================>............] - ETA: 3s - loss: 0.00652264/2264 - 3s - loss: 0.0047 - 3s/epoch - 1ms/step\n",
      " 918/1415 [==================>...........] - ETA: 3s - loss: 0.0026Epoch 21/28\n",
      "1017/1415 [====================>.........] - ETA: 2s - loss: 0.00282264/2264 - 3s - loss: 0.0019 - 3s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1038/1415 [=====================>........] - ETA: 2s - loss: 0.00262264/2264 - 3s - loss: 0.0019 - 3s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1078/1415 [=====================>........] - ETA: 2s - loss: 0.00262264/2264 - 3s - loss: 0.0048 - 3s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "1031/1415 [====================>.........] - ETA: 2s - loss: 0.00652264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "1313/1415 [==========================>...] - ETA: 0s - loss: 0.00672264/2264 - 3s - loss: 0.0041 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "1406/1415 [============================>.] - ETA: 0s - loss: 0.00272264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0065 - val_loss: 0.0080\n",
      "1350/1415 [===========================>..] - ETA: 0s - loss: 0.0067Epoch 20/28\n",
      "Epoch 20/28\n",
      "1061/1415 [=====================>........] - ETA: 2s - loss: 0.00272264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1135/1415 [=======================>......] - ETA: 1s - loss: 0.00282264/2264 - 3s - loss: 0.0040 - 3s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "1415/1415 [==============================] - 11s 8ms/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 20/28\n",
      "Epoch 20/28\n",
      "  36/1415 [..............................] - ETA: 8s - loss: 0.00312264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      " 139/1415 [=>............................] - ETA: 5s - loss: 0.00652264/2264 - 3s - loss: 0.0043 - 3s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 20/28\n",
      " 275/1415 [====>.........................] - ETA: 8s - loss: 0.00262264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "2264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      " 289/1415 [=====>........................] - ETA: 7s - loss: 0.00262264/2264 - 3s - loss: 0.0042 - 3s/epoch - 1ms/step\n",
      " 385/1415 [=======>......................] - ETA: 6s - loss: 0.0028Epoch 26/28\n",
      "Epoch 26/28\n",
      " 239/1415 [====>.........................] - ETA: 9s - loss: 0.00292264/2264 - 3s - loss: 0.0048 - 3s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      " 504/1415 [=========>....................] - ETA: 7s - loss: 0.00272264/2264 - 3s - loss: 0.0045 - 3s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      " 582/1415 [===========>..................] - ETA: 6s - loss: 0.00252264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      " 698/1415 [=============>................] - ETA: 6s - loss: 0.00252264/2264 - 3s - loss: 0.0042 - 3s/epoch - 1ms/step\n",
      "2264/2264 - 3s - loss: 0.0014 - 3s/epoch - 1ms/step\n",
      "2264/2264 - 3s - loss: 0.0058 - 3s/epoch - 1ms/step\n",
      " 815/1415 [================>.............] - ETA: 4s - loss: 0.0027Epoch 27/28\n",
      " 742/1415 [==============>...............] - ETA: 5s - loss: 0.0064Epoch 27/28\n",
      "2264/2264 - 3s - loss: 0.0040 - 3s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "Epoch 22/28\n",
      "1245/1415 [=========================>....] - ETA: 1s - loss: 0.00652264/2264 - 4s - loss: 0.0016 - 4s/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "1244/1415 [=========================>....] - ETA: 1s - loss: 0.00272264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "2264/2264 - 3s - loss: 0.0041 - 3s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "1172/1415 [=======================>......] - ETA: 2s - loss: 0.00642264/2264 - 3s - loss: 0.0044 - 3s/epoch - 2ms/step\n",
      "Epoch 23/28\n",
      "1140/1415 [=======================>......] - ETA: 2s - loss: 0.00252264/2264 - 3s - loss: 0.0042 - 3s/epoch - 2ms/step\n",
      "1415/1415 [==============================] - 12s 8ms/step - loss: 0.0065 - val_loss: 0.0124\n",
      "1261/1415 [=========================>....] - ETA: 1s - loss: 0.0027Epoch 21/28\n",
      "Epoch 26/28\n",
      " 142/1415 [==>...........................] - ETA: 5s - loss: 0.00622264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 12s 9ms/step - loss: 0.0027 - val_loss: 0.0024\n",
      "1101/1415 [======================>.......] - ETA: 2s - loss: 0.0027Epoch 24/28\n",
      "Epoch 21/28\n",
      " 194/1415 [===>..........................] - ETA: 4s - loss: 0.00282264/2264 - 3s - loss: 0.0042 - 3s/epoch - 1ms/step\n",
      " 405/1415 [=======>......................] - ETA: 5s - loss: 0.00612264/2264 - 3s - loss: 0.0015 - 3s/epoch - 1ms/step\n",
      "1325/1415 [===========================>..] - ETA: 0s - loss: 0.00262264/2264 - 3s - loss: 0.0041 - 3s/epoch - 1ms/step\n",
      "566/566 - 0s - loss: 0.0032 - 310ms/epoch - 548us/step\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      " 418/1415 [=======>......................] - ETA: 5s - loss: 0.0061566/566 - 0s - loss: 0.0017 - 301ms/epoch - 531us/step\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0064 - val_loss: 0.0057\n",
      "Epoch 24/28\n",
      "Epoch 21/28\n",
      "Epoch 21/28\n",
      "  49/1415 [>.............................] - ETA: 5s - loss: 0.00592264/2264 - 3s - loss: 0.0042 - 3s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      " 160/1415 [==>...........................] - ETA: 5s - loss: 0.00642264/2264 - 3s - loss: 0.0018 - 3s/epoch - 1ms/step\n",
      " 414/1415 [=======>......................] - ETA: 5s - loss: 0.00272264/2264 - 1s - loss: 0.0017 - 1s/epoch - 588us/step\n",
      "1415/1415 [==============================] - 13s 9ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "2264/2264 - 1s - loss: 0.0032 - 1s/epoch - 591us/step\n",
      "Epoch 25/28\n",
      " 604/1415 [===========>..................] - ETA: 4s - loss: 0.0062Epoch 21/28\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x14830e340>; total time= 7.4min\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1393cf340>; total time= 7.4min\n",
      " 370/1415 [======>.......................] - ETA: 5s - loss: 0.00622264/2264 - 2s - loss: 0.0039 - 2s/epoch - 1ms/step\n",
      " 392/1415 [=======>......................] - ETA: 5s - loss: 0.0023Epoch 28/28\n",
      " 177/1415 [==>...........................] - ETA: 4s - loss: 0.00272264/2264 - 3s - loss: 0.0060 - 3s/epoch - 1ms/step\n",
      " 415/1415 [=======>......................] - ETA: 4s - loss: 0.0062Epoch 25/28\n",
      " 428/1415 [========>.....................] - ETA: 4s - loss: 0.00262264/2264 - 2s - loss: 0.0014 - 2s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      " 851/1415 [=================>............] - ETA: 3s - loss: 0.00242264/2264 - 3s - loss: 0.0040 - 3s/epoch - 1ms/step\n",
      " 894/1415 [=================>............] - ETA: 2s - loss: 0.0063566/566 - 0s - loss: 0.0036 - 246ms/epoch - 435us/step\n",
      "2264/2264 - 2s - loss: 0.0039 - 2s/epoch - 1ms/step\n",
      " 882/1415 [=================>............] - ETA: 2s - loss: 0.0024Epoch 26/28\n",
      "1396/1415 [============================>.] - ETA: 0s - loss: 0.00252264/2264 - 2s - loss: 0.0015 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "2264/2264 - 1s - loss: 0.0037 - 1s/epoch - 492us/step\n",
      "1415/1415 [==============================] - 9s 6ms/step - loss: 0.0061 - val_loss: 0.0053\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 22/28\n",
      "Epoch 22/28\n",
      "1151/1415 [=======================>......] - ETA: 1s - loss: 0.0028[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x13ff505e0>; total time= 7.5min\n",
      " 290/1415 [=====>........................] - ETA: 2s - loss: 0.00262264/2264 - 2s - loss: 0.0043 - 2s/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0027 - val_loss: 0.0022\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0062 - val_loss: 0.0064\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 858us/step\n",
      "Epoch 28/28\n",
      "Epoch 22/28\n",
      "Epoch 22/28\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 8s 5ms/step - loss: 0.0026 - val_loss: 0.0023............] - ETA: 2s - loss: 0.006\n",
      " 303/1415 [=====>........................] - ETA: 2s - loss: 0.0063Epoch 22/28\n",
      " 946/1415 [===================>..........] - ETA: 1s - loss: 0.00622264/2264 - 2s - loss: 0.0040 - 2s/epoch - 978us/step\n",
      " 926/1415 [==================>...........] - ETA: 1s - loss: 0.0025Epoch 28/28\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 987us/step\n",
      " 784/1415 [===============>..............] - ETA: 2s - loss: 0.0063566/566 - 0s - loss: 0.0014 - 360ms/epoch - 637us/step\n",
      "1264/1415 [=========================>....] - ETA: 0s - loss: 0.00622264/2264 - 1s - loss: 0.0014 - 803ms/epoch - 355us/step\n",
      " 859/1415 [=================>............] - ETA: 2s - loss: 0.0022[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x13a861100>; total time= 7.6min\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0062 - val_loss: 0.0053\n",
      "Epoch 23/28\n",
      "  37/1415 [..............................] - ETA: 1s - loss: 0.00612264/2264 - 3s - loss: 0.0043 - 3s/epoch - 1ms/step\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "  83/1415 [>.............................] - ETA: 1s - loss: 0.0061566/566 - 0s - loss: 0.0046 - 219ms/epoch - 387us/step\n",
      "1006/1415 [====================>.........] - ETA: 1s - loss: 0.0025Epoch 23/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 6s 4ms/step - loss: 0.0023 - val_loss: 0.0108\n",
      "Epoch 23/28\n",
      " 103/1415 [=>............................] - ETA: 1s - loss: 0.00332264/2264 - 1s - loss: 0.0046 - 880ms/epoch - 389us/step\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0025 - val_loss: 0.0022\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x175ae0eb0>; total time= 7.6min\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0060 - val_loss: 0.0056\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0024 - val_loss: 0.0045\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0060 - val_loss: 0.0058\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 24/28\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0024 - val_loss: 0.0021: 0s - loss: 0.002\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0059 - val_loss: 0.0064\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0057 - val_loss: 0.0052\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0023 - val_loss: 0.0021 loss: 0.002\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "1410/1415 [============================>.] - ETA: 0s - loss: 0.0054Epoch 26/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0023 - val_loss: 0.0021\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      " 392/1415 [=======>......................] - ETA: 1s - loss: 0.0055Epoch 26/28\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0055 - val_loss: 0.0058\n",
      "1415/1415 [==============================] - 4s 2ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      " 207/1415 [===>..........................] - ETA: 1s - loss: 0.0023Epoch 27/28\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0023 - val_loss: 0.0020============>..] - ETA: 0s - loss: 0.002\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 4ms/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 5s 3ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0054 - val_loss: 0.0061\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0021 - val_loss: 0.0019\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0052 - val_loss: 0.0063\n",
      "1415/1415 [==============================] - 4s 3ms/step - loss: 0.0021 - val_loss: 0.0038\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "Epoch 1/28\n",
      "2264/2264 - 2s - loss: 0.0023 - 2s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "2264/2264 - 3s - loss: 0.0061 - 3s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "2264/2264 - 3s - loss: 0.0059 - 3s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "2264/2264 - 3s - loss: 0.0020 - 3s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "2264/2264 - 3s - loss: 0.0023 - 3s/epoch - 1ms/step\n",
      "Epoch 2/28\n",
      "2264/2264 - 3s - loss: 0.0021 - 3s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "2264/2264 - 2s - loss: 0.0053 - 2s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "2264/2264 - 3s - loss: 0.0058 - 3s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "2264/2264 - 2s - loss: 0.0021 - 2s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "2264/2264 - 2s - loss: 0.0023 - 2s/epoch - 1ms/step\n",
      "Epoch 3/28\n",
      "2264/2264 - 3s - loss: 0.0020 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "2264/2264 - 3s - loss: 0.0054 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "2264/2264 - 3s - loss: 0.0019 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "2264/2264 - 3s - loss: 0.0052 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "2264/2264 - 3s - loss: 0.0023 - 3s/epoch - 1ms/step\n",
      "Epoch 4/28\n",
      "2264/2264 - 2s - loss: 0.0021 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "2264/2264 - 2s - loss: 0.0052 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "2264/2264 - 2s - loss: 0.0019 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "2264/2264 - 2s - loss: 0.0051 - 2s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "2264/2264 - 3s - loss: 0.0021 - 3s/epoch - 1ms/step\n",
      "Epoch 5/28\n",
      "2264/2264 - 2s - loss: 0.0019 - 2s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "2264/2264 - 2s - loss: 0.0049 - 2s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "2264/2264 - 2s - loss: 0.0051 - 2s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "2264/2264 - 2s - loss: 0.0019 - 2s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "2264/2264 - 2s - loss: 0.0020 - 2s/epoch - 1ms/step\n",
      "Epoch 6/28\n",
      "2264/2264 - 2s - loss: 0.0019 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "2264/2264 - 2s - loss: 0.0049 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "2264/2264 - 3s - loss: 0.0051 - 3s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "2264/2264 - 2s - loss: 0.0019 - 2s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "2264/2264 - 3s - loss: 0.0020 - 3s/epoch - 1ms/step\n",
      "Epoch 7/28\n",
      "2264/2264 - 2s - loss: 0.0019 - 2s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "2264/2264 - 2s - loss: 0.0047 - 2s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "2264/2264 - 2s - loss: 0.0049 - 2s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "2264/2264 - 2s - loss: 0.0018 - 2s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "2264/2264 - 2s - loss: 0.0019 - 2s/epoch - 1ms/step\n",
      "Epoch 8/28\n",
      "2264/2264 - 2s - loss: 0.0021 - 2s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "2264/2264 - 2s - loss: 0.0051 - 2s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "2264/2264 - 2s - loss: 0.0050 - 2s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "2264/2264 - 2s - loss: 0.0018 - 2s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "2264/2264 - 2s - loss: 0.0019 - 2s/epoch - 1ms/step\n",
      "Epoch 9/28\n",
      "2264/2264 - 2s - loss: 0.0018 - 2s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "2264/2264 - 2s - loss: 0.0047 - 2s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "2264/2264 - 2s - loss: 0.0047 - 2s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "2264/2264 - 2s - loss: 0.0018 - 2s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "2264/2264 - 3s - loss: 0.0019 - 3s/epoch - 1ms/step\n",
      "Epoch 10/28\n",
      "2264/2264 - 3s - loss: 0.0019 - 3s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "2264/2264 - 3s - loss: 0.0045 - 3s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "2264/2264 - 3s - loss: 0.0046 - 3s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "2264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "2264/2264 - 2s - loss: 0.0020 - 2s/epoch - 1ms/step\n",
      "Epoch 11/28\n",
      "2264/2264 - 2s - loss: 0.0018 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "2264/2264 - 2s - loss: 0.0049 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "2264/2264 - 2s - loss: 0.0047 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "2264/2264 - 2s - loss: 0.0017 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "2264/2264 - 2s - loss: 0.0018 - 2s/epoch - 1ms/step\n",
      "Epoch 12/28\n",
      "2264/2264 - 2s - loss: 0.0019 - 2s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "2264/2264 - 2s - loss: 0.0044 - 2s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "2264/2264 - 2s - loss: 0.0045 - 2s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "2264/2264 - 2s - loss: 0.0017 - 2s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "2264/2264 - 2s - loss: 0.0019 - 2s/epoch - 1ms/step\n",
      "Epoch 13/28\n",
      "2264/2264 - 2s - loss: 0.0018 - 2s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "2264/2264 - 2s - loss: 0.0046 - 2s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "2264/2264 - 2s - loss: 0.0045 - 2s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "2264/2264 - 2s - loss: 0.0018 - 2s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "2264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "Epoch 14/28\n",
      "2264/2264 - 2s - loss: 0.0018 - 2s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "2264/2264 - 3s - loss: 0.0050 - 3s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "2264/2264 - 3s - loss: 0.0045 - 3s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "2264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "2264/2264 - 3s - loss: 0.0017 - 3s/epoch - 1ms/step\n",
      "Epoch 15/28\n",
      "2264/2264 - 2s - loss: 0.0017 - 2s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "2264/2264 - 2s - loss: 0.0043 - 2s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "2264/2264 - 2s - loss: 0.0045 - 2s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "2264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "2264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 16/28\n",
      "2264/2264 - 2s - loss: 0.0018 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "2264/2264 - 2s - loss: 0.0045 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "2264/2264 - 2s - loss: 0.0044 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "2264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 17/28\n",
      "2264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "2264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "2264/2264 - 3s - loss: 0.0052 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "2264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 1ms/step\n",
      "Epoch 18/28\n",
      "2264/2264 - 2s - loss: 0.0017 - 2s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "2264/2264 - 2s - loss: 0.0042 - 2s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "2264/2264 - 2s - loss: 0.0043 - 2s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "2264/2264 - 3s - loss: 0.0015 - 3s/epoch - 1ms/step\n",
      "Epoch 19/28\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "2264/2264 - 2s - loss: 0.0043 - 2s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "2264/2264 - 3s - loss: 0.0043 - 3s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 1ms/step\n",
      "Epoch 20/28\n",
      "2264/2264 - 2s - loss: 0.0017 - 2s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "2264/2264 - 2s - loss: 0.0048 - 2s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "2264/2264 - 3s - loss: 0.0041 - 3s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "2264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 1ms/step\n",
      "Epoch 21/28\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "2264/2264 - 3s - loss: 0.0043 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "2264/2264 - 3s - loss: 0.0046 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "2264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "2264/2264 - 3s - loss: 0.0014 - 3s/epoch - 1ms/step\n",
      "Epoch 22/28\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "2264/2264 - 3s - loss: 0.0041 - 3s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "2264/2264 - 3s - loss: 0.0040 - 3s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "2264/2264 - 2s - loss: 0.0015 - 2s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "2264/2264 - 2s - loss: 0.0015 - 2s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "2264/2264 - 3s - loss: 0.0015 - 3s/epoch - 1ms/step\n",
      "Epoch 23/28\n",
      "2264/2264 - 2s - loss: 0.0041 - 2s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "2264/2264 - 2s - loss: 0.0041 - 2s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "2264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "2264/2264 - 2s - loss: 0.0015 - 2s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "2264/2264 - 3s - loss: 0.0015 - 3s/epoch - 1ms/step\n",
      "Epoch 24/28\n",
      "2264/2264 - 3s - loss: 0.0044 - 3s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "2264/2264 - 3s - loss: 0.0041 - 3s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "2264/2264 - 2s - loss: 0.0014 - 2s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "2264/2264 - 2s - loss: 0.0015 - 2s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "2264/2264 - 3s - loss: 0.0015 - 3s/epoch - 1ms/step\n",
      "Epoch 25/28\n",
      "2264/2264 - 2s - loss: 0.0042 - 2s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "2264/2264 - 3s - loss: 0.0038 - 3s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "2264/2264 - 3s - loss: 0.0016 - 3s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "2264/2264 - 2s - loss: 0.0015 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "2264/2264 - 3s - loss: 0.0014 - 3s/epoch - 1ms/step\n",
      "Epoch 26/28\n",
      "2264/2264 - 2s - loss: 0.0046 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "2264/2264 - 2s - loss: 0.0047 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "2264/2264 - 2s - loss: 0.0014 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "2264/2264 - 2s - loss: 0.0015 - 2s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "2264/2264 - 2s - loss: 0.0014 - 2s/epoch - 1ms/step\n",
      "Epoch 27/28\n",
      "2264/2264 - 2s - loss: 0.0040 - 2s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "2264/2264 - 2s - loss: 0.0037 - 2s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "2264/2264 - 2s - loss: 0.0014 - 2s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "2264/2264 - 2s - loss: 0.0015 - 2s/epoch - 1ms/step\n",
      "2264/2264 - 2s - loss: 0.0016 - 2s/epoch - 1ms/step\n",
      "Epoch 28/28\n",
      "566/566 - 0s - loss: 0.0018 - 373ms/epoch - 658us/step\n",
      "2264/2264 - 2s - loss: 0.0041 - 2s/epoch - 1ms/step\n",
      "2264/2264 - 2s - loss: 0.0038 - 2s/epoch - 1ms/step\n",
      "2264/2264 - 2s - loss: 0.0014 - 2s/epoch - 1ms/step\n",
      "566/566 - 0s - loss: 0.0032 - 312ms/epoch - 551us/step\n",
      "2264/2264 - 1s - loss: 0.0017 - 1s/epoch - 474us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16d72fc70>; total time= 9.3min\n",
      "566/566 - 0s - loss: 0.0037 - 300ms/epoch - 530us/step\n",
      "566/566 - 0s - loss: 0.0013 - 297ms/epoch - 524us/step\n",
      "2264/2264 - 1s - loss: 0.0032 - 908ms/epoch - 401us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x13d9d5eb0>; total time= 9.3min\n",
      "2264/2264 - 2s - loss: 0.0013 - 2s/epoch - 881us/step\n",
      "2264/2264 - 1s - loss: 0.0037 - 875ms/epoch - 386us/step\n",
      "[CV] END activation=tanh, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x16e7b1eb0>; total time= 9.3min\n",
      "2264/2264 - 1s - loss: 0.0013 - 861ms/epoch - 380us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1572ef2b0>; total time= 9.3min\n",
      "566/566 - 0s - loss: 0.0012 - 257ms/epoch - 455us/step\n",
      "2264/2264 - 1s - loss: 0.0012 - 595ms/epoch - 263us/step\n",
      "[CV] END activation=relu, kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x1489a1100>; total time= 9.3min\n",
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1582 - val_loss: 0.0189\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0124 - val_loss: 0.0099\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 966us/step - loss: 0.0088 - val_loss: 0.0078\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 979us/step - loss: 0.0070 - val_loss: 0.0062\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 975us/step - loss: 0.0060 - val_loss: 0.0061\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 972us/step - loss: 0.0043 - val_loss: 0.0049\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 956us/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 965us/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 960us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0031 - val_loss: 0.0034\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 963us/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 1ms/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 978us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 992us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 2s 1ms/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 991us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 1/28\n",
      "2829/2829 - 2s - loss: 0.0021 - 2s/epoch - 664us/step\n",
      "Epoch 2/28\n",
      "2829/2829 - 2s - loss: 0.0020 - 2s/epoch - 661us/step\n",
      "Epoch 3/28\n",
      "2829/2829 - 2s - loss: 0.0019 - 2s/epoch - 646us/step\n",
      "Epoch 4/28\n",
      "2829/2829 - 2s - loss: 0.0020 - 2s/epoch - 644us/step\n",
      "Epoch 5/28\n",
      "2829/2829 - 2s - loss: 0.0019 - 2s/epoch - 652us/step\n",
      "Epoch 6/28\n",
      "2829/2829 - 2s - loss: 0.0018 - 2s/epoch - 659us/step\n",
      "Epoch 7/28\n",
      "2829/2829 - 2s - loss: 0.0018 - 2s/epoch - 648us/step\n",
      "Epoch 8/28\n",
      "2829/2829 - 2s - loss: 0.0018 - 2s/epoch - 643us/step\n",
      "Epoch 9/28\n",
      "2829/2829 - 2s - loss: 0.0018 - 2s/epoch - 651us/step\n",
      "Epoch 10/28\n",
      "2829/2829 - 2s - loss: 0.0017 - 2s/epoch - 651us/step\n",
      "Epoch 11/28\n",
      "2829/2829 - 2s - loss: 0.0017 - 2s/epoch - 646us/step\n",
      "Epoch 12/28\n",
      "2829/2829 - 2s - loss: 0.0016 - 2s/epoch - 646us/step\n",
      "Epoch 13/28\n",
      "2829/2829 - 2s - loss: 0.0016 - 2s/epoch - 646us/step\n",
      "Epoch 14/28\n",
      "2829/2829 - 2s - loss: 0.0017 - 2s/epoch - 648us/step\n",
      "Epoch 15/28\n",
      "2829/2829 - 2s - loss: 0.0015 - 2s/epoch - 646us/step\n",
      "Epoch 16/28\n",
      "2829/2829 - 2s - loss: 0.0016 - 2s/epoch - 641us/step\n",
      "Epoch 17/28\n",
      "2829/2829 - 2s - loss: 0.0015 - 2s/epoch - 646us/step\n",
      "Epoch 18/28\n",
      "2829/2829 - 2s - loss: 0.0015 - 2s/epoch - 651us/step\n",
      "Epoch 19/28\n",
      "2829/2829 - 2s - loss: 0.0015 - 2s/epoch - 644us/step\n",
      "Epoch 20/28\n",
      "2829/2829 - 2s - loss: 0.0015 - 2s/epoch - 637us/step\n",
      "Epoch 21/28\n",
      "2829/2829 - 2s - loss: 0.0015 - 2s/epoch - 642us/step\n",
      "Epoch 22/28\n",
      "2829/2829 - 2s - loss: 0.0014 - 2s/epoch - 646us/step\n",
      "Epoch 23/28\n",
      "2829/2829 - 2s - loss: 0.0014 - 2s/epoch - 648us/step\n",
      "Epoch 24/28\n",
      "2829/2829 - 2s - loss: 0.0014 - 2s/epoch - 651us/step\n",
      "Epoch 25/28\n",
      "2829/2829 - 2s - loss: 0.0014 - 2s/epoch - 642us/step\n",
      "Epoch 26/28\n",
      "2829/2829 - 2s - loss: 0.0014 - 2s/epoch - 652us/step\n",
      "Epoch 27/28\n",
      "2829/2829 - 2s - loss: 0.0014 - 2s/epoch - 636us/step\n",
      "Epoch 28/28\n",
      "2829/2829 - 2s - loss: 0.0013 - 2s/epoch - 637us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x345b95e80&gt;,\n",
       "                    n_jobs=-1,\n",
       "                    param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                                &#x27;optimizer&#x27;: [&#x27;sgd&#x27;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x345b900d0&gt;]},\n",
       "                    verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x345b95e80&gt;,\n",
       "                    n_jobs=-1,\n",
       "                    param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                                &#x27;optimizer&#x27;: [&#x27;sgd&#x27;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x345b900d0&gt;]},\n",
       "                    verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x345b95e80&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x345b95e80&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(error_score='raise',\n",
       "                    estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x345b95e80>,\n",
       "                    n_jobs=-1,\n",
       "                    param_grid={'activation': ['tanh', 'relu'],\n",
       "                                'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
       "                                'optimizer': ['sgd',\n",
       "                                              <keras.optimizers.legacy.adam.Adam object at 0x345b900d0>]},\n",
       "                    verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search selected hyperparameters\n",
    "\n",
    "# define base model function for grid search\n",
    "def create_model(optimizer=Adam(0.001), activation='relu', kernel_regularizer='l2'):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)),\n",
    "        Dense(128, activation=activation, kernel_regularizer=kernel_regularizer), \n",
    "        Dense(64, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)  # Output layer with 1 neuron for regression\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=epochs, batch_size=64, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'optimizer': ['sgd', Adam(learning_rate=0.001)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2']\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=epochs, verbose=2)\n",
    "\n",
    "# perform grid search hyperparameter tuning\n",
    "grid_search = HalvingGridSearchCV(estimator=keras_reg, param_grid=param_dist, cv=5, verbose=2, error_score='raise', n_jobs=-1)\n",
    "grid_search.fit(Xtrain, ytrain, sample_weight=wtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>n_resources</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>86.664786</td>\n",
       "      <td>0.718042</td>\n",
       "      <td>0.203581</td>\n",
       "      <td>0.044584</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054829</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.053360</td>\n",
       "      <td>-0.055927</td>\n",
       "      <td>-0.052909</td>\n",
       "      <td>-0.057002</td>\n",
       "      <td>-0.055412</td>\n",
       "      <td>-0.054922</td>\n",
       "      <td>0.001554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>99.468336</td>\n",
       "      <td>0.656485</td>\n",
       "      <td>0.174824</td>\n",
       "      <td>0.038194</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038918</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.039269</td>\n",
       "      <td>-0.038840</td>\n",
       "      <td>-0.039123</td>\n",
       "      <td>-0.039001</td>\n",
       "      <td>-0.038834</td>\n",
       "      <td>-0.039013</td>\n",
       "      <td>0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>89.113461</td>\n",
       "      <td>0.280190</td>\n",
       "      <td>0.197341</td>\n",
       "      <td>0.045196</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011238</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.011278</td>\n",
       "      <td>-0.011248</td>\n",
       "      <td>-0.011316</td>\n",
       "      <td>-0.011261</td>\n",
       "      <td>-0.011228</td>\n",
       "      <td>-0.011266</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>106.436394</td>\n",
       "      <td>4.210558</td>\n",
       "      <td>0.240596</td>\n",
       "      <td>0.052589</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006014</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.005144</td>\n",
       "      <td>-0.007006</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.007236</td>\n",
       "      <td>-0.005615</td>\n",
       "      <td>-0.005997</td>\n",
       "      <td>0.000943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>95.107283</td>\n",
       "      <td>0.786738</td>\n",
       "      <td>0.263743</td>\n",
       "      <td>0.038384</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073122</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.072357</td>\n",
       "      <td>-0.073513</td>\n",
       "      <td>-0.073163</td>\n",
       "      <td>-0.073625</td>\n",
       "      <td>-0.073175</td>\n",
       "      <td>-0.073167</td>\n",
       "      <td>0.000444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>113.244066</td>\n",
       "      <td>3.081708</td>\n",
       "      <td>0.408317</td>\n",
       "      <td>0.249198</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043034</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.043551</td>\n",
       "      <td>-0.043231</td>\n",
       "      <td>-0.043277</td>\n",
       "      <td>-0.042862</td>\n",
       "      <td>-0.042764</td>\n",
       "      <td>-0.043137</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>109.454873</td>\n",
       "      <td>0.890748</td>\n",
       "      <td>0.361256</td>\n",
       "      <td>0.120151</td>\n",
       "      <td>relu</td>\n",
       "      <td>l1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031268</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.030993</td>\n",
       "      <td>-0.034256</td>\n",
       "      <td>-0.030327</td>\n",
       "      <td>-0.030153</td>\n",
       "      <td>-0.030830</td>\n",
       "      <td>-0.031312</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>152.501894</td>\n",
       "      <td>14.050065</td>\n",
       "      <td>0.198814</td>\n",
       "      <td>0.070467</td>\n",
       "      <td>relu</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028019</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.028169</td>\n",
       "      <td>-0.027728</td>\n",
       "      <td>-0.028545</td>\n",
       "      <td>-0.027846</td>\n",
       "      <td>-0.027897</td>\n",
       "      <td>-0.028037</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>141.580026</td>\n",
       "      <td>2.052891</td>\n",
       "      <td>0.262241</td>\n",
       "      <td>0.072672</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010248</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.010321</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>-0.010234</td>\n",
       "      <td>-0.010304</td>\n",
       "      <td>-0.010246</td>\n",
       "      <td>-0.010282</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>225.880566</td>\n",
       "      <td>45.297840</td>\n",
       "      <td>0.283344</td>\n",
       "      <td>0.085744</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001745</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.001718</td>\n",
       "      <td>-0.001796</td>\n",
       "      <td>-0.001786</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.001904</td>\n",
       "      <td>-0.001760</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>210.230532</td>\n",
       "      <td>2.195742</td>\n",
       "      <td>0.165578</td>\n",
       "      <td>0.037235</td>\n",
       "      <td>relu</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039471</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.039276</td>\n",
       "      <td>-0.037940</td>\n",
       "      <td>-0.040991</td>\n",
       "      <td>-0.038956</td>\n",
       "      <td>-0.040930</td>\n",
       "      <td>-0.039619</td>\n",
       "      <td>0.001181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>10058</td>\n",
       "      <td>87.603570</td>\n",
       "      <td>35.224285</td>\n",
       "      <td>0.120712</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>relu</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030099</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.030308</td>\n",
       "      <td>-0.029977</td>\n",
       "      <td>-0.030358</td>\n",
       "      <td>-0.029921</td>\n",
       "      <td>-0.030010</td>\n",
       "      <td>-0.030115</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>30174</td>\n",
       "      <td>281.011689</td>\n",
       "      <td>21.458315</td>\n",
       "      <td>0.624389</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010533</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.010615</td>\n",
       "      <td>-0.010459</td>\n",
       "      <td>-0.010386</td>\n",
       "      <td>-0.010524</td>\n",
       "      <td>-0.010569</td>\n",
       "      <td>-0.010510</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>30174</td>\n",
       "      <td>270.761832</td>\n",
       "      <td>44.691981</td>\n",
       "      <td>0.494587</td>\n",
       "      <td>0.142809</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009212</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.009197</td>\n",
       "      <td>-0.009255</td>\n",
       "      <td>-0.009202</td>\n",
       "      <td>-0.009201</td>\n",
       "      <td>-0.009187</td>\n",
       "      <td>-0.009208</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>30174</td>\n",
       "      <td>3207.947498</td>\n",
       "      <td>559.181525</td>\n",
       "      <td>0.358809</td>\n",
       "      <td>0.116666</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004414</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.004534</td>\n",
       "      <td>-0.004305</td>\n",
       "      <td>-0.003972</td>\n",
       "      <td>-0.003835</td>\n",
       "      <td>-0.005381</td>\n",
       "      <td>-0.004405</td>\n",
       "      <td>0.000546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>30174</td>\n",
       "      <td>3549.804113</td>\n",
       "      <td>2.402317</td>\n",
       "      <td>0.161880</td>\n",
       "      <td>0.019160</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001777</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.001564</td>\n",
       "      <td>-0.001520</td>\n",
       "      <td>-0.002644</td>\n",
       "      <td>-0.001555</td>\n",
       "      <td>-0.001634</td>\n",
       "      <td>-0.001783</td>\n",
       "      <td>0.000432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>90522</td>\n",
       "      <td>492.021179</td>\n",
       "      <td>54.395932</td>\n",
       "      <td>0.429210</td>\n",
       "      <td>0.141490</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003676</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.004606</td>\n",
       "      <td>-0.003685</td>\n",
       "      <td>-0.003196</td>\n",
       "      <td>-0.003673</td>\n",
       "      <td>-0.003200</td>\n",
       "      <td>-0.003672</td>\n",
       "      <td>0.000514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>90522</td>\n",
       "      <td>513.813887</td>\n",
       "      <td>54.761040</td>\n",
       "      <td>0.385186</td>\n",
       "      <td>0.093419</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001205</td>\n",
       "      <td>-0.001749</td>\n",
       "      <td>-0.001256</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>-0.001689</td>\n",
       "      <td>-0.001454</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
       "0      0        10058      86.664786      0.718042         0.203581   \n",
       "1      0        10058      99.468336      0.656485         0.174824   \n",
       "2      0        10058      89.113461      0.280190         0.197341   \n",
       "3      0        10058     106.436394      4.210558         0.240596   \n",
       "4      0        10058      95.107283      0.786738         0.263743   \n",
       "5      0        10058     113.244066      3.081708         0.408317   \n",
       "6      0        10058     109.454873      0.890748         0.361256   \n",
       "7      0        10058     152.501894     14.050065         0.198814   \n",
       "8      0        10058     141.580026      2.052891         0.262241   \n",
       "9      0        10058     225.880566     45.297840         0.283344   \n",
       "10     0        10058     210.230532      2.195742         0.165578   \n",
       "11     0        10058      87.603570     35.224285         0.120712   \n",
       "12     1        30174     281.011689     21.458315         0.624389   \n",
       "13     1        30174     270.761832     44.691981         0.494587   \n",
       "14     1        30174    3207.947498    559.181525         0.358809   \n",
       "15     1        30174    3549.804113      2.402317         0.161880   \n",
       "16     2        90522     492.021179     54.395932         0.429210   \n",
       "17     2        90522     513.813887     54.761040         0.385186   \n",
       "\n",
       "    std_score_time param_activation param_kernel_regularizer  \\\n",
       "0         0.044584             tanh                       l1   \n",
       "1         0.038194             tanh                       l1   \n",
       "2         0.045196             tanh                       l2   \n",
       "3         0.052589             tanh                       l2   \n",
       "4         0.038384             tanh                    l1_l2   \n",
       "5         0.249198             tanh                    l1_l2   \n",
       "6         0.120151             relu                       l1   \n",
       "7         0.070467             relu                       l1   \n",
       "8         0.072672             relu                       l2   \n",
       "9         0.085744             relu                       l2   \n",
       "10        0.037235             relu                    l1_l2   \n",
       "11        0.014870             relu                    l1_l2   \n",
       "12        0.331250             tanh                       l2   \n",
       "13        0.142809             relu                       l2   \n",
       "14        0.116666             tanh                       l2   \n",
       "15        0.019160             relu                       l2   \n",
       "16        0.141490             tanh                       l2   \n",
       "17        0.093419             relu                       l2   \n",
       "\n",
       "                                      param_optimizer  \\\n",
       "0                                                 sgd   \n",
       "1   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "2                                                 sgd   \n",
       "3   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "4                                                 sgd   \n",
       "5   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "6                                                 sgd   \n",
       "7   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "8                                                 sgd   \n",
       "9   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "10                                                sgd   \n",
       "11  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "12                                                sgd   \n",
       "13                                                sgd   \n",
       "14  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "15  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "16  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "17  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "\n",
       "                                               params  ...  mean_test_score  \\\n",
       "0   {'activation': 'tanh', 'kernel_regularizer': '...  ...        -0.054829   \n",
       "1   {'activation': 'tanh', 'kernel_regularizer': '...  ...        -0.038918   \n",
       "2   {'activation': 'tanh', 'kernel_regularizer': '...  ...        -0.011238   \n",
       "3   {'activation': 'tanh', 'kernel_regularizer': '...  ...        -0.006014   \n",
       "4   {'activation': 'tanh', 'kernel_regularizer': '...  ...        -0.073122   \n",
       "5   {'activation': 'tanh', 'kernel_regularizer': '...  ...        -0.043034   \n",
       "6   {'activation': 'relu', 'kernel_regularizer': '...  ...        -0.031268   \n",
       "7   {'activation': 'relu', 'kernel_regularizer': '...  ...        -0.028019   \n",
       "8   {'activation': 'relu', 'kernel_regularizer': '...  ...        -0.010248   \n",
       "9   {'activation': 'relu', 'kernel_regularizer': '...  ...        -0.001745   \n",
       "10  {'activation': 'relu', 'kernel_regularizer': '...  ...        -0.039471   \n",
       "11  {'activation': 'relu', 'kernel_regularizer': '...  ...        -0.030099   \n",
       "12  {'activation': 'tanh', 'kernel_regularizer': '...  ...        -0.010533   \n",
       "13  {'activation': 'relu', 'kernel_regularizer': '...  ...        -0.009212   \n",
       "14  {'activation': 'tanh', 'kernel_regularizer': '...  ...        -0.004414   \n",
       "15  {'activation': 'relu', 'kernel_regularizer': '...  ...        -0.001777   \n",
       "16  {'activation': 'tanh', 'kernel_regularizer': '...  ...        -0.003676   \n",
       "17  {'activation': 'relu', 'kernel_regularizer': '...  ...        -0.001458   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0         0.001945               17           -0.053360           -0.055927   \n",
       "1         0.000149               14           -0.039269           -0.038840   \n",
       "2         0.000188               10           -0.011278           -0.011248   \n",
       "3         0.001040                6           -0.005144           -0.007006   \n",
       "4         0.001169               18           -0.072357           -0.073513   \n",
       "5         0.000352               16           -0.043551           -0.043231   \n",
       "6         0.001542               13           -0.030993           -0.034256   \n",
       "7         0.000272               11           -0.028169           -0.027728   \n",
       "8         0.000070                8           -0.010321           -0.010302   \n",
       "9         0.000106                2           -0.001718           -0.001796   \n",
       "10        0.001194               15           -0.039276           -0.037940   \n",
       "11        0.000160               12           -0.030308           -0.029977   \n",
       "12        0.000149                9           -0.010615           -0.010459   \n",
       "13        0.000033                7           -0.009197           -0.009255   \n",
       "14        0.000534                5           -0.004534           -0.004305   \n",
       "15        0.000403                3           -0.001564           -0.001520   \n",
       "16        0.000514                4           -0.004606           -0.003685   \n",
       "17        0.000224                1           -0.001205           -0.001749   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            -0.052909           -0.057002           -0.055412   \n",
       "1            -0.039123           -0.039001           -0.038834   \n",
       "2            -0.011316           -0.011261           -0.011228   \n",
       "3            -0.004984           -0.007236           -0.005615   \n",
       "4            -0.073163           -0.073625           -0.073175   \n",
       "5            -0.043277           -0.042862           -0.042764   \n",
       "6            -0.030327           -0.030153           -0.030830   \n",
       "7            -0.028545           -0.027846           -0.027897   \n",
       "8            -0.010234           -0.010304           -0.010246   \n",
       "9            -0.001786           -0.001596           -0.001904   \n",
       "10           -0.040991           -0.038956           -0.040930   \n",
       "11           -0.030358           -0.029921           -0.030010   \n",
       "12           -0.010386           -0.010524           -0.010569   \n",
       "13           -0.009202           -0.009201           -0.009187   \n",
       "14           -0.003972           -0.003835           -0.005381   \n",
       "15           -0.002644           -0.001555           -0.001634   \n",
       "16           -0.003196           -0.003673           -0.003200   \n",
       "17           -0.001256           -0.001370           -0.001689   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "0          -0.054922         0.001554  \n",
       "1          -0.039013         0.000167  \n",
       "2          -0.011266         0.000030  \n",
       "3          -0.005997         0.000943  \n",
       "4          -0.073167         0.000444  \n",
       "5          -0.043137         0.000288  \n",
       "6          -0.031312         0.001504  \n",
       "7          -0.028037         0.000292  \n",
       "8          -0.010282         0.000035  \n",
       "9          -0.001760         0.000101  \n",
       "10         -0.039619         0.001181  \n",
       "11         -0.030115         0.000181  \n",
       "12         -0.010510         0.000081  \n",
       "13         -0.009208         0.000024  \n",
       "14         -0.004405         0.000546  \n",
       "15         -0.001783         0.000432  \n",
       "16         -0.003672         0.000514  \n",
       "17         -0.001454         0.000224  \n",
       "\n",
       "[18 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "display(grid_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'activation': 'relu', 'kernel_regularizer': 'l2', 'optimizer': <keras.optimizers.legacy.adam.Adam object at 0x345b900d0>}\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "1415/1415 [==============================] - 3s 2ms/step - loss: 0.1527 - val_loss: 0.0159\n",
      "Epoch 2/28\n",
      "1415/1415 [==============================] - 1s 895us/step - loss: 0.0127 - val_loss: 0.0099\n",
      "Epoch 3/28\n",
      "1415/1415 [==============================] - 1s 887us/step - loss: 0.0088 - val_loss: 0.0075\n",
      "Epoch 4/28\n",
      "1415/1415 [==============================] - 1s 912us/step - loss: 0.0069 - val_loss: 0.0060\n",
      "Epoch 5/28\n",
      "1415/1415 [==============================] - 1s 817us/step - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 6/28\n",
      "1415/1415 [==============================] - 1s 882us/step - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 7/28\n",
      "1415/1415 [==============================] - 1s 882us/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 8/28\n",
      "1415/1415 [==============================] - 1s 930us/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 9/28\n",
      "1415/1415 [==============================] - 1s 884us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 10/28\n",
      "1415/1415 [==============================] - 1s 845us/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 11/28\n",
      "1415/1415 [==============================] - 1s 903us/step - loss: 0.0037 - val_loss: 0.0032\n",
      "Epoch 12/28\n",
      "1415/1415 [==============================] - 1s 938us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 13/28\n",
      "1415/1415 [==============================] - 1s 897us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 14/28\n",
      "1415/1415 [==============================] - 1s 892us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 15/28\n",
      "1415/1415 [==============================] - 1s 831us/step - loss: 0.0029 - val_loss: 0.0033\n",
      "Epoch 16/28\n",
      "1415/1415 [==============================] - 1s 887us/step - loss: 0.0030 - val_loss: 0.0058\n",
      "Epoch 17/28\n",
      "1415/1415 [==============================] - 1s 903us/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 18/28\n",
      "1415/1415 [==============================] - 1s 887us/step - loss: 0.0026 - val_loss: 0.0048\n",
      "Epoch 19/28\n",
      "1415/1415 [==============================] - 1s 837us/step - loss: 0.0026 - val_loss: 0.0024\n",
      "Epoch 20/28\n",
      "1415/1415 [==============================] - 1s 868us/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 21/28\n",
      "1415/1415 [==============================] - 1s 880us/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 22/28\n",
      "1415/1415 [==============================] - 1s 881us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 23/28\n",
      "1415/1415 [==============================] - 1s 876us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 24/28\n",
      "1415/1415 [==============================] - 1s 846us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 25/28\n",
      "1415/1415 [==============================] - 1s 874us/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 26/28\n",
      "1415/1415 [==============================] - 1s 881us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 27/28\n",
      "1415/1415 [==============================] - 1s 878us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 28/28\n",
      "1415/1415 [==============================] - 1s 825us/step - loss: 0.0020 - val_loss: 0.0019\n"
     ]
    }
   ],
   "source": [
    "# evaluate best model\n",
    "best_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)),\n",
    "    Dense(128, activation='relu', kernel_regularizer='l2'), \n",
    "    Dense(64, activation='relu', kernel_regularizer='l2'),\n",
    "    Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "    Dense(1)  # Output layer with 1 neuron for regression\n",
    "])\n",
    "\n",
    "best_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = best_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=28, batch_size=64, validation_data=(Xval, yval, wval))\n",
    "\n",
    "# save the best model\n",
    "best_model.save(\"models/tuned_dnn_model_processed_pd.h5\")\n",
    "\n",
    "# save model history in models\n",
    "with open('models/tuned_dnn_model_processed_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Training Huber Loss: 0.0020325426012277603\n",
      "Tuned Validation Huber Loss: 0.0018526243511587381\n"
     ]
    }
   ],
   "source": [
    "best_train_hl = best_model.history.history[\"loss\"][-1]\n",
    "best_val_hl = best_model.history.history[\"val_loss\"][-1]\n",
    "print(f\"Tuned Training Huber Loss: {best_train_hl}\")\n",
    "print(f\"Tuned Validation Huber Loss: {best_val_hl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 64-bit ('tf-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff044f01a3488bc48e7d3679a79037bfca5d9a382741a26c6ac0d6891b8e9545"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
